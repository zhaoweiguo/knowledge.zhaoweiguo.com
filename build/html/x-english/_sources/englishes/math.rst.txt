æ•°å­¦
####


* addition: åŠ 
* subtraction: å‡
* multiplication: ä¹˜
* product: ä¹˜ç§¯
* differentiable: å¯å¾®
  * differentiability: å¯å¾®æ€§
  * åå‘ä¼ æ’­ä»æœ€ç»ˆæŸå¤±å€¼å¼€å§‹ï¼Œä»æœ€é¡¶å±‚åå‘ä½œç”¨è‡³æœ€åº•å±‚ï¼Œåˆ©ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¯ä¸ªå‚æ•°å¯¹æŸå¤±å€¼çš„è´¡çŒ®å¤§å°ã€‚
  * Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter had in the loss value.
* derivate: n. å¯¼æ•°ï¼›æ´¾ç”Ÿè¯ï¼›æ´¾ç”Ÿçš„äº‹ç‰©
  * derivative:  n. [æ•°] å¯¼æ•° [åŒ–å­¦] è¡ç”Ÿç‰©ï¼›é‡‘èè¡ç”Ÿäº§å“ï¼›æ´¾ç”Ÿè¯ï¼›
  * Partial derivative: åå¯¼æ•°


* Linear Algebra[ËˆÃ¦ldÊ’ÉªbrÉ™]: çº¿æ€§ä»£æ•°
* Probability Statistics: æ¦‚ç‡ç»Ÿè®¡
* probability: æ¦‚ç‡è®º
* Calculus: å¾®ç§¯åˆ†å­¦
* deviation: åå·®
  * Standard deviation: æ ‡å‡†åå·®
* Covariance: åæ–¹å·®
* Entropy: ç†µ
* Multivariable Calculus: å¤šå…ƒå¾®ç§¯åˆ†
* squared error: å¹³æ–¹è¯¯å·®

* Geometric: å‡ ä½•


* scalar: n. [æ•°] æ ‡é‡, adj. æ ‡é‡çš„ï¼›æ•°é‡çš„ï¼›æ¢¯çŠ¶çš„ï¼Œåˆ†ç­‰çº§çš„
* vector: n. [æ•°] å‘é‡ï¼›çŸ¢é‡ï¼›å¸¦èŒè€…ï¼›èˆªçº¿, vt. ç”¨æ— çº¿ç”µå¯¼èˆª
* matrix: n. [æ•°] çŸ©é˜µï¼›æ¨¡å‹ï¼›ç¤¾ä¼šç¯å¢ƒï¼›åŸºè´¨ï¼›æ¯ä½“ï¼›å­å®«ï¼›è„‰çŸ³
* tensor: n. [æ•°] å¼ é‡ï¼›[è§£å‰–] å¼ è‚Œ
* logarithm: n. [æ•°] å¯¹æ•°
* exponential: n. [æ•°] æŒ‡æ•°
* Gradient: n. [æ•°] æ¢¯åº¦ï¼›å¡åº¦ï¼›å€¾æ–œåº¦
  * stochastic gradient descentï¼ˆSGDï¼‰
  * mini-batch SGD: å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™
  * true SGD: çœŸSGD
  * batch SGD: æ‰¹é‡ SGD


* Hadamard: é˜¿è¾¾ç›ï¼ˆHadamardï¼‰çŸ©é˜µï¼šç®€ç§°HçŸ©é˜µ. Â®HçŸ©é˜µæ˜¯ä¸€ä¸ªæ–¹é˜µï¼Œä»…æœ‰å…ƒç´ +1å’Œ-1æ„æˆï¼Œè€Œä¸”å…¶å„è¡Œï¼ˆå’Œåˆ—ï¼‰ æ˜¯äº’ç›¸æ­£äº¤çš„

* identity matrix: n. [æ•°] å•ä½çŸ©é˜µ
* the indicator function: æŒ‡æ ‡å‡½æ•°
* transpose of a vector or a matrix: ä¸€ä¸ªå‘é‡æˆ–çŸ©é˜µçš„è½¬ç½®
* Inverse of matrix X: çŸ©é˜µXçš„é€†çŸ©é˜µ
* Gradient of  ğ‘¦  with respect to  ğ±: yå…³äºxçš„æ¢¯åº¦
* Definite integral of  ğ‘“  from  ğ‘  to  ğ‘  with respect to  ğ‘¥: få…³äºxä»aåˆ°bçš„å®šç§¯åˆ†
* Random variable ğ‘§ has probability distribution ğ‘ƒ: éšæœºå˜é‡zçš„æ¦‚ç‡åˆ†å¸ƒä¸ºp

* loss function: æŸå¤±å‡½æ•°
* clustering algorithm: èšç±»ç®—æ³•
* Standard deviation: æ ‡å‡†åå·®
* normal equation: æ­£è§„æ–¹ç¨‹


* Probability distribution: æ¦‚ç‡åˆ†å¸ƒ
* Bernoulli distribution: ä¼¯åŠªåˆ©åˆ†å¸ƒ
* Gaussian distribution: é«˜æ–¯åˆ†å¸ƒ
* Monte Carlo tree: è’™ç‰¹å¡ç½—æ ‘
* Conditional probability: æ¡ä»¶æ¦‚ç‡
* Probability density function: [æ•°] æœºç‡å¯†åº¦å‡½æ•°ï¼›æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå‡½æ•°



* linear regression: çº¿æ€§å›å½’
* logistic regression: é€»è¾‘å›å½’



* stochastic gradient descent: éšæœºæ¢¯åº¦ä¸‹é™
* multi-layer perceptrons: å¤šå±‚æ„ŸçŸ¥å™¨

* pixel  /'pÉªks(É™)l/ /'pÉªksl/: åƒç´ ,åˆ†è¾¨ç‡

* the  ğ¿2  loss corresponds to the assumption that our data was corrupted by Gaussian noise, whereas the  ğ¿1  loss corresponds to an assumption of noise from a Laplace distribution.

* Bayesian analysis: è´å¶æ–¯åˆ†æ





