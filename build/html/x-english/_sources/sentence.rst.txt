常用句式
########


* 关键句: with respect to, 表示 “相对于”
* 关键句: Chain rule, 链式法则(微积分中的一种方法)
* The most common way of computing the loss gradients in a computation graph involves applying the chain rule from right to left, which is also called ``reverse-model automatic differentiation`` or ``backpropagation``. It means we start from the output layer (or the loss itself) and work backward through the network to the input layer. This is done to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training.
















