

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


<!-- start added 2025-08-06   增加对mermaid图的支持 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: { useMaxWidth: true }
    });
});
</script>
<!--  end added 2025-08-06   增加对mermaid图的支持 -->




  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>11.4. dataset &mdash; 新溪-gordon V2025.12 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="11.5. 数据集相关网站" href="website.html" />
    <link rel="prev" title="11.3. 中文图片相关数据集" href="chinese_image.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script src="../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.12
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/ml.html">1.3. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/deep_learning.html">1.5. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../normals/deep_learnings/normal.html">1.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../normals/deep_learnings/history.html">1.5.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/algorithm.html">1.7. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/tool.html">1.8. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/question.html">1.9. 常见问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. 机器人领域</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../theories/normal.html">2.1. 通用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../theories/normals/%E7%BB%8F%E5%85%B8%E8%AE%A4%E7%9F%A5%E7%A7%91%E5%AD%A6.html">2.1.1. 经典认知科学</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theories/key.html">2.2. 关键定义</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Recommenders/CF%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4.html">2.2.1. 协同过滤（Collaborative Filtering, CF）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Recommenders/MF%E6%A6%82%E7%8E%87%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html">2.2.2. MF(Matrix Factorization，矩阵分解)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Recommenders/PMF%E6%A6%82%E7%8E%87%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html">2.2.3. PMF（Probabilistic Matrix Factorization，概率矩阵分解）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Recommenders/Two-TowerModels%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B.html">2.2.4. Two-Tower Models（双塔模型）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Parallelism/normal.html">2.2.5. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Parallelism/PipelineParallelism.html">2.2.6. Pipeline Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/Parallelism/TensorParallesim.html">2.2.7. Tensor Parallesim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_GELU.html">2.2.8. 激活函数-GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_Leaky-ReLU.html">2.2.9. 激活函数-Leaky ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_RELU.html">2.2.10. 激活函数-ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_SiLU.html">2.2.11. 激活函数-SiLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_Sigmoid.html">2.2.12. 激活函数-Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/activation_Tanh.html">2.2.13. 激活函数-Tanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/normalization_L1.html">2.2.14. 归一化-L1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/normalization_L2.html">2.2.15. 归一化-L2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/probabilistic_Softmax.html">2.2.16. 概率分布-Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/probabilistic_Sparsemax.html">2.2.17. 概率分布-Sparsemax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_activations/probabilistic_logSoftmax.html">2.2.18. 概率分布-logsoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/classify_cross_entropy.html">2.2.19. 损失函数-分类-cross-entropy(交叉熵)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/classify_NLL.html">2.2.20. 损失函数-分类-负对数似然损失NLL Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/classify_log.html">2.2.21. 损失函数-分类-对数损失(Log Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/classify_kl.html">2.2.22. 损失函数-分类-KL 散度(KL Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/regression_MSE.html">2.2.23. 损失函数-回归-均方误差(MSE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/regression_MAE.html">2.2.24. 损失函数-回归-平均绝对误差(MAE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/regression_Huber.html">2.2.25. 损失函数-回归-Huber 损失</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/regression_log_cosh.html">2.2.26. 损失函数-回归-对数余弦损失(Log-Cosh Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_loss/%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F-L2%E6%AD%A3%E5%88%99%E5%8C%96.html">2.2.27. 权重衰减(L2正则化)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/GD.html">2.2.28. GD(梯度下降)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/SGD.html">2.2.29. SGD随机梯度下降</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/RMSprop.html">2.2.30. RMSprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/Adam.html">2.2.31. Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/AdamW.html">2.2.32. AdamW</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/func_optims/Momentum.html">2.2.33. Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/ners/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">2.2.34. HMM-隐马尔可夫模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/ners/WWM-%E5%85%A8%E8%AF%8DMask.html">2.2.35. WWM-Whole Word Masking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/ners/CRF-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html">2.2.36. CRF-条件随机场</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dls/ANN.html">2.2.37. ANN(NN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dls/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">2.2.38. 深度神经网络(Deep Neural Network, DNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dls/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">2.2.39. 卷积神经网络(Convolutional Neural Network, CNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dls/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91.html">2.2.40. RNN: 循环神经网(Recurrent Neural Network, RNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dls/LSTM-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86.html">2.2.41. LSTM: 长短时记忆(Long Short Term Memory, LSTM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/propagation.html">2.2.42. 前向/反向传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/LinearLayer.html">2.2.43. Linear Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/FFN.html">2.2.44. Feedforward Network-前馈网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/LayerNorm.html">2.2.45. LayerNorm(层归一化)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/WeightTying.html">2.2.46. Weight Tying</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/GreedyDecoding.html">2.2.47. Greedy Decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/ImageGrounding.html">2.2.48. Image Grounding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/dl_theorys/Perplexity.html">2.2.49. Perplexity(PPL)困惑度</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/ManhattanWorld%E6%9B%BC%E5%93%88%E9%A1%BF%E4%B8%96%E7%95%8C.html">2.2.50. Manhattan World(曼哈顿世界)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/HoughTransform%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2.html">2.2.51. Hough Transform（霍夫变换）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/PolarCoordinateSystem%E6%9E%81%E5%9D%90%E6%A0%87%E8%A1%A8%E7%A4%BA%E6%B3%95.html">2.2.52. 极坐标表示法(Polar Coordinate System)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/GaussianSphere%E9%AB%98%E6%96%AF%E7%90%83.html">2.2.53. Gaussian Sphere（高斯球）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/edge_direction%E8%BE%B9%E7%BC%98%E6%96%B9%E5%90%91.html">2.2.54. 边缘方向 Edge Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/3Ds/NormalVector%E6%B3%95%E5%90%91%E9%87%8F.html">2.2.55. NormalVector法向量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/AllReduce.html">2.2.56. AllReduce</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/BPE.html">2.2.57. BPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/Embedding%E6%A8%A1%E5%9E%8B.html">2.2.58. Embedding 模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">2.2.59. K-Means聚类算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/LLM.html">2.2.60. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/deeplearning.html">2.2.61. 深度学习相关</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/other.html">2.2.62. 其他</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8Bvs%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B.html">2.2.63. 判别式模型vs生成式模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%A9%BA%E9%97%B4.html">2.2.64. 欧几里得空间(Euclidean space)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/keys/others/%E7%9F%A2%E9%87%8F%E5%8C%96%E8%AE%A1%E7%AE%97.html">2.2.65. 矢量化计算(Vectorize calculations)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../theories/tmp.html">2.3. 临时</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/ReAct.html">2.3.1. ReAct框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/Reflection.html">2.3.2. Reflection反思</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/math.html">2.3.3. 数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/bag-of-words.html">2.3.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/word2vec.html">2.3.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/doc2vec.html">2.3.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/FastText.html">2.3.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/LDA.html">2.3.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/overfitting-underfitting.html">2.3.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/RAG.html">2.3.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/Agent.html">2.3.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/LLM.html">2.3.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/prompt_engineering.html">2.3.13. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/finetune.html">2.3.14. LLM调优(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/Workflow.html">2.3.15. Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../theories/tmps/0normal.html">2.3.16. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-推理模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. 模型量化(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/Quantizations/normal.html">3.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/Quantizations/GGUF.html">3.4.2. GGUF 文件</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/fileformat.html">3.5. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/fileformats/normal.html">3.5.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/fileformats/GGML.html">3.5.2. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/openai.html">3.6. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/openais/normal.html">3.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/prompt.html">3.7. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/prompts/demo_chinese.html">3.7.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/prompts/skill.html">3.7.3. 示例</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLMs/Android.html">3.8. Android版LLM相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/Androids/normal.html">3.8.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Android版部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../RAG.html">4. RAG相关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../NLPs/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../NLPs/preprocess.html">5.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/normal.html">5.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../NLPs/NER.html">5.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/NERs/normal.html">5.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/NERs/seq-label.html">5.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/NERs/history.html">5.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../NLPs/summary.html">5.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../NLPs/summarys/normal.html">5.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../library.html">6. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../libraries/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Image.html">6.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Video.html">6.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/IPythons/normal.html">6.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/IPythons/magic.html">6.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/IPythons/display.html">6.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/NumPys/normal.html">6.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/NumPys/Ndarray.html">6.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/NumPys/function.html">6.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/normal.html">6.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/example_subset.html">6.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/example_analysis.html">6.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/example_sql.html">6.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/example_default_value.html">6.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/example_multi_index.html">6.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/practice.html">6.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/api_input_output.html">6.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/normal.html">6.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/install.html">6.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/example.html">6.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Matplotlibs/pylab.html">6.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/SciPys/normal.html">6.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/sklearns/normal.html">6.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/sklearns/supervised.html">6.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/sklearns/unsupervised.html">6.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/OpenCVs/normal.html">6.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/OpenCVs/example.html">6.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/OpenCVs/struct.html">6.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/Seaborns/normal.html">6.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/jieba.html">6.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/gensim.html">6.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../libraries/gensims/normal.html">6.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../libraries/LAC.html">6.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../framework.html">7. 学习框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/normal.html">7.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/pytorchs/normal.html">7.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/pytorchs/nn.html">7.2.2. nn模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/huggingface.html">7.3. huggingface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/normal.html">7.3.1. 常用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/Transformers.html">7.3.2. Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/Transformers/Transformers.html">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/Transformers/Transformers_V4.45.2.html">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/Tokenizers_V0.13.3.html">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/huggingfaces/collect.html">7.3.7. 收集</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/collects/model.html">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/huggingfaces/collects/blog_decoding-methods.html">博文: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/vLLM.html">7.4. vLLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/vLLMs/normal.html">7.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/vLLMs/vLLM_doc.html">7.4.2. vLLM官方文档</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/llama.cpp.html">7.5. llama.cpp框架</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/llama.cpps/normal.html">7.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/mxnet.html">7.7. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/mxnets/ndarray.html">7.7.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/mxnets/gluon.html">7.7.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/mxnets/autograd.html">7.7.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/Keras/normal.html">7.9.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../frameworks/Keras/demo.html">7.9.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../frameworks/Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../frameworks/other.html">7.10. 其他</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../website.html">8. 关键网站</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../websites/ArXiv.html">8.3. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../websites/video.html">8.4. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../websites/normal.html">8.5. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/LLM-inference-accelerate.html">10.9. LLM推理加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/Evaluate.html">10.10. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opensources/platform.html">10.11. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../dataset.html">11. 数据集</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese.html">11.2. 中文数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="chinese_image.html">11.3. 中文图片相关数据集</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="website.html">11.5. 数据集相关网站</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../model.html">12. 常见模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cuda.html">13. 图形&amp;计算加速技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../cudas/normal.html">13.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Evaluate.html">14. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/normal.html">14.1. 通用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E4%BC%A0%E7%BB%9FAI.html">15. 传统AI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../dataset.html"><span class="section-number">11. </span>数据集</a> &raquo;</li>
        
      <li><span class="section-number">11.4. </span>dataset</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/datasets/huggingface.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">11.4. dataset</a><ul>
<li><a class="reference internal" href="#id2">11.4.1. 文本</a><ul>
<li><a class="reference internal" href="#load-dataset-imdb">load_dataset (“imdb”)</a></li>
<li><a class="reference internal" href="#load-dataset-wnut-17">load_dataset(“wnut_17”)</a></li>
<li><a class="reference internal" href="#load-dataset-squad">load_dataset(“squad”)</a></li>
<li><a class="reference internal" href="#load-dataset-eli5">load_dataset(“eli5”)</a></li>
<li><a class="reference internal" href="#load-dataset-opus-books">load_dataset(“opus_books”)</a></li>
<li><a class="reference internal" href="#load-dataset-billsum">load_dataset(“billsum”)</a></li>
<li><a class="reference internal" href="#load-dataset-swag">load_dataset(“swag”)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">11.4.2. 音频</a><ul>
<li><a class="reference internal" href="#load-dataset-polyai-minds14">load_dataset(“PolyAI/minds14”)</a></li>
<li><a class="reference internal" href="#load-dataset-mozilla-foundation-common-voice-13-0-en">load_dataset(“mozilla-foundation/common_voice_13_0”, “en”)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id4">11.4.3. 图像</a><ul>
<li><a class="reference internal" href="#load-dataset-beans">load_dataset(“beans”)</a></li>
<li><a class="reference internal" href="#load-dataset-food101">load_dataset(“food101”)</a></li>
<li><a class="reference internal" href="#load-dataset-scene-parse-150">load_dataset(“scene_parse_150”)</a></li>
<li><a class="reference internal" href="#load-dataset-cppe-5">load_dataset(“cppe-5”)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal">11.4.4. MULTIMODAL</a><ul>
<li><a class="reference internal" href="#load-dataset-lambdalabs-pokemon-blip-captions">load_dataset(“lambdalabs/pokemon-blip-captions”)</a></li>
<li><a class="reference internal" href="#load-dataset-nielsr-docvqa-1200-examples">load_dataset(“nielsr/docvqa_1200_examples”)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5">11.4.5. 视频</a><ul>
<li><a class="reference internal" href="#sayakpaul-ucf101-subset">sayakpaul/ucf101-subset</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="dataset">
<h1><span class="section-number">11.4. </span>dataset<a class="headerlink" href="#dataset" title="此标题的永久链接">¶</a></h1>
<section id="id2">
<h2><span class="section-number">11.4.1. </span>文本<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h2>
<section id="load-dataset-imdb">
<h3>load_dataset (“imdb”)<a class="headerlink" href="#load-dataset-imdb" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>imdb 是 IMDb 电影评价数据集，用于情感分类任务。</p></li>
<li><p>它包含来自 IMDb website 的 50,000 条影评，分为 25,000 条正面评论和 25,000 条负面评论。</p></li>
</ul>
<p>这个数据集的主要特点和应用如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>数据来源：IMDb 网站用户提交的影评。
任务：二分类任务，判断评论的情绪取向是正面还是负面。
数据格式：每个样本包含文本影评和一个标签 (正面 1 或负面 0)。
规模：总共 50,000 条样本，25,000 条每个类。
功能：可用于训练评价情感分类模型，检测文本中的情绪倾向。
使用：调用 load_dataset (&quot;imdb&quot;) 可加载整个数据集到内存中进行模型训练。
应用：情感分析、评论分类、情绪检测等领域任务常用的基准数据集。
</pre></div>
</div>
<p>总的来说，imdb 数据集提供了大量真实用户影评的情感标签，是训练和测试情感分类模型的重要公开可获得数据集。它的规模和任务清晰，是 NLP 任务中的一个基准应用。</p>
</section>
<section id="load-dataset-wnut-17">
<h3>load_dataset(“wnut_17”)<a class="headerlink" href="#load-dataset-wnut-17" title="此标题的永久链接">¶</a></h3>
<p>WNUT17数据集是一个新闻领域的命名实体识别数据集,全称为”Workshop on Noisy User-generated Text 2017”。
该数据集的一些关键特征:
- 数据来源:Twitter
- 包含4个类型的命名实体:位置(location)、人员(person)、组织(organization)和产品(product)。
- 训练集包含3,395条标注过的Twitter消息(tweet)。
- 开发集包含1,000条消息。
- 测试集包含3,857条消息。
- 消息均为非标准英语,包含语法错误、拼写错误和短语缩写。
- 数据集具有一定的噪声,这使其对命名实体识别算法的鲁棒性提出了挑战。
加载该数据集后,得到的wnut变量是一个DatasetDict对象,包含:
- train:训练集
- validation:开发集
- test:测试集
每个数据集又包含以下两个字段:
- tokens:标注的词语
- ner_tags:每个词的命名实体标签
WNUT17数据集可以用来训练对非标准、嘈杂文本的命名实体识别,评估模型的鲁棒性。它常被用作基准测试数据集。</p>
</section>
<section id="load-dataset-squad">
<h3>load_dataset(“squad”)<a class="headerlink" href="#load-dataset-squad" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://rajpurkar.github.io/SQuAD-explorer/">https://rajpurkar.github.io/SQuAD-explorer/</a></p></li>
</ul>
<p>SQuAD数据集是一个阅读理解数据集,全称为Stanford Question Answering Dataset。
squad数据集的主要特征包括:
- 数据来源是Wikipedia文章
- 包含10万多个阅读理解题及答案
- 问题是针对给定段落提出的
- 答案是段落的文本片段
- 训练/开发/测试集各5万多个样本
- 采用了 Rajpurkar et al.的常见SQuAD评估指标
这个数据集很好地模拟了机器阅读理解的任务,是阅读理解领域最常用的公开数据集之一。
加载后可直接用于fine-tuning语言表示模型,进行阅读理解任务的模型训练。
squad数据集有助于开发理解文本段落并回答相关问题的AI系统,是语言理解的一个重要benchmark。</p>
</section>
<section id="load-dataset-eli5">
<h3>load_dataset(“eli5”)<a class="headerlink" href="#load-dataset-eli5" title="此标题的永久链接">¶</a></h3>
<p>eli5数据集是一个问答数据集,全称为”Explain Like I’m 5”。
其主要特征包括:
- 来源于Reddit的类似的”explain like I’m 5”子版块的问答帖子
- 包含260,000个问题及详细的答案
- 问题涵盖各个领域,如科学、历史、技术等
- 答案试图以简单易懂的方式解释复杂的概念
- 可用于训练问答系统的读取理解能力
- 回答需要进行推理和概括能力
- 不同于factoid问答,更加复杂和深入
eli5提供了高质量的信息搜索型问答数据,需要模型具备深度语言理解和推理能力,是测试和提升问答系统复杂reasoning能力的有价值数据集。</p>
</section>
<section id="load-dataset-opus-books">
<h3>load_dataset(“opus_books”)<a class="headerlink" href="#load-dataset-opus-books" title="此标题的永久链接">¶</a></h3>
<p>opus_books是一个开源的多语言平行语料库,包含不同语言对的书籍数据。
其具体特性包括:
- 包含145种语言对的数据
- 总计超过1000本书的内容
- 语料涵盖小说、故事、寓言等文学作品
- 每种语言对平均超过100本书的数据
- 总数据量约1TB
- 文件格式为Moses平行语料库的标准格式
- 数据集可通过OPUS网站免费下载</p>
<p>opus_books数据集包含丰富的多语言平行文本,可用于训练机器翻译系统、进行跨语言对比研究等自然语言处理任务。
它覆盖面广、质量高,是语言研究和多语言应用的宝贵资源。加载后可直接用于下游任务的数据准备和模型训练。 （已编辑）</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;opus_books&quot;</span><span class="p">,</span> <span class="n">language_pair</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="s2">&quot;fr&quot;</span><span class="p">))</span>

<span class="n">books</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;opus_books&quot;</span><span class="p">,</span> <span class="s2">&quot;en-fr&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-dataset-billsum">
<h3>load_dataset(“billsum”)<a class="headerlink" href="#load-dataset-billsum" title="此标题的永久链接">¶</a></h3>
<p>billsum数据集是一个英文法案文档摘要数据集,主要有以下特征:
- 包含来自美国国会和欧盟委员会的法案文档
- 每个法案文档包含标题、文本内容和手工摘要
- 训练集包含39,357个文档,验证集463个,测试集250个
- 文本内容平均长度约为3,371 tokens
- 摘要平均长度约为205 tokens
- 文本覆盖多种主题的法案,如经济、医疗、教育等
- 提供原始文本、预处理后文本和摘要3类数据</p>
<p>billsum可用于文本摘要、长文本生成等研究。它提供真实法律文档的核心内容摘要,训练出的模型可以自动生成法律文档摘要。
该数据集质量高,文本具有代表性,是推动法律领域、公共政策等方面文本生成研究的重要资源。</p>
</section>
<section id="load-dataset-swag">
<h3>load_dataset(“swag”)<a class="headerlink" href="#load-dataset-swag" title="此标题的永久链接">¶</a></h3>
<p>SWAG数据集是一份大规模的语言推理数据集。
SWAG(Situations With Adversarial Generations)数据集包含113k个情景描述及4个备选连续(一个正确和三个错误)。该数据集旨在测试语言系统的常识推理能力。
SWAG数据集的一些关键特点:
- 情景描述都来自视频字幕
- 每个情景描述都有4个候选语,1个正确,3个错误
- 数据集规模很大,包含113222个训练样本,14823个评估样本
- 涵盖广泛的日常生活情景
- 需要语言模型具有常识推理能力才能判断正确的语义连续性
使用该数据集可以训练和评估自然语言推理模型的性能,考察模型在复杂语义连续任务上的推理和泛化能力。
SWAG数据集来自于2018年CMU发表在EMNLP的论文。该数据集常被用来作为语言理解评估基准之一。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">swag</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;swag&quot;</span><span class="p">,</span> <span class="s2">&quot;regular&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id3">
<h2><span class="section-number">11.4.2. </span>音频<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h2>
<section id="load-dataset-polyai-minds14">
<h3>load_dataset(“PolyAI/minds14”)<a class="headerlink" href="#load-dataset-polyai-minds14" title="此标题的永久链接">¶</a></h3>
<p>MINDS-14是一个针对多模态情感分析任务构建的英文数据集。它包含了来自Youtube视频的14个不同类别的对话,涵盖日常生活各个方面。
该数据集的一些关键统计信息:
- 音频长约40小时
- 包含3949段对话
- 平均对话长度约40秒
- 句子级标注的情感标签,包括Positive, Neutral和Negative 3类
研究者利用该数据集可以训练多模态模型来进行视频对话的情感分析,例如从语音和语言两方面判断某句对话的情感极性。
加载后的数据格式是一个字典,主要包含:
- audio: 音频waveform
- text: 对话转录文本
- emotion: 情感标签
以及一些对话的元信息,如视频ID,发言人ID等。
该数据集适合在多模态情感分析、对话系统等领域进行研究和模型训练。加载后可以直接用于情感分类等任务的Fine-tuning。</p>
</section>
<section id="load-dataset-mozilla-foundation-common-voice-13-0-en">
<h3>load_dataset(“mozilla-foundation/common_voice_13_0”, “en”)<a class="headerlink" href="#load-dataset-mozilla-foundation-common-voice-13-0-en" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0">https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0</a></p></li>
<li><p>原始: <a class="reference external" href="https://commonvoice.mozilla.org/en/datasets">https://commonvoice.mozilla.org/en/datasets</a></p></li>
<li><p>中文: <a class="reference external" href="https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0/viewer/zh-CN">https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0/viewer/zh-CN</a></p></li>
</ul>
</section>
</section>
<section id="id4">
<h2><span class="section-number">11.4.3. </span>图像<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h2>
<section id="load-dataset-beans">
<h3>load_dataset(“beans”)<a class="headerlink" href="#load-dataset-beans" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Beans leaf dataset with images of diseased and health leaves.</p></li>
</ul>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/QECGXu.png" src="https://img.zhaoweiguo.com/uPic/2023/08/QECGXu.png" />
</figure>
<p>Data Instances:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;image_file_path&#39;</span><span class="p">:</span> <span class="s1">&#39;/root/.cache/huggingface/datasets/downloads/extracted/0aaa78294d4bf5114f58547e48d91b7826649919505379a167decb629aa92b0a/train/bean_rust/bean_rust_train.109.jpg&#39;</span><span class="p">,</span>
    <span class="s1">&#39;image&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">PIL</span><span class="o">.</span><span class="n">JpegImagePlugin</span><span class="o">.</span><span class="n">JpegImageFile</span> <span class="n">image</span> <span class="n">mode</span><span class="o">=</span><span class="n">RGB</span> <span class="n">size</span><span class="o">=</span><span class="mi">500</span><span class="n">x500</span> <span class="n">at</span> <span class="mh">0x16BAA72A4A8</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="s1">&#39;labels&#39;</span><span class="p">:</span> <span class="mi">1</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="load-dataset-food101">
<h3>load_dataset(“food101”)<a class="headerlink" href="#load-dataset-food101" title="此标题的永久链接">¶</a></h3>
<p>Food-101数据集是一个食物图像分类数据集。
Food-101的数据集关键信息:
- 包含101个食物类别,总共101000张图片。
- 每类食物大约包含750张训练集图像和250张测试集图像。
- 图像分辨率为256x256像素。
- 图片包含常见的食物照片,如汉堡包、披萨等。
- 数据集采集自foodspotting网站,包含不同场景、角度拍摄的食物图片。
- 提供训练集和测试集划分。
加载后的数据集包含以下内容:
- images - 食物图像文件的路径
- labels - 食物类别的标签
- bboxes - 一些图像的边界框标注
该数据集常用于食物识别和图像分类任务,可以用来训练和评估各种图像分类模型的效果。也可用于食物检测模型的预训练。是计算机视觉领域较为常用的食物图像数据集之一。
通过fine-tuning,可以建立一个101分类食物识别模型。</p>
</section>
<section id="load-dataset-scene-parse-150">
<h3>load_dataset(“scene_parse_150”)<a class="headerlink" href="#load-dataset-scene-parse-150" title="此标题的永久链接">¶</a></h3>
<p>主要用于场景解析任务,包含150类语义类别的像素级标注图像。其关键信息如下:
- 包含20210张图像,分辨率为512x512
- 150个语义类别,涵盖人、动物、交通工具、建筑等常见对象
- 提供像素级标注,每个像素标记了所属的语义类别
- 数据采集自真实场景的街景和室内图像
- 数据来源:ADE20K数据集的子集
- 数据分为训练、验证、测试集
加载的数据包含图像路径image和Segmentation map两部分,map给出了每幅图像每个像素的语义类别。
这个数据集可用于训练像FCN、U-Net等图像语义分割模型,评估其对复杂场景的语义解析能力。
也可用于 few-shot 场景解析任务的研究,或迁移学习到其他相关任务。是计算机视觉领域重要的场景解析数据集之一。</p>
</section>
<section id="load-dataset-cppe-5">
<h3>load_dataset(“cppe-5”)<a class="headerlink" href="#load-dataset-cppe-5" title="此标题的永久链接">¶</a></h3>
<p>CPPE-5是一个用于医用个人防护设备细分类的新数据集。其主要特点为:
- 高质量图像和标注(每张图像约4.6个边界框)
- 真实生活场景中的图像,不同于当前任何类似数据集
- 大多数图像非标志性,可便于部署到实际环境
支持的任务和排行榜:
- 对象检测:可用于训练对象检测模型,具有活跃的排行榜。指标采用COCO检测评估标准,包括不同尺度和IoU阈值下的平均精度(mAP)。
- 该数据集允许研究医用防护设备的细分类,当前其他数据集更侧重大类目标。
- 图像真实而复杂,利于部署实际应用。
总之,CPPE-5是一个高质量的医疗域对象检测数据集,具有研究实际价值的特点,可推动这一领域的技术发展。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cppe5</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;cppe-5&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cppe5</span>
<span class="go">DatasetDict({</span>
<span class="go">    train: Dataset({</span>
<span class="go">        features: [&#39;image_id&#39;, &#39;image&#39;, &#39;width&#39;, &#39;height&#39;, &#39;objects&#39;],</span>
<span class="go">        num_rows: 1000</span>
<span class="go">    })</span>
<span class="go">    test: Dataset({</span>
<span class="go">        features: [&#39;image_id&#39;, &#39;image&#39;, &#39;width&#39;, &#39;height&#39;, &#39;objects&#39;],</span>
<span class="go">        num_rows: 29</span>
<span class="go">    })</span>
<span class="go">})</span>
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cppe5</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="go">{&#39;image_id&#39;: 15,</span>
<span class="go"> &#39;image&#39;: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at 0x7F9EC9E77C10&gt;,</span>
<span class="go"> &#39;width&#39;: 943,</span>
<span class="go"> &#39;height&#39;: 663,</span>
<span class="go"> &#39;objects&#39;: {</span>
<span class="go">        &#39;id&#39;: [114, 115, 116, 117],</span>
<span class="go">        &#39;area&#39;: [3796, 1596, 152768, 81002],</span>
<span class="go">        &#39;bbox&#39;: [</span>
<span class="go">                [302.0, 109.0, 73.0, 52.0],</span>
<span class="go">                [810.0, 100.0, 57.0, 28.0],</span>
<span class="go">                [160.0, 31.0, 248.0, 616.0],</span>
<span class="go">                [741.0, 68.0, 202.0, 401.0]</span>
<span class="go">        ],</span>
<span class="go">        &#39;category&#39;: [4, 4, 0, 0]</span>
<span class="go">  }</span>
<span class="go"> }</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/H4VOjg.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/H4VOjg.jpg" />
</figure>
</section>
</section>
<section id="multimodal">
<h2><span class="section-number">11.4.4. </span>MULTIMODAL<a class="headerlink" href="#multimodal" title="此标题的永久链接">¶</a></h2>
<section id="load-dataset-lambdalabs-pokemon-blip-captions">
<h3>load_dataset(“lambdalabs/pokemon-blip-captions”)<a class="headerlink" href="#load-dataset-lambdalabs-pokemon-blip-captions" title="此标题的永久链接">¶</a></h3>
<p>lambdalabs/pokemon-blip-captions是一个针对神奇宝贝(Pokémon)的图像描述数据集,主要由Anthropic的BLIP模型团队构建。以下是其主要特点:
- 数据集包含了超过6,000张不同神奇宝贝的图像。
- 每张图像都由人工标注了1-2句描述语料。
- 描述语料专注于图像中的主要神奇宝贝以及其外观特征和属性。
- 数据集覆盖了原版游戏中的151种神奇宝贝。
- 图片各具特色,包含官方插图和玩具模型照等。
- 描述语言流畅自然,数据集质量较高。
- 可用于图像字幕、视觉问答等任务的训练和评测。
- 提供了训练集、验证集和测试集分割。
- 可以通过TensorFlow Datasets加载使用。
- 数据量适中,可快速上手,也可扩充训练。
总体来说,这是一份质量较好的图像描述数据集,主题围绕明确的知识领域,适合作为概念学习和多模态任务的练习数据集,也可以作为更大规模数据集的补充。其作用在于提供高质量的图像-文本训练数据。</p>
</section>
<section id="load-dataset-nielsr-docvqa-1200-examples">
<h3>load_dataset(“nielsr/docvqa_1200_examples”)<a class="headerlink" href="#load-dataset-nielsr-docvqa-1200-examples" title="此标题的永久链接">¶</a></h3>
<p>数据集 “nielsr/docvqa_1200_examples” 是一个用于文档视觉问答（Document Visual Question Answering，简称 DocVQA）任务的数据集。这个数据集旨在支持模型在理解文档内容并回答与文档相关的问题方面的能力。</p>
<p>数据集中包含约 1200 个样本，每个样本都由以下元素组成：</p>
<ul class="simple">
<li><p>文档图像：每个样本都有一个文档图像，图像中可能包含文本块、段落、表格、图表等多种排版元素。</p></li>
<li><p>问题：每个样本都伴随着一个与文档内容相关的问题，这些问题可能需要对图像中的特定信息进行推理和理解。</p></li>
<li><p>答案：每个问题都有一个正确的答案，这个答案可以从文档图像中抽取出来。</p></li>
</ul>
<p>这个数据集的目标是促使模型学会从文档图像中抽取信息，然后根据问题理解并回答问题。这对于模拟真实世界中需要从文档中获取信息的情况非常有用，如阅读理解、信息提取等任务。</p>
<p>通过使用 “nielsr/docvqa_1200_examples” 数据集，研究人员和开发人员可以训练和评估模型在文档视觉问答任务上的表现，从而推动文档理解和信息提取领域的研究和应用。</p>
</section>
</section>
<section id="id5">
<h2><span class="section-number">11.4.5. </span>视频<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h2>
<section id="sayakpaul-ucf101-subset">
<h3>sayakpaul/ucf101-subset<a class="headerlink" href="#sayakpaul-ucf101-subset" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>UCF101官网: <a class="reference external" href="https://www.crcv.ucf.edu/data/UCF101.php">https://www.crcv.ucf.edu/data/UCF101.php</a></p></li>
</ul>
<p>sayakpaul/ucf101-subset 是一个在 Hugging Face Hub 上发布的 UCF101 数据集的子集。
UCF101 是一个行为识别(action recognition)的标准基准数据集,由佛罗里达大学收集和标注。该数据集包含 13320 个视频片段,涵盖 101 个人类行为类别,如篮球运球、跳舞、爬楼梯等。
该数据集子集的主要信息如下:
- 包含 UCF101 数据集的一个子集,视频数量更少但保留了所有 101 个行为类别。
- 视频格式为 AVI,分辨率为 320x240 像素。
- 提供了视频文件的路径列表和类别标签。
- 类别均匀分布,每个类别约有 100 个视频片段。
- 文件总大小约为 5.5GB。
这个子集的目的是提供一个小规模、易处理、包含多类别并保持类别均衡的行为识别数据集,用于算法开发、快速验证等目的。它保留了 UCF101 数据集多样性的同时,文件容量小,使用门槛更低。
总之,sayakpaul/ucf101-subset是一个行为识别领域常用的小数据集,适用于算法研究和模型训练等用途。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">hf_hub_download</span>

<span class="n">hf_dataset_identifier</span> <span class="o">=</span> <span class="s2">&quot;sayakpaul/ucf101-subset&quot;</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;UCF101_subset.tar.gz&quot;</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="n">hf_dataset_identifier</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="n">filename</span><span class="p">,</span> <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">)</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tarfile</span>
<span class="k">with</span> <span class="n">tarfile</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span> <span class="k">as</span> <span class="n">t</span><span class="p">:</span>
     <span class="n">t</span><span class="o">.</span><span class="n">extractall</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="website.html" class="btn btn-neutral float-right" title="11.5. 数据集相关网站" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="chinese_image.html" class="btn btn-neutral" title="11.3. 中文图片相关数据集" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'V2025.12',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../_static/copybutton.js"></script>
      <script type="text/javascript" src="../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>