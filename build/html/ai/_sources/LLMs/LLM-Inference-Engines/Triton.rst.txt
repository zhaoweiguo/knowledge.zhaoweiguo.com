Triton
######


* NVIDIA 开发的开源推理服务器，旨在为多模型、多框架的深度学习和机器学习应用提供高效的推理服务。它能够在云端、数据中心和边缘设备上高效地运行，支持多种部署和扩展方案。
* https://developer.nvidia.com/triton-inference-server
* 开发文档: https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html




主要特点
========

* 多框架支持：Triton 支持多种深度学习框架，包括 TensorFlow、PyTorch、ONNX、TensorRT 等。它可以运行不同框架的模型，使得在生产环境中混合使用不同框架的模型变得容易。
* 多模型部署：Triton 支持在同一个推理服务器上同时运行多个模型。这使得在一个服务器上部署不同任务的模型成为可能，并且支持动态加载和卸载模型。
* 高性能：Triton 旨在提供高性能的推理服务，支持 GPU 加速和 CPU 计算。它使用多种优化策略来提高模型的推理效率，适合处理高负载和高并发的场景。
* 灵活的部署选项：Triton 可以在 Kubernetes 等容器化环境中运行，支持弹性扩展和自动化部署。同时，它支持 RESTful 和 gRPC 接口，使得与其他服务的集成变得简单。
* 监控和可观察性：Triton 提供丰富的监控和可观察性功能，支持 Prometheus、Grafana 等工具。通过这些功能，您可以监控模型的性能、资源使用情况等。








































