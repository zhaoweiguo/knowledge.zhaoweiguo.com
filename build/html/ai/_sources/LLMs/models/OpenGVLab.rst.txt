OpenGVLab
#########

* Research group from Shanghai AI Lab focused on Vision-Centric AI research. The GV in our name, OpenGVLab, means general vision, a general understanding of vision, so little effort is needed to adapt to new vision-based tasks.


Models
======

* InternVL: a pioneering open-source alternative to GPT-4V.
* InternImage: a large-scale vision foundation models with deformable convolutions.
* InternVideo: large-scale video foundation models for multimodal understanding.
* VideoChat: an end-to-end chat assistant for video comprehension.
* All-Seeing-Project: towards panoptic visual recognition and understanding of the open world.





Datasets
========

* ShareGPT4o: a groundbreaking large-scale resource that we plan to open-source with 200K meticulously annotated images, 10K videos with highly descriptive captions, and 10K audio files with detailed * descriptions.
* InternVid: a large-scale video-text dataset for multimodal understanding and generation.
* MMPR: a high-quality, large-scale multimodal preference dataset.



Benchmarks
==========

* MVBench: a comprehensive benchmark for multimodal video understanding.
* CRPE: a benchmark covering all elements of the relation triplets (subject, predicate, object), providing a systematic platform for the evaluation of relation comprehension ability.
* MM-NIAH: a comprehensive benchmark for long multimodal documents comprehension.
* GMAI-MMBench: a comprehensive multimodal evaluation benchmark towards general medical AI.
































