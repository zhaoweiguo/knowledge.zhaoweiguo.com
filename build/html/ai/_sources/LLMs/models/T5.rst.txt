T5
###


基本
====


T5是Google在2020年提出的一个功效强大的文本-到-文本转化模型,具有以下核心特点::

	- 基于Transformer Encoder-Decoder结构设计
	- 提出了文本到文本传递(Text-to-Text Transfer Transformer)的统一框架,所有任务都表述为文本生成问题
	- 在Colossal Clean Crawled Corpus语料上进行预训练,数据量达到750GB
	- 支持多种下游任务,如翻译、文本摘要、问答等
	- 提供小规模到超大规模的5个模型,参数量从60 million到11 billion
	- T5-Base模型参数量约为220 million
	- 应用了自注意力机制和平衡损失函数
	- 支持PyTorch实现和访问
	- 模型性能在多项NLP任务上首次超过人类


5个模型
-------

1. T5-Small::

	- 参数量约为60 million
	- 基础的小规模T5模型
	- 模型文件大小约220MB

	- CPU:通用CPU即可运行
	- GPU:1-2块GPU,如Tesla V100
	- 内存:4-8GB

2. T5-Base::

	- 参数量约为220 million
	- 模型大小约为750MB
	- 性能和速度平衡的基础模型

	- CPU:通用CPU,但会比较慢
	- GPU:1-4块GPU,如Tesla V100
	- 内存:8-16GB

3. T5-Large::

	- 参数量约为770 million
	- 模型文件大小约为2.8GB
	- 性能较好的大模型

	- CPU:不建议,会非常缓慢
	- GPU:4-8块GPU,如Tesla V100
	- 内存:16-32GB

4. T5-3B::

	- 参数量约为3 billion
	- 模型大小约为11GB
	- 非常大的T5模型

	- CPU:不能运行
	- GPU:8-16块高端GPU,如Tesla V100 32GB
	- 内存:64-128GB

5. T5-11B::

	- 参数量最大,约为11 billion
	- 模型文件大小约为41GB
	- T5系列中性能最强的超大模型

	- CPU: 不能运行
	- GPU: 32+块高端GPU,专用集群
	- 内存:256GB+
	- 需使用专门的大规模分布式训练技术


示例
----

::

	tokenizer = AutoTokenizer.from_pretrained("t5-small")
	model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")


	tokenizer = T5Tokenizer.from_pretrained('t5-base')
	model = T5ForConditionalGeneration.from_pretrained("t5-base")











