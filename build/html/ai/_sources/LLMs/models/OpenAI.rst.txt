OpenAI
######



Whisper
=======

* https://huggingface.co/openai/whisper-large-v3



GPT
===


gpt2
----

GPT-2是一个重要的自回归语言模型,由OpenAI在2019年提出,其主要特性如下:
- 基于Transformer的Decoder结构
- 在WebText数据集上进行无监督预训练,数据量达40GB
- 提出了针对长文本生成的预训练语言模型
- 总参数量1.5亿,采用12层Decoder架构
- 模型文件大小570MB(GPT-2-small)
- 支持生成逼真的自然语言文本
- 可应用于机器翻译、问答、文本摘要、语音识别等任务
- 提供了117M、345M、774M、1558M四个模型规模
- 支持PyTorch实现
- 采用字元级BPE tokenizer
- 词汇量达到500万

GPT-2展示了预训练语言模型的强大能力,对自然语言理解和生成TASKS发挥关键作用,是深度学习处理文本的重要模型。

示例::

    model = GPT2LMHeadModel.from_pretrained('gpt2')
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')



distilgpt2
----------

distilgpt2是GPT-2模型的蒸馏版本,由HuggingFace的团队基于GPT-2训练而来。其具体特点如下::

    - 基于GPT-2模型进行知识蒸馏,保留语言生成能力
    - 参数量大幅减少,从1.5亿降至8200万
    - 模型大小从774M降至331M,更易部署
    - 训练/推理速度更快
    - 模型性能与GPT-2相近
    - 支持PyTorch和TensorFlow两种框架
    - 可进行文本生成、填充等下游任务
    - 实现了 transformer的解码器结构
    - 基于Transformer的自注意力机制
    - 采用了基于字节对的BPE编码


GPT4ALL
=======

* GPT4ALL（GPT for ALL）即是将大模型小型化做到极致的工具，该模型运行于计算机 CPU 上，无需互联网连接，也不会向外部服务器发送任何聊天数据（除非选择允许将您的聊天数据用于改进未来的 GPT4All 模型）。
* GPT4All Chat 适用于 Windows、Linux 和 macOS 操作系统，支持。
* GPT4All 基于 LLaMA 的开源大模型框架，通过调用 ChatGPT 来达到类似效果，并将其小型化到使用 CPU 即可部署。
* 对于 GPT4AII 来说，他们只花费了 4 天时间、$800 的 GPU 成本以及 $500 的 OpenAI API 调用成本之后，就成功模仿了 ChatGPT，实现了这个项目。

.. warning:: 不支持中文


* GitHub: https://github.com/nomic-ai/gpt4all
* 官网: https://gpt4all.io/index.html
* HuggingFace: https://huggingface.co/datasets/nomic-ai/gpt4all-j-prompt-generations
















