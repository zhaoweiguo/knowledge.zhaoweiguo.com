常用
####

* 深度学习的基本概念，包括神经网络、反向传播、激活函数等。






深度学习的应用::

    1. 计算机视觉
    2. 自动语音识别
    3. 自然语言处理
    4. 人机博弈
    5. 强化学习和统计建模



深度学习前身是神经网络（基于的神经网络模型和用数据编程的核心思想）。
机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。
深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。
仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。


时至今日，绝大多数神经网络都包含以下的核心原则::

    交替使用线性处理单元与非线性处理单元，它们经常被称为“层”
    使用链式法则（即反向传播）来更新网络的参数

单层神经网络：线性回归和softmax回归。
线性回归输出是一个连续值，因此适用于回归问题。
回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。
与回归问题不同，分类问题中模型的最终输出是一个离散值。
我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。
softmax回归则适用于分类问题。

建立基于输入x1和x2来计算输出y的表达式，也就是模型（model）
通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）
收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）
给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。
在机器学习里，将衡量误差的函数称为损失函数（loss function）。
这里使用的平方误差函数也称为平方损失（square loss）
当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。
大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。

在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。


模型训练所涉及的3个要素::

    1. 训练数据
    2. 损失函数
    3. 优化算法




* PyTorch版: https://tangshusen.me/Dive-into-DL-PyTorch/
* GitHub: https://github.com/ShusenTang/Dive-into-DL-PyTorch
* 英文版: https://d2l.ai/
* GitHub开源地址: https://github.com/d2l-ai/d2l-en
* 中文版: https://zh.d2l.ai/
* GitHub开源地址: https://github.com/d2l-ai/d2l-zh










