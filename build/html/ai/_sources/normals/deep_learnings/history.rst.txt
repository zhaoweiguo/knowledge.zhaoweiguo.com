历史
####

.. image:: https://img.zhaoweiguo.com/knowledge/images/ais/history_show1.jpeg

缘起
====

1943年，神经生理学家和神经元解剖学家 Warren McCulloch 和数学家 Walter Pitts 提出了神经元的数学描述和结构，并且证明了只要有足够的简单神经元，在它们互相连接并同步运行的情况下，可以模拟任何计算函数。

这样开创性的工作被认为是 NN 的起点。

几度兴衰
========

在60年代掀起了 NN 研究的第一次热潮::

    1958年，计算机学家 Frank Rosenblatt 提出了一种具有三级结构的 NN，称为“感知机”（Perceptron）。
    它实际上是一种二元线性分类器，可以被看作一种单层 NN.
    
    尽管结构简单，感知机能够学习并解决相当复杂的问题，在60年代掀起了 NN 研究的第一次热潮。
    很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。

    这股热潮持续了10年，终于因为感知机的作用终归有限（比如它不能处理线性不可分问题），
    在实践中无法产生实际的价值，而导致了 NN 发展的第一次低潮期。

直到80年代，NN 的研究才开始复苏::

    1986年，David Rumelhart、Geoffrey Hinton 和 Ronald Williams 将反向传播算法用于多层 NN 的训练，带来了 NN 的第二春。
    
    然而，训练 NN，最开始都是随机初始化权值。
    当 NN 的层数稍多之后，随机的初始值很可能导致反复迭代仍不收敛——根本训练不出来可用的 NN。
    进一步的研究和实际应用都受阻。

    基于统计的学习模型有严格的理论基础，可以在数学上严格地被证明为是凸优化问题。
    特别是在 SVM/SVR 出现后，用统计学习模型执行复杂任务也能得到不错的结果。

    而 NN 缺少数学理论支持——它的优化过程不是凸优化，根本不能从数学原理上证明最优解的存在；
    就算训练出了结果，也无法解释自己为什么有效；在实际运用的效果又不够好。

    如此种种，NN 研究进入第二次低谷。此后十几年的时间里，大多数研究人员都放弃了 NN。

2006年从 NN 到 DNN::

    Hinton 却矢志不渝地坚持着对 NN 的研究。终于在2006年迎来了划时代的成果。
    这一年，Hinton 发表了经典论文“Reducing the Dimensionality of Data with Neural Networks”。

    这篇论文提出了预训练（Pre-training）的方法（可以简单地想象成是“一层一层”地训练），分层初始化，
    使得深层神经网络（Deep Neural Network，DNN）的训练变得可能
    训练 NN 不必再局限在很少的一两层，四五层甚至八九层都成为了可能。

    由此，NN 重新回到大众的视线中，从此 NN 进入了 DNN 时代。

    我们说的深度学习一词，其实在30多年前就已经被提出来了。
    Rina Dechter 在1986年的论文中就提到了“ Shallow Learning”和“Deep Learning”。
    不过直到2000年，这个说法才被引入到 NN 领域。









