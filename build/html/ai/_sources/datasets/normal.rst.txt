常用
####




NLP评测
=======



TriviaQA
--------

* 类型：事实性问答
* 开放领域问答数据集，包含大量从trivia比赛中收集的问题和答案。它用于评估模型的事实性知识和信息检索能力，即模型在给定问题下能否从文本中提取正确的答案。


ARC
---

* AI2 Reasoning Challenge
* 类型：科学推理
* 用于测试模型科学推理能力的挑战数据集。它包含来自标准化测试的科学问题，分为简单和挑战两部分，测试模型在科学常识和复杂推理上的表现。


PIQA
----

* Physical Interaction: Question Answering
* 类型：物理常识推理
* 专注于物理世界的常识推理问题。它包含了与物理交互相关的选择题，测试模型理解和推理日常物理事件的能力。

Hellaswag
---------

* 类型：常识推理和故事完形填空
* 用于评估模型在常识推理和故事完形填空方面的能力。模型需要从多种选项中选择最符合上下文逻辑的结尾，测试模型的常识和语言理解能力。

QBQA
----

* 类型：教育类问答
* 问答数据集，包含大量从不同领域的考试题库中收集的问题。它用于评估模型在回答教育类问题上的能力，涵盖广泛的学科和题型。


Winogrande
----------

* 类型：常识推理
* 挑战数据集，用于评估模型的常识推理能力。它基于 Winograd Schema Challenge，包含需要模型在两个非常相似的选项中进行选择的问题，这些问题要求深层次的语言理解和常识推理能力。


CoT 复杂逻辑推理类数据登记本
============================

Long-Form Thought (LFT) Data 数据集
-----------------------------------

- 为训练和评估大语言模型（LLMs）在 长文本推理（Long-Form Reasoning） 和 复杂思维链（Complex Chain-of-Thought） 任务中的能力而构建的数据集。
- 它旨在帮助模型生成结构化、连贯且逻辑严谨的长文本回答，尤其适用于多步推理、论证构建和深入分析任务。
- 核心特点
    - 多步推理能力（Multi-Step Reasoning）
    - 长文本生成（Long-Form Generation）
    - 结构化数据格式（Structured Format）







Code: 评估代码编程生成模型基准数据集
====================================


MBPP (ManyBench Programming Problems)
-------------------------------------

* 来源: 由Austin等人于2021年提出。
* 内容: MBPP包含500个Python编程问题，这些问题是通过从不同的在线资源收集的，并经过人工筛选和验证。每个问题都附有一个问题描述、参考实现以及多组测试用例。这些测试用例用于验证生成的代码是否正确。
* 用途: MBPP用于评估代码生成模型在解决各种编程问题方面的性能，特别是模型生成的代码在运行测试用例时的通过率。

HumanEval
---------

* 来源: 由Chen等人于2021年提出，作为OpenAI Codex模型的评估基准。
* 内容: HumanEval由164个Python编程问题组成，这些问题专门设计用于评估代码生成模型。每个问题提供了详细的问题描述、函数签名以及一个或多个测试用例。与MBPP类似，这些测试用例用于评估生成代码的正确性。
* 用途: HumanEval主要用于评估代码生成模型在理解自然语言描述并生成相应代码的能力，特别是关注生成代码在运行时的正确性。

说明::

    HumanEval: 测试模型在 Python 语言上的代码生成能力。
    HumanEval Java: 测试模型在 Java 语言上的代码生成能力。
    HumanEval Bash: 测试模型在 Bash 语言上的代码生成能力。
    ...


APPS Dateset
------------

* Automated Programming Progress Standard
* 专门为评估和提升 大语言模型（LLMs）编程能力 而构建的大规模数据集。
* 它主要用于测试模型在自动代码生成、编程推理和问题求解方面的表现。APPS 数据集在 代码补全、代码生成和程序理解 领域具有重要意义，广泛用于训练和评估像 GPT-4、Codex、CodeT5 等代码模型。
* 核心特点
    * 多样化编程任务（Diverse Programming Tasks）
        * 包含 10,000+ 道编程题目，题目难度从简单的基础逻辑题到复杂的算法设计不等。
        * 涵盖的数据结构和算法包括：排序、搜索、动态规划、递归、图算法、数学建模等。
    * 真实世界场景（Real-World Problem Solving）
        * 题目来源于各类 在线编程竞赛平台，如 Codeforces、AtCoder、LeetCode 等，具有高度的实际应用价值。
    * 多语言支持（Multi-language Code Samples）
        * 尽管主要以 Python 为主，但也包括 C++、Java 等多种编程语言，适合跨语言模型的训练和测试。
    * 自动化评测机制（Automated Evaluation）
        * 每道题都配有输入输出样例和测试用例，方便对模型生成的代码进行自动化测试，快速判断代码的正确性。

* 数据集结构
    * Problem Statement（题目描述）： 自然语言形式的编程任务描述。
    * Input/Output Examples（输入输出示例）： 帮助模型理解题目要求。
    * Starter Code（初始代码，部分题目提供）： 提供框架代码，模型需补全关键逻辑。
    * Reference Solutions（参考答案）： 人类编写的正确代码作为参考。
    * Unit Tests（单元测试）： 自动评估代码正确性的测试用例。

* 🌍 任务类型
    * Code Generation
    * Code Completion
    * Bug Fixing
    * Code Understanding

* 评估指标
    * Pass@k： 衡量模型在 k 次尝试内至少成功生成正确代码 的概率（常见的指标有 Pass@1、Pass@5、Pass@10）。
    * Exact Match Accuracy： 模型生成代码是否与参考答案完全一致。
    * Execution Accuracy： 代码在测试用例上的正确率，关注代码的实际运行效果。

TACO(Text-to-Code)数据集
------------------------

- 专门设计用于研究和评估自然语言到代码生成（Text-to-Code Generation） 能力的数据集。
- 它的核心目标是帮助训练和测试大语言模型（LLMs）在将自然语言描述转换为可执行代码方面的表现。
- TACO 数据集主要用于代码生成、代码补全、程序理解等任务，广泛应用于自动化编程、智能 IDE 辅助、代码搜索等场景。




Spider
------

* Spider 是一个复杂的自然语言到 SQL 查询生成的评估数据集。它包含自然语言问题及其对应的 SQL 查询，测试模型将自然语言转换为正确 SQL 语句的能力。这个数据集涉及到复杂的数据库操作和多表查询。

CruxE
-----

* CruxE 是一个用于评估模型在代码生成和修复方面的能力的数据集。它包含编程问题和代码片段，要求模型生成或修复代码以解决给定的问题。这个数据集测试模型在理解现有代码和生成新代码上的能力。







Math: 评估数学专用的基准数据集
==============================

GSM8K
-----

* Grade School Math 8K
* GSM8K 是一种基准数据集，通常用于评估和测试自然语言处理（NLP）模型在数学问题求解方面的能力。
* 它代表了“Grade School Math 8K”，即包含大约 8000 道适合小学数学水平的问题。这些问题涉及多种数学主题，如算术、几何、代数等。


Odyssey Math maj@16
-------------------

* Odyssey Math 是 Compass Learning 旗下的一个在线数学学习平台，专为K-12学生设计。该平台提供互动性的数学课程和评估工具，旨在提高学生的数学技能和理解能力。
* maj@16: 面向16岁


GRE Math maj@16
---------------

研究生入学考试（Graduate Record Examination，简称 GRE）中的数学部分。GRE 是许多研究生院入学申请中的一项重要考试，而数学部分主要测试考生的基本数学技能，包括算术、代数、几何和数据分析。


AMC maj@16
----------

* American Mathematics Competitions
* 包括多个级别的数学竞赛，旨在促进中小学学生对数学的兴趣和理解。

    AMC 8：针对 8 年级及以下学生的竞赛。
    AMC 10：针对 10 年级及以下学生的竞赛。
    AMC 12：针对 12 年级及以下学生的竞赛。

AIME maj@16
-----------

* American Invitational Mathematics Examination
* 美国邀请数学考试（American Invitational Mathematics Examination）的缩写，是在 AMC 竞赛中表现优异的学生参加的更高级别的考试。AIME 旨在进一步挑战和激励有才华的数学学生，为他们提供更高水平的数学问题。这是数学奥林匹克竞赛（Mathematical Olympiad）的选拔过程的一部分，表现优异的学生可以进入更高级别的竞赛，如美国数学奥林匹克（USAMO）。


NuminaMath
----------

* 专门为数学推理和计算任务设计的数据集，主要用于测试和提升大语言模型（LLMs）在复杂数学场景下的推理能力和解题能力。
* 核心特点

    - 多领域数学覆盖（Multi-domain Math Coverage）：包含基础算术、代数、几何、概率、统计、微积分等多种数学领域的问题。
    - 推理与计算结合（Reasoning + Calculation）：不仅要求模型完成简单计算，还需要进行多步逻辑推理，类似于“链式推理（Chain of Thought）”的能力测试。
    - 多种题型（Diverse Problem Types）
        - 填空题：直接给出计算结果或推理结论。
        - 选择题：评估模型在多个备选答案中的判断能力。
        - 开放式问题：需要详细的解题步骤，考验模型的解释和推理能力。
    - 复杂公式和符号支持（Complex Notation Handling）：可能包含复杂的数学公式、LaTeX 表达式，或涉及数学符号的处理能力。





其他
====

ELI5
----

* ELI5（"Explain Like I'm Five"）是一个长形式问答数据集，最初构建于Reddit论坛，旨在帮助用户以简单易懂的方式理解复杂问题。ELI5数据集包含大量的“如何”、“为什么”或“什么”类型的问题，这些问题通常需要较长的回答和多个段落作为证据支持
* 在研究中，ELI5被广泛使用，因为它的多样性和挑战性使其成为评估大型语言模型（LLMs）在处理复杂问答任务时的一个重要基准。研究者们利用ELI5数据集来测试模型的生成能力，尤其是在生成准确且有引用支持的长答案方面


























