分词
####

* 中文跟英文最大的不同在于，英文是由一个个单词构成的，单词与单词之间有空格隔断。但是中文不一样，中文单词和单词之间除了标点符号没有别的隔断。这就给程序理解文本带来了一定的难度，分词的需求也应运而生。

NLP 分词工具::

    开源版:
    jieba、HanLP、THULAC

    腾讯、百度、阿里等公司也有相应的商业付费工具

jieba
=====

jieba 的使用示例::

    // 分词
    import jieba
    text = "极客时间棒呆啦"
    # jieba.cut 得到的是 generator 形式的结果
    seg = jieba.cut(text)  
    print(' '.join(seg)) 
    # Get： 极客 时间 棒呆 啦


    // 词性标注
    import jieba.posseg as posseg
    text = "一天不看极客时间我就浑身难受"
    # 形如 pair ('word, 'pos') 的结果
    seg = posseg.cut(text)  
    print([se for se in seg]) 
    # Get [ pair (' 一天 ', 'm'), pair (' 不 ', 'd'), pair (' 看 ', 'v'), pair (' 极客 ', 'n'), 
    #       pair (' 时间 ', 'n'), pair (' 我 ', 'r'), pair (' 就 ', 'd'), pair (' 浑身 ', 'n'), pair (' 难受 ', 'v')
    # ]
    # 说明:
    'm': 数词 (numeral)
    'd': 副词 (adverb)
    'n': 名词 (noun)




HanLP
=====

* https://www.hanlp.com/
* 面向生产环境的多语种自然语言处理（NLP）工具包
* 包括功能完善、易于构建和语料时新。它利用世界上最大的多语种语料库，支持包括简繁中文、英文、日文、俄文、法文、德文等 104 种语言上的 10 种联合任务
* HanLP 还自主研发了 Euler 大模型，该模型引入了全新分词算法、group-query attention 架构、flash-attention 算子、非均衡任务分配等新技术。Euler 大模型具有本地私有化部署、零门槛打开即用、支持本地推理和微调等特性，确保了使用的安全性和效率性。用户还可以生成私有大模型、自定义衍生人格，以及使用自有数据进行训练，从而创建个人专属的 AI 专家
* 这些任务包括但不限于::

    分词
    词性标注
    命名实体识别
    依存句法分析
    成分语法分析
    语义依存分析
    语义角色标注
    词干提取
    词法语法特征提取
    抽象意义表示
    指代消解
    语义文本相似度
    文本风格转换






































