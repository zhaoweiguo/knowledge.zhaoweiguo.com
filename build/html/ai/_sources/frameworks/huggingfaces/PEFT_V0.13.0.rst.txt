PEFT 0.13.0
###########

* GitHub: https://github.com/huggingface/peft


GET STARTED
===========


Quicktour
---------


train with the Trainer class
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

TrainingArguments::

    training_args = TrainingArguments(
        output_dir="your-name/bigscience/mt0-large-lora",
        learning_rate=1e-3,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        num_train_epochs=2,
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
    )

Trainer::

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    trainer.train()





Inference
^^^^^^^^^

load any PEFT-trained model for inference with the AutoPeftModel class::

    from peft import AutoPeftModelForCausalLM
    from transformers import AutoTokenizer
    import torch

    model = AutoPeftModelForCausalLM.from_pretrained("ybelkada/opt-350m-lora")
    tokenizer = AutoTokenizer.from_pretrained("facebook/opt-350m")

    model = model.to("cuda")
    model.eval()
    inputs = tokenizer("Preheat the oven to 350 degrees and place the cookie dough", return_tensors="pt")

    outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=50)
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])

    "Preheat the oven to 350 degrees and place ........."



PEFT method guides
==================

.. note:: æœ¬èŠ‚å†…å®¹ï¼ŒåŸå§‹ç‰ˆæœ¬éƒ½æœ‰è®²ï¼Œåªä¸è¿‡æ”¾åœ¨ã€ŒTASK GUIDESã€èŠ‚ï¼Œé¡ºåºæœ‰è°ƒæ•´

IA3
---

* IA3å°†æ¨¡å‹çš„æ¿€æ´»ï¼ˆè‡ªæ³¨æ„åŠ›å’Œç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å—ä¸­çš„é”®å’Œå€¼ï¼Œä»¥åŠä½ç½®å‰é¦ˆç½‘ç»œçš„ä¸­é—´æ¿€æ´»ï¼‰ä¹˜ä»¥ä¸‰ä¸ªå­¦ä¹ å‘é‡
* ä¸å¼•å…¥æƒé‡çŸ©é˜µè€Œä¸æ˜¯å‘é‡çš„ LoRA ç›¸æ¯”ï¼Œè¿™ç§ PEFT æ–¹æ³•å¼•å…¥çš„å¯è®­ç»ƒå‚æ•°æ•°é‡ç”šè‡³æ›´å°‘
* åŸå§‹æ¨¡å‹çš„å‚æ•°ä¿æŒå†»ç»“ï¼Œä»…æ›´æ–°è¿™äº›å‘é‡ã€‚å› æ­¤ï¼Œå¯¹æ–°çš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œå¾®è°ƒä¼šæ›´å¿«ã€æ›´ä¾¿å®œã€æ›´é«˜æ•ˆ

ç¤ºä¾‹::

    from peft import IA3Config, get_peft_model

    peft_config = IA3Config(task_type="SEQ_2_SEQ_LM")
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    "trainable params: 282,624 || all params: 1,229,863,936 || trainable%: 0.022980103060766553"




Developer guides
================

Model merging
-------------

* PEFT æä¾›äº†å¤šç§åˆå¹¶æ¨¡å‹çš„æ–¹æ³•ï¼Œä¾‹å¦‚çº¿æ€§æˆ– SVD ç»„åˆ
* ä¸¤ç§é€šè¿‡æ¶ˆé™¤å†—ä½™å‚æ•°æ›´æœ‰æ•ˆåœ°åˆå¹¶ LoRA é€‚é…å™¨çš„æ–¹æ³•::

    TIES - TrImã€Elect å’Œ Merge (TIES)
        1. é¦–å…ˆï¼Œä¿®å‰ªå†—ä½™å‚æ•°
        2. ç„¶åå°†å†²çªç¬¦å·åˆ†è§£ä¸ºèšåˆå‘é‡
        3. æœ€åå¯¹ç¬¦å·ä¸èšåˆç¬¦å·ç›¸åŒçš„å‚æ•°è¿›è¡Œå¹³å‡
        æ­¤æ–¹æ³•è€ƒè™‘åˆ°æŸäº›å€¼ï¼ˆå†—ä½™å’Œç¬¦å·ä¸ä¸€è‡´ï¼‰å¯èƒ½ä¼šé™ä½åˆå¹¶æ¨¡å‹çš„æ€§èƒ½

    DARE - Drop And REscale
        ä¸€ç§å¯ç”¨äºä¸º TIES ç­‰å…¶ä»–æ¨¡å‹åˆå¹¶æ–¹æ³•åšå‡†å¤‡çš„æ–¹æ³•
        å·¥ä½œåŸç†æ˜¯æ ¹æ®ä¸¢å¼ƒç‡éšæœºä¸¢å¼ƒå‚æ•°å¹¶é‡æ–°è°ƒæ•´å‰©ä½™å‚æ•°
        è¿™æœ‰åŠ©äºå‡å°‘å¤šä¸ªæ¨¡å‹ä¹‹é—´å†—ä½™å’Œæ½œåœ¨å¹²æ‰°å‚æ•°çš„æ•°é‡



Merge method
^^^^^^^^^^^^

åˆå¹¶ä¸‰ä¸ªå¾®è°ƒçš„ ``TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T`` æ¨¡å‹ï¼š ``tinyllama_lora_norobots`` ã€ ``tinyllama_lora_sql`` å’Œ ``tinyllama_lora_adcopy``::

    from peft import PeftConfig, PeftModel
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch

    config = PeftConfig.from_pretrained("smangrul/tinyllama_lora_norobots")
    model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, load_in_4bit=True, device_map="auto").eval()
    tokenizer = AutoTokenizer.from_pretrained("smangrul/tinyllama_lora_norobots")

    model = PeftModel.from_pretrained(model, "smangrul/tinyllama_lora_norobots", adapter_name="norobots")
    _ = model.load_adapter("smangrul/tinyllama_lora_sql", adapter_name="sql")
    _ = model.load_adapter("smangrul/tinyllama_lora_adcopy", adapter_name="adcopy")

ä½¿ç”¨ ``add_weighted_adapter()`` æ–¹æ³•è®¾ç½®(TIESæ–¹æ³•)::

    adapters = ["norobots", "adcopy", "sql"]
    weights = [2.0, 1.0, 1.0]
    adapter_name = "merge"
    density = 0.2
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="ties", density=density)

ä½¿ç”¨ ``add_weighted_adapter()`` æ–¹æ³•è®¾ç½®2(DAREæ–¹æ³•)::

    adapters = ["norobots", "adcopy", "sql"]
    weights = [2.0, 0.3, 0.7]
    adapter_name = "merge"
    density = 0.2
    model.add_weighted_adapter(adapters, weights, adapter_name, combination_type="dare_ties", density=density)

ä½¿ç”¨set_adapter()æ–¹æ³•å°†æ–°åˆå¹¶çš„æ¨¡å‹è®¾ç½®ä¸ºæ´»åŠ¨æ¨¡å‹::

    model.set_adapter("merge")


Merging (IA)Â³ Models
^^^^^^^^^^^^^^^^^^^^^^^

å°†ä¸‰ä¸ª (IA)Â³ é€‚é…å™¨åˆå¹¶åˆ° PEFT æ¨¡å‹ä¸­::

    adapters = ["adapter1", "adapter2", "adapter3"]
    weights = [0.4, 0.3, 0.3]
    adapter_name = "merge"
    model.add_weighted_adapter(adapters, weights, adapter_name)


å°†åˆå¹¶çš„æ¨¡å‹è®¾ç½®ä¸ºæ´»åŠ¨æ¨¡å‹::

    model.set_adapter("merge")



Quantization
------------

* é‡åŒ–ç›¸å…³åŸºæœ¬å‚è§: ``transformer_4.45.2``
* QLoRA æ˜¯ä¸€ç§å°†æ¨¡å‹é‡åŒ–ä¸º 4 ä½ï¼Œç„¶åä½¿ç”¨ LoRA å¯¹å…¶è¿›è¡Œè®­ç»ƒçš„æ–¹æ³•ã€‚æ­¤æ–¹æ³•å…è®¸æ‚¨åœ¨å•ä¸ª 48GB GPU ä¸Šå¾®è°ƒ 65B å‚æ•°æ¨¡å‹ï¼


LoRA
----

Initialization
^^^^^^^^^^^^^^

* LoRAæƒé‡çš„åˆå§‹åŒ–ç”±LoraConfigä¸­çš„å‚æ•°init_lora_weightsæ§åˆ¶ã€‚
* é»˜è®¤æƒ…å†µä¸‹ï¼ŒPEFT ä½¿ç”¨æƒé‡ A çš„ Kaiming-uniform å’Œæƒé‡ B çš„é›¶æ¥åˆå§‹åŒ– LoRA æƒé‡

::

    1. PiSSA
    2. OLoRA
    3. LoftQ
    4. Rank-stabilized LoRA
    5. Weight-Decomposed Low-Rank Adaptation (DoRA)
    6. QLoRA-style training
    7. Memory efficient Layer Replication with LoRA



Optimizers
^^^^^^^^^^

* LoRA è®­ç»ƒå¯ä»¥ä½¿ç”¨ `LoRA+ <https://arxiv.org/abs/2402.12354>`_ è¿›è¡Œä¼˜åŒ–ï¼ŒLoRA+ å¯¹é€‚é…å™¨çŸ©é˜µ A å’Œ B ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼Œå¯å°†å¾®è°ƒé€Ÿåº¦æé«˜ 2 å€ï¼Œæ€§èƒ½æé«˜ 1-2%ã€‚
* ç¤ºä¾‹::

    from peft import LoraConfig, get_peft_model
    from peft.optimizers import create_loraplus_optimizer
    from transformers import Trainer
    import bitsandbytes as bnb

    base_model = ...
    config = LoraConfig(...)
    model = get_peft_model(base_model, config)

    optimizer = create_loraplus_optimizer(
        model=model,
        optimizer_cls=bnb.optim.Adam8bit,
        lr=5e-5,
        loraplus_lr_ratio=16,
    )
    scheduler = None

    ...
    trainer = Trainer(
        ...,
        optimizers=(optimizer, scheduler),
    )



Merge LoRA weights into the base model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/11/B4t46d.png

    LoRA é€‚é…å™¨åˆå¹¶çš„ç¤ºæ„å›¾

how to run that using PEFT::

    base_model = ...
    from peft import PeftModel
    peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
    model = PeftModel.from_pretrained(base_model, peft_model_id)
    model.merge_and_unload()


keep a copy of the weights so you can unmerge the adapter later or delete and load different ones::

    model = PeftModel.from_pretrained(base_model, peft_model_id)
    model.merge_adapter()

    # unmerge the LoRA layers from the base model
    model.unmerge_adapter()

æ ¹æ®ç”¨æˆ·åœ¨weightså‚æ•°ä¸­æä¾›çš„æƒé‡æ–¹æ¡ˆå°†å¤šä¸ª LoRA åˆå¹¶åˆ°æ–°é€‚é…å™¨ä¸­::

    # åŠ è½½ç¬¬ä¸€ä¸ªé€‚é…å™¨
    peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
    model = PeftModel.from_pretrained(base_model, peft_model_id, adapter_name="sft")

    # åŠ è½½å¦ä¸€ä¸ªé€‚é…å™¨å¹¶å°†å…¶ä¸ç¬¬ä¸€ä¸ªé€‚é…å™¨åˆå¹¶
    weighted_adapter_name = "sft-dpo"
    model.load_adapter("alignment-handbook/zephyr-7b-dpo-lora", adapter_name="dpo")
    model.add_weighted_adapter(
        adapters=["sft", "dpo"],
        weights=[0.7, 0.3],
        adapter_name=weighted_adapter_name,
        combination_type="linear"
    )
    model.set_adapter(weighted_adapter_name)


Load adapters
^^^^^^^^^^^^^

ä½¿ç”¨load_adapter()å°†é€‚é…å™¨åŠ è½½åˆ°é¢„è®­ç»ƒæ¨¡å‹ä¸Š::

    peft_model_id = "alignment-handbook/zephyr-7b-sft-lora"
    model = PeftModel.from_pretrained(base_model, peft_model_id)

    # load different adapter
    model.load_adapter("alignment-handbook/zephyr-7b-dpo-lora", adapter_name="dpo")

    # set adapter as active
    model.set_adapter("dpo")

åˆ‡å›åŸºæœ¬æ¨¡å‹::

    # unload adapter(å¸è½½æ‰€æœ‰ LoRA æ¨¡å—)
    model.unload()

    # delete adapter(å®Œå…¨åˆ é™¤é€‚é…å™¨)
    model.delete_adapter("dpo")


Inference with different LoRA adapters in the same batch
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* é€šå¸¸ï¼Œæ¯ä¸ªæ¨ç†æ‰¹æ¬¡å¿…é¡»åœ¨ PEFT ä¸­ä½¿ç”¨ç›¸åŒçš„é€‚é…å™¨ã€‚
* å¯ä»¥ä½¿ç”¨adapter_nameå‚æ•°åœ¨åŒä¸€æ‰¹æ¬¡ä¸­æ··åˆä¸åŒçš„LoRAé€‚é…å™¨::

    # load the LoRA adapter for French
    peft_model = PeftModel.from_pretrained(model, <path>, adapter_name="adapter_fr")
    # next, load the LoRA adapter for German
    peft_model.load_adapter(<path>, adapter_name="adapter_de")

* å¯ä»¥ä½¿ç”¨adapter_nameså‚æ•°æ¥æŒ‡å®šæ¯ä¸ªæ ·æœ¬ä½¿ç”¨å“ªä¸ªé€‚é…å™¨::

    inputs = tokenizer(
        [
            "Hello, my dog is cute",            # è‹±è¯­
            "Salut, mon chien est mignon",      # æ³•è¯­
            "Hallo, mein Hund ist sÃ¼ÃŸ",         # å¾·è¯­
        ],
        return_tensors="pt",
        padding=True,
    )

    adapter_names = [
        "__base__",
        "adapter_fr",
        "adapter_de",
    ]
    # å¯¹æ¯ä¸€ä¸ªinputæŒ‡å®šadapter
    output = peft_model.generate(**inputs, adapter_names=adapter_names, max_new_tokens=20)


æ³¨æ„äº‹é¡¹
""""""""

* å®ƒä»…é€‚ç”¨äºæ¨ç†ï¼Œä¸é€‚ç”¨äºè®­ç»ƒã€‚
* çœ‹çœ‹ `LoRAX <https://github.com/predibase/lorax>`_ ã€ `punica <https://github.com/punica-ai/punica>`_ æˆ– `S_LoRA <https://github.com/S-LoRA/S-LoRA>`_ ç­‰æ›¿ä»£å®ç°ï¼Œå®ƒä»¬ä¸“é—¨ç”¨äºä¸å¤§é‡ä¸åŒçš„é€‚é…å™¨é…åˆä½¿ç”¨ã€‚



Custom models
-------------

.. note:: ä¸€äº›å¾®è°ƒæŠ€æœ¯ï¼ˆä¾‹å¦‚prompt tuningï¼‰ç‰¹å®šäºè¯­è¨€æ¨¡å‹ã€‚è¿™æ„å‘³ç€åœ¨ ğŸ¤— PEFT ä¸­ï¼Œé»˜è®¤ä½¿ç”¨ ğŸ¤— Transformers æ¨¡å‹ã€‚ç„¶è€Œï¼Œå…¶ä»–å¾®è°ƒæŠ€æœ¯ï¼ˆä¾‹å¦‚LoRA ï¼‰å¹¶ä¸å±€é™äºç‰¹å®šçš„æ¨¡å‹ç±»å‹ã€‚


Multilayer perceptron
^^^^^^^^^^^^^^^^^^^^^

ç®€å•çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå…·æœ‰è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚

.. code-block:: python

    from torch import nn
    class MLP(nn.Module):
        def __init__(self, num_units_hidden=2000):
            super().__init__()
            self.seq = nn.Sequential(
                nn.Linear(20, num_units_hidden),
                nn.ReLU(),
                nn.Linear(num_units_hidden, num_units_hidden),
                nn.ReLU(),
                nn.Linear(num_units_hidden, 2),
                nn.LogSoftmax(dim=-1),
            )

        def forward(self, X):
            return self.seq(X)

.. note:: åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©äº†å¤§é‡çš„éšè—å•å…ƒ(num_units_hidden=2000)æ¥çªå‡º PEFT çš„æ•ˆç‡å¢ç›Š

åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­ï¼Œéœ€è¦æˆ‘ä»¬ä½œä¸ºç”¨æˆ·æ¥é€‰æ‹©è¦è°ƒæ•´çš„å±‚::

    # æ‰“å°æ‰€æœ‰çš„å±‚
    >>> print([(n, type(m)) for n, m in MLP().named_modules()])
    [('', __main__.MLP),
     ('seq', torch.nn.modules.container.Sequential),
     ('seq.0', torch.nn.modules.linear.Linear),
     ('seq.1', torch.nn.modules.activation.ReLU),
     ('seq.2', torch.nn.modules.linear.Linear),
     ('seq.3', torch.nn.modules.activation.ReLU),
     ('seq.4', torch.nn.modules.linear.Linear),
     ('seq.5', torch.nn.modules.activation.LogSoftmax)]


    # ç¤ºä¾‹ï¼šå°† LoRA åº”ç”¨äºè¾“å…¥å±‚å’Œéšè—å±‚ï¼Œå³'seq.0'å’Œ'seq.2'ï¼Œè¦åœ¨æ²¡æœ‰ LoRA çš„æƒ…å†µä¸‹æ›´æ–°è¾“å‡ºå±‚ï¼Œå³'seq.4'
    from peft import LoraConfig
    config = LoraConfig(
        target_modules=["seq.0", "seq.2"],
        modules_to_save=["seq.4"],
    )

åˆ›å»º PEFT æ¨¡å‹å¹¶æ£€æŸ¥è®­ç»ƒå‚æ•°çš„æ¯”ä¾‹::

    from peft import get_peft_model

    model = MLP()
    peft_model = get_peft_model(model, config)
    peft_model.print_trainable_parameters()
    # prints trainable params: 56,164 || all params: 4,100,164 || trainable%: 1.369798866581922




Adapter injection
-----------------

* ä½¿ç”¨ PEFTï¼Œæ‚¨å¯ä»¥å°†å¯è®­ç»ƒé€‚é…å™¨æ³¨å…¥åˆ°ä»»ä½•torchæ¨¡å—ä¸­ï¼Œè¿™å…è®¸æ‚¨ä½¿ç”¨é€‚é…å™¨æ–¹æ³•ï¼Œè€Œæ— éœ€ä¾èµ– PEFT ä¸­çš„å»ºæ¨¡ç±»ã€‚ç›®å‰ï¼ŒPEFT æ”¯æŒå°†LoRA ã€ AdaLoRAå’ŒIA3æ³¨å…¥æ¨¡å‹ä¸­ï¼Œå› ä¸ºå¯¹äºè¿™äº›é€‚é…å™¨ï¼Œæ¨¡å‹çš„å°±åœ°ä¿®æ”¹è¶³ä»¥å¯¹å…¶è¿›è¡Œå¾®è°ƒã€‚
* ç¼ºç‚¹ï¼šä¸€æ˜¯éœ€è¦æ‰‹åŠ¨ç¼–å†™ Hugging Face ä¸­çš„from_pretrainedå’Œsave_pretrainedå®ç”¨å‡½æ•°æ¥ä¿å­˜å’ŒåŠ è½½é€‚é…å™¨ï¼›äºŒæ˜¯ä¸é€‚ç”¨äºPeftModelæä¾›çš„ä»»ä½•å®ç”¨æ–¹æ³•ï¼Œä¾‹å¦‚ç¦ç”¨å’Œåˆå¹¶é€‚é…å™¨
* ä¼˜ç‚¹ï¼šä¸€æ˜¯æ¨¡å‹å°±åœ°ä¿®æ”¹ï¼Œä¿ç•™æ‰€æœ‰åŸå§‹å±æ€§å’Œæ–¹æ³•ï¼›äºŒæ˜¯é€‚ç”¨äºä»»ä½•torchæ¨¡å—å’Œæ¨¡å¼


Creating a new PEFT model
^^^^^^^^^^^^^^^^^^^^^^^^^

* è¦æ‰§è¡Œé€‚é…å™¨æ³¨å…¥ï¼Œè¯·ä½¿ç”¨inject_adapter_in_model()æ–¹æ³•ã€‚æ­¤æ–¹æ³•é‡‡ç”¨ 3 ä¸ªå‚æ•°ï¼šPEFT é…ç½®ã€æ¨¡å‹å’Œå¯é€‰çš„é€‚é…å™¨åç§°ã€‚å¦‚æœæ‚¨ä½¿ç”¨ä¸åŒçš„é€‚é…å™¨åç§°å¤šæ¬¡è°ƒç”¨inject_adapter_in_model()ï¼Œæ‚¨è¿˜å¯ä»¥å°†å¤šä¸ªé€‚é…å™¨é™„åŠ åˆ°æ¨¡å‹ã€‚

å°† LoRA é€‚é…å™¨æ³¨å…¥åˆ°DummyModelæ¨¡å—çš„linearå­æ¨¡å—::

.. code-block:: python

    import torch
    from peft import inject_adapter_in_model, LoraConfig

    class DummyModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.embedding = torch.nn.Embedding(10, 10)
            self.linear = torch.nn.Linear(10, 10)
            self.lm_head = torch.nn.Linear(10, 10)

        def forward(self, input_ids):
            x = self.embedding(input_ids)
            x = self.linear(x)
            x = self.lm_head(x)
            return x

    lora_config = LoraConfig(
        lora_alpha=16,
        lora_dropout=0.1,
        r=64,
        bias="none",
        target_modules=["linear"],  # æ³¨å…¥äº†è¿™ä¸ªmodules
        )

    model = DummyModel()
    model = inject_adapter_in_model(lora_config, model)      # ä½¿ç”¨inject_adapter_in_modelæ³¨å…¥(ä¼šç”Ÿæˆéšæœºå‚æ•°çš„çŠ¶æ€å­—å…¸/æƒé‡)

    dummy_inputs = torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]])
    dummy_outputs = model(dummy_inputs)



æ‰“å°æ¨¡å‹ä»¥æŸ¥çœ‹é€‚é…å™¨æ˜¯å¦å·²æ­£ç¡®æ³¨å…¥::

    # æ³¨å…¥å‰
    DummyModel(
      (embedding): Embedding(10, 10)
      (linear): Linear(in_features=10, out_features=10, bias=True)
      (lm_head): Linear(in_features=10, out_features=10, bias=True)
    )
    # æ³¨å…¥å
    DummyModel(
      (embedding): Embedding(10, 10)
      (linear): Linear(
        in_features=10, out_features=10, bias=True
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.1, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=10, out_features=64, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=64, out_features=10, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
      (lm_head): Linear(in_features=10, out_features=10, bias=True)
    )




Saving the model
^^^^^^^^^^^^^^^^

ä½¿ç”¨ ``get_peft_model_state_dict`` è·å–modelé€‚é…å±‚çš„å…·ä½“æƒé‡::

    from peft import get_peft_model_state_dict
    peft_state_dict = get_peft_model_state_dict(model)
    print(peft_state_dict)

Loading the model
^^^^^^^^^^^^^^^^^

ä½¿ç”¨ ``set_peft_model_state_dict`` åŠ è½½é€‚é…å±‚çš„å…·ä½“æƒé‡::

    from peft import set_peft_model_state_dict

    model = DummyModel()
    model = inject_adapter_in_model(lora_config, model)             # ä¼šç”Ÿæˆéšæœºå‚æ•°çš„çŠ¶æ€å­—å…¸/æƒé‡
    outcome = set_peft_model_state_dict(model, peft_state_dict)     # æŒ‡å®šä½¿ç”¨ä¿å­˜çš„å‚æ•°çš„çŠ¶æ€å­—å…¸/æƒé‡
    # check that there were no wrong keys
    print(outcome.unexpected_keys)


æŸ¥çœ‹ç›¸å…³å‚æ•°-fromGPT
^^^^^^^^^^^^^^^^^^^^

æŸ¥çœ‹é€‚é…å™¨å‚æ•°çš„åˆå§‹çŠ¶æ€å’Œæ›´æ–°åçš„çŠ¶æ€::

    # æ£€æŸ¥é€‚é…å™¨å‚æ•°çš„çŠ¶æ€
    print("Before set_peft_model_state_dict:")
    for name, param in model.named_parameters():
        if 'lora' in name:  # å‡è®¾é€‚é…å™¨å±‚å‚æ•°åŒ…å« 'lora' å­—æ®µ
            print(f"{name} - Mean: {param.mean().item()}, Std: {param.std().item()}")


æ¯”è¾ƒæ¨¡å‹çš„è¾“å‡ºå˜åŒ–::

    # ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®æ¥æ¯”è¾ƒè¾“å‡º
    input_ids = torch.randint(0, 10, (1, 5))

    output_before = model(input_ids)
    print("Output before set_peft_model_state_dict:\n", output_before)

æ£€æŸ¥å‚æ•°æ˜¯å¦è¢«åŠ è½½ï¼ˆå“ˆå¸Œå€¼éªŒè¯ï¼‰::

    import hashlib

    def get_params_hash(model):
        params = torch.cat([p.flatten() for p in model.parameters() if 'lora' in p.name])
        return hashlib.md5(params.detach().cpu().numpy().tobytes()).hexdigest()

    # è®¡ç®—é€‚é…å™¨å‚æ•°çš„å“ˆå¸Œå€¼
    hash_before = get_params_hash(model)
    print(f"Hash before: {hash_before}")



Mixed adapter types
-------------------

* é€šå¸¸ï¼Œåœ¨ğŸ¤— PEFT ä¸­æ··åˆä¸åŒç±»å‹çš„é€‚é…å™¨æ˜¯ä¸å¯èƒ½çš„ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„ LoRA é€‚é…å™¨ï¼ˆå¯ä»¥æœ‰ä¸åŒçš„é…ç½®é€‰é¡¹ï¼‰åˆ›å»º PEFT æ¨¡å‹ï¼Œä½†æ— æ³•ç»„åˆ LoRA å’Œ LoHa é€‚é…å™¨ã€‚ç„¶è€Œï¼Œå¯¹äºPeftMixedModel ï¼Œåªè¦é€‚é…å™¨ç±»å‹å…¼å®¹ï¼Œè¿™å°±å¯ä»¥å·¥ä½œã€‚å…è®¸æ··åˆé€‚é…å™¨ç±»å‹çš„ä¸»è¦ç›®çš„æ˜¯ç»„åˆç»è¿‡è®­ç»ƒçš„é€‚é…å™¨è¿›è¡Œæ¨ç†ã€‚è™½ç„¶å¯ä»¥è®­ç»ƒæ··åˆé€‚é…å™¨æ¨¡å‹ï¼Œä½†å°šæœªç»è¿‡æµ‹è¯•ï¼Œå› æ­¤ä¸å»ºè®®è¿™æ ·åšã€‚

torch.compile
-------------

* åœ¨ PEFT ä¸­ï¼Œ torch.compileé€‚ç”¨äºæŸäº›åŠŸèƒ½ï¼Œä½†ä¸æ˜¯æ‰€æœ‰åŠŸèƒ½ã€‚
* å®ƒå¹¶ä¸æ€»æ˜¯æœ‰æ•ˆçš„åŸå› æ˜¯ PEFT åœ¨æŸäº›åœ°æ–¹æ˜¯é«˜åº¦åŠ¨æ€çš„ï¼ˆä¾‹å¦‚åœ¨å¤šä¸ªé€‚é…å™¨ä¹‹é—´åŠ è½½å’Œåˆ‡æ¢ï¼‰ï¼Œè¿™å¯èƒ½ä¼šç»™torch.compileå¸¦æ¥éº»çƒ¦ã€‚


PEFT checkpoint format
----------------------

PEFT files
^^^^^^^^^^

When you call ``save_pretrained()`` on a PEFT model, the PEFT model saves three files::

    1. adapter_model.safetensors or adapter_model.bin
        é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥safetensorsæ ¼å¼ä¿å­˜ï¼Œè¿™æ˜¯binæ ¼å¼çš„å®‰å…¨æ›¿ä»£æ–¹æ¡ˆ
        binæ ¼å¼å·²çŸ¥å®¹æ˜“å—åˆ°å®‰å…¨æ¼æ´çš„å½±å“ï¼Œå› ä¸ºå®ƒåœ¨åº•å±‚ä½¿ç”¨äº† pickle å®ç”¨ç¨‹åº
        ä¸¤ç§æ ¼å¼éƒ½å­˜å‚¨ç›¸åŒçš„state_dict ï¼Œå¹¶ä¸”å¯ä»¥äº’æ¢

    2. adapter_config.json
        åŒ…å«é€‚é…å™¨æ¨¡å—çš„é…ç½®ï¼Œè¿™æ˜¯åŠ è½½æ¨¡å‹æ‰€å¿…éœ€çš„
        åŒ…å«:
            å­˜å‚¨çš„é€‚é…å™¨æ¨¡å—ç±»å‹ï¼Œ "peft_type": "IA3"
            åŸºç¡€æ¨¡å‹çš„ä¿¡æ¯ï¼Œå¦‚ "base_model_name_or_path": "bert-base-uncased"
            æ¨¡å‹çš„ä¿®è®¢ç‰ˆï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œ "revision": null

    3. README.md

IAÂ³ é€‚é…å™¨çš„ ``adapter_config.json`` ç¤ºä¾‹::

    {
      "auto_mapping": {
        "base_model_class": "BertModel",
        "parent_library": "transformers.models.bert.modeling_bert"
      },
      "base_model_name_or_path": "bert-base-uncased",
      "fan_in_fan_out": false,
      "feedforward_modules": [
        "output.dense"
      ],
      "inference_mode": true,
      "init_ia3_weights": true,
      "modules_to_save": null,
      "peft_type": "IA3",
      "revision": null,
      "target_modules": [
        "key",
        "value",
        "output.dense"
      ],
      "task_type": null
    }


Convert to PEFT format
^^^^^^^^^^^^^^^^^^^^^^

adapter_model::

    1. é»˜è®¤æƒ…å†µï¼Œå¯¹äº BERT æ¨¡å‹ï¼ŒLoRA åº”ç”¨äºæ³¨æ„åŠ›æ¨¡å—çš„queryå±‚å’Œvalueå±‚
        è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ‚¨ä¼šåœ¨æ¯å±‚çš„é”®åç§°ä¸­çœ‹åˆ°attention.self.queryå’Œattention.self.value
    2. LoRA å°†æƒé‡åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µï¼š lora_Aå’Œlora_B
    3. LoRA çŸ©é˜µè¢«å®ç°ä¸ºnn.Linearå±‚ï¼Œå› æ­¤å‚æ•°å­˜å‚¨åœ¨.weightå±æ€§ä¸­
    4. é»˜è®¤æƒ…å†µä¸‹ï¼ŒLoRA ä¸åº”ç”¨äº BERT çš„åµŒå…¥å±‚ï¼Œå› æ­¤æ²¡æœ‰lora_A_embeddingå’Œlora_B_embeddingçš„æ¡ç›®


``adapter_config.json`` è‡³å°‘åº”åŒ…å«ä»¥ä¸‹æ¡ç›®::

    {
      "target_modules": ["query", "value"],
      "peft_type": "LORA"
    }




Accelerate integrations
=======================


DeepSpeed
---------

* DeepSpeedæ˜¯ä¸€ä¸ªä¸“ä¸ºå…·æœ‰æ•°åäº¿å‚æ•°çš„å¤§å‹æ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒçš„é€Ÿåº¦å’Œè§„æ¨¡è€Œè®¾è®¡çš„åº“ã€‚
* å…¶æ ¸å¿ƒæ˜¯é›¶å†—ä½™ä¼˜åŒ–å™¨ (ZeRO)ï¼Œå®ƒå°†ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-1)ã€æ¢¯åº¦ (ZeRO-2) å’Œå‚æ•° (ZeRO-3) è·¨æ•°æ®å¹¶è¡Œè¿›ç¨‹è¿›è¡Œåˆ†ç‰‡ã€‚
* è¿™å¤§å¤§å‡å°‘äº†å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿æ‚¨èƒ½å¤Ÿå°†è®­ç»ƒæ‰©å±•åˆ°åäº¿ä¸ªå‚æ•°æ¨¡å‹ã€‚ä¸ºäº†é‡Šæ”¾æ›´é«˜çš„å†…å­˜æ•ˆç‡ï¼ŒZeRO-Offload åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­åˆ©ç”¨ CPU èµ„æºæ¥å‡å°‘ GPU è®¡ç®—å’Œå†…å­˜ã€‚


Fully Sharded Data Parallel
---------------------------

* å…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ(FSDP) ä¸“ä¸ºé«˜è¾¾ 1T å‚æ•°çš„å¤§å‹é¢„è®­ç»ƒæ¨¡å‹çš„åˆ†å¸ƒå¼è®­ç»ƒè€Œå¼€å‘ã€‚
* FSDP é€šè¿‡è·¨æ•°æ®å¹¶è¡Œè¿›ç¨‹å¯¹æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€è¿›è¡Œåˆ†ç‰‡æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å®ƒè¿˜å¯ä»¥å°†åˆ†ç‰‡çš„æ¨¡å‹å‚æ•°å¸è½½åˆ° CPUã€‚
* FSDP æä¾›çš„å†…å­˜æ•ˆç‡å…è®¸æ‚¨å°†è®­ç»ƒæ‰©å±•åˆ°æ›´å¤§çš„æ‰¹æ¬¡æˆ–æ¨¡å‹å¤§å°ã€‚






Conceptual guides
=================

Adapters
--------

* åŸºäºé€‚é…å™¨çš„æ–¹æ³•åœ¨å†»ç»“é¢„è®­ç»ƒæ¨¡å‹çš„æ³¨æ„åŠ›å±‚(attention)å’Œå…¨è¿æ¥å±‚(fully-connected)ä¹‹åæ·»åŠ é¢å¤–çš„å¯è®­ç»ƒå‚æ•°ï¼Œä»¥å‡å°‘å†…å­˜ä½¿ç”¨å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚
* è¯¥æ–¹æ³•å› é€‚é…å™¨è€Œå¼‚ï¼Œå®ƒå¯èƒ½åªæ˜¯ä¸€ä¸ªé¢å¤–çš„é™„åŠ å±‚ï¼Œä¹Ÿå¯èƒ½å°†æƒé‡æ›´æ–° Î”W è¡¨ç¤ºä¸ºæƒé‡çŸ©é˜µçš„ä½ç§©åˆ†è§£ã€‚
* æ— è®ºå“ªç§æ–¹å¼ï¼Œé€‚é…å™¨é€šå¸¸éƒ½å¾ˆå°ï¼Œä½†è¡¨ç°å‡ºä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹ç›¸å½“çš„æ€§èƒ½ï¼Œå¹¶ä¸”èƒ½å¤Ÿç”¨æ›´å°‘çš„èµ„æºè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚


Low-Rank Adaptation (LoRA)
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note:: LoRA æ˜¯æœ€æµè¡Œçš„ PEFT æ–¹æ³•ä¹‹ä¸€ï¼Œå¦‚æœæ‚¨åˆšåˆšå¼€å§‹ä½¿ç”¨ PEFTï¼Œé‚£ä¹ˆè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚å®ƒæœ€åˆæ˜¯ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹å¼€å‘çš„ï¼Œä½†ç”±äºå…¶æ•ˆç‡å’Œæœ‰æ•ˆæ€§ï¼Œå®ƒæ˜¯æ‰©æ•£æ¨¡å‹çš„ä¸€ç§éå¸¸æµè¡Œçš„è®­ç»ƒæ–¹æ³•ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2024/11/lora_animated.gif

    LoRA é€šè¿‡ä½ç§©åˆ†è§£ç”¨ä¸¤ä¸ªè¾ƒå°çš„çŸ©é˜µï¼ˆç§°ä¸ºæ›´æ–°çŸ©é˜µï¼‰è¡¨ç¤ºæƒé‡æ›´æ–° Î”Wã€‚å¯ä»¥è®­ç»ƒè¿™äº›æ–°çŸ©é˜µä»¥é€‚åº”æ–°æ•°æ®ï¼ŒåŒæ—¶ä¿æŒè¾ƒä½çš„å‚æ•°æ€»æ•°ã€‚åŸå§‹æƒé‡çŸ©é˜µä¿æŒå†»ç»“çŠ¶æ€ï¼Œä¸ä¼šæ”¶åˆ°ä»»ä½•è¿›ä¸€æ­¥çš„æ›´æ–°ã€‚ä¸ºäº†äº§ç”Ÿæœ€ç»ˆç»“æœï¼Œå°†åŸå§‹æƒé‡å’Œé¢å¤–è°ƒæ•´çš„æƒé‡ç›¸ç»“åˆã€‚æ‚¨è¿˜å¯ä»¥å°†é€‚é…å™¨æƒé‡ä¸åŸºæœ¬æ¨¡å‹åˆå¹¶ï¼Œä»¥æ¶ˆé™¤æ¨ç†å»¶è¿Ÿã€‚

ä¼˜ç‚¹::

    1. LoRA é€šè¿‡å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡æ¥æé«˜å¾®è°ƒçš„æ•ˆç‡ã€‚
    2. åŸå§‹é¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥æ‹¥æœ‰å¤šä¸ªè½»é‡çº§ä¾¿æºå¼ LoRA æ¨¡å‹ï¼Œç”¨äºåœ¨å…¶ä¹‹ä¸Šæ„å»ºçš„å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚
    3. LoRA ä¸å…¶ä»–å‚æ•°æœ‰æ•ˆçš„æ–¹æ³•æ­£äº¤ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä¸­è®¸å¤šæ–¹æ³•ç»„åˆã€‚
    4. ä½¿ç”¨ LoRA å¾®è°ƒçš„æ¨¡å‹çš„æ€§èƒ½ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚


Mixture of LoRA Experts (X-LoRA)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://arxiv.org/abs/2402.07148
* X-LoRAæ˜¯ LoRA çš„æ··åˆä¸“å®¶æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä½¿ç”¨å¯†é›†æˆ–ç¨€ç–é—¨æ§æ¥åŠ¨æ€æ¿€æ´» LoRA ä¸“å®¶ã€‚
* LoRA ä¸“å®¶ä»¥åŠåŸºç¡€æ¨¡å‹åœ¨è®­ç»ƒæœŸé—´è¢«å†»ç»“ï¼Œå¯¼è‡´å‚æ•°æ•°é‡è¾ƒä½ï¼Œå› ä¸ºåªæœ‰é—¨æ§å±‚å¿…é¡»è¿›è¡Œè®­ç»ƒã€‚
* ç‰¹åˆ«æ˜¯ï¼Œé—¨æ§å±‚è¾“å‡ºç¼©æ”¾ï¼ˆå–å†³äºé…ç½®ï¼‰åœ¨å±‚å’Œä»¤ç‰Œçº§åˆ«ä¸Šæ˜¯ç»†ç²’åº¦çš„ã€‚æ­¤å¤–ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒX-LoRA åŠ¨æ€æ¿€æ´» LoRA é€‚é…å™¨æ¥å›å¿†çŸ¥è¯†å¹¶æœ‰æ•ˆåœ°æ··åˆå®ƒä»¬

* å¯¹äºæ¯ä¸ªæ­¥éª¤ï¼ŒX-LoRA è¦æ±‚åŸºæœ¬æ¨¡å‹è¿è¡Œä¸¤æ¬¡::

    1. é¦–å…ˆï¼Œåœ¨æ²¡æœ‰ä»»ä½• LoRA é€‚é…å™¨çš„æƒ…å†µä¸‹è·å–hidden states
    2. å…¶æ¬¡ï¼Œç”¨hidden statesè®¡ç®—scalings(åº”ç”¨äº LoRA é€‚é…å™¨)å¹¶ç¬¬äºŒæ¬¡è¿è¡Œæ¨¡å‹ã€‚
    ç¬¬äºŒæ¬¡è¿è¡Œçš„è¾“å‡ºæ˜¯æ¨¡å‹æ­¥éª¤çš„ç»“æœã€‚

Low-Rank Hadamard Product (LoHa)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://huggingface.co/papers/2108.06098
* ä½ç§©åˆ†è§£ä¼šå½±å“æ€§èƒ½ï¼Œå› ä¸ºæƒé‡æ›´æ–°ä»…é™äºä½ç§©ç©ºé—´ï¼Œè¿™ä¼šé™åˆ¶æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
* ä½†æ˜¯ï¼Œæ‚¨ä¸ä¸€å®šè¦ä½¿ç”¨æ›´å¤§çš„Rank(ç§©)ï¼Œå› ä¸ºå®ƒä¼šå¢åŠ å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚
* ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œ LoHa ï¼ˆä¸€ç§æœ€åˆä¸ºè®¡ç®—æœºè§†è§‰å¼€å‘çš„æ–¹æ³•ï¼‰è¢«åº”ç”¨äºæ‰©æ•£æ¨¡å‹(diffusion models)ï¼Œå…¶ä¸­ç”Ÿæˆä¸åŒå›¾åƒçš„èƒ½åŠ›æ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ã€‚
* LoHa è¿˜åº”è¯¥é€‚ç”¨äºä¸€èˆ¬æ¨¡å‹ç±»å‹ï¼Œä½†åµŒå…¥å±‚ç›®å‰å°šæœªåœ¨ PEFT ä¸­å®ç°ã€‚

* LoHa ä½¿ç”¨Hadamard ä¹˜ç§¯ï¼ˆé€å…ƒç´ ä¹˜ç§¯ï¼‰è€Œä¸æ˜¯çŸ©é˜µä¹˜ç§¯ã€‚ Î”W ç”±å››ä¸ªè¾ƒå°çš„çŸ©é˜µè¡¨ç¤ºï¼Œè€Œä¸æ˜¯ LoRA ä¸­çš„ä¸¤ä¸ªçŸ©é˜µï¼Œå¹¶ä¸”æ¯å¯¹è¿™äº›ä½ç§©çŸ©é˜µéƒ½ä¸ Hadamard ä¹˜ç§¯ç›¸ç»“åˆã€‚å› æ­¤ï¼ŒÎ”W å¯ä»¥å…·æœ‰ç›¸åŒæ•°é‡çš„å¯è®­ç»ƒå‚æ•°ï¼Œä½†å…·æœ‰æ›´é«˜çš„ç§©å’Œè¡¨è¾¾èƒ½åŠ›ã€‚


Low-Rank Kronecker Product (LoKr)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://hf.co/papers/2309.14859
* LoKrä¸ LoRA å’Œ LoHa éå¸¸ç›¸ä¼¼ï¼Œå®ƒä¹Ÿä¸»è¦åº”ç”¨äºæ‰©æ•£æ¨¡å‹(diffusion models)ï¼Œå°½ç®¡æ‚¨ä¹Ÿå¯ä»¥å°†å®ƒä¸å…¶ä»–æ¨¡å‹ç±»å‹ä¸€èµ·ä½¿ç”¨ã€‚ 
* LoKr å°†çŸ©é˜µä¹˜ç§¯æ›¿æ¢ä¸ºå…‹ç½—å†…å…‹ä¹˜ç§¯ã€‚
* å…‹ç½—å†…å…‹ä¹˜ç§¯åˆ†è§£åˆ›å»ºä¸€ä¸ªå—çŸ©é˜µï¼Œè¯¥çŸ©é˜µä¿ç•™åŸå§‹æƒé‡çŸ©é˜µçš„ç§©ã€‚å…‹ç½—å†…å…‹ä¹˜ç§¯çš„å¦ä¸€ä¸ªå¥½å¤„æ˜¯å®ƒå¯ä»¥é€šè¿‡å †å çŸ©é˜µåˆ—æ¥çŸ¢é‡åŒ–ã€‚è¿™å¯ä»¥åŠ å¿«è¯¥è¿‡ç¨‹ï¼Œå› ä¸ºæ‚¨å¯ä»¥é¿å…å®Œå…¨é‡å»º Î”Wã€‚


Orthogonal Finetuning (OFT)
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://hf.co/papers/2306.07280
* OFTæ˜¯ä¸€ç§ä¸»è¦å…³æ³¨åœ¨å¾®è°ƒæ¨¡å‹ä¸­ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹çš„ç”Ÿæˆæ€§èƒ½(generative performance)çš„æ–¹æ³•ã€‚
* å®ƒè¯•å›¾åœ¨ä¸€å±‚ä¸­æ‰€æœ‰æˆå¯¹ç¥ç»å…ƒä¹‹é—´ä¿æŒç›¸åŒçš„ä½™å¼¦ç›¸ä¼¼æ€§ï¼ˆè¶…çƒé¢èƒ½é‡ï¼‰ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥æ›´å¥½åœ°æ•è·ç¥ç»å…ƒä¹‹é—´çš„è¯­ä¹‰ä¿¡æ¯ã€‚
* è¿™æ„å‘³ç€OFTæ›´èƒ½ä¿å­˜ä¸»é¢˜ï¼Œå¹¶ä¸”æ›´é€‚åˆå¯æ§ç”Ÿæˆï¼ˆç±»ä¼¼äºControlNet ï¼‰ã€‚


Orthogonal Butterfly (BOFT)
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://hf.co/papers/2311.06243
* åº”è¯¥ç±»ä¼¼ OFT


Adaptive Low-Rank Adaptation (AdaLoRA)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* https://hf.co/papers/2303.10512
* ``AdaLoRA`` é€šè¿‡ä¸ºæ›´é€‚åˆä»»åŠ¡çš„é‡è¦æƒé‡çŸ©é˜µåˆ†é…æ›´å¤šå‚æ•°ï¼ˆå³æ›´é«˜çš„ç§© rï¼‰å¹¶ä¿®å‰ªä¸å¤ªé‡è¦çš„æƒé‡çŸ©é˜µæ¥ç®¡ç†ä» LoRA å¼•å…¥çš„å‚æ•°é¢„ç®—ã€‚
* ç§©ç”±ç±»ä¼¼äºå¥‡å¼‚å€¼åˆ†è§£ (SVD, singular value decomposition) çš„æ–¹æ³•æ§åˆ¶ã€‚
* âˆ†W é€šè¿‡ä¸¤ä¸ªæ­£äº¤çŸ©é˜µå’Œä¸€ä¸ªåŒ…å«å¥‡å¼‚å€¼çš„å¯¹è§’çŸ©é˜µè¿›è¡Œå‚æ•°åŒ–ã€‚
* è¿™ç§å‚æ•°åŒ–æ–¹æ³•é¿å…äº†è¿­ä»£åº”ç”¨è®¡ç®—æˆæœ¬é«˜æ˜‚çš„ SVDã€‚
* åŸºäºæ­¤æ–¹æ³•ï¼Œâˆ†W çš„ç§©æ ¹æ®é‡è¦æ€§åˆ†æ•°è¿›è¡Œè°ƒæ•´ã€‚
* âˆ†W è¢«åˆ†ä¸ºä¸‰å…ƒç»„ï¼Œæ¯ä¸ªä¸‰å…ƒç»„æ ¹æ®å…¶å¯¹æ¨¡å‹æ€§èƒ½çš„è´¡çŒ®è¿›è¡Œè¯„åˆ†ã€‚é‡è¦æ€§åˆ†æ•°ä½çš„ä¸‰å…ƒç»„è¢«ä¿®å‰ªï¼Œé‡è¦æ€§åˆ†æ•°é«˜çš„ä¸‰å…ƒç»„ä¿ç•™ä»¥è¿›è¡Œå¾®è°ƒã€‚


Llama-Adapter
^^^^^^^^^^^^^

* https://hf.co/papers/2303.16199
* Llama-Adapter æ˜¯ä¸€ç§å°† Llama æ¨¡å‹è°ƒæ•´ä¸ºèƒ½å¤Ÿç†è§£å’Œæ‰§è¡ŒæŒ‡ä»¤ï¼ˆinstruction-followingï¼‰çš„æ–¹æ³•ã€‚
* ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿé€‚åº”è¿™ç§â€œæŒ‡ä»¤è·Ÿéšâ€çš„ä»»åŠ¡ï¼Œç ”ç©¶äººå‘˜å¯¹ Llama-Adapter è¿›è¡Œäº†è®­ç»ƒï¼Œä½¿ç”¨çš„æ•°æ®é›†åŒ…å« 52,000 æ¡æŒ‡ä»¤åŠå…¶ç›¸åº”çš„è¾“å‡ºã€‚


Soft prompts
------------

* å‚è§å‰ç‰ˆæœ¬çš„ Conceptual guides -> Prompting


Multitask prompt tuning
^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/11/n6SoOG.png

* Multitask prompt tuning (MPT) ä»å¤šä¸ªä»»åŠ¡ç±»å‹çš„æ•°æ®ä¸­å­¦ä¹ å•ä¸ªæç¤ºï¼Œè¿™äº›ä»»åŠ¡ç±»å‹å¯ä»¥ä¸ºä¸åŒçš„ç›®æ ‡ä»»åŠ¡å…±äº«ã€‚å…¶ä»–ç°æœ‰æ–¹æ³•ä¸ºæ¯ä¸ªä»»åŠ¡å­¦ä¹ ä¸€ä¸ªå•ç‹¬çš„è½¯æç¤ºï¼Œéœ€è¦æ£€ç´¢æˆ–èšåˆè¿™äº›æç¤ºä»¥é€‚åº”ç›®æ ‡ä»»åŠ¡ã€‚
* MPTç”±ä¸¤ä¸ªé˜¶æ®µç»„æˆ::

    1. source training
        å¯¹äºæ¯ä¸ªä»»åŠ¡ï¼Œå…¶soft promptè¢«åˆ†è§£ä¸ºç‰¹å®šäºä»»åŠ¡çš„å‘é‡ã€‚
        å°†ç‰¹å®šäºä»»åŠ¡çš„å‘é‡ç›¸ä¹˜ä»¥å½¢æˆå¦ä¸€ä¸ªçŸ©é˜µWï¼Œå¹¶ä¸”åœ¨Wå’Œå…±äº«æç¤ºçŸ©é˜µPä¹‹é—´ä½¿ç”¨Hadamardä¹˜ç§¯æ¥ç”Ÿæˆç‰¹å®šäºä»»åŠ¡çš„æç¤ºçŸ©é˜µã€‚
        ç‰¹å®šäºä»»åŠ¡çš„æç¤ºè¢«æç‚¼æˆåœ¨æ‰€æœ‰ä»»åŠ¡ä¹‹é—´å…±äº«çš„å•ä¸ªæç¤ºçŸ©é˜µã€‚è¯¥æç¤ºæ˜¯é€šè¿‡å¤šä»»åŠ¡è®­ç»ƒè¿›è¡Œè®­ç»ƒçš„ã€‚
    2. target adaptation
        ä¸ºäº†é€‚åº”ç›®æ ‡ä»»åŠ¡çš„å•ä¸ªæç¤ºï¼Œç›®æ ‡æç¤ºè¢«åˆå§‹åŒ–å¹¶è¡¨ç¤ºä¸ºå…±äº«æç¤ºçŸ©é˜µå’Œç‰¹å®šäºä»»åŠ¡çš„ä½ç§©æç¤ºçŸ©é˜µçš„Hadamardä¹˜ç§¯ã€‚






IA3
---

* ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œæ—¨åœ¨æ”¹è¿›LoRA ã€‚
* ä¸ºäº†ä½¿å¾®è°ƒæ›´åŠ æœ‰æ•ˆï¼ŒIA3ï¼ˆé€šè¿‡æŠ‘åˆ¶å’Œæ”¾å¤§å†…éƒ¨æ¿€æ´»çš„æ³¨å…¥å¼é€‚é…å™¨Infused Adapter by Inhibiting and Amplifying Inner Activationsï¼‰ä½¿ç”¨å­¦ä¹ å‘é‡(learned vectors)é‡æ–°è°ƒæ•´å†…éƒ¨æ¿€æ´»ã€‚
* è¿™äº›å­¦ä¹ åˆ°çš„å‘é‡è¢«æ³¨å…¥åˆ°å…¸å‹çš„åŸºäºå˜å‹å™¨çš„æ¶æ„ä¸­çš„æ³¨æ„åŠ›å’Œå‰é¦ˆæ¨¡å—(attention and feedforward modules)ä¸­ã€‚
* è¿™äº›å­¦ä¹ åˆ°çš„å‘é‡æ˜¯å¾®è°ƒæœŸé—´å”¯ä¸€å¯è®­ç»ƒçš„å‚æ•°ï¼Œå› æ­¤åŸå§‹æƒé‡ä¿æŒå†»ç»“ã€‚
* å¤„ç†å­¦ä¹ å‘é‡ï¼ˆä¸å­¦ä¹  LoRA ç­‰æƒé‡çŸ©é˜µçš„ä½ç§©æ›´æ–°ç›¸åï¼‰å¯ä»¥ä½¿å¯è®­ç»ƒå‚æ•°çš„æ•°é‡å°‘å¾—å¤šã€‚


ä¼˜ç‚¹
^^^^

* IA3 é€šè¿‡å¤§å¹…å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä½¿å¾®è°ƒæ›´åŠ é«˜æ•ˆã€‚ ï¼ˆå¯¹äº T0ï¼ŒIA3 æ¨¡å‹åªæœ‰å¤§çº¦ 0.01% å¯è®­ç»ƒå‚æ•°ï¼Œè€Œå³ä½¿ LoRA ä¹Ÿæœ‰ > 0.1%ï¼‰
* åŸå§‹çš„é¢„è®­ç»ƒæƒé‡ä¿æŒå†»ç»“çŠ¶æ€ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥æ‹¥æœ‰å¤šä¸ªè½»é‡çº§ä¾¿æºå¼ IA3 æ¨¡å‹ï¼Œç”¨äºåœ¨å…¶ä¹‹ä¸Šæ„å»ºçš„å„ç§ä¸‹æ¸¸ä»»åŠ¡ã€‚
* ä½¿ç”¨ IA3 å¾®è°ƒçš„æ¨¡å‹çš„æ€§èƒ½ä¸å®Œå…¨å¾®è°ƒçš„æ¨¡å‹çš„æ€§èƒ½ç›¸å½“ã€‚
* IA3 ä¸ä¼šå¢åŠ ä»»ä½•æ¨ç†å»¶è¿Ÿï¼Œå› ä¸ºé€‚é…å™¨æƒé‡å¯ä»¥ä¸åŸºæœ¬æ¨¡å‹åˆå¹¶ã€‚


.. note:: ä¸Šè¿°ä¼˜ç‚¹ä¸ LoRA ç±»ä¼¼

* åŸåˆ™ä¸Šï¼ŒIA3 å¯ä»¥åº”ç”¨äºç¥ç»ç½‘ç»œä¸­æƒé‡çŸ©é˜µçš„ä»»ä½•å­é›†ï¼Œä»¥å‡å°‘å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æ ¹æ®ä½œè€…çš„å®ç°ï¼ŒIA3 æƒé‡è¢«æ·»åŠ åˆ° Transformer æ¨¡å‹çš„é”®ã€å€¼å’Œå‰é¦ˆå±‚(key, value and feedforward layers)ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹äº Transformer æ¨¡å‹ï¼ŒIA3 æƒé‡è¢«æ·»åŠ åˆ°é”®å±‚å’Œå€¼å±‚(key and value layers)çš„è¾“å‡ºä»¥åŠæ¯ä¸ª Transformer å—ä¸­ç¬¬äºŒå‰é¦ˆå±‚çš„è¾“å…¥ã€‚
* ç»™å®šæ³¨å…¥ IA3 å‚æ•°çš„ç›®æ ‡å±‚ï¼Œå¯è®­ç»ƒå‚æ•°çš„æ•°é‡å¯ä»¥æ ¹æ®æƒé‡çŸ©é˜µçš„å¤§å°ç¡®å®šã€‚



Common IA3 parameters in PEFT
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    target_modules
        åº”ç”¨ IA3 å‘é‡çš„æ¨¡å—ï¼ˆä¾‹å¦‚ï¼Œæ³¨æ„åŠ›å—, attention blocksï¼‰
    feedforward_modules
        åœ¨target_modulesä¸­è¢«è§†ä¸ºå‰é¦ˆå±‚çš„æ¨¡å—åˆ—è¡¨
        è™½ç„¶å­¦ä¹ å‘é‡(learned vectors)ä¸æ³¨æ„åŠ›å—çš„è¾“å‡ºæ¿€æ´»(output activation for attention blocks)ç›¸ä¹˜ï¼Œä½†å‘é‡ä¸ç»å…¸å‰é¦ˆå±‚çš„è¾“å…¥ç›¸ä¹˜
        è¯·æ³¨æ„ï¼Œ feedforward_moduleså¿…é¡»æ˜¯target_modulesçš„å­é›†
    modules_to_save
        é™¤äº† IA3 å±‚ä¹‹å¤–çš„æ¨¡å—åˆ—è¡¨ï¼Œè¦è®¾ç½®ä¸ºå¯è®­ç»ƒå¹¶ä¿å­˜åœ¨æœ€ç»ˆæ£€æŸ¥ç‚¹ä¸­ã€‚è¿™äº›é€šå¸¸åŒ…æ‹¬æ¨¡å‹çš„è‡ªå®šä¹‰å¤´ï¼Œè¯¥å¤´æ˜¯ä¸ºå¾®è°ƒä»»åŠ¡éšæœºåˆå§‹åŒ–çš„











å‚è€ƒ
====




* å®è·µ: notebook collection: https://huggingface.co/spaces/PEFT/causal-language-modeling


















