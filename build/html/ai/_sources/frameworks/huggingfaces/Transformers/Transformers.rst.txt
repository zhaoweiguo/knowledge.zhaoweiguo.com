Transformers
############

.. note:: æœ¬æ–‡æ¡£æ˜¯å‚è€ƒè‡ª ``v4.34.1`` ç‰ˆæœ¬

* from: https://huggingface.co/docs/transformers/v4.23.1/en/index


ç®€ä»‹
====


1. GET STARTED::

    provides a quick tour of the library and installation instructions to get up and running.

2. TUTORIALS::

    are a great place to start if youâ€™re a beginner.
    This section will help you gain the basic skills you need to start using the library.

3. HOW-TO GUIDES::

    show you how to achieve a specific goal, 
    like finetuning a pretrained model for language modeling 
    or how to write and share a custom model.

4. CONCEPTUAL GUIDES::

    offers more discussion and explanation of the underlying concepts 
        and ideas behind models, tasks, and the design philosophy of ğŸ¤— Transformers.

5. API: describes all classes and functions::

    MAIN CLASSES
        details the most important classes like configuration, model, tokenizer, and pipeline.
    MODELS
        details the classes and functions related to each model implemented in the library.
    INTERNAL
        HELPERS details utility classes and functions used internally.


GET STARTED
===========

Quick tour
----------

å®‰è£…::

    pip install transformers datasets
    # optional
    sentiment
    pip install tensorflow


Pipeline
^^^^^^^^

Task List::

    +------------------------------+-----------------+-----------------------------------------------+
    | Task                         | Modality        | Pipeline identifier                           |
    +==============================+=================+===============================================+
    | Text classification          | NLP             | pipeline(task="sentiment-analysis")           |
    +------------------------------+-----------------+-----------------------------------------------+
    | Text generation              | NLP             | pipeline(task="text-generation")              |
    +------------------------------+-----------------+-----------------------------------------------+
    | Summarization                | NLP             | pipeline(task="summarization")                |
    +------------------------------+-----------------+-----------------------------------------------+
    | Image classification         | Computer vision | pipeline(task="image-classification")         |
    +------------------------------+-----------------+-----------------------------------------------+
    | Image segmentation           | Computer vision | pipeline(task="image-segmentation")           |
    +------------------------------+-----------------+-----------------------------------------------+
    | Object detection             | Computer vision | pipeline(task="object-detection")             |
    +------------------------------+-----------------+-----------------------------------------------+
    | Audio classification         | Audio           | pipeline(task="audio-classification")         |
    +------------------------------+-----------------+-----------------------------------------------+
    | Automatic speech recognition | Audio           | pipeline(task="automatic-speech-recognition") |
    +------------------------------+-----------------+-----------------------------------------------+
    | Visual question answering    | Multimodal      | pipeline(task="vqa")                          |
    +------------------------------+-----------------+-----------------------------------------------+
    | Document question answering  | Multimodal      | pipeline(task="document-question-answering")  |
    +------------------------------+-----------------+-----------------------------------------------+
    | Image captioning             | Multimodal      | pipeline(task="image-to-text")                |
    +------------------------------+-----------------+-----------------------------------------------+

::

    >>> from transformers import pipeline

    >>> classifier = pipeline("sentiment-analysis")
    >>> classifier("We are very happy to show you the ğŸ¤— Transformers library.")
    [{'label': 'POSITIVE', 'score': 0.9998}]


Example: iterate over an entire dataset of automatic speech::

    import torch
    from transformers import pipeline

    # è¯­éŸ³è¯†åˆ«pipeline(speech_recognizer)
    sr = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")

    # è½½å…¥æ•°æ®
    from datasets import load_dataset, Audio
    dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")

    # ç¡®ä¿ç›¸åŒçš„ sampling rate
    dataset = dataset.cast_column("audio", Audio(sampling_rate=sr.feature_extractor.sampling_rate))

    # æ‰§è¡Œtask
    result = sr(dataset[:4]["audio"])
    print([d["text"] for d in result])


Example: Use another model and tokenizer in the pipeline::

    model_name = "nlptown/bert-base-multilingual-uncased-sentiment"

    from transformers import AutoTokenizer, AutoModelForSequenceClassification

    model = AutoModelForSequenceClassification.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # æŒ‡å®šmodelå’Œtokenizer
    classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

    # æ‰§è¡Œ
    classifier("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.")



AutoClass
^^^^^^^^^

AutoTokenizer::

    from transformers import AutoTokenizer

    model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # Pass your text to the tokenizer:
    encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
    print(encoding)
    # {
    #         'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 
    #        'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 
    #        'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
    # }

    # accept a list of inputs, and pad and truncate the text to return a batch with uniform length
    pt_batch = tokenizer(
        ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt",
    )

AutoModel::

    # For text (or sequence) classification, you should load `AutoModelForSequenceClassification`
    from transformers import AutoModelForSequenceClassification

    model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
    pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)

    # pass your preprocessed batch of inputs directly to the model
    pt_outputs = pt_model(**pt_batch)

    from torch import nn

    # outputs the final activations in the logits attribute
    >>> pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
    >>> print(pt_predictions)
    tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
            [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)


Save a model::

    pt_save_directory = "./pt_save_pretrained"
    tokenizer.save_pretrained(pt_save_directory)
    pt_model.save_pretrained(pt_save_directory)

    # load
    pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")


Custom model builds
^^^^^^^^^^^^^^^^^^^

::

    # ä½¿ç”¨AutoConfigåŠ è½½è¦ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆè‡ªå®šä¹‰é…ç½®
    from transformers import AutoConfig
    my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)

    # ä½¿ç”¨AutoModelåŸºäºè‡ªå®šä¹‰é…ç½®åˆ›å»ºæ¨¡å‹
    from transformers import AutoModel
    my_model = AutoModel.from_config(my_config)

Trainer
^^^^^^^

1. A PreTrainedModel or a torch.nn.Module::

    from transformers import AutoModelForSequenceClassification
    model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

2. TrainingArguments åŒ…å«å¯ä»¥æ›´æ”¹çš„æ¨¡å‹è¶…å‚æ•°ï¼Œä¾‹å¦‚å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°å’Œè¦è®­ç»ƒçš„å‘¨æœŸæ•°::

    from transformers import TrainingArguments

    training_args = TrainingArguments(
        output_dir="path/to/save/folder/",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        num_train_epochs=2,
    )

3. é¢„å¤„ç†ç±»ï¼Œå¦‚tokenizer, image processor, feature extractor, or processor::

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

4. åŠ è½½æ•°æ®é›†::

    from datasets import load_dataset
    dataset = load_dataset("rotten_tomatoes")  # doctest: +IGNORE_RESULT

5. ä½¿ç”¨mapåº”ç”¨æ•´ä¸ªæ•°æ®é›†::

    def tokenize_dataset(dataset):
        return tokenizer(dataset["text"])

    dataset = dataset.map(tokenize_dataset, batched=True)

6. ä½¿ç”¨DataCollatorWithPaddingä»æ•°æ®é›†åˆ›å»ºä¸€æ‰¹ç¤ºä¾‹::

    from transformers import DataCollatorWithPadding
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

ä½¿ç”¨Trainer::

    from transformers import Trainer

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        tokenizer=tokenizer,
        data_collator=data_collator,
    )  # doctest: +SKIP

å¼€å§‹è®­ç»ƒ::

    trainer.train()




Installation
------------

* default install::

    pip install transformers

    # éªŒè¯
    python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"

* cpuç‰ˆå®‰è£…(install Transformers and a deep learning library in one line)::

    pip install 'transformers[torch]'         # å®‰è£… ğŸ¤— Transformers å’Œ PyTorch

    pip install 'transformers[tf-cpu]'        # å®‰è£… ğŸ¤— Transformers å’Œ TensorFlow 2.0


* æºç å®‰è£…::

    pip install git+https://github.com/huggingface/transformers


    git clone https://github.com/huggingface/transformers.git
    cd transformers
    pip install -e .


æ£€æŸ¥æ˜¯å¦å®‰è£…æ­£ç¡®::

    python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"

    # æŸ¥çœ‹ç‰ˆæœ¬
    print(transformers.__version__)

é™„åŠ æ¨¡å—::

    pip install 'transformers[audio]'
    pip install 'transformers[torch]'
    pip install 'transformers[tf-cpu]'






ç¯å¢ƒå˜é‡
--------

ä½¿ç”¨condaä¸‹è½½çš„æ¨¡å‹æ–‡ä»¶åœ°å€::

    <env_path>/lib/pythonX.Y/site-packages/transformers/models
    ç¤ºä¾‹:
    /home/username/miniconda/envs/myenv/lib/python3.7/site-packages/transformers/models


    æ¯ä¸ªç¯å¢ƒéƒ½æœ‰è‡ªå·±çš„æ¨¡å‹æ–‡ä»¶å‰¯æœ¬,å¹¶ä¸”ç¯å¢ƒä¹‹é—´ç›¸äº’éš”ç¦»ã€‚
    å¯ä»¥é€šè¿‡è®¾ç½®`TRANSFORMERS_CACHE`ç¯å¢ƒå˜é‡æ¥è¦†ç›–è¿™ä¸€é»˜è®¤è¡Œä¸º







Fetch models and tokenizers to use offline
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Use the from_pretrained() and save_pretrained() workflow
""""""""""""""""""""""""""""""""""""""""""""""""""""""""


1. Download your files ahead of time with PreTrainedModel.from_pretrained()::

    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
    model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")

2. Save your files to a specified directory with PreTrainedModel.save_pretrained()::

    tokenizer.save_pretrained("./your/path/bigscience_t0")
    model.save_pretrained("./your/path/bigscience_t0")

3. Now when youâ€™re offline, reload your files with PreTrainedModel.from_pretrained() from the specified directory::


    tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
    model = AutoModel.from_pretrained("./your/path/bigscience_t0")

Programmatically download files with the huggingface_hub library
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

1. Install the huggingface_hub library in your virtual environment::

    python -m pip install huggingface_hub

2. Use the hf_hub_download function to download a file to a specific path::

    from huggingface_hub import hf_hub_download

    hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")

3. Once your file is downloaded and locally cached, specify itâ€™s local path to load and use it::

    from transformers import AutoConfig
    config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")



TUTORIALS
=========

Pipelines for inference
-----------------------

Start by creating a pipeline() and specify an inference task::

    from transformers import pipeline
    generator = pipeline(task="automatic-speech-recognition")

Pass your input text to the pipeline()::

    generator("https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac")
    {'text': 'I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES'}



Load pretrained instances with an AutoClass
-------------------------------------------

.. note:: Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, BERT is an **architecture**, while bert-base-uncased is a **checkpoint**. Model is a general term that can mean either architecture or checkpoint.


AutoTokenizer
^^^^^^^^^^^^^

.. note:: Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.

Load a tokenizer with AutoTokenizer.from_pretrained()::

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

tokenize your input as shown below::

    >>> sequence = "In a hole in the ground there lived a hobbit."
    >>> encoded_input=tokenizer(sequence)
    >>> print(encoded_input)
    {'input_ids': [101, 1999, 1037, 4920, ...], 
     'token_type_ids': [0, 0, 0, 0, 0, 0, ...], 
     'attention_mask': [1, 1, 1, 1, 1, 1, ...]}

Return your input by decoding the input_ids::

    >>> tokenizer.decode(encoded_input["input_ids"])
    "[CLS] in a hole in the ground there lived a hobbit.[SEP]"
    # è¯´æ˜: two special tokens
    # CLS: classifier
    # SEP: separator


AutoImageProcessor
^^^^^^^^^^^^^^^^^^

For vision tasks, an image processor processes the image into the correct input format::

    from transformers import AutoImageProcessor
    image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")


AutoFeatureExtractor
^^^^^^^^^^^^^^^^^^^^

For audio tasks, a feature extractor processes the audio signal the correct input format::

    from transformers import AutoFeatureExtractor
    feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")



AutoProcessor
^^^^^^^^^^^^^

* Multimodal tasks require a processor that combines two types of preprocessing tools. 
* For example, the ``LayoutLMV2`` model requires an image processor to handle images and a tokenizer to handle text; a processor combines both of them.

::

    from transformers import AutoProcessor
    processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")


AutoModel
^^^^^^^^^

AutoModelFor classes let you load a pretrained model for a given task(sequence classification)::

    from transformers import AutoModelForSequenceClassification
    model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

reuse the same checkpoint to load an architecture for a different task(token classification)::

    from transformers import AutoModelForTokenClassification
    model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")

.. note:: Generally, we recommend using the AutoTokenizer class and the AutoModelFor class to load pretrained instances of models. This will ensure you load the correct architecture every time. 




Preprocess data
---------------

* Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format.
* Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 

Transformers provides a set of preprocessing classes to help prepare your data for the model::

    1. Text
        use `Tokenizer` to convert text into a sequence of tokens, and assemble them into tensors.
    2. Speech and audio
        use `Feature` extractor to extract sequential features 
            from audio waveforms and convert them into tensors.
    3. Image inputs 
        use `ImageProcessor` to convert images into tensors.
    4. Multimodal inputs, 
        use `Processor` to combine a tokenizer and a feature extractor or image processor.


.. note:: ``AutoProcessor`` always works and automatically chooses the correct class for the model youâ€™re using, whether youâ€™re using a tokenizer, image processor, feature extractor or processor.


Natural Language Processing
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* The main tool for preprocessing textual data is a tokenizer. 
* A tokenizer splits text into tokens according to a set of rules. 
* The tokens are converted into numbers and then tensors, which become the model inputs. 
* Any additional inputs required by the model are added by the tokenizer.


Pad
"""

* Sentences arenâ€™t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. 
* Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.

ç¤ºä¾‹::

    >>> batch_sentences = [
    >>>     "But what about second breakfast?",
    >>>     "Don't think he knows about second breakfast, Pip.",
    >>>     "What about elevensies?",
    >>> ]
    >>> encoded_input = tokenizer(batch_sentences, padding=True)
    >>> print(encoded_input)
    {'input_ids': [[101, 1252, 1184, 1164, ..., 0, 0, 0, 0, 0, 0, 0],
                   [101, 1790, 112, 189, ..., 6462, 117, 21902, 1643, 119, 102],
                   [101, 1327, 1164, 545, ..., 0, 0, 0, 0, 0, 0, 0, 0]],
     'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
     'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}



Truncation
""""""""""

* On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. 
* In this case, youâ€™ll need to truncate the sequence to a shorter length.
* Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model

ç¤ºä¾‹::

    >>> batch_sentences = [
    >>>     "But what about second breakfast?",
    >>>     "Don't think he knows about second breakfast, Pip.",
    >>>     "What about elevensies?",
    >>> ]
    >>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
    >>> print(encoded_input)
    {'input_ids': [[101, 1252, 1184, 1164, ..., 0, 0, 0, 0, 0, 0, 0],
                   [101, 1790, 112, 189, ..., 6462, 117, 21902, 1643, 119, 102],
                   [101, 1327, 1164, 545, ..., 0, 0, 0, 0, 0, 0, 0, 0]],
     'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],
     'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}

Build tensors
"""""""""""""

* Finally, you want the tokenizer to return the actual tensors that get fed to the model.
* Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow

ç¤ºä¾‹::

    >> batch_sentences = [
    >>     "But what about second breakfast?",
    >>     "Don't think he knows about second breakfast, Pip.",
    >>     "What about elevensies?",
    >> ]
    >> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
    >> print(encoded_input)
    {'input_ids': tensor([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],
                          [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],
                          [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]]),
     'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),
     'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],
                               [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                               [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}


Audio
^^^^^

* For audio tasks, youâ€™ll need a ``feature extractor`` to prepare your dataset for the model. 
* The feature extractor is designed to extract features from raw audio data, and convert them into tensors.

.. note:: Remember you should always resample your audio datasetâ€™s sampling rate to match the sampling rate of the dataset used to pretrain a model!

è·å–æ•°æ®::

    from datasets import load_dataset, Audio
    dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")

    # upsample the sampling rate to 16kHz:
    dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))


Load the feature extractor::

    from transformers import AutoFeatureExtractor
    feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")


Pass the audio array to the feature extractor::

    audio_input = [dataset[0]["audio"]["array"]]
    feature_extractor(audio_input, sampling_rate=16000)


Pading
""""""

æŸ¥çœ‹æ•°æ®é•¿åº¦::

    dataset[0]["audio"]["array"].shape
    (173398,)

    dataset[1]["audio"]["array"].shape
    (106496,)

è¡¥é½::

    def preprocess_function(examples):
        audio_arrays = [x["array"] for x in examples["audio"]]
        inputs = feature_extractor(
            audio_arrays,
            sampling_rate=16000,
            padding=True,
            max_length=100000,
            truncation=True,
        )
        return inputs
    processed_dataset = preprocess_function(dataset[:5])

ä¸¤æ¬¡æŸ¥çœ‹æ•°æ®é•¿åº¦::

    dataset[0]["audio"]["array"].shape
    (100000,)

    dataset[1]["audio"]["array"].shape
    (100000,)

Computer vision
^^^^^^^^^^^^^^^

* For computer vision tasks, youâ€™ll need an ``image processor`` to prepare your dataset for the model. 
* Image preprocessing consists of several steps that convert images into the input expected by the model. 
* These steps include but are not limited to resizing, normalizing, color channel correction, and converting images to tensors.

è½½å…¥æ•°æ®::

    from datasets import load_dataset
    dataset = load_dataset("food101", split="train[:100]")

æŸ¥çœ‹å›¾ç‰‡::

    dataset[0]["image"]


Load the image processor::

    from transformers import AutoImageProcessor
    image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")

image augmentation
""""""""""""""""""

.. note:: è¿™å„¿ç”¨çš„æ˜¯torchvisionâ€™s transforms moduleï¼Œè¿˜å¯ä»¥ç”¨å…¶ä»–å›¾åƒå¢å¼ºæ–¹æ³•ï¼Œå¦‚: `Albumentations <https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb>`_ å’Œ `Kornia <https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb>`_

resizing::

    from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose

    size = (
        image_processor.size["shortest_edge"]
        if "shortest_edge" in image_processor.size
        else (image_processor.size["height"], image_processor.size["width"])
    )

    # éšæœºè£å‰ªå’Œå˜åŒ–é¢œè‰²
    # RandomResizedCropä¼šéšæœºè£å‰ªå›¾ç‰‡çš„åŒºåŸŸã€‚
    # ColorJitterä¼šéšæœºæ”¹å˜å›¾åƒçš„äº®åº¦ã€å¯¹æ¯”åº¦ç­‰å‚æ•°ã€‚
    _transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])

combines image augmentation and image preprocessing for a batch of images and generates pixel_values::

    # å¯¹æ¯ä¸ªå›¾åƒexampleåº”ç”¨_transforms
    # å¹¶å°†è½¬æ¢åçš„å›¾åƒä¿å­˜åœ¨exampleçš„pixel_valuesä¸­
    def transforms(examples):
        images = [_transforms(img.convert("RGB")) for img in examples["image"]]
        examples["pixel_values"] = image_processor(images, do_resize=False, return_tensors="pt")["pixel_values"]
        return examples

apply the transforms on the fly::

    dataset.set_transform(transforms)


The image has been randomly cropped and itâ€™s color properties are different::

    import numpy as np
    import matplotlib.pyplot as plt

    img = dataset[0]["pixel_values"]
    plt.imshow(img.permute(1, 2, 0))

.. note:: dataset[0]["pixel_values"]æ¯æ¬¡ç»“æœä¸ä¸€æ ·ã€‚åŸå› æ˜¯ä½¿ç”¨äº†``dataset.set_transform(transforms)``ï¼Œæ¯æ¬¡éå†datasetæ—¶,è¿™äº›éšæœºæ“ä½œéƒ½ä¼šé‡æ–°åº”ç”¨,æ‰€ä»¥åŒä¸€ä¸ªæ ·æœ¬ç»è¿‡å¢å¼ºä¹‹åçš„pixel_valueså°±ä¼šæœ‰æ‰€ä¸åŒã€‚è¿™ä¹Ÿæ­£æ˜¯æ•°æ®å¢å¼ºçš„ç›®çš„,é€šè¿‡éšæœºæ“ä½œåˆ›é€ æ›´å¤šä¸åŒçš„è®­ç»ƒæ ·æœ¬,æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚æ€»ç»“æ¥è¯´,dataset[0]æœ¬èº«ä¸å˜,ä½†å¢å¼ºåpixel_valuesä¸åŒ,æ˜¯å› ä¸ºéšæœºå¢å¼ºå¼•èµ·çš„ã€‚è¿™å¯¹æé«˜æ¨¡å‹é²æ£’æ€§æ˜¯æœ‰å¸®åŠ©çš„ã€‚


Pading
""""""

::

    def collate_fn(batch):
        pixel_values = [item["pixel_values"] for item in batch]
        encoding = image_processor.pad(pixel_values, return_tensors="pt")
        labels = [item["labels"] for item in batch]
        batch = {}
        batch["pixel_values"] = encoding["pixel_values"]
        batch["pixel_mask"] = encoding["pixel_mask"]
        batch["labels"] = labels
        return batch


* åœ¨PyTorchä¸­,collate_fnå‡½æ•°çš„ä½œç”¨æ˜¯åœ¨ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®æ—¶å¯¹ä¸€ä¸ªbatchçš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ã€‚
* collate_fnä¼šåœ¨æ¯ä¸ªbatchè¢«åŠ è½½åæ‰§è¡Œ,å®ƒæ¥å—ä¸€ä¸ªbatchçš„æ•°æ®ä½œä¸ºè¾“å…¥,å¹¶è¿”å›batchçš„æ•°æ®ä½œä¸ºè¾“å‡ºã€‚
 
å¸¸è§çš„ä½¿ç”¨collate_fnçš„åœºæ™¯æœ‰::

    - å½“æ ·æœ¬çš„æ•°æ®æ ¼å¼ä¸åŒæ—¶,collate_fnå¯ä»¥å°†å…¶è½¬æ¢ä¸ºç›¸åŒæ ¼å¼ã€‚
        ä¾‹å¦‚æ ·æœ¬åŒ…æ‹¬å›¾åƒå’Œæ–‡æœ¬,collate_fnå¯ä»¥å°†å…¶è½¬æ¢ä¸ºåŒæ ·çš„å¼ é‡æ ¼å¼ã€‚
    - å½“batchä¸­çš„æ ·æœ¬é•¿åº¦ä¸åŒæ—¶,collate_fnå¯ä»¥é€šè¿‡paddingå°†å…¶è¡¥é½åˆ°ç›¸åŒé•¿åº¦ã€‚
        ä¾‹å¦‚å¤„ç†NLPä»»åŠ¡ä¸­çš„æ–‡æœ¬æ•°æ®ã€‚
    - å¯¹batchä¸­çš„æ ·æœ¬è¿›è¡Œé¢å¤–çš„é¢„å¤„ç†
        ä¾‹å¦‚å›¾åƒå¢å¼ºã€æ–‡æœ¬tokenizeç­‰ã€‚
    - æ„å»ºè‡ªå®šä¹‰çš„æ•°æ®ç»“æ„ä½œä¸ºbatchçš„è¾“å‡º
        ä¾‹å¦‚ä¸ºæ£€æµ‹ä»»åŠ¡æ„å»º(images, targets)çš„ç»“æ„ã€‚
    - åœ¨è®­ç»ƒè¯­éŸ³è¯†åˆ«æ¨¡å‹æ—¶,collate_fnå¯ä»¥å°†éŸ³é¢‘æ ·æœ¬paddingåˆ°ç›¸åŒé•¿åº¦,å¹¶æ„å»ºé•¿åº¦å˜é‡ç­‰ã€‚



Fine-tune a pretrained model
----------------------------

å®‰è£…åŒ…::

    !pip install datasets transformers accelerate evaluate

load data::

    >>> from datasets import load_dataset
    >>> dataset = load_dataset("yelp_review_full")
    >>> dataset["train"][100]
    {'label': 0,
     'text': 'My expectations for McDonal...'}

token::

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

model::

    from transformers import AutoModelForSequenceClassification
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)


å–å°éƒ¨åˆ†æ•°æ®ä»¥èŠ‚çœæ—¶é—´(å¯é€‰)::

    from datasets import DatasetDict, Dataset

    small_train_dataset = dataset["train"].shuffle(seed=42).select(range(100))
    small_test_dataset = dataset["test"].shuffle(seed=42).select(range(100))
    small_dataset = DatasetDict({
        'train': small_train_dataset, 
        'test': small_test_dataset
    })

æ‰¹å¤„ç†token::

    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True)

    tokenized_datasets = small_dataset.map(tokenize_function, batched=True)
    small_tokenized_train_dataset = tokenized_datasets["train"]
    small_tokenized_test_dataset = tokenized_datasets["test"]



Train with PyTorch Trainer
^^^^^^^^^^^^^^^^^^^^^^^^^^

Training hyperparameters
""""""""""""""""""""""""

Specify where to save the checkpoints from your training::

    from transformers import TrainingArguments
    training_args = TrainingArguments(output_dir="test_trainer")


monitor your evaluation metrics during fine-tuning(å¯é€‰)::

    from transformers import TrainingArguments, Trainer
    training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")

Evaluate
""""""""

* ``Trainer`` does not automatically evaluate model performance during training.
* You should add ``compute_metrics`` param to ``Trainer`` object.


`Evaluate <https://huggingface.co/docs/evaluate/index>`_ library provides a simple accuracy function::

    import numpy as np
    import evaluate

    metric = evaluate.load("accuracy")

convert the predictions to logits (remember all ğŸ¤— Transformers models return logits)::

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)



Trainer
"""""""

Create a Trainer object::

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=small_tokenized_train_dataset,
        eval_dataset=small_tokenized_eval_dataset,
        compute_metrics=compute_metrics,
    )


fine-tune begin::

    trainer.train()




Train in native PyTorch
^^^^^^^^^^^^^^^^^^^^^^^

æ¸…é™¤ç¯å¢ƒèŠ‚çœèµ„æº::

    del model
    del trainer
    torch.cuda.empty_cache()


manually postprocess tokenized_dataset to prepare it for training::

    tokenized_datasets = tokenized_datasets.remove_columns(["text"])
    tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

    # Set the format of the dataset to return PyTorch tensors instead of lists:
    tokenized_datasets.set_format("torch")

    small_tokenized_train_dataset = tokenized_datasets["train"]
    small_tokenized_test_dataset = tokenized_datasets["test"]


DataLoader
""""""""""


Create a DataLoader::

    from torch.utils.data import DataLoader

    train_dataloader = DataLoader(small_tokenized_train_dataset, shuffle=True, batch_size=8)
    eval_dataloader = DataLoader(small_tokenized_test_dataset, batch_size=8)

Optimizer and learning rate scheduler
"""""""""""""""""""""""""""""""""""""

Create an optimizer and learning rate scheduler to fine-tune the model::

    from torch.optim import AdamW
    optimizer = AdamW(model.parameters(), lr=5e-5)

Create the default learning rate scheduler from Trainer::

    from transformers import get_scheduler
    num_epochs = 3
    num_training_steps = num_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
    )

specify device to use a GPU::

    import torch
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    model.to(device)


Training loop
"""""""""""""

åŸºæœ¬çš„å¾ªç¯è®­ç»ƒé€»è¾‘::

    from tqdm.auto import tqdm

    progress_bar = tqdm(range(num_training_steps))

    model.train()
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)       # å‰å‘ä¼ æ’­
            loss = outputs.loss
            loss.backward()                # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦

            optimizer.step()               # ä½¿ç”¨ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
            lr_scheduler.step()            # ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨æ›´æ–°å­¦ä¹ ç‡
            optimizer.zero_grad()          # æ¸…é›¶ä¼˜åŒ–å™¨çš„æ¢¯åº¦
            progress_bar.update(1)




Evaluate
""""""""


åŸºæœ¬çš„æ¨¡å‹è¯„ä¼°é€»è¾‘::

    import evaluate

    metric = evaluate.load("accuracy")    # åŠ è½½è¯„ä¼°æŒ‡æ ‡
    model.eval()                          # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    for batch in eval_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        with torch.no_grad():             # å…³é—­autograd engineè¿›è¡Œæ¨ç†
            outputs = model(**batch)      # æ¨¡å‹å‰å‘ä¼ æ’­è®¡ç®—

        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)        # è®¡ç®—é¢„æµ‹ç±»åˆ«
        metric.add_batch(predictions=predictions, references=batch["labels"])     # å°†é¢„æµ‹ç»“æœå’Œæ ‡ç­¾ä¼ å…¥metricè¿›è¡ŒæŒ‡æ ‡è®¡ç®—

    metric.compute()                      # èšåˆæ‰¹æ¬¡ç»“æœ,å¾—åˆ°æœ€ç»ˆè¯„ä¼°æŒ‡æ ‡æ•°é‡




Train with a script
-------------------

* æœ¬èŠ‚ä¸»è¦å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ç°æˆçš„è„šæœ¬æ¥ç›´æ¥å®ç°ç›¸åº”çš„åŠŸèƒ½
* ä¸»è¦å¦‚ä¸‹é¢2ä¸ªç”±ç¤¾åŒºè´¡çŒ®çš„è„šæœ¬ç¤ºä¾‹ `research projects <https://github.com/huggingface/transformers/tree/main/examples/research_projects>`_ å’Œ `legacy examples <https://github.com/huggingface/transformers/tree/main/examples/legacy>`_

.. warning:: These scripts are not actively maintained and require a specific version of ğŸ¤— Transformers that will most likely be incompatible with the latest version of the library.

è¿è¡Œè„šæœ¬ç¤ºä¾‹::

    python examples/pytorch/summarization/run_summarization.py \
        --model_name_or_path t5-small \
        --do_train \
        --do_eval \
        --dataset_name cnn_dailymail \
        --dataset_config "3.0.0" \
        --source_prefix "summarize: " \
        --output_dir /tmp/tst-summarization \
        --per_device_train_batch_size=4 \
        --per_device_eval_batch_size=4 \
        --overwrite_output_dir \
        --predict_with_generate


Distributed training with Accelerate
------------------------------------

* æœ¬èŠ‚ä¸»è¦è®²äº†ä¸€ä¸ªåˆ†å¸ƒå¼è®­ç»ƒçš„å·¥å…·: ``Accelerate``

å®‰è£…::

    pip install accelerate

Backward
^^^^^^^^


ä½¿ç”¨ ``Accelerate`` åªéœ€è¦åšå¦‚ä¸‹ä¿®æ”¹::

    + from accelerate import Accelerator
      from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

    + accelerator = Accelerator()

      model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
      optimizer = AdamW(model.parameters(), lr=3e-5)

    - device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    - model.to(device)

    + train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    +     train_dataloader, eval_dataloader, model, optimizer
    + )

      num_epochs = 3
      num_training_steps = num_epochs * len(train_dataloader)
      lr_scheduler = get_scheduler(
          "linear",
          optimizer=optimizer,
          num_warmup_steps=0,
          num_training_steps=num_training_steps
      )

      progress_bar = tqdm(range(num_training_steps))

      model.train()
      for epoch in range(num_epochs):
          for batch in train_dataloader:
    -         batch = {k: v.to(device) for k, v in batch.items()}
              outputs = model(**batch)
              loss = outputs.loss
    -         loss.backward()
    +         accelerator.backward(loss)

              optimizer.step()
              lr_scheduler.step()
              optimizer.zero_grad()
              progress_bar.update(1)


Transformers Agent
------------------

.. warning:: Transformers Agent is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change.

* building on the concept of tools and agents.
* In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools.

ç¤ºä¾‹
^^^^

å‘½ä»¤::

    agent.run("Caption the following image", image=image)


.. image:: https://img.zhaoweiguo.com/uPic/2023/08/B49pjD.png


å‘½ä»¤::

    agent.run("Read the following text out loud", text=text)


.. image:: https://img.zhaoweiguo.com/uPic/2023/08/vtKK41.png


å‘½ä»¤::

    agent.run(
        "In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?",
        document=document,
    )


.. image:: https://img.zhaoweiguo.com/uPic/2023/08/2cLcOq.png


Quickstart
^^^^^^^^^^

å®‰è£…::

    pip install transformers[agents]

logging in to have access to the Inference API::

    from huggingface_hub import login
    login("<YOUR_TOKEN>")

instantiate the agent::

    from transformers import HfAgent

    # Starcoder
    agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoder")
    # StarcoderBase
    # agent = HfAgent("https://api-inference.huggingface.co/models/bigcode/starcoderbase")
    # OpenAssistant
    # agent = HfAgent(url_endpoint="https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5")

    ## OpenAI
    # pip install openai
    # from transformers import OpenAiAgent
    # agent = OpenAiAgent(model="text-davinci-003", api_key="<your_api_key>")


Single execution (run)
""""""""""""""""""""""

::

    agent.run("Draw me a picture of rivers and lakes.")


    picture = agent.run("Generate a picture of rivers and lakes.")
    updated_picture = agent.run("Transform the image in `picture` to add an island to it.", picture=picture)


Chat-based execution (chat)
"""""""""""""""""""""""""""

::

    agent.chat("Generate a picture of rivers and lakes")
    agent.chat("Transform the picture so that there is a rock in there")


åŸç†
^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/1kTPz2.jpg

Agents
""""""

* The â€œagentâ€ here is a large language model, and weâ€™re prompting it so that it has access to a specific set of tools.

Tools
"""""

* Tools are very simple: theyâ€™re a single function, with a name, and a description. We then use these toolsâ€™ descriptions to prompt the agent. Through the prompt, we show the agent how it would leverage tools to perform what was requested in the query.

Resource
^^^^^^^^

A curated set of tools
""""""""""""""""""""""

* Document question answering: given a document (such as a PDF) in image format, answer a question on this document (`Donut <https://huggingface.co/docs/transformers/model_doc/donut>`_)
* Text question answering: given a long text and a question, answer the question in the text (`Flan_T5 <https://huggingface.co/docs/transformers/model_doc/flan-t5>`_)
* Unconditional image captioning: Caption the image! (`BLIP <https://huggingface.co/docs/transformers/model_doc/blip>`_)
* Image question answering: given an image, answer a question on this image (`VILT <https://huggingface.co/docs/transformers/model_doc/vilt>`_)
* Image segmentation: given an image and a prompt, output the segmentation mask of that prompt (`CLIPSeg <https://huggingface.co/docs/transformers/model_doc/clipseg>`_)
* Speech to text: given an audio recording of a person talking, transcribe the speech into text (`Whisper <https://huggingface.co/docs/transformers/model_doc/whisper>`_)
* Text to speech: convert text to speech (`SpeechT5 <https://huggingface.co/docs/transformers/model_doc/speecht5>`_)
* Zero-shot text classification: given a text and a list of labels, identify to which label the text corresponds the most (`BART <https://huggingface.co/docs/transformers/model_doc/bart>`_)
* Text summarization: summarize a long text in one or a few sentences (`BART <https://huggingface.co/docs/transformers/model_doc/bart>`_)
* Translation: translate the text into a given language (`NLLB <https://huggingface.co/docs/transformers/model_doc/nllb>`_)

Custom tools
""""""""""""

* Text downloader: to download a text from a web URL
* Text to image: generate an image according to a prompt, leveraging stable diffusion. `huggingface-tools/text-to-image <https://huggingface.co/spaces/huggingface-tools/text-to-image>`_
* Image transformation: modify an image given an initial image and a prompt, leveraging instruct pix2pix stable diffusion
* Text to video: generate a small video according to a prompt, leveraging damo-vilab


Code generation
^^^^^^^^^^^^^^^

ç¤ºä¾‹::

    >>> agent.run("Draw me a picture of rivers and lakes", return_code=True)

    ==Code generated by the agent==
    from transformers import load_tool
    image_generator = load_tool("huggingface-tools/text-to-image")
    image = image_generator(prompt="rivers and lakes")

ç¤ºä¾‹::

    >>> agent.run("Draw me a picture of the sea then transform the picture to add an island", return_code=True)

    ==Code generated by the agent==
    from transformers import load_tool
    image_transformer = load_tool("huggingface-tools/image-transformation")
    image_generator = load_tool("huggingface-tools/text-to-image")
    image = image_generator(prompt="a picture of the sea")
    image = image_transformer(image, prompt="an island")

ç¤ºä¾‹::

    >>> picture = agent.run("Generate a picture of rivers and lakes.")
    >>> updated_picture = agent.run("Transform the image in `picture` to add an boat to it.", picture=picture, return_code=True)

    ==Code generated by the agent==
    image = image_transformer(image=picture, prompt="a boat")


Practice
^^^^^^^^

* https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj#scrollTo=Q9rx-nKzDpAW







TASK GUIDES
===========

NATURAL LANGUAGE PROCESSING
---------------------------

NLP::

    Text classification
    Token classification
      One of the most common token classification tasks is Named Entity Recognition (NER). 
      NER attempts to find a label for each entity in a sentence, 
        such as a person, location, or org.
    Question answering
    Causal language modeling(There are two types of language modeling: `causal` and `masked`)
      Causal language models are frequently used for text generation.
      You can use these models for creative applications 
        like choosing your own text adventure or an intelligent coding assistant 
            like Copilot or CodeParrot.
    Masked language modeling
      it predicts a masked token in a sequence, and the model can attend to tokens bidirectionally
      it is great for tasks that require a good contextual understanding of an entire sequence.
      BERT is an example of a masked language model.
    Translation
    Summarization
    Multiple choice



AUDIO
-----

::

    Audio classification
    Automatic speech recognition


COMPUTER VISION
---------------

::

    Image classification
    Semantic segmentation
      Semantic segmentation assigns a label or class to each individual pixel of an image.
      Common real-world applications of semantic segmentation include:
          training self-driving cars to identify pedestrians and important traffic information, 
          identifying cells and abnormalities in medical imagery, 
          monitoring environmental changes from satellite imagery.
    Video classification
    Object detection
      This task is commonly used in autonomous driving for detecting things 
        like pedestrians, road signs, and traffic lights. 
      Other applications include counting objects in images, image search, and more.
    Zero-shot object detection
    Zero-shot image classification
    Depth estimation

    è¯´æ˜:
    è¯­ä¹‰åˆ†å‰²éœ€è¦å¤„ç†æ‰€æœ‰åƒç´ ,ç›®æ ‡æ£€æµ‹åªå¤„ç†æ„Ÿå…´è¶£çš„ç›®æ ‡åŒºåŸŸã€‚
    è¯­ä¹‰åˆ†å‰²ä¾§é‡å¯¹æ•´ä¸ªåœºæ™¯å…¨é¢ç†è§£,ç›®æ ‡æ£€æµ‹ä¾§é‡æ£€æµ‹ç‰¹å®šæ„Ÿå…´è¶£ç›®æ ‡ã€‚


MULTIMODAL
----------

::

    Image captioning
    Document Question Answering
    Text to speech







DEVELOPER GUIDES
================

ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹åŒ…æ‹¬::

    GPT2
    XLNet
    OpenAI GPT
    CTRL
    TransformerXL
    XLM
    Bart
    T5
    GIT
    Whisper

Transformers Notebooks with examples
------------------------------------

* https://huggingface.co/docs/transformers/notebooks


Community resources
-------------------

* https://huggingface.co/docs/transformers/community






PERFORMANCE AND SCALABILITY
===========================

Trainer supports four hyperparameter search backends currently::

     optuna, sigopt, raytune and wandb




CONCEPTUAL GUIDES
=================


Philosophy
----------

three standard classes required to use each model::

    1. configuration
    2. models
    3. a preprocessing class

         1) tokenizer for NLP(AutoTokenizer)
         2) image processor for vision(AutoImageProcessor)
         3) feature extractor for audio(AutoFeatureExtractor)
         4) processor for multimodal inputs(AutoProcessor)


On top of those three base classes, the library provides two APIs::

    1. pipeline()
        for quickly using a model for inference on a given task
    2. Trainer
        to quickly train or fine-tune a PyTorch model



Main concepts
^^^^^^^^^^^^^

* Model classes can be PyTorch models (torch.nn.Module), Keras models (tf.keras.Model) or JAX/Flax models (flax.linen.Module) that work with the pretrained weights provided in the library.
* Configuration classes store the hyperparameters required to build a model (such as the number of layers and hidden size). You donâ€™t always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).
* Preprocessing classes convert the raw data into a format accepted by the model. A tokenizer stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. Image processors preprocess vision inputs, feature extractors preprocess audio inputs, and a processor handles multimodal inputs.


All these classes have these three methods::

    from_pretrained()
    save_pretrained()
    push_to_hub()


Glossary
--------

::

    attention mask
        è¯¥å‚æ•°å‘æ¨¡å‹æŒ‡ç¤ºå“ªäº›æ ‡è®°åº”è¯¥è¢«å…³æ³¨ï¼Œå“ªäº›æ ‡è®°ä¸åº”è¯¥è¢«å…³æ³¨ã€‚
        ä¸¤ä¸ªæœ‰ä¸åŒçš„é•¿åº¦çš„åºåˆ—æ”¾åœ¨åŒä¸€ä¸ªå¼ é‡ä¸­æ—¶ä½¿ç”¨
        æ³¨æ„æ©ç æ˜¯ä¸€ä¸ªäºŒè¿›åˆ¶å¼ é‡ï¼ŒæŒ‡ç¤ºå¡«å……ç´¢å¼•çš„ä½ç½®ï¼Œä»¥ä¾¿æ¨¡å‹ä¸ä¼šå…³æ³¨å®ƒä»¬

    autoencoding models
        See `encoder models` and `masked language modeling`

    autoregressive models
        See `causal language modeling` and `decoder models`

    backbone
        backbone is the network (embeddings and layers) that outputs the raw hidden states or features.
        It is usually connected to a head which accepts the features as its input to make a prediction.
        For example, 
            `ViTModel` is a `backbone` without a specific head on top. 
            Other models can also use `VitModel` as a `backbone` such as DPT.

    causal language modeling
        é¢„è®­ç»ƒä»»åŠ¡ï¼Œæ¨¡å‹æŒ‰é¡ºåºè¯»å–æ–‡æœ¬å¹¶å¿…é¡»é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯

    channel
        å½©è‰²å›¾åƒç”±çº¢ã€ç»¿ã€è“ (RGB) ä¸‰ä¸ªé€šé“ä¸­çš„å€¼çš„æŸç§ç»„åˆç»„æˆï¼Œè€Œç°åº¦å›¾åƒåªæœ‰ä¸€ä¸ªé€šé“
        åœ¨ ğŸ¤— Transformers ä¸­ï¼Œé€šé“å¯ä»¥æ˜¯å›¾åƒå¼ é‡çš„ç¬¬ä¸€ä¸ªæˆ–æœ€åä¸€ä¸ªç»´åº¦ï¼š[ n_channels , height , width ] æˆ– [ height , width , n_channels ]

    connectionist temporal classification (CTC)
        ä¸€ç§å…è®¸æ¨¡å‹åœ¨ä¸ç¡®åˆ‡çŸ¥é“è¾“å…¥å’Œè¾“å‡ºå¦‚ä½•å¯¹é½çš„æƒ…å†µä¸‹è¿›è¡Œå­¦ä¹ çš„ç®—æ³•
         CTC é€šå¸¸ç”¨äºè¯­éŸ³è¯†åˆ«ä»»åŠ¡ï¼Œç”±äºå¤šç§åŸå› ï¼ˆä¾‹å¦‚è¯´è¯è€…çš„è¯­é€Ÿä¸åŒï¼‰ï¼Œè¯­éŸ³å¹¶ä¸æ€»æ˜¯ä¸æ–‡æœ¬å®Œå…¨ä¸€è‡´ã€‚

    convolution
        ç¥ç»ç½‘ç»œä¸­çš„ä¸€ç§å±‚ï¼Œå…¶ä¸­è¾“å…¥çŸ©é˜µä¸ä¸€ä¸ªè¾ƒå°çš„çŸ©é˜µï¼ˆå†…æ ¸æˆ–è¿‡æ»¤å™¨ï¼‰ç›¸ä¹˜ï¼Œå¹¶å°†å€¼æ±‚å’Œåˆ°ä¸€ä¸ªæ–°çŸ©é˜µä¸­
        å·ç§¯ç¥ç»ç½‘ç»œ (CNN) å¸¸ç”¨äºè®¡ç®—æœºè§†è§‰

    decoder input IDs
        This input is specific to encoder-decoder models, and contains the input IDs that will be fed to the decoder. 
        These inputs should be used for sequence to sequence tasks, such as translation or summarization, and are usually built in a way specific to each model.

    decoder models
        ä¹Ÿå«: autoregressive models
        decoder models involve a pretraining task (called causal language modeling) where the model reads the texts in order and has to predict the next word. 
        Itâ€™s usually done by reading the whole sentence with a mask to hide future tokens at a certain timestep.

    encoder models
        ä¹Ÿå«: autoencoding models
        encoder models take an input (such as text or images) and transform them into a condensed(å‹ç¼©) numerical representation called an embedding. 
        Oftentimes, encoder models are pretrained using techniques like `masked language modeling`, 
            which masks parts of the input sequence and forces the model to create more meaningful representations.

    feature extraction
        The process of selecting and transforming raw data into a set of features that are more informative and useful for machine learning algorithms. 
        Some examples of feature extraction include 
            transforming raw text into word embeddings 
            and extracting important features such as edges or shapes from image/video data.

    feed forward chunking
        å‚è§ä¸‹é¢è¯¦è§£

    finetuned models
        Finetuning is a form of transfer learning which involves 
            taking a pretrained model, freezing its weights, and replacing the output layer with a newly added model head. 
        The model head is trained on your target dataset.

    head
        The model head refers to the last layer of a neural network that accepts the raw hidden states and projects them onto a different dimension. 
        There is a different model head for each task. 
        For example:
            GPT2ForSequenceClassification is a sequence classification head - a linear layer - on top of the base GPT2Model.
            ViTForImageClassification is an image classification head - a linear layer on top of the final hidden state of the CLS token - on top of the base ViTModel.
            Wav2Vec2ForCTC is a language modeling head with CTC on top of the base Wav2Vec2Model.

    image patch
        å‚è§ä¸‹é¢è¯¦è§£
        Vision-based Transformers models split an image into smaller patches which are linearly embedded, 
            and then passed as a sequence to the model. 
        You can find the patch_size - or resolution - of the model in its configuration.


    inference
        Inference is the process of evaluating a model on new data after training is complete. 

    input IDs
        The input ids are often the only required parameters to be passed to the model as input. 
        They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.

    labels
        The labels are an optional argument which can be passed in order for the model to compute the loss itself.
        These labels should be the expected prediction of the model: 
            it will use the standard loss in order to compute the loss between its predictions and the expected value (the label).

    masked language modeling (MLM)
        A pretraining task where the model sees a corrupted version of the texts, 
            usually done by masking some tokens randomly, and has to predict the original text.

    multimodal
        A task that combines texts with another kind of inputs (for instance images).

    pipeline
        Transformers ä¸­çš„ç®¡é“æ˜¯ä¸€ä¸ªæŠ½è±¡ï¼ŒæŒ‡çš„æ˜¯æŒ‰ç‰¹å®šé¡ºåºæ‰§è¡Œçš„ä¸€ç³»åˆ—æ­¥éª¤ï¼Œç”¨äºé¢„å¤„ç†å’Œè½¬æ¢æ•°æ®å¹¶ä»æ¨¡å‹è¿”å›é¢„æµ‹

    pixel values
        ä¼ é€’ç»™æ¨¡å‹çš„å›¾åƒæ•°å€¼è¡¨ç¤ºçš„å¼ é‡ã€‚
        pixel valuesçš„å½¢çŠ¶ä¸º [ batch_size , num_channels , height , width ]ï¼Œç”±å›¾åƒå¤„ç†å™¨ç”Ÿæˆã€‚

    pooling
        é€šè¿‡å–æ± åŒ–ç»´åº¦çš„æœ€å¤§å€¼æˆ–å¹³å‡å€¼ï¼Œå°†çŸ©é˜µç¼©å‡ä¸ºæ›´å°çš„çŸ©é˜µçš„æ“ä½œ
        æ± åŒ–å±‚é€šå¸¸ä½äºå·ç§¯å±‚ä¹‹é—´ï¼Œç”¨äºå¯¹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œä¸‹é‡‡æ ·

    position IDs

    representation learning
        æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå­¦ä¹ åŸå§‹æ•°æ®çš„æœ‰æ„ä¹‰çš„è¡¨ç¤ºã€‚
        è¡¨ç¤ºå­¦ä¹ æŠ€æœ¯çš„ä¸€äº›ç¤ºä¾‹åŒ…æ‹¬è¯åµŒå…¥ã€è‡ªåŠ¨ç¼–ç å™¨å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN)ã€‚

    self-attention
        Each element of the input finds out which other elements of the input they should attend to.
        è¾“å…¥çš„æ¯ä¸ªå…ƒç´ éƒ½ä¼šæ‰¾å‡ºå®ƒä»¬åº”è¯¥å…³æ³¨è¾“å…¥çš„å“ªäº›å…¶ä»–å…ƒç´ ã€‚

    self-supervised learning
    semi-supervised learning

    sequence-to-sequence (seq2seq)
        ä»è¾“å…¥ç”Ÿæˆæ–°åºåˆ—çš„æ¨¡å‹ï¼Œä¾‹å¦‚ç¿»è¯‘æ¨¡å‹æˆ–æ‘˜è¦æ¨¡å‹

    stride
        åœ¨å·ç§¯æˆ–æ± åŒ–ä¸­ï¼Œæ­¥å¹…æ˜¯æŒ‡å†…æ ¸åœ¨çŸ©é˜µä¸Šç§»åŠ¨çš„è·ç¦»ã€‚
        æ­¥å¹…ä¸º 1 è¡¨ç¤ºå†…æ ¸ä¸€æ¬¡ç§»åŠ¨ä¸€ä¸ªåƒç´ ï¼Œæ­¥å¹…ä¸º 2 è¡¨ç¤ºå†…æ ¸ä¸€æ¬¡ç§»åŠ¨ä¸¤ä¸ªåƒç´ ã€‚

    token
        A part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a punctuation symbol.

    token Type IDs
        è¿™äº›éœ€è¦å°†ä¸¤ä¸ªä¸åŒçš„åºåˆ—è¿æ¥åˆ°å•ä¸ªâ€œinput_idsâ€æ¡ç›®ä¸­ï¼Œè¿™é€šå¸¸æ˜¯åœ¨ç‰¹æ®Šæ ‡è®°çš„å¸®åŠ©ä¸‹æ‰§è¡Œçš„ï¼Œ
        ä¾‹å¦‚åˆ†ç±»å™¨ï¼ˆclassifier [CLS] ï¼‰å’Œåˆ†éš”ç¬¦ï¼ˆseparator [SEP] ï¼‰æ ‡è®°ã€‚

    transfer learning
        ä¸€ç§æ¶‰åŠé‡‡ç”¨é¢„è®­ç»ƒæ¨¡å‹å¹¶å°†å…¶é€‚åº”ç‰¹å®šäºæ‚¨çš„ä»»åŠ¡çš„æ•°æ®é›†çš„æŠ€æœ¯ã€‚
        æ‚¨å¯ä»¥åˆ©ç”¨ä»ç°æœ‰æ¨¡å‹è·å¾—çš„çŸ¥è¯†ä½œä¸ºèµ·ç‚¹ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚è¿™åŠ å¿«äº†å­¦ä¹ è¿‡ç¨‹å¹¶å‡å°‘äº†æ‰€éœ€çš„è®­ç»ƒæ•°æ®é‡ã€‚



feed forward chunking
^^^^^^^^^^^^^^^^^^^^^

* from llm
* Feed Forward Chunking æ˜¯ä¸€ç§ç”¨äºå‡å°‘å†…å­˜æ¶ˆè€—çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨ Transformer æ¨¡å‹ä¸­çš„å‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼ˆFeed Forward Layersï¼‰çš„è®¡ç®—ä¸­

èƒŒæ™¯ï¼šTransformer æ¨¡å‹ä¸­çš„å‰é¦ˆç½‘ç»œ::

    åœ¨ Transformer æ¨¡å‹çš„æ¯ä¸ªæ®‹å·®æ³¨æ„åŠ›å—ï¼ˆResidual Attention Blockï¼‰ä¸­ï¼Œé€šå¸¸åŒ…å«ä»¥ä¸‹ä¸¤éƒ¨åˆ†ï¼š
        1. è‡ªæ³¨æ„åŠ›å±‚ï¼ˆSelf-Attention Layerï¼‰
        2. å‰é¦ˆç½‘ç»œï¼ˆFeed Forward Networkï¼ŒFFNï¼‰
    å‰é¦ˆç½‘ç»œé€šå¸¸ç”±ä¸¤å±‚çº¿æ€§å±‚ç»„æˆï¼š
        ç¬¬ä¸€å±‚å°†è¾“å…¥åµŒå…¥ï¼ˆembeddingï¼‰ä»éšè—å±‚å¤§å°ï¼ˆhidden_sizeï¼‰æŠ•å½±åˆ°ä¸€ä¸ªæ›´é«˜ç»´åº¦çš„ä¸­é—´å±‚å¤§å°ï¼ˆintermediate_sizeï¼‰
        ç¬¬äºŒå±‚å°†ä¸­é—´å±‚çš„è¾“å‡ºå†æŠ•å½±å›éšè—å±‚å¤§å°ã€‚

* ä¾‹ï¼šå¯¹äº BERT æ¨¡å‹ï¼Œintermediate_size æ¯” hidden_size æ›´å¤§ï¼Œè¿™æ ·åšæ˜¯ä¸ºäº†å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚ç„¶è€Œï¼Œç”±äºè¾“å…¥çš„å°ºå¯¸æ˜¯ [batch_size, sequence_length, hidden_size]ï¼Œä¸­é—´å±‚çš„å°ºå¯¸ä¼šå˜ä¸º [batch_size, sequence_length, intermediate_size]ï¼Œåœ¨ intermediate_size è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œè¿™ä¼šå ç”¨å¤§é‡å†…å­˜ã€‚
* ã€é—®é¢˜ï¼šå†…å­˜å¼€é”€ã€‘å½“è¾“å…¥æœ‰è¾ƒé•¿çš„åºåˆ—é•¿åº¦ï¼ˆsequence_lengthï¼‰æ—¶ï¼Œå­˜å‚¨è¿™äº›ä¸­é—´åµŒå…¥ä¼šå¯¼è‡´å·¨å¤§çš„å†…å­˜å ç”¨ã€‚å¯¹äºå¤§å‹ Transformer æ¨¡å‹ï¼Œè¿™ç§å†…å­˜å¼€é”€æ˜¯ç“¶é¢ˆä¹‹ä¸€ã€‚
* ã€è§£å†³æ–¹æ¡ˆFeed Forward Chunkingã€‘ä¸»è¦æ€æƒ³æ˜¯ï¼šä¸å†ä¸€æ¬¡æ€§å¯¹æ•´ä¸ªè¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®è®¡ç®—å‰é¦ˆç½‘ç»œçš„è¾“å‡ºï¼Œè€Œæ˜¯å°†è¾“å…¥åºåˆ—æŒ‰å—ï¼ˆchunkï¼‰è¿›è¡Œåˆ†å‰²ï¼Œé€å—è®¡ç®—å‰é¦ˆå±‚çš„è¾“å‡ºã€‚
* å…·ä½“æ­¥éª¤å¦‚ä¸‹::

    1. å°†è¾“å…¥çš„ [batch_size, sequence_length, hidden_size] åˆ‡åˆ†ä¸ºè‹¥å¹²å°å—ï¼Œæ¯ä¸€å°å—çš„å¤§å°ä¸º [batch_size, chunk_size, hidden_size]
    2. å¯¹æ¯ä¸ªå°å—å•ç‹¬è¿›è¡Œå‰é¦ˆç½‘ç»œçš„è®¡ç®—ï¼Œå¾—åˆ°å¯¹åº”çš„è¾“å‡º
    3. å°†è¿™äº›å°å—çš„è¾“å‡ºæ‹¼æ¥ï¼ˆconcatï¼‰æˆå®Œæ•´çš„è¾“å‡º [batch_size, sequence_length, hidden_size]

* ã€å…³é”®ç‚¹ï¼šä¸ºä»€ä¹ˆèƒ½è¿™æ ·åšã€‘å‰é¦ˆç½‘ç»œçš„è®¡ç®—æ˜¯ä½ç½®ç‹¬ç«‹çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œåºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„å‰é¦ˆç½‘ç»œè¾“å‡ºåªä¾èµ–äºè¯¥ä½ç½®çš„è¾“å…¥ï¼Œè€Œä¸ä¼šå—å…¶ä»–ä½ç½®çš„å½±å“ã€‚å› æ­¤ï¼Œå°†åºåˆ—æŒ‰å—åˆ†å‰²å¹¶é€å—è®¡ç®—ï¼Œæœ€ç»ˆæ‹¼æ¥èµ·æ¥çš„ç»“æœå’Œä¸€æ¬¡æ€§è®¡ç®—æ•´ä¸ªåºåˆ—çš„ç»“æœæ˜¯æ•°å­¦ç­‰ä»·çš„ã€‚
* ã€å†…å­˜ä¸è®¡ç®—æ—¶é—´çš„æƒè¡¡ã€‘1.å‡å°‘å†…å­˜æ¶ˆè€—ï¼šé€šè¿‡å°†åºåˆ—åˆ†å—ï¼Œåªéœ€è¦ä¸ºæ¯ä¸ªå°å—åˆ†é…å†…å­˜ï¼Œè€Œä¸æ˜¯æ•´ä¸ªåºåˆ—ï¼Œè¿™æ˜¾è‘—å‡å°‘äº†ä¸­é—´åµŒå…¥çš„å†…å­˜å¼€é”€ã€‚2.å¢åŠ è®¡ç®—æ—¶é—´ï¼šè™½ç„¶å†…å­˜å¼€é”€å‡å°‘äº†ï¼Œä½†è®¡ç®—æ—¶é—´ä¼šå¢åŠ ã€‚åŸå› æ˜¯ï¼Œæ¨¡å‹ä¸èƒ½ä¸€æ¬¡æ€§å¹¶è¡Œè®¡ç®—æ‰€æœ‰åºåˆ—ä½ç½®ï¼Œè€Œæ˜¯éœ€è¦é€å—è®¡ç®—ï¼Œè¿™ä¼šå¸¦æ¥é¢å¤–çš„æ—¶é—´å¼€é”€ã€‚
* ã€æ€»ç»“ã€‘Feed Forward Chunking æ˜¯ä¸€ç§é€šè¿‡é€å—è®¡ç®—å‰é¦ˆç½‘ç»œæ¥é™ä½å†…å­˜å ç”¨çš„æŠ€æœ¯ï¼Œå®ƒåœ¨ Transformer æ¨¡å‹ä¸­å°¤ä¸ºé‡è¦ï¼Œç‰¹åˆ«æ˜¯å½“è¾“å…¥åºåˆ—è¾ƒé•¿ã€éšè—å±‚å’Œä¸­é—´å±‚è¾ƒå¤§æ—¶ã€‚è™½ç„¶è¿™ç§æ–¹æ³•ä¼šå¢åŠ è®¡ç®—æ—¶é—´ï¼Œä½†é€šè¿‡é€‚å½“é€‰æ‹© chunk_sizeï¼Œå¯ä»¥åœ¨è®¡ç®—æ—¶é—´å’Œå†…å­˜ä½¿ç”¨ä¹‹é—´æ‰¾åˆ°ä¸€ä¸ªå¹³è¡¡ã€‚


image patch
^^^^^^^^^^^

* Image patch æ˜¯è§†è§‰Transformerï¼ˆVision Transformersï¼ŒViTsï¼‰æ¨¡å‹ä¸­çš„ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼Œç”¨äºå°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºé€‚åˆ Transformer å¤„ç†çš„åºåˆ—æ ¼å¼ã€‚
* åœ¨ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¸­ï¼Œè¾“å…¥å›¾åƒä¼šç»è¿‡å·ç§¯æ ¸ï¼ˆfiltersï¼‰é€å±‚æå–å±€éƒ¨ç‰¹å¾ã€‚CNNæ“…é•¿æ•æ‰å±€éƒ¨çš„ç©ºé—´å…³ç³»ï¼Œä½†åœ¨å»ºæ¨¡è¿œè·ç¦»ä¾èµ–ï¼ˆlong-range dependenciesï¼‰ä¸Šå­˜åœ¨ä¸€å®šçš„é™åˆ¶ã€‚
* Vision Transformer åˆ™å¼•å…¥äº†å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ Transformer ç±»ä¼¼çš„æ¶æ„ï¼Œè¯•å›¾å°†å›¾åƒä¹Ÿå¤„ç†æˆä¸€ç§åºåˆ—åŒ–è¾“å…¥ï¼Œå¹¶é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰æ¥æ•æ‰å…¨å±€å’Œå±€éƒ¨ç‰¹å¾ã€‚
* ã€å®šä¹‰ã€‘åœ¨ Vision Transformer æ¨¡å‹ä¸­ï¼Œå›¾åƒé¦–å…ˆè¢«åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„å°å—ï¼Œç§°ä¸º image patchesï¼Œæ¯ä¸ªå°å—éƒ½æ˜¯å›¾åƒçš„ä¸€ä¸ªå±€éƒ¨åŒºåŸŸã€‚å…·ä½“æ¥è¯´ï¼š

    * 1.åˆ’åˆ†å›¾åƒï¼šVision Transformer å°†ä¸€å¼ å›¾åƒï¼ˆä¾‹å¦‚ 224x224 çš„ RGB å›¾åƒï¼‰æŒ‰ç…§å›ºå®šå¤§å°ï¼ˆä¾‹å¦‚ 16x16ï¼‰çš„ç½‘æ ¼åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ªå°å—ï¼ˆpatchesï¼‰ã€‚å¦‚æœå›¾åƒæ˜¯ 224x224ï¼Œä¸” patch å¤§å°æ˜¯ 16x16ï¼Œé‚£ä¹ˆæœ€ç»ˆä¼šå¾—åˆ° (224 / 16) * (224 / 16) = 14 * 14 = 196 ä¸ªå›¾åƒå—ï¼Œæ¯ä¸ªå›¾åƒå—çš„å°ºå¯¸æ˜¯ 16x16x3ï¼ˆRGB å›¾åƒæœ‰ 3 ä¸ªé€šé“ï¼‰ã€‚
    * 2. çº¿æ€§åµŒå…¥ï¼šæ¯ä¸ª 16x16x3 çš„å›¾åƒå—ä¼šè¢«å±•å¹³æˆä¸€ä¸ªå‘é‡ï¼ˆ1Dï¼‰ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼ˆçº¿æ€§å±‚ï¼‰å°†å…¶æ˜ å°„åˆ°æ¨¡å‹çš„åµŒå…¥ç»´åº¦ï¼ˆembedding dimensionï¼‰ï¼Œè¿™ä¸ä¼ ç»Ÿ Transformer çš„è¯å‘é‡ç±»ä¼¼ã€‚
    * 3. ä½œä¸ºåºåˆ—è¾“å…¥ Transformerï¼šè¿™äº›åµŒå…¥åçš„å›¾åƒå—ï¼ˆpatchesï¼‰è¢«è§†ä¸ºåºåˆ—çš„å…ƒç´ ï¼Œç±»ä¼¼äº NLP ä¸­çš„è¯å‘é‡ï¼Œä½œä¸ºè¾“å…¥åºåˆ—ä¼ é€’ç»™ Transformerã€‚æ¯ä¸ª patch ç›¸å½“äºè¾“å…¥åºåˆ—ä¸­çš„ä¸€ä¸ªâ€œè¯â€ã€‚

* å› æ­¤ï¼ŒVision Transformer æ¨¡å‹å°†ä¸€å¼ äºŒç»´å›¾åƒè½¬æ¢æˆäº†ä¸€ä¸ªåºåˆ—åŒ–çš„è¡¨ç¤ºå½¢å¼ï¼Œå¹¶é€šè¿‡ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œå¤„ç†ã€‚

* ã€1. Patch Size çš„é€‰æ‹©ã€‘Patch size æ˜¯æŒ‡åˆ’åˆ†å›¾åƒæ—¶æ¯ä¸ªå°å—çš„å¤§å°ã€‚è¾ƒå¤§çš„ patch size ä¼šå°†æ›´å¤šçš„å±€éƒ¨ä¿¡æ¯é›†ä¸­åˆ°ä¸€ä¸ª patch ä¸­ï¼Œä½†åŒæ—¶ä¹Ÿä¼šé™ä½å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œå¯èƒ½ä¼šä¸¢å¤±ä¸€äº›ç»†èŠ‚ã€‚è¾ƒå°çš„ patch size åˆ™ä¼šå¢åŠ åºåˆ—é•¿åº¦ï¼ˆæ›´å¤šçš„ patchesï¼‰ï¼Œä½¿å¾— Transformer èƒ½å¤Ÿæ•æ‰æ›´å¤šçš„ç»†èŠ‚ï¼Œä½†ä¹Ÿå¢åŠ äº†è®¡ç®—å¤æ‚åº¦ã€‚ä¾‹å¦‚ï¼Œå¯¹äº 224x224 çš„å›¾åƒï¼Œä½¿ç”¨ 16x16 çš„ patch ä¼šç”Ÿæˆ 196 ä¸ª patchesï¼Œè€Œä½¿ç”¨ 8x8 çš„ patch åˆ™ä¼šç”Ÿæˆ 784 ä¸ª patchesã€‚
* ã€2. çº¿æ€§åµŒå…¥ã€‘æ¯ä¸ª image patch è¢«å±•å¹³æˆä¸€ç»´å‘é‡ï¼Œå¹¶é€šè¿‡çº¿æ€§å±‚å°†å…¶æ˜ å°„åˆ°ä¸€ä¸ªå›ºå®šçš„ç»´åº¦ï¼ˆæ¯”å¦‚ 768 ç»´ï¼‰ï¼Œè¿™ä¸ªæ“ä½œæ˜¯ä¸ºäº†ä½¿æ‰€æœ‰ patches çš„è¡¨ç¤ºç»´åº¦ä¸ Transformer æ¨¡å‹çš„è¾“å…¥ç»´åº¦åŒ¹é…ã€‚
* ã€3. è‡ªæ³¨æ„åŠ›æœºåˆ¶å¦‚ä½•ä½œç”¨äº Patchesã€‘åœ¨ Vision Transformer ä¸­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šå…³æ³¨æ¯ä¸ª patch å’Œå…¶ä»–æ‰€æœ‰ patches ä¹‹é—´çš„å…³ç³»ï¼Œä»è€Œå»ºæ¨¡å…¨å±€ç‰¹å¾ã€‚ä¸ CNN ä¸åŒï¼ŒVision Transformer ä¸å±€é™äºå±€éƒ¨æ„Ÿå—é‡ï¼ˆreceptive fieldï¼‰ï¼Œå®ƒå¯ä»¥ç›´æ¥æ•æ‰å›¾åƒä¸­è¿œè·ç¦»çš„ä¾èµ–å…³ç³»ã€‚
* ã€4. é…ç½®ä¸­çš„ patch_sizeã€‘æ¨¡å‹çš„é…ç½®æ–‡ä»¶é€šå¸¸ä¼šåŒ…å« patch_sizeï¼Œè¿™è¡¨ç¤ºæ¯ä¸ª patch çš„åˆ†è¾¨ç‡ï¼ˆä¾‹å¦‚ 16x16ï¼‰ï¼Œå®ƒç›´æ¥å†³å®šäº†å›¾åƒè¢«åˆ†å‰²çš„ç²’åº¦ã€‚å¯ä»¥ç†è§£ä¸º patch_size æ§åˆ¶äº† Transformer å¤„ç†å›¾åƒçš„â€œå•ä½â€ï¼Œç±»ä¼¼äºåœ¨ NLP ä¸­çš„ tokenï¼ˆè¯å…ƒï¼‰ã€‚

* ã€å›¾åƒå—ï¼ˆpatchesï¼‰çš„é‡è¦æ€§ã€‘

    * å±€éƒ¨ä¸å…¨å±€ç‰¹å¾ç»“åˆï¼šå›¾åƒå—åœ¨æ¨¡å‹ä¸­æ‰®æ¼”äº†è¯å…ƒï¼ˆtokensï¼‰çš„è§’è‰²ï¼Œå®ƒä»¬ä»£è¡¨å›¾åƒçš„å±€éƒ¨ä¿¡æ¯ï¼Œè€Œ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥åœ¨è¿™äº›å±€éƒ¨ä¿¡æ¯ä¹‹é—´è¿›è¡Œå…¨å±€å»ºæ¨¡ã€‚è¿™ç§æœºåˆ¶ä½¿å¾— ViTs èƒ½å¤ŸåŒæ—¶æ•æ‰åˆ°å›¾åƒçš„ç»†èŠ‚ç‰¹å¾ï¼ˆå±€éƒ¨å—ä¹‹é—´çš„å…³ç³»ï¼‰å’Œæ•´ä½“ç»“æ„ï¼ˆä¸åŒå—çš„è¿œè·ç¦»å…³ç³»ï¼‰ã€‚
    * å‡å°‘è®¡ç®—å¤æ‚åº¦ï¼šé€šè¿‡å°†å›¾åƒåˆ’åˆ†ä¸ºè¾ƒå¤§çš„å›¾åƒå—ï¼ŒViT æ¨¡å‹èƒ½å¤Ÿå‡å°‘åºåˆ—é•¿åº¦ï¼Œä»è€Œé™ä½è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦ã€‚åºåˆ—é•¿åº¦è¶ŠçŸ­ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—é‡è¶Šå°ã€‚


* ã€æ€»ç»“ã€‘Image patch æ˜¯ Vision Transformer å°†è¾“å…¥å›¾åƒè½¬æ¢ä¸ºåºåˆ—æ ¼å¼çš„å…³é”®æ­¥éª¤ã€‚é€šè¿‡å°†å›¾åƒåˆ’åˆ†ä¸ºå°çš„å›¾åƒå—ï¼Œå¹¶å°†è¿™äº›å—é€šè¿‡çº¿æ€§åµŒå…¥æ˜ å°„åˆ°å‘é‡è¡¨ç¤ºï¼ŒViT å¯ä»¥åˆ©ç”¨ Transformer çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥æ•æ‰å›¾åƒçš„å±€éƒ¨å’Œå…¨å±€ç‰¹å¾ã€‚Patch size çš„é€‰æ‹©ä¼šç›´æ¥å½±å“æ¨¡å‹çš„è®¡ç®—å¤æ‚åº¦å’Œå¯¹å›¾åƒç»†èŠ‚çš„æ•æ‰èƒ½åŠ›ã€‚






How Transformers solve tasks
----------------------------


.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/ZNMFdF.png

    å¤„ç†è¿‡ç¨‹å›¾: Tokenizer-> Model -> Post-Processing


* ``Wav2Vec2`` for audio classification and automatic speech recognition (ASR)
* ``Vision Transformer (ViT)`` and ``ConvNeXT`` for image classification
* ``DETR`` for object detection
* ``Mask2Former`` for image segmentation
* ``GLPN`` for depth estimation
* ``BERT`` for NLP tasks like text classification, token classification and question answering that use an encoder
* ``GPT2`` for NLP tasks like text generation that use a decoder
* ``BART`` for NLP tasks like summarization and translation that use an encoder-decoder


.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/YtfN6S.jpg

    Vision Transformer

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/NiMpg0.jpg

    Object detection



.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/3getUU.jpg

    Image segmentation

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/TU69J8.jpg

    Depth estimation



The Transformer model family
----------------------------

Computer vision
^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/RYzYvS.jpg


Natural language processing
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/FB5ONz.jpg


Audio
^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/2MKAWt.jpg


Multimodal
^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/tnCfmL.jpg


Reinforcement learning
^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2023/08/eSDNPe.jpg



Summary of the tokenizers
-------------------------


3 tokenization algorithms::

    1. word-based
        very large vocabularies
        large quantity of out-of-vocabulary tokens
        loss of meaning across very similar words
    2. character-based
        very long sequences
        less meaningful individual tokens
    3. subword-based
        principles:
            frequently used words should not be split into subwords
            rare words should be decompose into meaningful subwords


Subword tokenization::

    1. Byte-Pair Encoding (BPE)
        GPT-2
        RoBERTa
    2. WordPiece
        BERT
        DistilBERT
        Electra
    3. Unigram+SentencePiece(é€‚ç”¨äºéç©ºæ ¼åˆ†éš”çš„è¯­è¨€)
        XLNet
        ALBERT
        Marian
        T5



Padding and truncation
----------------------

* The following table summarizes the recommended way to setup padding and truncation

padding strategy(boolean or a string)::

    True or 'longest'
    max_length
    False or 'do_not_pad

truncation strategy(boolean or a string)::

    True or 'longest_first'
    only_second
        å½“è¾“å…¥æ¨¡å‹çš„æ˜¯ä¸¤ä¸ªåºåˆ—çš„é…å¯¹ï¼ˆä¾‹å¦‚ï¼šå¥å­å¯¹ï¼Œæˆ–æ–‡æœ¬å¯¹ï¼‰
        è¿™ç§æˆªæ–­æ–¹å¼é€‚ç”¨äºéœ€è¦ä¿æŒç¬¬ä¸€ä¸ªåºåˆ—å®Œæ•´ï¼Œè€Œå¯¹ç¬¬äºŒä¸ªåºåˆ—è¿›è¡Œè£å‰ªçš„åœºæ™¯ã€‚
    only_first
        ä¸only_secondç›¸å
    False or 'do_not_truncate'


no truncation::

    +-----------------------------------+-----------------------------------------------------------------+
    | no padding                        | tokenizer(batch_sentences)                                      |
    +===================================+=================================================================+
    | padding to max sequence in batch  | tokenizer(batch_sentences, padding=True) or                     |
    +-----------------------------------+-----------------------------------------------------------------+
    |                                   | tokenizer(batch_sentences, padding='longest')                   |
    +-----------------------------------+-----------------------------------------------------------------+
    | padding to max model input length | tokenizer(batch_sentences, padding='max_length')                |
    +-----------------------------------+-----------------------------------------------------------------+
    | padding to specific length        | tokenizer(batch_sentences, padding='max_length', max_length=42) |
    +-----------------------------------+-----------------------------------------------------------------+
    | padding to a multiple of a value  | tokenizer(batch_sentences, padding=True, pad_to_multiple_of=8)  |
    +-----------------------------------+-----------------------------------------------------------------+


truncation to max model input length::

    +-----------------------------------+-----------------------------------------------------------------------+
    | no padding                        | tokenizer(batch_sentences, truncation=True) or                        |
    +===================================+=======================================================================+
    |                                   | tokenizer(batch_sentences, truncation=STRATEGY)                       |
    +-----------------------------------+-----------------------------------------------------------------------+
    | padding to max sequence in batch  | tokenizer(batch_sentences, padding=True, truncation=True) or          |
    +-----------------------------------+-----------------------------------------------------------------------+
    |                                   | tokenizer(batch_sentences, padding=True, truncation=STRATEGY)         |
    +-----------------------------------+-----------------------------------------------------------------------+
    | padding to max model input length | tokenizer(batch_sentences, padding='max_length', truncation=True) or  |
    +-----------------------------------+-----------------------------------------------------------------------+
    |                                   | tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY) |
    +-----------------------------------+-----------------------------------------------------------------------+
    | padding to specific length        | Not possible                                                          |
    +-----------------------------------+-----------------------------------------------------------------------+


truncation to specific length::

    +-----------------------------------+--------------------------------------------------------------------------------------+
    | no padding                        | tokenizer(batch_sentences, truncation=True, max_length=42) or                        |
    +===================================+======================================================================================+
    |                                   | tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)                       |
    +-----------------------------------+--------------------------------------------------------------------------------------+
    | padding to max sequence in batch  | tokenizer(batch_sentences, padding=True, truncation=True, max_length=42) or          |
    +-----------------------------------+--------------------------------------------------------------------------------------+
    |                                   | tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)         |
    +-----------------------------------+--------------------------------------------------------------------------------------+
    | padding to max model input length | Not possible                                                                         |
    +-----------------------------------+--------------------------------------------------------------------------------------+
    | padding to specific length        | tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42) or  |
    +-----------------------------------+--------------------------------------------------------------------------------------+
    |                                   | tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42) |
    +-----------------------------------+--------------------------------------------------------------------------------------+


Model training anatomy
----------------------

* å½“æ¨¡å‹åŠ è½½åˆ° GPU æ—¶ï¼Œ ``CUDA çš„è¿è¡Œæ—¶åº“å’Œå†…æ ¸ï¼ˆkernelsï¼‰`` ä¹Ÿä¼šåŠ è½½ï¼Œè¿™å¯èƒ½ä¼šå ç”¨ 1-2GB å†…å­˜ã€‚ä¸ºäº†æŸ¥çœ‹å®ƒæœ‰å¤šå°‘ï¼Œæˆ‘ä»¬å°†ä¸€ä¸ªå¾®å°çš„å¼ é‡åŠ è½½åˆ° GPU ä¸­ï¼Œè¿™ä¹Ÿä¼šè§¦å‘å†…æ ¸çš„åŠ è½½ã€‚
* å¯ä»¥ç”¨ä¸‹é¢è¯­å¥éªŒè¯::

    import torch
    torch.ones((1, 1)).to("cuda")   # 
    print_gpu_utilization()         # è¿™å„¿åŸºæœ¬å°±æ˜¯å†…æ ¸å ç”¨çš„gpuæ•°

    è¯´æ˜:
        å†…æ ¸å ç”¨å¤šå°‘å’ŒGPUå‹å·æœ‰å…³
        L20(48G):     390M   NVIDIA L20
        V100(32G):    366M   V100-SXM2
            Volta æ¶æ„
        V100(8G):     449M   Tesla P4
            Pascal æ¶æ„


* è¯·æ³¨æ„ï¼Œåœ¨è¾ƒæ–°çš„ GPU ä¸Šï¼Œæ¨¡å‹æœ‰æ—¶ä¼šå ç”¨æ›´å¤šç©ºé—´ï¼Œå› ä¸ºæƒé‡ä»¥ä¼˜åŒ–çš„æ–¹å¼åŠ è½½ï¼Œå¯ä»¥åŠ å¿«æ¨¡å‹çš„ä½¿ç”¨é€Ÿåº¦ã€‚


Anatomy of Modelâ€™s Operations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Transformers æ¶æ„åŒ…æ‹¬ 3 ä¸ªä¸»è¦æ“ä½œç»„ï¼ŒæŒ‰è®¡ç®—å¼ºåº¦åˆ†ç»„å¦‚ä¸‹::

    1. Tensor Contractions
        å¤šå¤´æ³¨æ„åŠ›çš„çº¿æ€§å±‚å’Œç»„ä»¶éƒ½è¿›è¡Œæ‰¹é‡çŸ©é˜µ-çŸ©é˜µä¹˜æ³•ã€‚è¿™äº›æ“ä½œæ˜¯è®­ç»ƒ Transformer æ—¶è®¡ç®—é‡æœ€å¤§çš„éƒ¨åˆ†
    2. Statistical Normalizations
        Softmax å’Œå±‚å½’ä¸€åŒ–æ¯”å¼ é‡æ”¶ç¼©çš„è®¡ç®—å¼ºåº¦è¦å°ï¼Œå¹¶ä¸”æ¶‰åŠä¸€ä¸ªæˆ–å¤šä¸ªå½’çº¦æ“ä½œï¼Œç„¶åé€šè¿‡æ˜ å°„åº”ç”¨å…¶ç»“æœã€‚
    3. Element-wise Operators
        è¿™äº›æ˜¯å‰©ä½™çš„è¿ç®—ç¬¦ï¼šåå·®ã€ä¸¢å¤±ã€æ¿€æ´»å’Œå‰©ä½™è¿æ¥ã€‚è¿™äº›æ˜¯è®¡ç®—å¼ºåº¦æœ€å°çš„æ“ä½œ


.. note:: åœ¨åˆ†ææ€§èƒ½ç“¶é¢ˆæ—¶äº†è§£è¿™äº›çŸ¥è¯†å¾ˆæœ‰å¸®åŠ©ã€‚This summary is derived from `Data Movement Is All You Need: A Case Study on Optimizing Transformers 2020 <https://arxiv.org/abs/2007.00072>`_


Anatomy of Modelâ€™s Memory
^^^^^^^^^^^^^^^^^^^^^^^^^

è®­ç»ƒæ¨¡å‹ä½¿ç”¨çš„å†…å­˜æ¯”ä»…ä»…å°†æ¨¡å‹æ”¾åœ¨ GPU ä¸Šè¦å¤šå¾—å¤šã€‚è¿™æ˜¯å› ä¸ºè®­ç»ƒæœŸé—´æœ‰è®¸å¤šç»„ä»¶ä½¿ç”¨ GPU å†…å­˜ã€‚ GPU å†…å­˜çš„ç»„æˆéƒ¨åˆ†å¦‚ä¸‹::

    1. model weights æ¨¡å‹æƒé‡
    2. optimizer states ä¼˜åŒ–å™¨çŠ¶æ€
    3. gradients æ¸å˜
    4. forward activations saved for gradient computation ä¸ºæ¢¯åº¦è®¡ç®—ä¿å­˜å‰å‘æ¿€æ´»
    5. temporary buffers ä¸´æ—¶ç¼“å†²åŒº
    6. functionality-specific memory ç‰¹å®šåŠŸèƒ½å­˜å‚¨å™¨


details::

    1. Model Weights:
        4 bytes * number of parameters for fp32 training
        6 bytes * number of parameters for mixed precision training (maintains a model in fp32 and one in fp16 in memory)
    2. Optimizer States:
        8 bytes * number of parameters for normal AdamW (maintains 2 states)
        2 bytes * number of parameters for 8-bit AdamW optimizers like bitsandbytes
        4 bytes * number of parameters for optimizers like SGD with momentum (maintains only 1 state)
    3. Gradients
        4 bytes * number of parameters for either fp32 or mixed precision training (gradients are always kept in fp32)
    4. Forward Activations
        size depends on many factors, the key ones being sequence length, hidden size and batch size.
    5. Temporary Memory
        å„ç§ä¸´æ—¶å˜é‡
    6. Functionality-specific memory
        ç‰¹æ®Šçš„å†…å­˜éœ€æ±‚ã€‚ä¾‹å¦‚ï¼Œå½“ä½¿ç”¨é›†æŸæœç´¢ç”Ÿæˆæ–‡æœ¬æ—¶ï¼Œè½¯ä»¶éœ€è¦ç»´æŠ¤è¾“å…¥å’Œè¾“å‡ºçš„å¤šä¸ªå‰¯æœ¬









API
===

MAIN CLASSES
------------

Agents
^^^^^^

three types of Agents::

    1. `HfAgent` uses inference endpoints for opensource models
    2. `LocalAgent` uses a model of your choice locally
    3. `OpenAiAgent` uses OpenAI closed models



Auto Classes
^^^^^^^^^^^^

Generic classes::

    AutoConfig
    AutoModel
    AutoTokenizer

    AutoFeatureExtractor
    AutoImageProcessor
    AutoProcessor

Generic pretraining classes::

    AutoModelForPreTraining

Natural Language Processing::

    AutoModelForCausalLM
    AutoModelForMaskedLM
    AutoModelForMaskGeneration
    AutoModelForSeq2SeqLM
    AutoModelForSequenceClassification
    AutoModelForMultipleChoice
    AutoModelForNextSentencePrediction
    AutoModelForTokenClassification
    AutoModelForQuestionAnswering
    AutoModelForTextEncoding

Computer vision::

    AutoModelForDepthEstimation
    AutoModelForImageClassification
    AutoModelForVideoClassification
    AutoModelForMaskedImageModeling
    AutoModelForObjectDetection
    AutoModelForImageSegmentation
    AutoModelForSemanticSegmentation
    AutoModelForInstanceSegmentation
    AutoModelForUniversalSegmentation
    AutoModelForZeroShotImageClassification
    AutoModelForZeroShotObjectDetection

Audio::

    AutoModelForAudioClassification
    AutoModelForAudioFrameClassification
    AutoModelForCTC
    AutoModelForSpeechSeq2Seq
    AutoModelForAudioXVector


Multimodal::

    AutoModelForTableQuestionAnswering
    AutoModelForDocumentQuestionAnswering
    AutoModelForVisualQuestionAnswering
    AutoModelForVision2Seq

Callbacks
^^^^^^^^^

* The main class that implements callbacks is TrainerCallback.
* By default a Trainer will use the following callbacks::

    `DefaultFlowCallback` which handles the default behavior for logging, saving and evaluation.
    `PrinterCallback` or `ProgressCallback` to display progress and print the logs
        the first one is used if you deactivate tqdm through the TrainingArguments
        otherwise itâ€™s the second one
    ...

Logging
^^^^^^^

æ—¥å¿—é»˜è®¤Warningçº§åˆ«ï¼Œå¯ä»¥è°ƒæ•´æˆinfoçº§åˆ«::

    import transformers
    transformers.logging.set_verbosity_info()

ç¯å¢ƒå˜é‡è®¾ç½®::

    TRANSFORMERS_VERBOSITY

Usage::

    from transformers.utils import logging

    logging.set_verbosity_info()
    logger = logging.get_logger("transformers")
    logger.info("INFO")
    logger.warning("WARN")



















































































































