Transformers 4.45.2
###################


.. note:: æœ¬æ–‡æ¡£æ˜¯è‡ª ``v4.23.1`` ç‰ˆæœ¬åˆ°æœ¬ç‰ˆæœ¬çš„å˜åŠ¨éƒ¨åˆ†

* from: https://huggingface.co/docs/transformers/v4.45.2/en/index


Tutorials
=========






Load pretrained instances with an AutoClass
-------------------------------------------

.. note:: å…¶ä»– AutoXXX å‚è§åŸå§‹transformersæ–‡æ¡£

AutoBackbone
^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/KqpDhA.png

    A Swin backbone with multiple stages for outputting a feature map.

* AutoBackbone å…è®¸æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œä»ä¸»å¹²çš„ä¸åŒé˜¶æ®µè·å–ç‰¹å¾å›¾ã€‚
* from_pretrained() å‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°:
    * out_indices æ˜¯è¦ä»ä¸­è·å–ç‰¹å¾å›¾çš„å±‚çš„ç´¢å¼•
    * out_features æ˜¯è¦ä»ä¸­è·å–ç‰¹å¾å›¾çš„å›¾å±‚çš„åç§°



.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/8tHkPG.png

    A feature map from the first stage of the backbone. The patch partition refers to the model stem.


.. code-block:: python

    from transformers import AutoImageProcessor, AutoBackbone
    import torch
    from PIL import Image
    import requests
    url = "http://images.cocodataset.org/val2017/000000039769.jpg"
    image = Image.open(requests.get(url, stream=True).raw)
    processor = AutoImageProcessor.from_pretrained("microsoft/swin-tiny-patch4-window7-224")
    model = AutoBackbone.from_pretrained("microsoft/swin-tiny-patch4-window7-224", out_indices=(1,))

    inputs = processor(image, return_tensors="pt")
    outputs = model(**inputs)
    feature_maps = outputs.feature_maps

    >> list(feature_maps[0].shape)



Generation with LLMs
--------------------

å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“::

    pip install transformers bitsandbytes>=0.39.0 -q


Generate text
^^^^^^^^^^^^^

* é’ˆå¯¹ ``causal language modeling`` è¿›è¡Œè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†ä¸€ç³»åˆ—æ–‡æœ¬æ ‡è®°ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒã€‚


.. raw:: html

   <figure>  
       <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/genLLM1.mov" width="620"></video>
       <figcaption style="display: block; margin-top: 10px; text-align: center;">ğŸ”µ"Forward pass of an LLM"</figcaption>  
   </figure>

* ä»¥è¿­ä»£æ–¹å¼é‡å¤ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œæ¨¡å‹åº”è¯¥å­¦ä¹ ä½•æ—¶è¾“å‡ºåºåˆ—ç»“æŸ ï¼ˆEOSï¼‰ ä»¤ç‰Œã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œåˆ™å½“è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œç”Ÿæˆå°†åœæ­¢ã€‚

.. raw:: html

   <figure>  
       <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/gen_LLM2.mov" width="620"></video>
       <figcaption style="display: block; margin-top: 10px; text-align: center;">ğŸ”µ"Autoregressive generation iteratively selects the next token from a probability distribution to generate text"</figcaption>  
   </figure>

åŠ è½½æ¨¡å‹::

    from transformers import AutoModelForCausalLM

    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
    )

    è¯´æ˜:
    device_map ç¡®ä¿å°†æ¨¡å‹ç§»åŠ¨åˆ°æ‚¨çš„ GPU
    load_in_4bit åº”ç”¨ 4 ä½åŠ¨æ€é‡åŒ–ï¼Œå¤§å¹…é™ä½èµ„æºéœ€æ±‚

preprocess your text input with a tokenizer::

    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
    model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")

call the generate() method to returns the generated tokens::

    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # 'A list of colors: red, blue, green, yellow, orange, purple, pink,'


æ‰¹å¤„ç†ï¼Œè¿™å°†ä»¥è¾ƒå°çš„å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬å¤§å¤§æé«˜ååé‡::

    tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
    model_inputs = tokenizer(
        ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
    ).to("cuda")
    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    # ['A list of colors: red, blue, green, yellow, orange, purple, pink,',
    #    'Portugal is a country in southwestern Europe, on the Iber']


Common pitfalls
^^^^^^^^^^^^^^^

.. note:: ç”Ÿæˆç­–ç•¥æœ‰å¾ˆå¤šï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹

ç¤ºä¾‹::

    from transformers import AutoModelForCausalLM, AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
    model = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
    )

Generated output is too short/long
""""""""""""""""""""""""""""""""""

* å¦‚æœæœªåœ¨ GenerationConfig æ–‡ä»¶ä¸­æŒ‡å®šï¼Œåˆ™ generate é»˜è®¤æœ€å¤šè¿”å› 20 ä¸ªä»¤ç‰Œ
* å¼ºçƒˆå»ºè®®åœ¨ generate è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½® max_new_tokens ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°ä»¤ç‰Œæ•°ã€‚
  
.. note:: LLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…è§£ç å™¨æ¨¡å‹ï¼‰ä¹Ÿä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚Keep in mind LLMs (more precisely, decoder-only models) also return the input prompt as part of the output.

ç¤ºä¾‹::

    model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

    # By default, the output will contain up to 20 tokens
    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º
    'A sequence of numbers: 1, 2, 3, 4, 5'


    # Setting `max_new_tokens` allows you to control the maximum length
    generated_ids = model.generate(**model_inputs, max_new_tokens=50)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º
    'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'



Incorrect generation mode
"""""""""""""""""""""""""

* é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨ GenerationConfig æ–‡ä»¶ä¸­æŒ‡å®šï¼Œå¦åˆ™ generate åœ¨æ¯æ¬¡è¿­ä»£ï¼ˆè´ªå©ªè§£ç ï¼‰æ—¶é€‰æ‹©æœ€å¯èƒ½çš„ tokenã€‚
* æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œé€‰æ‹©ä¸åŒçš„æ–¹æ³•
* åƒèŠå¤©æœºå™¨äººæˆ–å†™è®ºæ–‡è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡é€‚åˆæŒ‡å®š ``do_sample=True``
* è€ŒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘ç­‰åŸºäºè¾“å…¥çš„ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚
* ç›¸å…³åšå®¢æ–‡ç« : https://huggingface.co/blog/how-to-generate


ç¤ºä¾‹::

    # Set seed for reproducibility -- you don't need this unless you want full reproducibility
    from transformers import set_seed
    set_seed(42)

    model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

    # LLM + greedy decoding = repetitive, boring output
    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º:
    'I am a cat. I am a cat. I am a cat. I am a cat'


    # With sampling, the output becomes more creative!
    generated_ids = model.generate(**model_inputs, do_sample=True)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º
    'I am a cat.  Specifically, I am an indoor-only cat.  I'



Wrong padding side
""""""""""""""""""

* LLMs are decoder-only architectures, meaning they continue to iterate on your input prompt. 
* If your inputs do not have the same length, they need to be padded.(ä¸‹é¢ç¤ºä¾‹é‡Œé¢çš„123å’ŒABCDEé•¿åº¦ä¸åŒ)
* Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded. 
* Make sure you also donâ€™t forget to pass the attention mask to generate!


ç¤ºä¾‹::

    # The tokenizer initialized above has right-padding active by default: the 1st sequence,
    # which is shorter, has padding on the right side. Generation fails to capture the logic.
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="right")  # é»˜è®¤æ˜¯right
    model_inputs = tokenizer(
        ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
    ).to("cuda")
    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º
    # '1, 2, 33333333333'



    # With left-padding, it works as expected!
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
    tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
    model_inputs = tokenizer(
        ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
    ).to("cuda")
    generated_ids = model.generate(**model_inputs)
    tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # è¾“å‡º
    # '1, 2, 3, 4, 5, 6,'



Wrong prompt
""""""""""""

* æŸäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦æŸç§è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚
* æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œè¯·å‚é˜… ``Task Guides -> Prompting -> LLM prompting guide`` ã€‚
* ä¸‹é¢çœ‹ä¸€ä¸ª ``chat templating`` ä¾‹å­(ä½¿ç”¨ ``tokenizer.apply_chat_template()`` å‡½æ•°)

.. code-block:: python

    tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
    model = AutoModelForCausalLM.from_pretrained(
        "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
    )

    set_seed(0)
    prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
    model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
    input_length = model_inputs.input_ids.shape[1]
    generated_ids = model.generate(**model_inputs, max_new_tokens=20)
    print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
    # è¾“å‡º
    # "I'm not a thug, but i can tell you that a human cannot eat"
    # è¯´æ˜
    # Oh no, it did not follow our instruction to reply as a thug!


    # write a better prompt and use the right template for this model
    # (through `tokenizer.apply_chat_template`)
    set_seed(0)
    messages = [
        {
            "role": "system",
            "content": "You are a friendly chatbot who always responds in the style of a thug",
        },
        {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
    ]
    model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
    input_length = model_inputs.shape[1]
    generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
    print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
    # è¾“å‡º
    # 'None, you thug. How bout you try to focus on more useful questions?'
    # è¯´æ˜
    # As we can see, it followed a proper thug style ğŸ˜




Chatting with Transformers
--------------------------


Choosing a chat model
^^^^^^^^^^^^^^^^^^^^^

* â€œ8Bâ€æˆ–â€œ70Bâ€ã€‚è¿™æ˜¯æ¨¡å‹ä¸­çš„å‚æ•°æ•°ã€‚å¦‚æœæ²¡æœ‰é‡åŒ–ï¼Œæ¯ä¸ªå‚æ•°å¤§çº¦éœ€è¦ 2 å­—èŠ‚çš„å†…å­˜ã€‚è¿™æ„å‘³ç€å…·æœ‰ 80 äº¿ä¸ªå‚æ•°çš„â€œ8Bâ€æ¨¡å‹å°†éœ€è¦å¤§çº¦ 16GB çš„å†…å­˜æ¥é€‚åº”å‚æ•°ï¼Œå†åŠ ä¸Šä¸€äº›é¢å¤–çš„å…¶ä»–å¼€é”€ã€‚å®ƒéå¸¸é€‚åˆå…·æœ‰ 24GB æ˜¾å­˜
* â€œMixed of Expertsâ€ æ¨¡å‹ã€‚è¿™äº›å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼åˆ—å‡ºå®ƒä»¬çš„å°ºå¯¸ï¼Œä¾‹å¦‚â€œ8x7Bâ€æˆ–â€œ141B-A35Bâ€ã€‚è¿™é‡Œçš„æ•°å­—æœ‰ç‚¹æ¨¡ç³Šï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸ºæ¨¡å‹åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹å¤§çº¦æœ‰ 56 ï¼ˆ8x7ï¼‰ äº¿ä¸ªå‚æ•°ï¼Œåœ¨ç¬¬äºŒç§æƒ…å†µä¸‹æœ‰ 1410 äº¿ä¸ªå‚æ•°ã€‚







ç›¸å…³ä»£ç 
^^^^^^^^

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch

    # Prepare the input as before
    chat = [
        {"role": "system", "content": "You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986."},
        {"role": "user", "content": "Hey, can you tell me any fun things to do in New York?"}
    ]

    # 1: Load the model and tokenizer
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

    # 2: Apply the chat template
    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    print("Formatted chat:\n", formatted_chat)

    # 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)
    inputs = tokenizer(formatted_chat, return_tensors="pt", add_special_tokens=False)
    # Move the tokenized inputs to the same device the model is on (GPU/CPU)
    inputs = {key: tensor.to(model.device) for key, tensor in inputs.items()}
    print("Tokenized inputs:\n", inputs)

    # 4: Generate text from the model
    outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.1)
    print("Generated tokens:\n", outputs)

    # 5: Decode the output back to a string
    decoded_output = tokenizer.decode(outputs[0][inputs['input_ids'].size(1):], skip_special_tokens=True)
    print("Decoded output:\n", decoded_output)

Performance, memory and hardware
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Memory considerations
"""""""""""""""""""""

* å¤§å¤šæ•°ç°ä»£è¯­è¨€æ¨¡å‹éƒ½ä»¥â€œbfloat16â€ç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªå‚æ•°ä»…ä½¿ç”¨ 2 ä¸ªå­—èŠ‚ï¼Œè€Œä¸ä½¿ç”¨å  4 ä¸ªå­—èŠ‚çš„float32
* ä½¿ç”¨ â€œquantizationâ€ å¯ä»¥é™ä½åˆ° 16 ä½ä»¥ä¸‹ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æŸå‹ç¼©æ¨¡å‹æƒé‡çš„æ–¹æ³•ã€‚è¿™å…è®¸å°†æ¯ä¸ªå‚æ•°å‹ç¼©åˆ° 8 ä½ã€4 ä½ç”šè‡³æ›´å°‘ã€‚


é‡åŒ–::

    from transformers import AutoModelForCausalLM, BitsAndBytesConfig

    quantization_config = BitsAndBytesConfig(load_in_8bit=True)  # You can also try load_in_4bit
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct", device_map="auto", quantization_config=quantization_config)


Performance considerations
""""""""""""""""""""""""""

* èŠå¤©æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ç›¸å¯¹ä¸åŒï¼Œå› ä¸ºå®ƒçš„ç“¶é¢ˆæ˜¯ **å†…å­˜å¸¦å®½** è€Œä¸æ˜¯ **è®¡ç®—èƒ½åŠ›** ï¼Œå› ä¸ºå®ƒå¿…é¡»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ª token ä»å†…å­˜ä¸­è¯»å–æ¯ä¸€ä¸ªactive parameterã€‚è¿™æ„å‘³ç€æ‚¨æ¯ç§’å¯ä»¥ä»èŠå¤©æ¨¡å‹ç”Ÿæˆçš„ token æ•°é‡é€šå¸¸ä¸å®ƒè¿™ä¸ªè¡¨è¾¾å¼æˆæ­£æ¯”ï¼š``å†…å­˜æ€»å¸¦å®½é™¤ä»¥æ¨¡å‹çš„å¤§å°`` ã€‚ä¸€ä¸ª8Bçš„æ¨¡å‹ï¼Œä»¥ bfloat16 ç²¾åº¦åŠ è½½æ—¶ï¼Œæ¨¡å‹å¤§å°ä¸º ~16GBã€‚è¿™æ„å‘³ç€å¿…é¡»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ªä»¤ç‰Œä»å†…å­˜ä¸­è¯»å– 16GBã€‚
* æ€»å†…å­˜å¸¦å®½ä»æ¶ˆè´¹ç±» CPU çš„ ``20-100GB/ç§’`` åˆ°æ¶ˆè´¹ç±» GPUã€Intel Xeonã€AMD Threadripper/Epyc æˆ–é«˜ç«¯ Apple Silicon ç­‰ä¸“ç”¨ CPU çš„ ``200-900GB/ç§’`` ä¸ç­‰ï¼Œæœ€åé«˜è¾¾ ``2-3TB/ç§’`` çš„æ•°æ®ä¸­å¿ƒ GPUï¼Œå¦‚ Nvidia A100 æˆ– H100ã€‚è¿™åº”è¯¥å¯ä»¥è®©æ‚¨å¾ˆå¥½åœ°äº†è§£è¿™äº›ä¸åŒç¡¬ä»¶ç±»å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚
* assisted generationçš„å˜ä½“ï¼šä¹Ÿç§°ä¸º â€œæ¨æµ‹æ€§é‡‡æ ·(speculative sampling)â€ï¼Œé€šå¸¸ä½¿ç”¨è¾ƒå°çš„â€œè‰ç¨¿æ¨¡å‹(draft model)â€å°è¯•ä¸€æ¬¡çŒœæµ‹å¤šä¸ªæœªæ¥çš„ tokenï¼Œç„¶åç”¨èŠå¤©æ¨¡å‹ç¡®è®¤è¿™äº›generationsã€‚å¦‚æœé€šè¿‡èŠå¤©æ¨¡å‹éªŒè¯äº†çŒœæµ‹ç»“æœï¼Œåˆ™æ¯æ¬¡forward passå¯ä»¥ç”Ÿæˆå¤šä¸ª Tokenï¼Œå¤§å¤§ç¼“è§£äº†å¸¦å®½ç“¶é¢ˆï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚
* MoE æ¨¡å‹ï¼šå‡ ç§æµè¡Œçš„èŠå¤©æ¨¡å‹ï¼Œå¦‚ Mixtralã€Qwen-MoE å’Œ DBRXï¼Œéƒ½æ˜¯ MoE æ¨¡å‹ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œå¹¶éæ¯ä¸ªå‚æ•°å¯¹äºç”Ÿæˆçš„æ¯ä¸ª Token éƒ½å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚å› æ­¤ï¼ŒMoE æ¨¡å‹é€šå¸¸å…·æœ‰ä½å¾—å¤šçš„å†…å­˜å¸¦å®½è¦æ±‚ï¼Œå³ä½¿å®ƒä»¬çš„æ€»å¤§å°å¯èƒ½ç›¸å½“å¤§ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯ä»¥æ¯”ç›¸åŒå¤§å°çš„æ™®é€š â€œå¯†é›†â€ æ¨¡å‹å¿«å‡ å€ã€‚ç„¶è€Œï¼Œåƒè¾…åŠ©ç”Ÿæˆ(assisted generation)è¿™æ ·çš„æŠ€æœ¯é€šå¸¸å¯¹è¿™äº›æ¨¡å‹æ— æ•ˆï¼Œå› ä¸ºæ¯ä¸ªæ–°çš„æ¨æµ‹ä»¤ç‰Œéƒ½ä¼šæœ‰æ›´å¤šçš„å‚æ•°å˜å¾—æ´»è·ƒï¼Œè¿™å°†æŠµæ¶ˆ MoE æ¶æ„æä¾›çš„å¸¦å®½å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚








TASK GUIDES
===========



COMPUTER VISION
---------------


Image-to-Image
^^^^^^^^^^^^^^

::

    image enhancement (super resolution, low light enhancement, deraining and so on)
        å›¾åƒå¢å¼º(è¶…åˆ†è¾¨ç‡ã€å¼±å…‰å¢å¼ºã€å»æ±¡ç­‰)
    image inpainting
        å›¾åƒä¿®å¤

Image Feature Extraction
^^^^^^^^^^^^^^^^^^^^^^^^

::

    image similarity
        å›¾åƒç›¸ä¼¼åº¦
    image retrieval
        å›¾åƒæ£€ç´¢

* remove the task-specific head (image classification, object detection etc) and get the features
* These features are very useful on a higher level: edge detection, corner detection and so on. 
* They may also contain information about the real world (e.g. what a cat looks like) depending on how deep the model is. 
* Therefore, these outputs can be used to train new classifiers on a specific dataset.


Mask Generation
^^^^^^^^^^^^^^^

* **Mask Generation** ä¸ºå›¾åƒç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ©æ¨¡çš„ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡ä¸å›¾åƒåˆ†å‰²éå¸¸ç›¸ä¼¼ï¼Œä½†å­˜åœ¨è®¸å¤šå·®å¼‚ã€‚å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä»…é™äºå®ƒä»¬åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°çš„ç±»ï¼›ç»™å®šå›¾åƒï¼Œå®ƒä»¬è¿”å›ä¸€ç»„æ©ç å’Œç›¸åº”çš„ç±»ã€‚
* Mask generation is the task of generating semantically meaningful masks for an image. This task is very similar to image segmentation, but many differences exist. 
* ``Image segmentation`` models are trained on labeled datasets and are limited to the classes they have seen during training; they return a set of masks and corresponding classes, given an image.

Mask generation models are trained on large amounts of data and operate in two modes::

    1. Prompting mode: 
        æ¨¡å‹æ¥æ”¶å›¾åƒå’Œæç¤ºï¼Œå…¶ä¸­æç¤ºå¯ä»¥æ˜¯å¯¹è±¡å†…å›¾åƒä¸­çš„ 2D ç‚¹ä½ç½®ï¼ˆXY åæ ‡ï¼‰æˆ–å¯¹è±¡å‘¨å›´çš„è¾¹ç•Œæ¡†ã€‚
        åœ¨æç¤ºæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä»…è¿”å›æç¤ºæ‰€æŒ‡å‘çš„å¯¹è±¡ä¸Šçš„mask
    2. Segment Everything mode:
        ç»™å®šä¸€å¼ å›¾åƒï¼Œæ¨¡å‹ä¼šç”Ÿæˆå›¾åƒä¸­çš„æ¯ä¸ªè’™ç‰ˆã€‚
        ä¸ºæ­¤ï¼Œå°†ç”Ÿæˆä¸€ä¸ªç‚¹ç½‘æ ¼å¹¶å°†å…¶å åŠ åœ¨å›¾åƒä¸Šä»¥è¿›è¡Œæ¨ç†ã€‚



.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/hvrrET.png

::

    Point Prompting
    Box Prompting


.. note:: å…·ä½“ç»†çœ‹æ–‡æ¡£å§(æœ‰ç©ºè¿è¡Œä¸€ä¸‹ç›¸å…³ä»£ç å†ç»†åˆ†æå§)



Keypoint Detection
^^^^^^^^^^^^^^^^^^

* å…³é”®ç‚¹æ£€æµ‹è¯†åˆ«å¹¶å®šä½å›¾åƒä¸­çš„ç‰¹å®šå…´è¶£ç‚¹ã€‚è¿™äº›å…³é”®ç‚¹ä¹Ÿç§°ä¸ºåœ°æ ‡ï¼Œä»£è¡¨å¯¹è±¡çš„æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œä¾‹å¦‚é¢éƒ¨ç‰¹å¾æˆ–å¯¹è±¡éƒ¨åˆ†ã€‚
* Keypoint detection identifies and locates specific points of interest within an image. These keypoints, also known as landmarks, represent meaningful features of objects, such as facial features(é¢éƒ¨ç‰¹å¾) or object parts(å¯¹è±¡éƒ¨ä½).

These models take an image input and return the following outputs::

    1. Keypoints and Scores: 
        å…´è¶£ç‚¹åŠå…¶ç½®ä¿¡åº¦åˆ†æ•°
        Points of interest and their confidence scores.
    2. Descriptors: 
        æ¯ä¸ªå…³é”®ç‚¹å‘¨å›´çš„å›¾åƒåŒºåŸŸçš„è¡¨ç¤ºå½¢å¼ï¼Œæ•è·å…¶çº¹ç†ã€æ¸å˜ã€æ–¹å‘å’Œå…¶ä»–å±æ€§
        A representation of the image region surrounding each keypoint, 
            capturing its texture, gradient, orientation and other properties.

.. figure:: https://img.zhaoweiguo.com/uPic/2024/11/KEzcue.png




Knowledge Distillation for Computer Vision
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). 



MULTIMODAL
----------

* Visual Question Answering: åŸºäºå›¾åƒå›ç­”å¼€æ”¾å¼é—®é¢˜çš„ä»»åŠ¡ã€‚æ”¯æŒæ­¤ä»»åŠ¡çš„æ¨¡å‹çš„è¾“å…¥é€šå¸¸æ˜¯å›¾åƒå’Œé—®é¢˜çš„ç»„åˆï¼Œè¾“å‡ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„ç­”æ¡ˆã€‚
* Image-text-to-tex: ä¹Ÿç§°ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ (VLM: vision language models)ï¼Œæ˜¯é‡‡ç”¨å›¾åƒè¾“å…¥çš„è¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œä»è§†è§‰é—®ç­”(visual question answering)åˆ°å›¾åƒåˆ†å‰²(image segmentation)ã€‚æ­¤ä»»åŠ¡ä¸å›¾åƒåˆ°æ–‡æœ¬(image-to-text)æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ï¼Œå¹¶ä¸”åœ¨ä¸€äº›ä½¿ç”¨åœºæ™¯ä¸Šæœ‰é‡å ï¼Œå¦‚ï¼šå›¾åƒå­—å¹•(image captioning)ã€‚å›¾åƒåˆ°æ–‡æœ¬(Image-to-text)æ¨¡å‹ä»…æ¥å—å›¾åƒè¾“å…¥å¹¶ä¸”é€šå¸¸å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œè€Œ VLM æ¥å—å¼€æ”¾å¼æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶ä¸”æ˜¯æ›´é€šç”¨çš„æ¨¡å‹ã€‚
* Video-text-to-text: ä¹Ÿç§°ä¸ºè§†é¢‘è¯­è¨€æ¨¡å‹(video language models)æˆ–å…·æœ‰è§†é¢‘è¾“å…¥çš„è§†è§‰è¯­è¨€æ¨¡å‹(vision language models with video input)ï¼Œæ˜¯é‡‡ç”¨è§†é¢‘è¾“å…¥çš„è¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œä»è§†é¢‘é—®ç­”(video question answering)åˆ°è§†é¢‘å­—å¹•(video captioning)ã€‚





Generation
----------


Text generation strategies
^^^^^^^^^^^^^^^^^^^^^^^^^^

* The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy that the generate() method will use.
* é€‰æ‹©è¾“å‡ºtokenæ¥ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ç§°ä¸º decoding ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰generate()æ–¹æ³•å°†ä½¿ç”¨çš„è§£ç ç­–ç•¥(decoding strategy)
* ä¿®æ”¹è§£ç ç­–ç•¥ä¸ä¼šæ”¹å˜ä»»ä½•å¯è®­ç»ƒå‚æ•°çš„å€¼ã€‚ä½†æ˜¯ï¼Œå®ƒä¼šå¯¹ç”Ÿæˆçš„è¾“å‡ºçš„è´¨é‡äº§ç”Ÿæ˜¾ç€å½±å“ã€‚


Default text generation configuration
"""""""""""""""""""""""""""""""""""""

å½“æ‚¨æ˜¾å¼åŠ è½½æ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ ``model.generation_config`` æ£€æŸ¥æ¨¡å‹é™„å¸¦çš„ç”Ÿæˆé…ç½®::

    >>> from transformers import AutoModelForCausalLM
    >>> model = AutoModelForCausalLM.from_pretrained("distilbert/distilgpt2")
    >>> model.generation_config
    GenerationConfig {
      "bos_token_id": 50256,
      "eos_token_id": 50256
    }
    <BLANKLINE>

.. note:: æ‰“å°model.generation_configä»…æ˜¾ç¤ºä¸é»˜è®¤ç”Ÿæˆé…ç½®ä¸åŒçš„å€¼ï¼Œå¹¶ä¸”ä¸åˆ—å‡ºä»»ä½•é»˜è®¤å€¼ã€‚

default generation configuration::

    input prompt to a maximum: 20 token
    default decoding strategy is greedy search

Customize text generation
"""""""""""""""""""""""""

é€šè¿‡å°†å‚æ•°åŠå…¶å€¼ç›´æ¥ä¼ é€’ç»™generateæ–¹æ³•æ¥è¦†ç›–ä»»ä½•generation_config::

    my_model.generate(**inputs, num_beams=4, do_sample=True)

ç»å¸¸è°ƒæ•´çš„å‚æ•°::

    1. max_new_tokens: 
        è¦ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°(the maximum number of tokens to generate)
        è¾“å‡ºåºåˆ—çš„å¤§å°ï¼Œä¸åŒ…æ‹¬è¾“å…¥prompt
    2. num_beams: 
        é€šè¿‡æŒ‡å®šå¤§äº 1 çš„æ³¢æŸæ•°é‡ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆåœ°ä»è´ªå©ªæœç´¢åˆ‡æ¢åˆ°æ³¢æŸæœç´¢
    3. do_sample: 
        å¦‚æœè®¾ç½®ä¸ºTrue ï¼Œæ­¤å‚æ•°å¯ç”¨è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚å¤šé¡¹å¼é‡‡æ ·ã€æ³¢æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€Top-K é‡‡æ ·å’Œ Top-p é‡‡æ ·ã€‚
    4. num_return_sequences: 
        æ¯ä¸ªè¾“å…¥è¿”å›çš„åºåˆ—å€™é€‰æ•°
        è¯¥é€‰é¡¹ä»…é€‚ç”¨äºæ”¯æŒå¤šä¸ªåºåˆ—å€™é€‰çš„è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚æ³¢æŸæœç´¢(beam_search)å’Œ é‡‡æ ·(sampling)
        è´ªå©ªæœç´¢(greedy_search)å’Œå¯¹æ¯”æœç´¢(contrastive_search)ç­‰è§£ç ç­–ç•¥è¿”å›å•ä¸ªè¾“å‡ºåºåˆ—


Save a custom decoding strategy with your model
"""""""""""""""""""""""""""""""""""""""""""""""

specific generation configuration::

    from transformers import AutoModelForCausalLM, GenerationConfig

    model = AutoModelForCausalLM.from_pretrained("my_account/my_model")
    generation_config = GenerationConfig(
        max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id
    )
    generation_config.save_pretrained("my_account/my_model")


å¦‚æœæ‚¨æƒ³ä¸ºå•ä¸ªæ¨¡å‹å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ç§ç”¨äºé€šè¿‡é‡‡æ ·ç”Ÿæˆåˆ›æ„æ–‡æœ¬ï¼Œä¸€ç§ç”¨äºé€šè¿‡é›†æŸæœç´¢è¿›è¡Œæ‘˜è¦ï¼‰æ—¶ä¼šå¾ˆæœ‰ç”¨::

    # ä½¿ç”¨GenerationConfig.save_pretrained()ä¸­çš„config_file_nameå‚æ•°å°†å¤šä¸ªç”Ÿæˆé…ç½®å­˜å‚¨åœ¨å•ä¸ªç›®å½•ä¸­ã€‚
    # ä½¿ç”¨GenerationConfig.from_pretrained()å®ä¾‹åŒ–å®ƒä»¬

    from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig

    tokenizer = AutoTokenizer.from_pretrained("google-t5/t5-small")
    model = AutoModelForSeq2SeqLM.from_pretrained("google-t5/t5-small")

    translation_generation_config = GenerationConfig(
        num_beams=4,
        early_stopping=True,
        decoder_start_token_id=0,
        eos_token_id=model.config.eos_token_id,
        pad_token=model.config.pad_token_id,
    )

    # è¯´æ˜ï¼šé€šè¿‡æŒ‡ä»¤é…ç½®æ–‡ä»¶åæŠŠç›¸å…³é…ç½®å†™å…¥åˆ°æŒ‡å®šæ–‡ä»¶å’Œä»æŒ‡å®šæ–‡ä»¶åŠ è½½
    translation_generation_config.save_pretrained("/tmp", "translation_generation_config.json")
    generation_config = GenerationConfig.from_pretrained("/tmp", "translation_generation_config.json")
    inputs = tokenizer("translate English to French: Configuration files are easy to use!", return_tensors="pt")
    outputs = model.generate(**inputs, generation_config=generation_config)
    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))

Streaming
"""""""""

.. warning:: æµåª’ä½“ç±»çš„ API ä»åœ¨å¼€å‘ä¸­ï¼Œå°†æ¥å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

    tok = AutoTokenizer.from_pretrained("openai-community/gpt2")
    model = AutoModelForCausalLM.from_pretrained("openai-community/gpt2")
    inputs = tok(["An increasing sequence: one,"], return_tensors="pt")
    streamer = TextStreamer(tok)

    # Despite returning the usual output, the streamer will also print the generated text to stdout.
    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)
    # è¾“å‡º:
    An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,


Watermarking
""""""""""""

* è®ºæ–‡: On the Reliability of Watermarks for Large Language Models: https://arxiv.org/abs/2306.04634


Decoding strategies
"""""""""""""""""""

* å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†: https://huggingface.co/blog/how-to-generate
* å½±å“æ¨¡å‹çš„generate()ç»“æœæœ‰2
* 1. è§£ç ç­–ç•¥(decoding strategies)ä¸»è¦åŸºäº Logits(ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå› æ­¤é€‰æ‹©ä¸€ä¸ªå¥½çš„ Logitsæ“ä½œç­–ç•¥(logits manipulation strategy)å¯ä»¥å¤§æœ‰å¸®åŠ©ï¼
* 2. é™¤äº†é€‰æ‹©è§£ç ç­–ç•¥ä¹‹å¤–ï¼Œæ“ä½œé€»è¾‘(manipulating the logits)æ˜¯æ‚¨å¯ä»¥é‡‡å–çš„å¦ä¸€ä¸ªæ–¹æ³•ã€‚æµè¡Œçš„ logits æ“ä½œç­–ç•¥åŒ…æ‹¬top_p ã€ min_på’Œrepetition_penalty


Greedy Search
+++++++++++++

* é»˜è®¤ä½¿ç”¨è´ªå©ªæœç´¢è§£ç ï¼Œå› æ­¤æ‚¨ä¸å¿…ä¼ é€’ä»»ä½•å‚æ•°æ¥å¯ç”¨å®ƒã€‚
* è¿™æ„å‘³ç€å‚æ•°num_beamsè®¾ç½®ä¸º 1 ä¸”do_sample=False ã€‚

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer
    prompt = "I look forward to"
    checkpoint = "distilbert/distilgpt2"

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForCausalLM.from_pretrained(checkpoint)
    outputs = model.generate(**inputs)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)
    # è¾“å‡º
    ['I look forward to seeing you all again!\n\n\n\n\n\n\n\n\n\n\n']



Contrastive search
++++++++++++++++++

* è®ºæ–‡A Contrastive Framework for Neural Text Generation: https://arxiv.org/abs/2202.06417
* å¯¹æ¯”æœç´¢çš„å·¥ä½œåŸç†: https://huggingface.co/blog/introducing-csearch
* å¯ç”¨å’Œæ§åˆ¶å¯¹æ¯”æœç´¢è¡Œä¸ºçš„ä¸¤ä¸ªä¸»è¦å‚æ•°æ˜¯penalty_alphaå’Œtop_k

.. code-block:: python

    from transformers import AutoTokenizer, AutoModelForCausalLM

    checkpoint = "openai-community/gpt2-large"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForCausalLM.from_pretrained(checkpoint)

    prompt = "Hugging Face Company is"
    inputs = tokenizer(prompt, return_tensors="pt")

    outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)
    # ['Hugging Face Company is a family owned and operated business. .... We look forward to hearing from you!']


Multinomial sampling
++++++++++++++++++++

* ä¸æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°çš„è´ªå©ªæœç´¢ç›¸åï¼Œå¤šé¡¹å¼é‡‡æ ·ï¼ˆä¹Ÿç§°ä¸ºç¥–å…ˆé‡‡æ ·ï¼‰æ ¹æ®æ¨¡å‹ç»™å‡ºçš„æ•´ä¸ªè¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚
* æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„ä»¤ç‰Œéƒ½æœ‰è¢«é€‰æ‹©çš„æœºä¼šï¼Œä»è€Œé™ä½äº†é‡å¤çš„é£é™©ã€‚
* è®¾ç½®do_sample=Trueå’Œnum_beams=1

.. code-block:: python

    from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
    set_seed(0)  # For reproducibility

    checkpoint = "openai-community/gpt2-large"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForCausalLM.from_pretrained(checkpoint)

    prompt = "Today was an amazing day because"
    inputs = tokenizer(prompt, return_tensors="pt")

    outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)


Beam-search decoding
++++++++++++++++++++

* ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œæ³¢æŸæœç´¢è§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™å¤šä¸ªå‡è®¾ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—æ€»ä½“æ¦‚ç‡æœ€é«˜çš„å‡è®¾ã€‚
* è¿™æ ·åšçš„ä¼˜ç‚¹æ˜¯å¯ä»¥è¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å§‹çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œå¹¶ä¸”ä¼šè¢«è´ªå©ªæœç´¢å¿½ç•¥ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/XNPDom.png


* äº¤äº’å¼æ¼”ç¤º: https://huggingface.co/spaces/m-ric/beam_search_visualizer
* è¦å¯ç”¨æ­¤è§£ç ç­–ç•¥ï¼Œè¯·æŒ‡å®šå¤§äº 1 çš„num_beams ï¼ˆä¹Ÿç§°ä¸ºè¦è·Ÿè¸ªçš„å‡è®¾æ•°ï¼‰

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer

    prompt = "It is astonishing how one can"
    checkpoint = "openai-community/gpt2-medium"

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForCausalLM.from_pretrained(checkpoint)

    outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)


Beam-search multinomial sampling
++++++++++++++++++++++++++++++++

* è¿™ç§è§£ç ç­–ç•¥å°†æ³¢æŸæœç´¢ä¸å¤šé¡¹å¼é‡‡æ ·ç›¸ç»“åˆã€‚
* æŒ‡å®šnum_beamså¤§äº 1ï¼Œå¹¶è®¾ç½®do_sample=Trueæ‰èƒ½ä½¿ç”¨æ­¤è§£ç ç­–ç•¥ã€‚

.. code-block:: python

    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed
    set_seed(0)  # For reproducibility

    prompt = "translate English to German: The house is wonderful."
    checkpoint = "google-t5/t5-small"

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

    outputs = model.generate(**inputs, num_beams=5, do_sample=True)
    tokenizer.decode(outputs[0], skip_special_tokens=True)


Diverse beam search decoding
++++++++++++++++++++++++++++

* å¤šæ ·åŒ–æ³¢æŸæœç´¢è§£ç ç­–ç•¥æ˜¯æ³¢æŸæœç´¢ç­–ç•¥çš„æ‰©å±•ï¼Œå…è®¸ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ³¢æŸåºåˆ—é›†ä»¥ä¾›é€‰æ‹©ã€‚
* å·¥ä½œåŸç†ï¼Œè¯·å‚é˜…Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models: https://arxiv.org/pdf/1610.02424.pdf
* è¯¥æ–¹æ³•å…·æœ‰ä¸‰ä¸ªä¸»è¦å‚æ•°ï¼š ``num_beams`` ã€ ``num_beam_groups`` å’Œ ``diversity_penalty`` 
* å¤šæ ·æ€§æƒ©ç½šç¡®ä¿è¾“å‡ºåœ¨ç»„ä¹‹é—´æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç»„å†…ä½¿ç”¨æ³¢æŸæœç´¢ã€‚


.. code-block:: python

    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

    checkpoint = "google/pegasus-xsum"
    prompt = (
        "The Permaculture Design Principles are a set of universal design principles "
        ...
        "efficient way possible."
    )

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

    outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)
    tokenizer.decode(outputs[0], skip_special_tokens=True)


Speculative Decoding
++++++++++++++++++++

* æ¨æµ‹è§£ç ï¼ˆä¹Ÿç§°ä¸ºè¾…åŠ©è§£ç ï¼‰æ˜¯ä¸Šè¿°è§£ç ç­–ç•¥çš„ä¸€ç§ä¿®æ”¹ï¼Œå®ƒä½¿ç”¨è¾…åŠ©æ¨¡å‹ï¼ˆæœ€å¥½æ˜¯æ›´å°çš„æ¨¡å‹ï¼‰æ¥ç”Ÿæˆä¸€äº›å€™é€‰æ ‡è®°ã€‚
* ç„¶åï¼Œä¸»æ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰æ ‡è®°ï¼Œä»è€ŒåŠ å¿«è§£ç è¿‡ç¨‹ã€‚
* å¦‚æœdo_sample=True ï¼Œåˆ™ä½¿ç”¨æ¨æµ‹è§£ç è®ºæ–‡ä¸­å¼•å…¥çš„å¸¦æœ‰é‡é‡‡æ ·çš„ä»¤ç‰ŒéªŒè¯ã€‚
* è¾…åŠ©è§£ç å‡è®¾ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ç›¸åŒçš„åˆ†è¯å™¨ï¼Œå¦åˆ™ï¼Œè¯·å‚é˜…ä¸‹é¢çš„é€šç”¨è¾…åŠ©è§£ç ã€‚
* ç›®å‰è¾…åŠ©è§£ç ä»…æ”¯æŒè´ªå©ªæœç´¢å’Œé‡‡æ ·ï¼Œè¾…åŠ©è§£ç ä¸æ”¯æŒæ‰¹é‡è¾“å…¥ã€‚
* è¦äº†è§£æœ‰å…³è¾…åŠ©è§£ç çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤åšå®¢æ–‡ç« : https://huggingface.co/blog/assisted-generation


* è¦å¯ç”¨è¾…åŠ©è§£ç ï¼Œè¯·ä½¿ç”¨æ¨¡å‹è®¾ç½®assistant_modelå‚æ•°ã€‚

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer

    prompt = "Alice and Bob"
    checkpoint = "EleutherAI/pythia-1.4b-deduped"
    assistant_checkpoint = "EleutherAI/pythia-160m-deduped"

    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForCausalLM.from_pretrained(checkpoint)
    assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
    outputs = model.generate(**inputs, assistant_model=assistant_model)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)


Universal Assisted Decoding
+++++++++++++++++++++++++++

* é€šç”¨è¾…åŠ©è§£ç  (UAD) æ·»åŠ äº†å¯¹å…·æœ‰ä¸åŒæ ‡è®°å™¨çš„ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹çš„æ”¯æŒã€‚
* è¦ä½¿ç”¨å®ƒï¼Œåªéœ€ä½¿ç”¨tokenizerå’Œassistant_tokenizerå‚æ•°ä¼ é€’æ ‡è®°å™¨ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚
* åœ¨å†…éƒ¨ï¼Œä¸»æ¨¡å‹è¾“å…¥æ ‡è®°è¢«é‡æ–°ç¼–ç ä¸ºè¾…åŠ©æ¨¡å‹æ ‡è®°ï¼Œç„¶ååœ¨è¾…åŠ©ç¼–ç ä¸­ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œè¿™äº›å€™é€‰æ ‡è®°åˆè¢«é‡æ–°ç¼–ç ä¸ºä¸»æ¨¡å‹å€™é€‰æ ‡è®°ã€‚ç„¶åéªŒè¯æŒ‰ç…§ä¸Šé¢çš„è§£é‡Šè¿›è¡Œã€‚é‡æ–°ç¼–ç æ­¥éª¤æ¶‰åŠå°†ä»¤ç‰Œ ID è§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸åŒçš„ä»¤ç‰Œç”Ÿæˆå™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ç”±äºé‡æ–°ç¼–ç ä»¤ç‰Œå¯èƒ½ä¼šå¯¼è‡´ä»¤ç‰ŒåŒ–å·®å¼‚ï¼Œå› æ­¤ UAD ä¼šæ‰¾åˆ°æºç¼–ç å’Œç›®æ ‡ç¼–ç ä¹‹é—´çš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼Œä»¥ç¡®ä¿æ–°ä»¤ç‰ŒåŒ…å«æ­£ç¡®çš„æç¤ºåç¼€ã€‚


* å¦‚æœä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ä¸åŒçš„æ ‡è®°å™¨ï¼Œè¯·ä½¿ç”¨é€šç”¨è¾…åŠ©è§£ç ã€‚

.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer

    prompt = "Alice and Bob"
    checkpoint = "google/gemma-2-9b"
    assistant_checkpoint = "double7/vicuna-68m"

    assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    inputs = tokenizer(prompt, return_tensors="pt")

    model = AutoModelForCausalLM.from_pretrained(checkpoint)
    assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)
    outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)
    tokenizer.batch_decode(outputs, skip_special_tokens=True)


DoLa Decoding
+++++++++++++

* Decoding by Contrasting Layers (DoLa) æ˜¯ä¸€ç§å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜äº‹å®æ€§å¹¶å‡å°‘LLMsçš„å¹»è§‰
* å¦‚ ICLR 2024 DoLa çš„è®ºæ–‡æ‰€è¿°ï¼šDecoding by Contrasting Layers Improves Factuality in Large Language Models: https://arxiv.org/abs/2309.03883
* DoLa æ˜¯é€šè¿‡å¯¹æ¯”æœ€ç»ˆå±‚ä¸æ—©æœŸå±‚è·å¾—çš„ logits å·®å¼‚æ¥å®ç°çš„ï¼Œä»è€Œæ”¾å¤§äº†å˜å‹å™¨å±‚ç‰¹å®šéƒ¨åˆ†çš„äº‹å®çŸ¥è¯†ã€‚

.. note:: æ›´è¯¦ç»†çš„å†ç»†çœ‹å—


Best Practices for Generation with Cache
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


What is Cache and why we should care
""""""""""""""""""""""""""""""""""""

* KV Cache
* æ¨¡å‹ä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªtokenï¼Œå¹¶ä¸”æ¯ä¸ªæ–°é¢„æµ‹éƒ½å–å†³äºå…ˆå‰çš„ä¸Šä¸‹æ–‡

.. code-block:: python

    past_key_values = None # past_key_values is the key-value cache
    generated_tokens = []
    next_token_id = tokenizer(prompt, return_tensors="pt")["input_ids"].to("cuda")

    for _ in range(5):
      next_logits, past_key_values = model(next_token_id, past_key_values=past_key_values, use_cache=True).to_tuple()
      next_logits = next_logits[:, -1:]
      next_token_id = torch.argmax(next_logits, dim=-1)

      print("shape of input_ids", next_token_id.shape)
      print("length of key-value cache", len(past_key_values[0][0]))  # past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]
      generated_tokens.append(next_token_id.item())

    generated_text = tokenizer.batch_decode(generated_tokens)
    generated_text

    # è¾“å‡º
    shape of input_ids torch.Size([1, 1])
    length of key-value cache 20
    shape of input_ids torch.Size([1, 1])
    length of key-value cache 21
    shape of input_ids torch.Size([1, 1])
    length of key-value cache 22
    shape of input_ids torch.Size([1, 1])
    length of key-value cache 23
    shape of input_ids torch.Size([1, 1])
    length of key-value cache 24
    [' Here', ' is', ' a', ' Python', ' function']



* è¿™æ„å‘³ç€ï¼Œè¦åœ¨ Generation ä¸­é¢„æµ‹ç¼–å·ä¸º 1000 çš„tokenï¼Œæ‚¨éœ€è¦æ¥è‡ªä¹‹å‰ 999 ä¸ªtokençš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä»¥ token å½¢å¼çš„çŸ©é˜µä¹˜æ³•è®¡ç®—ã€‚
* ä½†æ˜¯è¦é¢„æµ‹ä»¤ç‰Œç¼–å· 1001ï¼Œæ‚¨è¿˜éœ€è¦å‰ 999 ä¸ªä»¤ç‰Œä¸­çš„ç›¸åŒä¿¡æ¯ï¼Œä»¥åŠä»¤ç‰Œç¼–å· 1000 ä¸­çš„é™„åŠ ä¿¡æ¯ã€‚
* è¿™å°±æ˜¯ä½¿ç”¨é”®å€¼ç¼“å­˜(KV Cache)æ¥ä¼˜åŒ–é¡ºåºç”Ÿæˆè¿‡ç¨‹çš„åœ°æ–¹ï¼Œæ–¹æ³•æ˜¯å­˜å‚¨å…ˆå‰çš„è®¡ç®—ä»¥ä¾¿åœ¨åç»­ä¸­é‡ç”¨ä»¤ç‰Œï¼Œå› æ­¤ä¸éœ€è¦å†æ¬¡è®¡ç®—å®ƒä»¬ã€‚


.. note:: è¯·æ³¨æ„ï¼Œç¼“å­˜åªèƒ½åœ¨æ¨ç†ä¸­ä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶åº”ç¦ç”¨ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ„å¤–é”™è¯¯ã€‚


Generate with Cache
"""""""""""""""""""

* é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ç¼“å­˜ç”Ÿæˆï¼Œå…¶ä¸­ã€œDynamicCacheç±»æ˜¯å¤§å¤šæ•°æ¨¡å‹çš„é»˜è®¤ç¼“å­˜ã€‚
* å¦‚æœç”±äºæŸç§åŸå› æ‚¨ä¸æƒ³ä½¿ç”¨ç¼“å­˜ï¼Œåˆ™å¯ä»¥å°†use_cache=Falseä¼ é€’åˆ°generate()æ–¹æ³•ä¸­ã€‚
* ç¼“å­˜ç±»å¯ä»¥åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨cache_implementationå‚æ•°è¿›è¡Œè®¾ç½®ã€‚



Quantized Cache
+++++++++++++++

* é”®å’Œå€¼ç¼“å­˜ä¼šå ç”¨å¾ˆå¤§ä¸€éƒ¨åˆ†å†…å­˜ï¼Œæˆä¸ºé•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚
* ä½¿ç”¨generate()æ—¶ä½¿ç”¨é‡åŒ–ç¼“å­˜å¯ä»¥æ˜¾ç€å‡å°‘å†…å­˜éœ€æ±‚ï¼Œä½†ä»£ä»·æ˜¯é€Ÿåº¦ã€‚
* transformersä¸­çš„ ``KV Cacheé‡åŒ–`` å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°æ­¤è®ºæ–‡å¯å‘: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache: https://arxiv.org/abs/2402.02750

* å¦‚æœæ‚¨ä½¿ç”¨quantoåç«¯ï¼Œå»ºè®®å°†ç¼“å­˜é…ç½®ä¸­çš„axis-key/axis-valueå‚æ•°è®¾ç½®ä¸º0ï¼›å¦‚æœæ‚¨ä½¿ç”¨HQQåç«¯ï¼Œå»ºè®®å°†å…¶è®¾ç½®ä¸º1 ã€‚å¯¹äºå…¶ä»–é…ç½®å€¼ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼



.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
    inputs = tokenizer("I like rock music because", return_tensors="pt").to(model.device)

    out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="quantized", cache_config={"nbits": 4, "backend": "quanto"})
    print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])

    out = model.generate(**inputs, do_sample=False, max_new_tokens=20)
    print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])


Offloaded Cache
+++++++++++++++

* ä¸ KV ç¼“å­˜é‡åŒ–ç±»ä¼¼ï¼Œ ~OffloadedCacheç­–ç•¥æ—¨åœ¨å‡å°‘ GPU VRAM ä½¿ç”¨ã€‚
* å®ƒé€šè¿‡å°†å¤§å¤šæ•°å±‚çš„ KV ç¼“å­˜ç§»è‡³ CPU æ¥å®ç°è¿™ä¸€ç‚¹ã€‚
* å½“æ¨¡å‹çš„forward()æ–¹æ³•è¿­ä»£å„å±‚æ—¶ï¼Œè¯¥ç­–ç•¥ä¼šåœ¨GPUä¸Šç»´æŠ¤å½“å‰å±‚ç¼“å­˜ã€‚åŒæ—¶ï¼Œå®ƒå¼‚æ­¥é¢„å–ä¸‹ä¸€å±‚ç¼“å­˜ï¼Œå¹¶å°†ä¸Šä¸€å±‚ç¼“å­˜å‘é€å› CPUã€‚
* ä¸ KV ç¼“å­˜é‡åŒ–ä¸åŒï¼Œæ­¤ç­–ç•¥å§‹ç»ˆäº§ç”Ÿä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥ä½œä¸ºå®ƒçš„ç›´æ¥æ›¿ä»£å“æˆ–åå¤‡æ–¹æ¡ˆã€‚

.. note:: ``Cache offloading`` éœ€è¦ GPUï¼Œå¹¶ä¸”å¯èƒ½æ¯” ``dynamic KV cache`` æ…¢ã€‚å¦‚æœæ‚¨é‡åˆ° CUDA å†…å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·ä½¿ç”¨å®ƒã€‚Cache offloading requires a GPU and can be slower than dynamic KV cache. Use it if you are getting CUDA out of memory errors.


* ç¤ºä¾‹-å¦‚ä½•ä½¿ç”¨ KV ç¼“å­˜å¸è½½ä½œä¸ºåå¤‡ç­–ç•¥

.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    def resilient_generate(model, *args, **kwargs):
        oom = False
        try:
            return model.generate(*args, **kwargs)
        except torch.cuda.OutOfMemoryError as e:
            print(e)
            print("retrying with cache_implementation='offloaded'")
            oom = True
        if oom:  # å¦‚æœOOM,åˆ™å¯åŠ¨åå¤‡ç­–ç•¥
            torch.cuda.empty_cache()
            kwargs["cache_implementation"] = "offloaded"
            return model.generate(*args, **kwargs)

    >>> ckpt = "microsoft/Phi-3-mini-4k-instruct"
    >>> tokenizer = AutoTokenizer.from_pretrained(ckpt)
    >>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to("cuda:0")
    >>> prompt = ["okay "*1000 + "Fun fact: The most"]
    >>> inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    >>> beams = { "num_beams": 40, "num_beam_groups": 40, "num_return_sequences": 40, "diversity_penalty": 1.0, "max_new_tokens": 23, "early_stopping": True, }
    >>> out = resilient_generate(model, **inputs, **beams)
    >>> responses = tokenizer.batch_decode(out[:,-28:], skip_special_tokens=True)


Static Cache
++++++++++++

* ç”±äºâ€œDynamicCacheâ€éšç€æ¯ä¸ªç”Ÿæˆæ­¥éª¤åŠ¨æ€å¢é•¿ï¼Œå› æ­¤å®ƒä¼šé˜»æ­¢æ‚¨åˆ©ç”¨ JIT ä¼˜åŒ–ã€‚ 
* ~StaticCacheä¸ºé”®å’Œå€¼é¢„å…ˆåˆ†é…ç‰¹å®šçš„æœ€å¤§å¤§å°ï¼Œå…è®¸æ‚¨ç”Ÿæˆæœ€å¤§é•¿åº¦è€Œæ— éœ€ä¿®æ”¹ç¼“å­˜å¤§å°ã€‚
* æœ‰å…³é™æ€ç¼“å­˜å’Œ JIT ç¼–è¯‘çš„æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹StaticCache & torchcompile: https://huggingface.co/docs/transformers/main/en/llm_optims#static-kv-cache-and-torchcompile

.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")
    inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

    # simply pass the cache implementation="static"
    out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="static")
    tokenizer.batch_decode(out, skip_special_tokens=True)[0]



Offloaded Static Cache
++++++++++++++++++++++

.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16, device_map="auto")
    inputs = tokenizer("Hello, my name is", return_tensors="pt").to(model.device)

    # simply pass the cache implementation="static"
    out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation="offloaded_static")
    tokenizer.batch_decode(out, skip_special_tokens=True)[0]


Sliding Window Cache
++++++++++++++++++++

.. warning:: æ³¨æ„ï¼Œæ‚¨åªèƒ½å°†æ­¤ç¼“å­˜ç”¨äºæ”¯æŒæ»‘åŠ¨çª—å£çš„æ¨¡å‹ï¼Œä¾‹å¦‚ Mistral æ¨¡å‹ã€‚


.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache

    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-v0.1", torch_dtype=torch.float16).to("cuda:0")
    inputs = tokenizer("Yesterday I was on a rock concert and.", return_tensors="pt").to(model.device)

    # can be used by passing in cache implementation
    out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation="sliding_window")
    tokenizer.batch_decode(out, skip_special_tokens=True)[0]

Sink Cache
++++++++++

* è®ºæ–‡: Efficient Streaming Language Models with Attention Sinks: https://arxiv.org/abs/2309.17453
* å…è®¸æ‚¨ç”Ÿæˆé•¿æ–‡æœ¬åºåˆ—ï¼ˆæ ¹æ®è®ºæ–‡â€œæ— é™é•¿åº¦â€ï¼‰ï¼Œæ— éœ€ä»»ä½•å¾®è°ƒã€‚è¿™æ˜¯é€šè¿‡æ™ºèƒ½å¤„ç†ä»¥å‰çš„é”®å’Œå€¼æ¥å®ç°çš„ï¼Œç‰¹åˆ«æ˜¯å®ƒä¿ç•™äº†åºåˆ—ä¸­çš„ä¸€äº›åˆå§‹æ ‡è®°ï¼Œç§°ä¸ºâ€œæ¥æ”¶å™¨æ ‡è®°â€ã€‚è¿™æ˜¯åŸºäºè¿™æ ·çš„è§‚å¯Ÿï¼šè¿™äº›åˆå§‹ä»¤ç‰Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸å¼•äº†å¾ˆå¤§ä¸€éƒ¨åˆ†æ³¨æ„åŠ›åˆ†æ•°ã€‚ â€œæ¥æ”¶å™¨ä»¤ç‰Œâ€ä¹‹åçš„ä»¤ç‰Œå°†åœ¨æ»‘åŠ¨çª—å£çš„åŸºç¡€ä¸Šè¢«ä¸¢å¼ƒï¼Œä»…ä¿ç•™æœ€æ–°çš„window_sizeä»¤ç‰Œã€‚é€šè¿‡å°†è¿™äº›åˆå§‹æ ‡è®°ä¿ç•™ä¸ºâ€œæ³¨æ„åŠ›æ± â€ï¼Œå³ä½¿åœ¨å¤„ç†å¾ˆé•¿çš„æ–‡æœ¬æ—¶ï¼Œæ¨¡å‹ä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼Œä»è€Œä¸¢å¼ƒå¤§éƒ¨åˆ†å…ˆå‰çš„çŸ¥è¯†ã€‚

.. code-block:: python

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache

    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
    model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float16).to("cuda:0")
    inputs = tokenizer("This is a long story about unicorns, fairies and magic.", return_tensors="pt").to(model.device)

    # get our cache, specify number of sink tokens and window size
    # Note that window size already includes sink tokens, so has to be larger
    past_key_values = SinkCache(window_length=256, num_sink_tokens=4)
    out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)
    tokenizer.batch_decode(out, skip_special_tokens=True)[0]

.. note:: ä¸å…¶ä»–ç¼“å­˜ç±»ä¸åŒï¼Œè¿™ä¸ªç¼“å­˜ç±»ä¸èƒ½é€šè¿‡æŒ‡ç¤ºcache_implementationæ¥ç›´æ¥ä½¿ç”¨ã€‚æ‚¨å¿…é¡»åœ¨è°ƒç”¨generate()ä¹‹å‰åˆå§‹åŒ–ç¼“å­˜


Encoder-Decoder Cache
+++++++++++++++++++++

* ~EncoderDecoderCacheæ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œæ—¨åœ¨å¤„ç†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ç¼“å­˜éœ€æ±‚ã€‚è¿™ç§ç¼“å­˜ç±»å‹æ˜¯ä¸“é—¨ä¸ºç®¡ç†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ç¼“å­˜è€Œæ„å»ºçš„ï¼Œç¡®ä¿å­˜å‚¨å’Œæ£€ç´¢è¿™äº›å¤æ‚æ¨¡å‹æ‰€éœ€çš„è¿‡å»çš„é”®/å€¼ã€‚


Model-specific Cache Classes
""""""""""""""""""""""""""""

* æœ‰äº›æ¨¡å‹éœ€è¦ä»¥ç‰¹å®šçš„æ–¹å¼å­˜å‚¨ä»¥å‰çš„é”®ã€å€¼æˆ–çŠ¶æ€ï¼Œå¹¶ä¸”ä¸èƒ½ä½¿ç”¨ä¸Šè¿°ç¼“å­˜ç±»ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æœ‰å‡ ä¸ªä¸“ä¸ºç‰¹å®šæ¨¡å‹è®¾è®¡çš„ä¸“ç”¨ç¼“å­˜ç±»ã€‚
* ç¤ºä¾‹åŒ…æ‹¬ç”¨äºGemma2ç³»åˆ—æ¨¡å‹çš„~HybridCacheæˆ–ç”¨äºMambaæ¶æ„æ¨¡å‹çš„~MambaCache ã€‚





Prompting
---------

Image tasks with IDEFICS
^^^^^^^^^^^^^^^^^^^^^^^^

* å¯¹äºæŠŠå›¾åƒå…ˆè½¬ä¸ºæ–‡æœ¬å†è¿›è¡Œåˆ†æçš„LLMï¼Œè¿™ç§å›¾åƒç±»çš„taskä¹Ÿå¯ä»¥åƒæ™®é€šçš„è¯­è¨€LLMä¸€æ ·ä½¿ç”¨prompt


LLM prompting guide
^^^^^^^^^^^^^^^^^^^

* ç¼–ç å™¨-è§£ç å™¨å¼æ¨¡å‹é€šå¸¸ç”¨äºè¾“å‡ºä¸¥é‡ä¾èµ–è¾“å…¥çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘å’Œæ‘˜è¦ã€‚ä»…è§£ç å™¨æ¨¡å‹ç”¨äºæ‰€æœ‰å…¶ä»–ç±»å‹çš„ç”Ÿæˆä»»åŠ¡ã€‚
* å…·ä½“çœ‹promptç›¸å…³æ–‡æ¡£




Developer guides
================

Use fast tokenizers from ğŸ¤— Tokenizers
------------------------------------------

* PreTrainedTokenizerï¼šè¿™æ˜¯ä¸€ä¸ªçº¯ Python å®ç°çš„åˆ†è¯å™¨åŸºç±»ï¼Œæ‰€æœ‰çš„åˆ†è¯å’Œç¼–ç æ“ä½œéƒ½æ˜¯é€šè¿‡ Python ä»£ç æ‰§è¡Œçš„ã€‚
* PreTrainedTokenizerFastï¼šåŸºäº Rust ç¼–å†™çš„ ğŸ¤— Tokenizers åº“ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„åˆ†è¯ç®—æ³•ã€‚PreTrainedTokenizerFast é€šè¿‡ç»‘å®š Rust å®ç°ï¼Œæä¾›äº†æ›´å¿«çš„åˆ†è¯é€Ÿåº¦ã€‚
* éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰æ¨¡å‹çš„åˆ†è¯å™¨éƒ½æœ‰å¯¹åº”çš„ â€œFastâ€ å®ç°ï¼Œç‰¹åˆ«æ˜¯åŸºäº SentencePiece çš„åˆ†è¯å™¨ï¼ˆå¦‚ T5ã€ALBERTã€CamemBERTã€XLMRoBERTa å’Œ XLNet ç­‰æ¨¡å‹ï¼‰ç›®å‰å°šæ—  â€œFastâ€ ç‰ˆæœ¬å¯ç”¨ 



* åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿåˆ†è¯å™¨(dummy tokenizer)

.. code-block:: python

    from tokenizers import Tokenizer
    from tokenizers.models import BPE
    from tokenizers.trainers import BpeTrainer
    from tokenizers.pre_tokenizers import Whitespace

    tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
    trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

    tokenizer.pre_tokenizer = Whitespace()
    files = [...]
    tokenizer.train(files, trainer)

    # ä¿å­˜
    tokenizer.save("tokenizer.json")



Loading directly from the tokenizer object
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

::

    from transformers import PreTrainedTokenizerFast
    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)


Loading from a JSON file
^^^^^^^^^^^^^^^^^^^^^^^^

::

    from transformers import PreTrainedTokenizerFast
    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")



Use model-specific APIs
----------------------------

* Create a custom architecture
* AutoClassè‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ **å»ºè®®** ä½¿ç”¨AutoClassæ¥ç”Ÿæˆä¸æ£€æŸ¥ç‚¹æ— å…³çš„ä»£ç ã€‚
* æœ¬èŠ‚ä¸»è¦äº†è§£å¦‚ä½•åˆ›å»ºä¸ä½¿ç”¨AutoClassè‡ªå®šä¹‰æ¨¡å‹

Configuration
^^^^^^^^^^^^^

* Configuration æŒ‡æ¨¡å‹çš„ç‰¹å®šå±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§
* ç¤ºä¾‹ ``DistilBertConfig`` displays all the default attributes used to build a base ``DistilBertModel``

.. code-block:: python

    >>> from transformers import DistilBertConfig
    >>> config = DistilBertConfig()
    >>> print(config)
    DistilBertConfig {
      "activation": "gelu",
      "attention_dropout": 0.1,
      "dim": 768,
      "dropout": 0.1,
      "hidden_dim": 3072,
      "initializer_range": 0.02,
      "max_position_embeddings": 512,
      "model_type": "distilbert",
      "n_heads": 12,
      "n_layers": 6,
      "pad_token_id": 0,
      "qa_dropout": 0.1,
      "seq_classif_dropout": 0.2,
      "sinusoidal_pos_embds": false,
      "transformers_version": "4.16.2",
      "vocab_size": 30522
    }


* æ‰€æœ‰å±æ€§å‡å¯å®šåˆ¶ï¼Œå¦‚ä¸‹ç¤ºä¾‹

.. code-block:: python

    >>> my_config = DistilBertConfig(activation="relu", attention_dropout=0.4)
    >>> print(my_config)
    DistilBertConfig {
      "activation": "relu",             # gelu->relu
      "attention_dropout": 0.4,         # 0.1->0.4
      "dim": 768,
      "dropout": 0.1,
      "hidden_dim": 3072,
      "initializer_range": 0.02,
      "max_position_embeddings": 512,
      "model_type": "distilbert",
      "n_heads": 12,
      "n_layers": 6,
      "pad_token_id": 0,
      "qa_dropout": 0.1,
      "seq_classif_dropout": 0.2,
      "sinusoidal_pos_embds": false,
      "transformers_version": "4.16.2",
      "vocab_size": 30522
    }

ä¿å­˜&åŠ è½½::

    my_config = DistilBertConfig.from_pretrained("distilbert/distilbert-base-uncased", activation="relu", attention_dropout=0.4)
    # ä¿å­˜
    my_config.save_pretrained(save_directory="./your_model_save_path")
    # åŠ è½½
    my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")

Model
^^^^^

åŠ è½½::

    # å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­
    # è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹
    # æ³¨æ„ï¼šåœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•æœ‰ç”¨çš„äº‹æƒ…
    from transformers import DistilBertModel
    my_config = DistilBertConfig.from_pretrained("./your_model_save_path/config.json")
    model = DistilBertModel(my_config)

    # è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®çš„é¢„è®­ç»ƒæ¨¡å‹
    model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased")

    # ä½¿ç”¨è‡ªå·±çš„æ¨¡å‹é…ç½®å±æ€§
    model = DistilBertModel.from_pretrained("distilbert/distilbert-base-uncased", config=my_config)

Model heads
"""""""""""

* At this point, you have a base ``DistilBERT`` model which outputs the ``hidden states``. 
* The ``hidden states`` are passed as inputs to a ``model head`` to produce the final output. 
* ğŸ¤— Transformers provides a different model head for each task as long as a model supports the task
*  (i.e., you canâ€™t use DistilBERT for a sequence-to-sequence task like translation).

* ç¤ºä¾‹
* ``DistilBertForSequenceClassification`` is a base ``DistilBERT`` model with a ``sequence classification`` head. 
* The sequence classification head is a linear layer on top of the ``pooled outputs``.

::

    from transformers import DistilBertForSequenceClassification
    model = DistilBertForSequenceClassification.from_pretrained("distilbert/distilbert-base-uncased")


* é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„ model headï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤checkpointé‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚
* å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæ‚¨å°†ä½¿ç”¨ ``DistilBertForQuestionAnswering`` æ¨¡å‹å¤´(model head)ã€‚
* The ``question answering head`` is similar to the ``sequence classification head`` except it is a linear layer on top of the ``hidden states output``.

::

    from transformers import DistilBertForQuestionAnswering
    model = DistilBertForQuestionAnswering.from_pretrained("distilbert/distilbert-base-uncased")


Tokenizer
^^^^^^^^^

* PreTrainedTokenizer ï¼šåˆ†è¯å™¨çš„ Python å®ç°ã€‚
* PreTrainedTokenizerFast ï¼šæ¥è‡ªæˆ‘ä»¬åŸºäº Rust çš„ğŸ¤— Tokenizeråº“çš„ tokenizerã€‚

.. warning:: å¹¶éæ¯ä¸ªæ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚æŸ¥çœ‹æ­¤ `è¡¨ <https://huggingface.co/docs/transformers/main/en/index#supported-frameworks>`_ ä»¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦å…·æœ‰å¿«é€Ÿåˆ†è¯å™¨æ”¯æŒã€‚


å¦‚æœæ‚¨æƒ³è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼Œåˆ™å¯ä»¥ä»è¯æ±‡è¡¨æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨::

    from transformers import DistilBertTokenizer
    my_tokenizer = DistilBertTokenizer(vocab_file="my_vocab_file.txt", do_lower_case=False, padding_side="left")


åˆ›å»ºå…·æœ‰é¢„è®­ç»ƒæ¨¡å‹è¯æ±‡è¡¨çš„åˆ†è¯å™¨::

    from transformers import DistilBertTokenizer
    slow_tokenizer = DistilBertTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
    # fast_tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert/distilbert-base-uncased")


.. note:: By default, ``AutoTokenizer`` will try to load a ``fast tokenizer``. You can disable this behavior by setting ``use_fast=False`` in from_pretrained.


Image processor
^^^^^^^^^^^^^^^

* todo
* å›¾åƒå¤„ç†å™¨å¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ªImageProcessingMixinåŸºç±»ã€‚

Backbone
^^^^^^^^

* todo

.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/bgPiyo.png

    Computer vision models consist of a ``backbone``, ``neck``, and ``head``. 

* **backbone** ä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ï¼Œ **neck** ç»„åˆå¹¶å¢å¼ºæå–çš„ç‰¹å¾ï¼Œ **head** ç”¨äºä¸»è¦ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡æ£€æµ‹ï¼‰ã€‚
* é¦–å…ˆåœ¨æ¨¡å‹é…ç½®ä¸­åˆå§‹åŒ–ä¸»å¹²ï¼Œå¹¶æŒ‡å®šæ˜¯å¦è¦åŠ è½½é¢„è®­ç»ƒçš„æƒé‡æˆ–åŠ è½½éšæœºåˆå§‹åŒ–çš„æƒé‡ã€‚ç„¶åæ‚¨å¯ä»¥å°†æ¨¡å‹é…ç½®ä¼ é€’ç»™æ¨¡å‹å¤´ã€‚
* The backbone extracts features from an input image, the neck combines and enhances the extracted features, and the head is used for the main task (e.g., object detection). 
* Start by initializing a backbone in the model config and specify whether you want to load pretrained weights or load randomly initialized weights. Then you can pass the model config to the model head.

Feature extractor
^^^^^^^^^^^^^^^^^

* todo
* ç»§æ‰¿è‡ªFeatureExtractionMixinåŸºç±»ï¼Œä¹Ÿå¯ä»¥ç»§æ‰¿SequenceFeatureExtractorç±»æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚

Processor
^^^^^^^^^

* todo
* å¯¹äºæ”¯æŒå¤šæ¨¡å¼ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç­‰å¤„ç†ç±»åŒ…è£…åˆ°å•ä¸ªå¯¹è±¡ä¸­ã€‚



Building custom models
^^^^^^^^^^^^^^^^^^^^^^

* è®²äº†å¦‚ä½•è‡ªå·±å†™ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹
* è®²äº†AutoXXXå¦‚ä½•å®ç°åŠ è½½æ¨¡å‹çš„



Chat Templates
--------------

Introduce
^^^^^^^^^

* An increasingly common use case for LLMs is chat.
* åœ¨èŠå¤©ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯¥æ¨¡å‹ä¸æ˜¯ç»§ç»­å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆå¦‚æ ‡å‡†è¯­è¨€æ¨¡å‹çš„æƒ…å†µï¼‰ï¼Œè€Œæ˜¯ç»§ç»­ç”±ä¸€æ¡æˆ–å¤šæ¡æ¶ˆæ¯ç»„æˆçš„å¯¹è¯ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½åŒ…å«ä¸€ä¸ªè§’è‰²ï¼Œä¾‹å¦‚â€œuserâ€æˆ–â€œassistsâ€ï¼Œä»¥åŠæ¶ˆæ¯æ–‡æœ¬ã€‚
* ä¸æ ‡è®°åŒ–(tokenization)éå¸¸ç›¸ä¼¼ï¼Œä¸åŒçš„æ¨¡å‹æœŸæœ›èŠå¤©çš„è¾“å…¥æ ¼å¼æˆªç„¶ä¸åŒã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ·»åŠ èŠå¤©æ¨¡æ¿ä½œä¸ºä¸€é¡¹åŠŸèƒ½çš„åŸå› ã€‚
* èŠå¤©æ¨¡æ¿æ˜¯æ ‡è®°å™¨(tokenizer)çš„ä¸€éƒ¨åˆ†ã€‚å®ƒä»¬æŒ‡å®šå¦‚ä½•å°†è¡¨ç¤ºä¸ºæ¶ˆæ¯åˆ—è¡¨çš„å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›æ ¼å¼çš„å•ä¸ªå¯æ ‡è®°å­—ç¬¦ä¸²ã€‚

ç¤ºä¾‹::

    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")

    chat = [
      {"role": "user", "content": "Hello, how are you?"},
      {"role": "assistant", "content": "I'm doing great. How can I help you today?"},
      {"role": "user", "content": "I'd like to show off how chat templating works!"},
    ]

    tokenizer.apply_chat_template(chat, tokenize=False)

    # æ³¨: æ ‡è®° [INST] å’Œ [/INST] æ¥æŒ‡ç¤ºç”¨æˆ·æ¶ˆæ¯çš„å¼€å§‹å’Œç»“æŸ
    # å…¶ä»–æ¨¡å‹å¯èƒ½ä½¿ç”¨åˆ«çš„æ ‡è®°æ¥æŒ‡ç¤º


How do I use chat templates
^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. code-block:: python

    from transformers import AutoModelForCausalLM, AutoTokenizer

    checkpoint = "HuggingFaceH4/zephyr-7b-beta"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForCausalLM.from_pretrained(checkpoint)  # You may want to use bfloat16 and/or move to GPU here

    messages = [
        {
            "role": "system",
            "content": "You are a friendly chatbot who always responds in the style of a pirate",
        },
        {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
     ]
    tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")
    print(tokenizer.decode(tokenized_chat[0]))
    # è¾“å‡º
    """
    <|system|>
    You are a friendly chatbot who always responds in the style of a pirate</s> 
    <|user|>
    How many helicopters can a human eat in one sitting?</s> 
    <|assistant|>
    """

æ¨¡å‹è¾“å‡º::

    outputs = model.generate(tokenized_chat, max_new_tokens=128) 
    print(tokenizer.decode(outputs[0]))
    # è¾“å‡º
    <|system|>
    You are a friendly chatbot who always responds in the style of a pirate</s> 
    <|user|>
    How many helicopters can a human eat in one sitting?</s> 
    <|assistant|>
    Matey, I'm afraid I must .......






æ ¸å¿ƒå‚æ•°
^^^^^^^^

add_generation_prompt å‚æ•°
""""""""""""""""""""""""""

* ç±»å‹ï¼šbool
* åŠŸèƒ½ï¼šæ˜¯å¦åœ¨èŠå¤©æ¨¡æ¿çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªæç¤ºï¼Œç”¨äºæŒ‡ç¤ºæ¨¡å‹ç”Ÿæˆä¸‹ä¸€æ¡æ¶ˆæ¯ã€‚è¿™å¯¹äºä¸€äº›èŠå¤©æ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä¸€ä¸ªç‰¹å®šçš„è§¦å‘æ ‡è®°æ¥å¼€å§‹ç”Ÿæˆã€‚

å®ä¾‹::

    messages = [
        {"role": "user", "content": "Hi there!"},
        {"role": "assistant", "content": "Nice to meet you!"},
        {"role": "user", "content": "Can I ask a question?"}
    ]

 without a generation prompt::

    >>> tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
    """<|im_start|>user
    Hi there!<|im_end|>
    <|im_start|>assistant
    Nice to meet you!<|im_end|>
    <|im_start|>user
    Can I ask a question?<|im_end|>
    """

with a generation prompt::

    >>> tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    """<|im_start|>user
    Hi there!<|im_end|>
    <|im_start|>assistant
    Nice to meet you!<|im_end|>
    <|im_start|>user
    Can I ask a question?<|im_end|>
    <|im_start|>assistant               # æ·»åŠ ç”Ÿæˆæç¤º
    """

continue_final_message å‚æ•°
"""""""""""""""""""""""""""

* ç±»å‹ï¼šbool
* åŠŸèƒ½ï¼šæ˜¯å¦åœ¨æœ€åä¸€æ¡æ¶ˆæ¯çš„ content æœ«å°¾ç»§ç»­ç”Ÿæˆã€‚è¿™åœ¨éœ€è¦æ¨¡å‹æ¥ç€æœªå®Œæˆçš„å¥å­ç”Ÿæˆæ—¶å¾ˆæœ‰ç”¨ã€‚



::

    chat = [
        {"role": "user", "content": "Can you format the answer in JSON?"},
        {"role": "assistant", "content": '{"name": "'},
    ]

    formatted_chat = tokenizer.apply_chat_template(chat, tokenize=True, return_dict=True, continue_final_message=True)
    model.generate(**formatted_chat)


.. note:: add_generation_promptæ·»åŠ å¼€å§‹æ–°æ¶ˆæ¯çš„æ ‡è®°ï¼Œè€Œcontinue_final_messageä»æœ€ç»ˆæ¶ˆæ¯ä¸­åˆ é™¤ä»»ä½•æ¶ˆæ¯ç»“æŸæ ‡è®°ï¼Œå› æ­¤å°†å®ƒä»¬ä¸€èµ·ä½¿ç”¨æ²¡æœ‰æ„ä¹‰ã€‚


tokenize å‚æ•°
"""""""""""""

.. note:: é»˜è®¤æƒ…å†µä¸‹ï¼ŒæŸäº›æ ‡è®°ç”Ÿæˆå™¨ä¼šå°†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚<bos>å’Œ<eos>æ·»åŠ åˆ°å®ƒä»¬æ ‡è®°çš„æ–‡æœ¬ä¸­ã€‚èŠå¤©æ¨¡æ¿åº”è¯¥å·²ç»åŒ…å«å®ƒä»¬éœ€è¦çš„æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œå› æ­¤é¢å¤–çš„ç‰¹æ®Šæ ‡è®°é€šå¸¸ä¼šä¸æ­£ç¡®æˆ–é‡å¤ï¼Œè¿™ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚

* å¦‚æœæ‚¨ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è®¾ç½®æ–‡æœ¬æ ¼å¼ apply_chat_template(tokenize=False) ï¼Œå½“æ‚¨ç¨åæ ‡è®°è¯¥æ–‡æœ¬æ—¶ï¼Œæ‚¨åº”è¯¥è®¾ç½®å‚æ•°add_special_tokens=False ã€‚å¦‚æœä½ ä½¿ç”¨ apply_chat_template(tokenize=True) ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªï¼

Advanced: How do chat templates work?
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* æ¨¡å‹çš„èŠå¤©æ¨¡æ¿å­˜å‚¨åœ¨tokenizer.chat_templateå±æ€§ä¸­

ç¤ºä¾‹(Jinja æ¨¡æ¿)::

    {%- for message in messages %}
        {{- '<|' + message['role'] + |>\n' }}
        {{- message['content'] + eos_token }}
    {%- endfor %}
    {%- if add_generation_prompt %}
        {{- '<|assistant|>\n' }}
    {%- endif %}


ç¤ºä¾‹2::

    {%- for message in messages %}
        {%- if message['role'] == 'user' %}
            {{- bos_token + '[INST] ' + message['content'] + ' [/INST]' }}
        {%- elif message['role'] == 'system' %}
            {{- '<<SYS>>\\n' + message['content'] + '\\n<</SYS>>\\n\\n' }}
        {%- elif message['role'] == 'assistant' %}
            {{- ' '  + message['content'] + ' ' + eos_token }}
        {%- endif %}
    {%- endfor %}


Advanced: Adding and editing chat templates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

How do I create a chat template?
""""""""""""""""""""""""""""""""

åŸºäºåˆ«çš„tokenè¿›è¡Œä¿®æ”¹::

    template = tokenizer.chat_template
    template = template.replace("SYS", "SYSTEM")  # Change the system token

    tokenizer.chat_template = template  # Set the new template
    tokenizer.push_to_hub("model_name")  # Upload your new template to the Hub!


ä¸€ç§æµè¡Œçš„é€‰æ‹©æ˜¯ChatMLæ ¼å¼::

    {%- for message in messages %}
        {{- '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n' }}
    {%- endfor %}


Advanced: Template writing tips
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Trimming whitespace
"""""""""""""""""""

å¼ºçƒˆå»ºè®®ä½¿ç”¨æ ¼å¼::

    {%- for message in messages %}
        {{- message['role'] + message['content'] }}
    {%- endfor %}

ä¸è¦ä½¿ç”¨æ ¼å¼::

    {% for message in messages %}
        {{ message['role'] + message['content'] }}
    {% endfor %}


Callable functions
""""""""""""""""""

::

    1. raise_exception(msg)
    2. strftime_now(format_str)

Compatibility with non-Python Jinja
"""""""""""""""""""""""""""""""""""

.. note:: é Python å®ç°åœ¨éƒ¨ç½²ç¯å¢ƒä¸­å°¤å…¶å¸¸è§ï¼Œå…¶ä¸­ JS å’Œ Rust éå¸¸æµè¡Œã€‚

* 1.Replace Python methods with Jinja filters::

    string.lower()      => string|lower
    dict.items()        => dict|items
    string.strip()      => string|trim

* 2.Replace True, False and None, which are Python-specific, with true, false and none.
* 3.æ·»åŠ tojsonè¿‡æ»¤å™¨, é¿å…ç›´æ¥æ¸²æŸ“å­—å…¸æˆ–åˆ—è¡¨å¯èƒ½ä¼šåœ¨å…¶ä»–å®ç°ä¸­ç»™å‡ºä¸åŒçš„ç»“æœ


* Jinjaå†…ç½®Filter: https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters



Trainer
-------

Basic usage::

    1. perform a training step to calculate the loss(æ‰§è¡Œè®­ç»ƒæ­¥éª¤æ¥è®¡ç®—æŸå¤±)
    2. calculate the gradients with the backward method(ä½¿ç”¨åå‘æ–¹æ³•è®¡ç®—æ¢¯åº¦)
    3. update the weights based on the gradients(æ ¹æ®æ¢¯åº¦æ›´æ–°æƒé‡)
    4. repeat this process until youâ€™ve reached a predetermined number of epochs(é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šepochs)

class::

    Trainer
    Seq2SeqTrainer
    trl.SFTTrainer 

TrainingArguments class::

    from transformers import TrainingArguments

    training_args = TrainingArguments(
        output_dir="your-model",
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=2,
        weight_decay=0.01,
        eval_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        push_to_hub=True,
    )

Export to ONNX
--------------

* ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œå¯ä»¥é€šè¿‡å…¶exportersæ¨¡å—å°†æ¨¡å‹ä» PyTorch å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼(serialized format)ï¼Œä¾‹å¦‚ ONNX å’Œ TFLiteã€‚
* ONNXï¼ˆOpen Neural Network eXchangeï¼‰æ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨è¿ç®—ç¬¦å’Œé€šç”¨æ–‡ä»¶æ ¼å¼ï¼Œä»¥è¡¨ç¤ºå„ç§æ¡†æ¶ï¼ˆåŒ…æ‹¬ PyTorch å’Œ TensorFlowï¼‰ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼æ—¶ï¼Œè¿™äº›è¿ç®—ç¬¦ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸ºä¸­é—´è¡¨ç¤ºï¼‰ï¼Œè¯¥è®¡ç®—å›¾è¡¨ç¤ºé€šè¿‡ç¥ç»ç½‘ç»œçš„æ•°æ®æµã€‚
* é€šè¿‡ä½¿ç”¨æ ‡å‡†åŒ–è¿ç®—ç¬¦å’Œæ•°æ®ç±»å‹å…¬å¼€å›¾è¡¨ï¼ŒONNX å¯ä»¥è½»æ¾åœ°åœ¨æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç„¶åå¯¼å…¥åˆ° TensorFlow ä¸­ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚

Exporting a ğŸ¤— Transformers model to ONNX with CLI
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼Œè¯·é¦–å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹::

    $ pip install optimum[exporters]

ç¤ºä¾‹-å¯¼å‡º::

    # ä» ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹
    $ optimum-cli export onnx --model distilbert/distilbert distilbert/

    # å¯¼å‡ºæœ¬åœ°æ¨¡å‹
    $ optimum-cli export onnx --model local_path --task question-answering distilbert/

ä½¿ç”¨ONNX RuntimeåŠ è½½å¹¶è¿è¡Œæ¨¡å‹::

    from transformers import AutoTokenizer
    from optimum.onnxruntime import ORTModelForQuestionAnswering

    tokenizer = AutoTokenizer.from_pretrained("distilbert")
    model = ORTModelForQuestionAnswering.from_pretrained("distilbert")
    inputs = tokenizer("What am I using?", "Using DistilBERT with ONNX Runtime!", return_tensors="pt")
    outputs = model(**inputs)

Exporting a ğŸ¤— Transformers model to ONNX with optimum.onnxruntime
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

ç¤ºä¾‹-å¯¼å‡º::

    from optimum.onnxruntime import ORTModelForSequenceClassification
    from transformers import AutoTokenizer

    model_checkpoint = "distilbert_base_uncased_squad"
    save_directory = "onnx/"

    # Load a model from transformers and export it to ONNX
    ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

    # Save the onnx model and tokenizer
    ort_model.save_pretrained(save_directory)
    tokenizer.save_pretrained(save_directory)


Exporting a model with transformers.onnx
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. warning:: transformers.onnxä¸å†ç»´æŠ¤ï¼Œè¯·ä½¿ç”¨ä¸Šé¢2èŠ‚çš„ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚

ç¤ºä¾‹-å¯¼å‡º::

    pip install transformers[onnx]

    python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/


ç¤ºä¾‹-è¿è¡Œ::

    from transformers import AutoTokenizer
    from onnxruntime import InferenceSession

    tokenizer = AutoTokenizer.from_pretrained("distilbert/distilbert-base-uncased")
    session = InferenceSession("onnx/model.onnx")
    # ONNX Runtime expects NumPy arrays as input
    inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
    outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))



* FP16 stands for mixed-precision meaning that computations within the model are done using a mixture of 16-bit and 32-bit floating-point operations
* https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half



Interoperability with GGUF files
--------------------------------

* The GGUF file format is used to store models for inference with `GGML <https://github.com/ggerganov/ggml>`_ and other libraries that depend on itï¼ˆå¦‚: llama.cpp or whisper.cppï¼‰
* å®ƒæ˜¯Hugging Face Hub æ”¯æŒçš„ä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œå…·æœ‰å…è®¸å¿«é€Ÿæ£€æŸ¥æ–‡ä»¶ä¸­çš„å¼ é‡(tensors)å’Œå…ƒæ•°æ®(metadata)çš„åŠŸèƒ½ã€‚
* è¿™ç§æ–‡ä»¶æ ¼å¼è¢«è®¾è®¡ä¸ºâ€œå•æ–‡ä»¶æ ¼å¼(single-file-format)â€ï¼Œå…¶ä¸­å•ä¸ªæ–‡ä»¶é€šå¸¸åŒ…å«é…ç½®å±æ€§(configuration attributes)ã€åˆ†è¯å™¨è¯æ±‡(tokenizer vocabulary)å’Œå…¶ä»–å±æ€§ï¼Œä»¥åŠè¦åœ¨æ¨¡å‹ä¸­åŠ è½½çš„æ‰€æœ‰å¼ é‡ã€‚


Supported quantization types::

    F32
    F16
    BF16
    Q4_0
    Q4_1
    Q5_0
    Q5_1
    Q8_0
    Q2_K
    Q3_K
    Q4_K
    Q5_K
    Q6_K
    IQ1_S
    IQ1_M
    IQ2_XXS
    IQ2_XS
    IQ2_S
    IQ3_XXS
    IQ3_S
    IQ4_XS
    IQ4_NL

Supported model architectures::

    LLaMa
    Mistral
    Qwen2
    Qwen2Moe
    Phi3

Example::

    # åŠ è½½ GGUF æ–‡ä»¶æ ¼å¼
    from transformers import AutoTokenizer, AutoModelForCausalLM
    model_id = "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"
    filename = "tinyllama-1.1b-chat-v1.0.Q6_K.gguf"
    tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
    model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)


    # ä¿å­˜æ¨¡å‹å¹¶å°†å…¶å¯¼å‡ºå›gguf
    tokenizer.save_pretrained('directory')
    model.save_pretrained('directory')
    !python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}




Quantization Methods
====================


.. note:: æœ¬èŠ‚ç®€å•æ•´ç†ï¼Œéœ€è¦æ—¶ç»†çœ‹

Quantization
------------

Quantization method::

    bitsandbytes
    GPTQ
    AWQ
    AQLM
    Quanto
    EETQ
    HQQ
    FBGEMM_FP8
    Optimum
    TorchAO
    compressed-tensors
    Contribute new quantization method

.. figure:: https://img.zhaoweiguo.com/uPic/2024/10/7nWOfI.png

bitsandbytes
------------

* https://github.com/TimDettmers/bitsandbytes
* bitsandbytes is the easiest option for quantizing a model to 8 and 4-bit.

* å®šä¹‰ï¼šå¼‚å¸¸å€¼(Outliers)æ˜¯æŒ‡åœ¨æ•°æ®é›†ä¸­æ˜æ˜¾åç¦»å…¶ä»–æ•°æ®ç‚¹çš„æ•°å€¼ã€‚å®ƒä»¬ä¸æ•°æ®é›†çš„å¹³å‡è¶‹åŠ¿æˆ–èŒƒå›´ç›¸æ¯”ï¼Œè¡¨ç°å¾—éå¸¸å¼‚å¸¸ï¼Œå¯èƒ½ç”±äºæµ‹é‡é”™è¯¯ã€æç«¯æƒ…å†µæˆ–æ•°æ®åˆ†å¸ƒä¸­çš„ç¨€æœ‰äº‹ä»¶å¼•èµ·ã€‚
* å®šä¹‰ï¼šéå¼‚å¸¸å€¼(Non-Outliers)æ˜¯æŒ‡åœ¨æ•°æ®é›†ä¸­ç¬¦åˆæ€»ä½“è¶‹åŠ¿ã€èŒƒå›´æˆ–åˆ†å¸ƒçš„æ•°å€¼ã€‚å®ƒä»¬ä¸ä¼šæ˜æ˜¾åç¦»æ•°æ®çš„ä¸»æµç‰¹å¾ï¼Œé€šå¸¸ä½äºæ•°æ®çš„å¹³å‡å€¼é™„è¿‘ã€‚
* åœ¨æœºå™¨å­¦ä¹ ä¸­çš„è¡¨ç°ï¼šåœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒæŸäº›æƒé‡æˆ–æ¿€æ´»å€¼å¯èƒ½éå¸¸å¤§æˆ–éå¸¸å°ï¼ˆç›¸å¯¹äºå…¶ä»–å€¼ï¼‰ï¼Œè¿™äº›å€¼ä¼šè¢«ç§°ä¸ºå¼‚å¸¸å€¼(Outliers)ã€‚å¦‚æœç›´æ¥ä½¿ç”¨ä½ç²¾åº¦ï¼ˆå¦‚8-bitï¼‰çš„é‡åŒ–ï¼Œå¼‚å¸¸å€¼å¯èƒ½å¯¼è‡´è¾ƒå¤§çš„ç²¾åº¦æŸå¤±ã€‚
* å¤„ç†æ–¹å¼ï¼šåœ¨8-bité‡åŒ–è¿‡ç¨‹ä¸­ï¼Œå¼‚å¸¸å€¼å¾€å¾€ä¸ä¼šç›´æ¥é‡åŒ–ä¸º8ä½æ•´æ•°ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´ç²¾åº¦æŸå¤±ã€‚é€šå¸¸ï¼Œè¿™äº›å¼‚å¸¸å€¼ä¼šä¿ç•™åœ¨æ›´é«˜ç²¾åº¦çš„æ ¼å¼ï¼ˆå¦‚FP16ï¼‰ä¸­å•ç‹¬å¤„ç†ã€‚
* ã€é‡åŒ–è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‘åœ¨é‡åŒ–ç¥ç»ç½‘ç»œæ—¶ï¼Œoutliers å’Œ non-outliers è¢«åˆ†å¼€å¤„ç†ã€‚éå¼‚å¸¸å€¼é€‚åˆç›´æ¥ç”¨8-bitè¡¨ç¤ºï¼Œèƒ½æå¤§åœ°å‡å°‘è®¡ç®—å’Œå­˜å‚¨çš„èµ„æºéœ€æ±‚ã€‚è€Œå¼‚å¸¸å€¼å› ä¸ºå¯èƒ½å¯¼è‡´ç²¾åº¦æŸå¤±ï¼Œé€šå¸¸ç”¨æ›´é«˜ç²¾åº¦çš„FP16è¡¨ç¤ºã€‚éšåï¼Œå°†è¿™ä¸¤éƒ¨åˆ†ï¼ˆFP16çš„å¼‚å¸¸å€¼å’ŒINT8çš„éå¼‚å¸¸å€¼ï¼‰ç›¸ä¹˜ã€åŠ æ€»ï¼Œä»¥ä¿æŒè®¡ç®—ç»“æœçš„ç²¾ç¡®æ€§ã€‚

.. note:: é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ—¢èƒ½åˆ©ç”¨ä½ç²¾åº¦é‡åŒ–çš„ä¼˜åŠ¿ï¼ˆå‡å°‘æ¨¡å‹å¤§å°å’ŒåŠ é€Ÿæ¨ç†ï¼‰ï¼Œåˆèƒ½åœ¨å¤„ç†å¼‚å¸¸å€¼æ—¶ä¿æŒä¸€å®šçš„ç²¾åº¦ã€‚



* 8 ä½é‡åŒ–å°† fp16 ä¸­çš„å¼‚å¸¸å€¼(outliers)ä¸ int8 ä¸­çš„éå¼‚å¸¸å€¼(non-outliers)ç›¸ä¹˜ï¼Œå°†éå¼‚å¸¸å€¼è½¬æ¢å› fp16ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ä»¥è¿”å› fp16 ä¸­çš„æƒé‡ã€‚è¿™å‡å°‘äº†å¼‚å¸¸å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
* 8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16.
* 4 ä½é‡åŒ–å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ï¼Œé€šå¸¸ä¸QLoRAä¸€èµ·ä½¿ç”¨æ¥å¾®è°ƒé‡åŒ–çš„LLMs ã€‚


8bit::

    from transformers import AutoModelForCausalLM, BitsAndBytesConfig

    quantization_config = BitsAndBytesConfig(load_in_8bit=True)

    model_8bit = AutoModelForCausalLM.from_pretrained(
        "bigscience/bloom-1b7", 
        quantization_config=quantization_config
    )


4bit::

    from transformers import AutoModelForCausalLM, BitsAndBytesConfig

    quantization_config = BitsAndBytesConfig(load_in_4bit=True)

    model_4bit = AutoModelForCausalLM.from_pretrained(
        "bigscience/bloom-1b7",
        quantization_config=quantization_config
    )


GPTQ
----

* https://github.com/PanQiWei/AutoGPTQ
* AutoGPTQåº“å®ç°äº† GPTQ ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒåé‡åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œéƒ½è¢«ç‹¬ç«‹é‡åŒ–ï¼Œä»¥æ‰¾åˆ°æœ€å°åŒ–è¯¯å·®çš„æƒé‡ç‰ˆæœ¬ã€‚
* è¿™äº›æƒé‡è¢«é‡åŒ–ä¸º int4ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šå³æ—¶æ¢å¤ä¸º fp16ã€‚
* These weights are quantized to int4, but theyâ€™re restored to fp16 on the fly during inference.

::

    from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig

    model_id = "facebook/opt-125m"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    gptq_config = GPTQConfig(bits=4, dataset="c4", tokenizer=tokenizer)
    quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto", quantization_config=gptq_config)
    # è®¾ç½®device_map="auto"å¯è‡ªåŠ¨å°†æ¨¡å‹å¸è½½åˆ° CPUï¼Œä»¥å¸®åŠ©å°†æ¨¡å‹æ”¾å…¥å†…å­˜ä¸­ï¼Œå¹¶å…è®¸æ¨¡å‹æ¨¡å—åœ¨ CPU å’Œ GPU ä¹‹é—´ç§»åŠ¨ä»¥è¿›è¡Œé‡åŒ–ã€‚

ExLlama
^^^^^^^

* https://github.com/turboderp/exllama
* ExLlamaæ˜¯Llamaæ¨¡å‹çš„ Python/C++/CUDA å®ç°ï¼Œæ—¨åœ¨ä½¿ç”¨ 4 ä½ GPTQ æƒé‡è¿›è¡Œæ›´å¿«çš„æ¨ç†

::

    import torch
    from transformers import AutoModelForCausalLM, GPTQConfig

    gptq_config = GPTQConfig(bits=4, exllama_config={"version":2})
    model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="auto", quantization_config=gptq_config)


ä»…å½“æ•´ä¸ªæ¨¡å‹ä½äº GPU ä¸Šæ—¶æ‰æ”¯æŒ ExLlama å†…æ ¸ã€‚å¦‚æœæ‚¨ä½¿ç”¨ AutoGPTQï¼ˆç‰ˆæœ¬ > 0.4.2ï¼‰åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œåˆ™éœ€è¦ç¦ç”¨ ExLlama å†…æ ¸::

    import torch
    from transformers import AutoModelForCausalLM, GPTQConfig
    gptq_config = GPTQConfig(bits=4, use_exllama=False)
    model = AutoModelForCausalLM.from_pretrained("{your_username}/opt-125m-gptq", device_map="cpu", quantization_config=gptq_config)


AWQ
---

* Activation-aware Weight Quantization(AWQ): https://hf.co/papers/2306.00978
* ä¸ä¼šé‡åŒ–æ¨¡å‹ä¸­çš„æ‰€æœ‰æƒé‡ï¼Œè€Œæ˜¯ä¿ç•™å¯¹LLMæ€§èƒ½å¾ˆé‡è¦çš„ä¸€å°éƒ¨åˆ†æƒé‡ã€‚è¿™æ˜¾ç€å‡å°‘äº†é‡åŒ–æŸå¤±ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä»¥ 4 ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½•æ€§èƒ½ä¸‹é™ã€‚
* é€šè¿‡å¯¹æ¨¡å‹çš„æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡å¤„ç†ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ•æ‰æƒé‡åˆ†å¸ƒçš„ç‰¹ç‚¹ã€‚AWQåœ¨ä¿ç•™æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¤æ‚åº¦ã€‚ä¸€ç§æ”¹è¿›çš„é‡åŒ–æ–¹æ³•ï¼Œå®ƒé’ˆå¯¹ç¥ç»ç½‘ç»œçš„æƒé‡åˆ†å¸ƒç‰¹ç‚¹ï¼Œé€šè¿‡åŠ æƒå¹³å‡çš„æ–¹å¼é‡åŒ–å‚æ•°ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹çš„ç²¾åº¦ã€‚åœ¨æ¨ç†æ—¶ï¼ŒAWQ å¯ä»¥ä½¿ç”¨ä½ç²¾åº¦çš„æƒé‡è¡¨ç¤ºï¼Œå‡å°‘å­˜å‚¨å’Œè®¡ç®—çš„æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½çš„ç¨³å®šã€‚ä¸ä¼ ç»Ÿçš„é‡åŒ–æŠ€æœ¯ï¼ˆå¦‚ç›´æ¥çš„é€å±‚æˆ–é€é€šé“é‡åŒ–ï¼‰ç›¸æ¯”ï¼ŒAWQ å¯¹æƒé‡åˆ†å¸ƒçš„å¤„ç†æ›´åŠ ç²¾ç»†ï¼Œå› æ­¤åœ¨åŒç­‰é‡åŒ–ç²¾åº¦ä¸‹èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ¨ç†ç»“æœã€‚(ğŸˆ³from LLM)
* æœ‰å‡ ä¸ªç”¨äºä½¿ç”¨ AWQ ç®—æ³•é‡åŒ–æ¨¡å‹çš„åº“ï¼Œä¾‹å¦‚ 
* llm-awq: https://github.com/mit-han-lab/llm-awq
* autoawq: https://github.com/casper-hansen/AutoAWQ>
* optimization-intel: 

Fused modules::

    import torch
    from transformers import AwqConfig, AutoModelForCausalLM

    model_id = "TheBloke/Mistral-7B-OpenOrca-AWQ"

    quantization_config = AwqConfig(
        bits=4,
        fuse_max_seq_len=512,
        do_fuse=True,
    )
    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(0)


AQLM
----

* Additive Quantization of Language Models (AQLM): ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚å®ƒå°†å¤šä¸ªæƒé‡ä¸€èµ·é‡åŒ–å¹¶åˆ©ç”¨å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚ AQLM å°† 8-16 ä¸ªæƒé‡ç»„è¡¨ç¤ºä¸ºå¤šä¸ªçŸ¢é‡ä»£ç çš„æ€»å’Œã€‚

::

    from transformers import AutoTokenizer, AutoModelForCausalLM

    quantized_model = AutoModelForCausalLM.from_pretrained(
        "ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf",
        torch_dtype="auto", 
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf")

Quanto
------

* https://github.com/huggingface/quanto
* Quantoåº“æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ pytorch é‡åŒ–å·¥å…·åŒ…ã€‚ä½¿ç”¨çš„é‡åŒ–æ–¹æ³•æ˜¯çº¿æ€§é‡åŒ–

EETQ
----

* https://github.com/NetEase-FuXi/EETQ
* EETQåº“æ”¯æŒ NVIDIA GPUS çš„ int8 æ¯é€šé“ä»…æƒé‡é‡åŒ–ã€‚
* é«˜æ€§èƒ½GEMMå’ŒGEMVå†…æ ¸æ¥è‡ªFasterTransformerå’ŒTensorRT- LLM ã€‚
* å®ƒä¸éœ€è¦æ ¡å‡†æ•°æ®é›†ï¼Œä¹Ÿä¸éœ€è¦é¢„å…ˆé‡åŒ–æ‚¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç”±äºæ¯é€šé“é‡åŒ–ï¼Œç²¾åº¦ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚


HQQ
---

* https://github.com/mobiusml/hqq/
* Half-Quadratic Quantization (HQQ): é€šè¿‡å¿«é€Ÿé²æ£’ä¼˜åŒ–(fast robust optimization)å®ç°åŠ¨æ€é‡åŒ–(on-the-fly quantization)ã€‚
* å®ƒä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼Œå¯ç”¨äºé‡åŒ–ä»»ä½•æ¨¡å‹ã€‚


FBGEMM FP8
----------

* https://github.com/pytorch/FBGEMM
* the weights will be quantized in 8bit (FP8) per channel
* the activation will be quantized in 8bit (FP8) per token

Optimum
-------

* https://huggingface.co/docs/optimum/index
* Optimumåº“æ”¯æŒ Intelã€Furiosaã€ONNX Runtimeã€GPTQ å’Œè¾ƒä½çº§åˆ« PyTorch é‡åŒ–å‡½æ•°çš„é‡åŒ–ã€‚
* å¦‚æœæ‚¨ä½¿ç”¨ç‰¹å®šçš„ä¼˜åŒ–ç¡¬ä»¶ï¼ˆä¾‹å¦‚ Intel CPUã€Furiosa NPU æˆ– ONNX Runtime ç­‰æ¨¡å‹åŠ é€Ÿå™¨ï¼‰ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ Optimum è¿›è¡Œé‡åŒ–ã€‚

TorchAO
-------

* https://github.com/pytorch/ao
* TorchAOæ˜¯ PyTorch çš„æ¶æ„ä¼˜åŒ–åº“ï¼Œå®ƒæä¾›äº†ç”¨äºæ¨ç†å’Œè®­ç»ƒçš„é«˜æ€§èƒ½æ•°æ®ç±»å‹ã€ä¼˜åŒ–æŠ€æœ¯å’Œå†…æ ¸ï¼Œå…·æœ‰ä¸torch.compile ã€ FSDP ç­‰åŸç”Ÿ PyTorch åŠŸèƒ½çš„å¯ç»„åˆæ€§ã€‚


Compressed Tensors
------------------

* https://github.com/neuralmagic/compressed-tensors
* æä¾›äº†ä¸€ç§é€šç”¨ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥å­˜å‚¨å’Œç®¡ç†å‹ç¼©æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚
* è¯¥åº“æ”¯æŒå„ç§é‡åŒ–å’Œç¨€ç–æ–¹æ¡ˆï¼Œä½¿å…¶æˆä¸ºå¤„ç†ä¸åŒæ¨¡å‹ä¼˜åŒ–çš„ç»Ÿä¸€æ ¼å¼ï¼Œä¾‹å¦‚ GPTQã€AWQã€SmoothQuantã€INT8ã€FP8ã€SparseGPT ç­‰ã€‚



Performance and scalability
===========================


LLM inference optimization
--------------------------


Static kv-cache and torch.compile
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


* ä½¿ç”¨ kv-cache æ¥å­˜å‚¨è¿‡å»çš„é”®å’Œå€¼ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—å®ƒä»¬ã€‚
* ç„¶è€Œï¼Œç”±äº kv-cache åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤éƒ½æ˜¯åŠ¨æ€ä¸”å˜åŒ–çš„ï¼Œå› æ­¤å®ƒä¼šé˜»æ­¢æ‚¨åˆ©ç”¨ torch.compile ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ä¼˜åŒ–å·¥å…·ï¼Œå¯å°† PyTorch ä»£ç èåˆåˆ°å¿«é€Ÿä¸”ä¼˜åŒ–çš„å†…æ ¸ä¸­ã€‚
* kv-cache çš„å®Œæ•´æŒ‡å—: å‚è§ä¸Šé¢çš„ ``Best Practices for Generation with Cache``

* static kv-cache é€šè¿‡å°† kv-cache å¤§å°é¢„å…ˆåˆ†é…ä¸ºæœ€å¤§å€¼æ¥è§£å†³æ­¤é—®é¢˜ï¼Œè¿™å…è®¸æ‚¨å°†å…¶ä¸torch.compileç»“åˆä½¿ç”¨ï¼Œæœ€é«˜å¯æé«˜ 4 å€çš„é€Ÿåº¦ã€‚ç›®å‰ï¼Œåªæœ‰Llamaå’Œå…¶ä»–ä¸€äº›æ¨¡å‹æ”¯æŒ static kv-cache å’Œtorch.compile ã€‚

é™æ€ kv ç¼“å­˜çš„ä½¿ç”¨åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼Œå…·ä½“å–å†³äºä»»åŠ¡çš„å¤æ‚æ€§::

    1. Basic usage: simply set a flag in generation_config (recommended);
    2. Advanced usage: handle a cache object for multi-turn generation or a custom generation loop;
    3. Advanced usage: compile the entire generate function into a single graph, if having a single graph is relevant for you.


Speculative decoding
^^^^^^^^^^^^^^^^^^^^

* æ¨æµ‹è§£ç 
* æ·±å…¥å‚è§åšå®¢æ–‡ç« : Assisted Generation: a new direction toward low-latency text generation: https://hf.co/blog/assisted-generation
* è‡ªå›å½’çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¯¹äºæ¯ä¸ªè¾“å…¥æ ‡è®°ï¼Œæ‚¨éœ€è¦åœ¨å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­æ¯æ¬¡åŠ è½½æ¨¡å‹æƒé‡ã€‚
* å¯¹äºæ‹¥æœ‰æ•°åäº¿å‚æ•°çš„LLMsæ¥è¯´ï¼Œè¿™æ—¢ç¼“æ…¢åˆéº»çƒ¦ã€‚æ¨æµ‹æ€§è§£ç é€šè¿‡ä½¿ç”¨ç¬¬äºŒä¸ªæ›´å°ã€æ›´å¿«çš„è¾…åŠ©æ¨¡å‹æ¥ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œå¹¶åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ç”±æ›´å¤§çš„LLMè¿›è¡ŒéªŒè¯ï¼Œä»è€Œç¼“è§£äº†è¿™ç§é€Ÿåº¦ä¸‹é™çš„æƒ…å†µã€‚
* å¦‚æœéªŒè¯çš„ä»¤ç‰Œæ˜¯æ­£ç¡®çš„ï¼Œ LLMåŸºæœ¬ä¸Šå¯ä»¥â€œå…è´¹â€è·å¾—å®ƒä»¬ï¼Œè€Œä¸å¿…è‡ªå·±ç”Ÿæˆå®ƒä»¬ã€‚

Prompt lookup decoding
""""""""""""""""""""""

* Prompt lookup decoding æ˜¯æ¨æµ‹è§£ç çš„ä¸€ç§å˜ä½“
* æç¤ºæŸ¥æ‰¾å¯¹äºåŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚æ‘˜è¦ï¼‰ç‰¹åˆ«æœ‰æ•ˆï¼Œå…¶ä¸­æç¤ºå’Œè¾“å‡ºä¹‹é—´ç»å¸¸å­˜åœ¨é‡å çš„å•è¯ã€‚è¿™äº›é‡å çš„ n å…ƒè¯­æ³•è¢«ç”¨ä½œLLMå€™é€‰tokenã€‚


Attention optimizations
^^^^^^^^^^^^^^^^^^^^^^^

* Transformer æ¨¡å‹çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—å’Œå†…å­˜ä¸­éšç€è¾“å…¥æ ‡è®°çš„æ•°é‡å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚
* è¿™ç§é™åˆ¶ä¼šåœ¨å¤„ç†æ›´é•¿åºåˆ—çš„LLMsä¸­è¢«æ”¾å¤§ã€‚
* ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯·å°è¯• FlashAttention2 æˆ– PyTorch çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (scaled dot product attention, SDPA)ï¼Œå®ƒä»¬æ˜¯å†…å­˜æ•ˆç‡æ›´é«˜çš„æ³¨æ„åŠ›å®ç°ï¼Œå¯ä»¥åŠ é€Ÿæ¨ç†ã€‚


FlashAttention-2
""""""""""""""""

* FlashAttention å’ŒFlashAttention-2å°†æ³¨æ„åŠ›è®¡ç®—åˆ†è§£ä¸ºæ›´å°çš„å—ï¼Œå¹¶å‡å°‘å¯¹ GPU å†…å­˜çš„ä¸­é—´è¯»/å†™æ“ä½œçš„æ•°é‡ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚
* FlashAttention-2 é€šè¿‡åœ¨åºåˆ—é•¿åº¦ç»´åº¦ä¸Šå¹¶è¡ŒåŒ–ä»¥åŠæ›´å¥½çš„ç¡¬ä»¶åˆ†åŒºå·¥ä½œæ¥æ”¹è¿›åŸå§‹ FlashAttention ç®—æ³•ï¼Œä»¥å‡å°‘åŒæ­¥å’Œé€šä¿¡å¼€é”€ã€‚



PyTorch scaled dot product attention
""""""""""""""""""""""""""""""""""""

* PyTorch 2.0 ä¸­è‡ªåŠ¨å¯ç”¨äº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (SDPA)ï¼Œå®ƒæ”¯æŒ FlashAttentionã€xFormers å’Œ PyTorch çš„ C++ å®ç°ã€‚
* å¦‚æœæ‚¨ä½¿ç”¨ CUDA åç«¯ï¼ŒSDPA ä¼šé€‰æ‹©æ€§èƒ½æœ€ä½³çš„æ³¨æ„åŠ›ç®—æ³•ã€‚å¯¹äºå…¶ä»–åç«¯ï¼ŒSDPA é»˜è®¤ä½¿ç”¨ PyTorch C++ å®ç°ã€‚
* åªè¦æ‚¨å®‰è£…äº†æœ€æ–°çš„ PyTorch ç‰ˆæœ¬ï¼ŒSDPA å°±æ”¯æŒ FlashAttention-2ã€‚

Quantization
^^^^^^^^^^^^

* å¦‚æœæ‚¨ä¸å— GPU çš„é™åˆ¶ï¼Œåˆ™ä¸ä¸€å®šéœ€è¦é‡åŒ–æ¨¡å‹ï¼Œå› ä¸ºé‡åŒ–å’Œåé‡åŒ–æƒé‡æ‰€éœ€çš„é¢å¤–æ­¥éª¤å¯èƒ½ä¼šäº§ç”Ÿè¾ƒå°çš„å»¶è¿Ÿæˆæœ¬ï¼ˆAWQ å’Œèåˆ AWQ æ¨¡å—é™¤å¤–ï¼‰ã€‚




Efficient training techniques
-----------------------------


Methods and tools for efficient training on a single GPU
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘ä¸¤ä¸ªæ–¹é¢::

    1. Data throughput/training time æ•°æ®ååé‡/è®­ç»ƒæ—¶é—´
    2. Model performance æ¨¡å‹æ€§èƒ½

Method/toolä¸å¯¹åº”çš„æ•ˆæœ::

    +----------------------------------------+-------------------------+------------------------------+
    | Method/tool                            | Improves training speed | Optimizes memory utilization |
    +========================================+=========================+==============================+
    | Batch size choice                      | Yes                     | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | Gradient accumulation                  | No                      | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | Gradient checkpointing                 | No                      | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | Mixed precision training               | Yes                     | Maybe*                       |
    +----------------------------------------+-------------------------+------------------------------+
    | torch_empty_cache_steps                | No                      | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | Optimizer choice                       | Yes                     | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | Data preloading                        | Yes                     | No                           |
    +----------------------------------------+-------------------------+------------------------------+
    | DeepSpeed Zero                         | No                      | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+
    | torch.compile                          | Yes                     | No                           |
    +----------------------------------------+-------------------------+------------------------------+
    | Parameter-Efficient Fine Tuning (PEFT) | No                      | Yes                          |
    +----------------------------------------+-------------------------+------------------------------+

Batch size choice
"""""""""""""""""

* ä¸ºäº†å®ç°æœ€ä½³æ€§èƒ½ï¼Œé¦–å…ˆè¦ç¡®å®šé€‚å½“çš„æ‰¹é‡å¤§å°ã€‚å»ºè®®ä½¿ç”¨å¤§å°ä¸º 2^N çš„æ‰¹é‡å¤§å°å’Œè¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°ã€‚é€šå¸¸å®ƒæ˜¯ 8 çš„å€æ•°ï¼Œä½†ä¹Ÿå¯èƒ½æ›´é«˜ï¼Œå…·ä½“å–å†³äºæ‰€ä½¿ç”¨çš„ç¡¬ä»¶å’Œæ¨¡å‹çš„æ•°æ®ç±»å‹ã€‚
* ä½œä¸ºå‚è€ƒï¼Œè¯·æŸ¥çœ‹ NVIDIA å¯¹äºå…¨è¿æ¥å±‚ï¼ˆæ¶‰åŠ GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ï¼‰çš„ `è¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•° <https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features>`_ å’Œ `æ‰¹é‡å¤§å° <https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size>`_ çš„å»ºè®®ã€‚
* For reference, check out NVIDIAâ€™s recommendation for input/output neuron counts and batch size for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).
* å¼ é‡æ ¸å¿ƒè¦æ±‚æ ¹æ®æ•°æ®ç±»å‹å’Œç¡¬ä»¶å®šä¹‰ä¹˜æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº fp16 æ•°æ®ç±»å‹ï¼Œå»ºè®®ä½¿ç”¨ 8 çš„å€æ•°ï¼Œé™¤éæ˜¯ A100 GPUï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ 64 çš„å€æ•°ã€‚



Gradient Accumulation
"""""""""""""""""""""

* æ¢¯åº¦ç´¯ç§¯æ–¹æ³•æ—¨åœ¨ä»¥è¾ƒå°çš„å¢é‡è®¡ç®—æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡è®¡ç®—æ•´ä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦ã€‚
* è¿™ç§æ–¹æ³•æ¶‰åŠé€šè¿‡å‘å‰å’Œå‘åéå†æ¨¡å‹å¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­ç´¯ç§¯æ¢¯åº¦æ¥è¿­ä»£è®¡ç®—è¾ƒå°æ‰¹æ¬¡çš„æ¢¯åº¦ã€‚ä¸€æ—¦ç§¯ç´¯äº†è¶³å¤Ÿæ•°é‡çš„æ¢¯åº¦ï¼Œå°±ä¼šæ‰§è¡Œæ¨¡å‹çš„ä¼˜åŒ–æ­¥éª¤ã€‚
* é€šè¿‡é‡‡ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥å°†æœ‰æ•ˆæ‰¹é‡å¤§å°(effective batch size)å¢åŠ åˆ°è¶…å‡º GPU å†…å­˜å®¹é‡çš„é™åˆ¶ã€‚
* ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¢¯åº¦ç´¯ç§¯å¼•å…¥çš„é¢å¤–å‰å‘å’Œåå‘ä¼ é€’å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚

ç¤ºä¾‹::

    training_args = TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4, **default_args)
    # è¯´æ˜
        1. é€šè¿‡å°†gradient_accumulation_stepså‚æ•°æ·»åŠ åˆ°TrainingArgumentsæ¥å¯ç”¨æ¢¯åº¦ç´¯ç§¯
        2. æœ‰æ•ˆæ‰¹é‡å¤§å°å˜ä¸º 4

Gradient Checkpointing
""""""""""""""""""""""

* å³ä½¿æ‰¹é‡å¤§å°è®¾ç½®ä¸º 1 å¹¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œä¸€äº›å¤§å‹æ¨¡å‹ä»ç„¶å¯èƒ½é¢ä¸´å†…å­˜é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºè¿˜æœ‰å…¶ä»–ç»„ä»¶ä¹Ÿéœ€è¦å†…å­˜å­˜å‚¨ã€‚
* ä¿å­˜å‰å‘ä¼ é€’ä¸­çš„æ‰€æœ‰æ¿€æ´»ä»¥ä¾¿åœ¨åå‘ä¼ é€’æœŸé—´è®¡ç®—æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´æ˜¾ç€çš„å†…å­˜å¼€é”€ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨å‘åä¼ é€’è¿‡ç¨‹ä¸­ä¸¢å¼ƒæ¿€æ´»å¹¶åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å®ƒä»¬ï¼Œè¿™ä¼šå¸¦æ¥ç›¸å½“å¤§çš„è®¡ç®—å¼€é”€å¹¶å‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚
* **æ¢¯åº¦æ£€æŸ¥ç‚¹** æä¾›äº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„æŠ˜è¡·æ–¹æ¡ˆï¼Œå¹¶åœ¨æ•´ä¸ªè®¡ç®—å›¾ä¸­ä¿å­˜äº†æˆ˜ç•¥é€‰æ‹©çš„æ¿€æ´»ï¼Œå› æ­¤åªéœ€ä¸ºæ¢¯åº¦é‡æ–°è®¡ç®—ä¸€å°éƒ¨åˆ†æ¿€æ´»ã€‚æœ‰å…³æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ·±å…¥è§£é‡Šï¼Œè¯·å‚é˜…è¿™ç¯‡ `ç²¾å½©æ–‡ç«  <https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9>`_

ç¤ºä¾‹::

    training_args = TrainingArguments(
        per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
    )


Mixed precision training
""""""""""""""""""""""""

* **æ··åˆç²¾åº¦è®­ç»ƒ** æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡å¯¹æŸäº›å˜é‡ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°å€¼æ ¼å¼æ¥ä¼˜åŒ–è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æ•ˆç‡çš„æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œå¤§å¤šæ•°æ¨¡å‹ä½¿ç”¨ 32 ä½æµ®ç‚¹ç²¾åº¦ï¼ˆfp32 æˆ– float32ï¼‰æ¥è¡¨ç¤ºå’Œå¤„ç†å˜é‡ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å˜é‡éƒ½éœ€è¦å¦‚æ­¤é«˜ç²¾åº¦æ‰èƒ½è·å¾—å‡†ç¡®çš„ç»“æœã€‚é€šè¿‡å°†æŸäº›å˜é‡çš„ç²¾åº¦é™ä½ä¸ºè¾ƒä½çš„æ•°å€¼æ ¼å¼ï¼Œä¾‹å¦‚ 16 ä½æµ®ç‚¹ï¼ˆfp16 æˆ– float16ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚ç”±äºåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæœ‰äº›è®¡ç®—æ˜¯ä»¥åŠç²¾åº¦æ‰§è¡Œçš„ï¼Œè€Œæœ‰äº›è®¡ç®—ä»ç„¶æ˜¯å…¨ç²¾åº¦çš„ï¼Œå› æ­¤è¯¥æ–¹æ³•ç§°ä¸ºæ··åˆç²¾åº¦è®­ç»ƒã€‚

* æœ€å¸¸è§çš„æ··åˆç²¾åº¦è®­ç»ƒæ˜¯é€šè¿‡ä½¿ç”¨ fp16 (float16) æ•°æ®ç±»å‹æ¥å®ç°çš„ï¼Œä½†æ˜¯ï¼Œä¸€äº› GPU æ¶æ„ï¼ˆä¾‹å¦‚ Ampere æ¶æ„ï¼‰æä¾› bf16 å’Œ tf32ï¼ˆCUDA å†…éƒ¨æ•°æ®ç±»å‹ï¼‰æ•°æ®ç±»å‹ã€‚æŸ¥çœ‹ `NVIDIAåšå®¢ <https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/>`_


fp16
++++

* æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸»è¦ä¼˜ç‚¹æ¥è‡ªäºä»¥åŠç²¾åº¦ï¼ˆfp16ï¼‰ä¿å­˜æ¿€æ´»ã€‚å°½ç®¡æ¢¯åº¦ä¹Ÿæ˜¯ä»¥åŠç²¾åº¦è®¡ç®—çš„ï¼Œä½†å®ƒä»¬åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ä¼šè½¬æ¢å›å…¨ç²¾åº¦ï¼Œå› æ­¤æ­¤å¤„ä¸ä¼šèŠ‚çœå†…å­˜ã€‚
* è™½ç„¶æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œä½†å®ƒä¹Ÿä¼šå¯¼è‡´ä½¿ç”¨æ›´å¤š GPU å†…å­˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°æ‰¹é‡å¤§å°ã€‚è¿™æ˜¯å› ä¸ºè¯¥æ¨¡å‹ç°åœ¨ä»¥ 16 ä½å’Œ 32 ä½ç²¾åº¦ï¼ˆGPU ä¸ŠåŸå§‹æ¨¡å‹çš„ 1.5 å€ï¼‰å‘ˆç°åœ¨ GPU ä¸Šã€‚

è¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¯·å°†fp16æ ‡å¿—è®¾ç½®ä¸ºTrue::

    training_args = TrainingArguments(per_device_train_batch_size=4, fp16=True, **default_args)

BF16
++++

* å¦‚æœæ‚¨å¯ä»¥ä½¿ç”¨ Ampere æˆ–æ›´æ–°çš„ç¡¬ä»¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ bf16 è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒå’Œè¯„ä¼°ã€‚
* è™½ç„¶ bf16 çš„ç²¾åº¦æ¯” fp16 å·®ï¼Œä½†å®ƒçš„åŠ¨æ€èŒƒå›´è¦å¤§å¾—å¤šã€‚åœ¨ fp16 ä¸­ï¼Œæ‚¨å¯ä»¥æ‹¥æœ‰çš„æœ€å¤§æ•°å­—æ˜¯65504 ï¼Œä»»ä½•é«˜äºè¯¥æ•°å­—çš„æ•°å­—éƒ½ä¼šå¯¼è‡´æº¢å‡ºã€‚ bf16 æ•°å­—å¯ä»¥å¤§åˆ°3.39e+38 (!)ï¼Œè¿™ä¸ fp32 å¤§è‡´ç›¸åŒ - å› ä¸ºä¸¤è€…éƒ½æœ‰ 8 ä½ç”¨äºæ•°å­—èŒƒå›´ã€‚

åœ¨ ğŸ¤— Trainer ä¸­å¯ç”¨ BF16::

    training_args = TrainingArguments(bf16=True, **default_args)


TF32
++++

* Ampere ç¡¬ä»¶ä½¿ç”¨ä¸€ç§åä¸º tf32 çš„ç¥å¥‡æ•°æ®ç±»å‹ã€‚
* å®ƒå…·æœ‰ä¸ fp32ï¼ˆ8 ä½ï¼‰ç›¸åŒçš„æ•°å€¼èŒƒå›´ï¼Œä½†ä¸æ˜¯ 23 ä½ç²¾åº¦ï¼Œè€Œæ˜¯åªæœ‰ 10 ä½ï¼ˆä¸ fp16 ç›¸åŒï¼‰ï¼Œå¹¶ä¸”æ€»å…±åªä½¿ç”¨ 19 ä½ã€‚
* å®ƒçš„â€œç¥å¥‡â€ä¹‹å¤„åœ¨äºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ™®é€šçš„ fp32 è®­ç»ƒå’Œ/æˆ–æ¨ç†ä»£ç ï¼Œå¹¶ä¸”é€šè¿‡å¯ç”¨ tf32 æ”¯æŒï¼Œæ‚¨å¯ä»¥è·å¾—é«˜è¾¾ 3 å€çš„ååé‡æå‡ã€‚

æ‚¨éœ€è¦åšçš„å°±æ˜¯å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­::

    import torch
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True


ç¤ºä¾‹::

    TrainingArguments(tf32=True, **default_args)

.. note:: tf32 æ— æ³•ç›´æ¥é€šè¿‡tensor.to(dtype=torch.tf32)è®¿é—®ï¼Œå› ä¸ºå®ƒæ˜¯å†…éƒ¨ CUDA æ•°æ®ç±»å‹ã€‚æ‚¨éœ€è¦torch>=1.7æ‰èƒ½ä½¿ç”¨ tf32 æ•°æ®ç±»å‹ã€‚

Flash Attention 2
"""""""""""""""""

* å¯ä»¥é€šè¿‡åœ¨ Transformer ä¸­ä½¿ç”¨ Flash Attention 2 é›†æˆæ¥åŠ é€Ÿè®­ç»ƒååé‡
* å…·ä½“å‚è§åé¢çš„æ¨ç†ä¼˜åŒ–(Optimizing inference)



Optimizer choice
""""""""""""""""

* ç”¨äºè®­ç»ƒ Transformer æ¨¡å‹çš„æœ€å¸¸è§ä¼˜åŒ–å™¨æ˜¯ Adam æˆ– AdamWï¼ˆå¸¦æœ‰æƒé‡è¡°å‡çš„ Adamï¼‰ã€‚
* Adamé€šè¿‡å­˜å‚¨ä¹‹å‰æ¢¯åº¦çš„æ»šåŠ¨å¹³å‡å€¼å®ç°äº†è‰¯å¥½çš„æ”¶æ•›ï¼›ç„¶è€Œï¼Œå®ƒå¢åŠ äº†æ¨¡å‹å‚æ•°æ•°é‡æ•°é‡çº§çš„é¢å¤–å†…å­˜å ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›¿ä»£ä¼˜åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä¸º NVIDIA GPU å®‰è£…äº†NVIDIA/apex ï¼Œæˆ–ä¸º AMD GPU å®‰è£…äº†ROCmSoftwarePlatform/apex ï¼Œ adamw_apex_fusedå°†ä¸ºæ‚¨æä¾›æ‰€æœ‰å—æ”¯æŒçš„ AdamW ä¼˜åŒ–å™¨ä¸­æœ€å¿«çš„è®­ç»ƒä½“éªŒã€‚
* Traineré›†æˆäº†å„ç§å¯ç«‹å³ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼š adamw_hf ã€ adamw_torch ã€ adamw_torch_fused ã€ adamw_apex_fused ã€ adamw_anyprecision ã€ adafactoræˆ–adamw_bnb_8bit ã€‚å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å®ç°æ’å…¥æ›´å¤šä¼˜åŒ–å™¨ã€‚

Data preloading
"""""""""""""""

* é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€åˆ‡éƒ½å‘ç”Ÿåœ¨ä¸»è¿›ç¨‹ä¸­ï¼Œå®ƒå¯èƒ½æ— æ³•è¶³å¤Ÿå¿«åœ°ä»ç£ç›˜è¯»å–æ•°æ®ï¼Œä»è€Œäº§ç”Ÿç“¶é¢ˆï¼Œå¯¼è‡´ GPU åˆ©ç”¨ç‡ä¸è¶³

DeepSpeed ZeRO
""""""""""""""

* DeepSpeed æ˜¯ä¸€ä¸ªå¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œä¸ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerate é›†æˆã€‚å®ƒæä¾›äº†å¹¿æ³›çš„åŠŸèƒ½å’Œä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚
* å¦‚æœæ‚¨çš„æ¨¡å‹é€‚åˆå•ä¸ª GPU å¹¶ä¸”æ‚¨æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥å®¹çº³å°æ‰¹é‡å¤§å°ï¼Œåˆ™ä¸éœ€è¦ä½¿ç”¨ DeepSpeedï¼Œå› ä¸ºå®ƒåªä¼šå‡æ…¢é€Ÿåº¦ã€‚
* ä½†æ˜¯ï¼Œå¦‚æœæ¨¡å‹ä¸é€‚åˆå•ä¸ª GPUï¼Œæˆ–è€…æ‚¨æ— æ³•é€‚åº”å°æ‰¹é‡ï¼Œåˆ™å¯ä»¥åˆ©ç”¨ DeepSpeed ZeRO + CPU Offload æˆ– NVMe Offload æ¥å¤„ç†æ›´å¤§çš„æ¨¡å‹ã€‚


Using torch.compile
"""""""""""""""""""

ç¤ºä¾‹::

    training_args = TrainingArguments(torch_compile=True, **default_args)

* torch.compile uses Pythonâ€™s frame evaluation API to automatically ``create a graph`` from existing PyTorch programs. After capturing the graph,å¯ä»¥éƒ¨ç½²ä¸åŒçš„åç«¯æ¥å°†å›¾è¡¨é™ä½åˆ°ä¼˜åŒ–çš„å¼•æ“ã€‚
* create a graph: é€šè¿‡ torch.compile è‡ªåŠ¨å°†ç°æœ‰çš„ PyTorch ç¨‹åºè½¬æ¢æˆè®¡ç®—å›¾ï¼ˆcomputation graphï¼‰

    * å…·ä½“æ¥è¯´ï¼ŒPyTorch é€šå¸¸æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼ˆå³åŠ¨æ€å›¾ï¼Œä¹Ÿå« eager executionï¼‰ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªæ“ä½œï¼ˆå¦‚å¼ é‡åŠ æ³•ã€çŸ©é˜µä¹˜æ³•ç­‰ï¼‰éƒ½ä¼šç«‹å³æ‰§è¡Œã€‚
    * è€Œ torch.compile ä½¿ç”¨ Python çš„ "frame evaluation API"ï¼Œå°†è¿™äº›åŠ¨æ€çš„æ“ä½œæ•è·ä¸‹æ¥ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªä¼˜åŒ–åçš„é™æ€è®¡ç®—å›¾ï¼ˆstatic computation graphï¼‰ã€‚
    * è¿™ä¸ªè®¡ç®—å›¾åŒ…å«äº†æ•´ä¸ªæ¨¡å‹çš„æ“ä½œé¡ºåºå’Œä¾èµ–å…³ç³»ï¼Œç›¸å½“äºä¸€ç§é«˜æ•ˆçš„è¡¨è¾¾æ–¹å¼ã€‚é€šè¿‡å°†æ¨¡å‹çš„æ“ä½œå˜æˆå›¾ç»“æ„ï¼Œåç«¯å¯ä»¥å¯¹å…¶è¿›è¡Œä¼˜åŒ–å’ŒåŠ é€Ÿï¼Œåˆ©ç”¨ç¡¬ä»¶æ›´å¥½åœ°æ‰§è¡Œè¿™äº›æ“ä½œï¼Œæ¯”å¦‚é€šè¿‡ç¼–è¯‘æˆæ›´é«˜æ•ˆçš„ä»£ç æˆ–è€…åœ¨ä¸åŒçš„ç¡¬ä»¶æ¶æ„ä¸Šæ‰§è¡Œã€‚
    * å› æ­¤ï¼Œ"create a graph" çš„æ„æ€æ˜¯ï¼štorch.compile å°†åŸæœ¬æŒ‰æ­¥éª¤æ‰§è¡Œçš„æ¨¡å‹ä»£ç è½¬æ¢ä¸ºä¸€ä¸ªå¯ä¼˜åŒ–çš„å›¾ç»“æ„ï¼Œä¾¿äºè¿›ä¸€æ­¥çš„æ€§èƒ½ä¼˜åŒ–ã€‚


æœ€å¸¸ç”¨çš„åç«¯::

    1. Debugging backends:
        dynamo.optimize("eager")
        dynamo.optimize("aot_eager")

    2. Training & inference backends:
        dynamo.optimize("inductor")
        dynamo.optimize("nvfuser")
        dynamo.optimize("aot_nvfuser")
        dynamo.optimize("aot_cudagraphs")

    3. Inference-only backends:
        dynamo.optimize("ofi")
        dynamo.optimize("fx2trt")
        dynamo.optimize("onnxrt")
        dynamo.optimize("ipex")


* æœ‰å…³å°†torch.compileä¸ ğŸ¤— Transformer ç»“åˆä½¿ç”¨çš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡å…³äºä½¿ç”¨æœ€æ–° PyTorch 2.0 åŠŸèƒ½å¾®è°ƒç”¨äºæ–‡æœ¬åˆ†ç±»çš„ BERT æ¨¡å‹çš„åšå®¢æ–‡ç« : https://www.philschmid.de/getting-started-pytorch-2-0-transformers




Using ğŸ¤— PEFT
"""""""""""""""""

* å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å™¨ï¼‰ã€‚
* As a result the memory associated to the ``optimizer states and gradients`` are greatly reduced.

å¯¹äºæ™®é€š AdamWï¼Œä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜è¦æ±‚ä¸º::

    1. fp32 copy of parameters: 4 bytes/param
    2. Momentum: 4 bytes/param
    3. Variance: 4 bytes/param

ä¸€ä¸ª7Bçš„æ¨¡å‹ and 200 million parameters injected with ``Low Rank Adapters`` ::

    æ™®é€šæ¨¡å‹ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚: 
        12 * 7 = 84 GB
    æ·»åŠ  Lora ä¼šç¨å¾®å¢åŠ ä¸æ¨¡å‹æƒé‡ç›¸å…³çš„å†…å­˜ï¼Œä½†ä¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚å¤§å¹…é™ä½è‡³ 
        12 * 0.2 = 2.4GB



Using ğŸ¤— Accelerate
""""""""""""""""""""""

ç¤ºä¾‹::

    training_args = TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        gradient_checkpointing=True,
        fp16=True,
        **default_args,
    )
    # å®Œæ•´ç¤ºä¾‹è®­ç»ƒå¾ªç¯
    from accelerate import Accelerator
    from torch.utils.data.dataloader import DataLoader

    dataloader = DataLoader(ds, batch_size=training_args.per_device_train_batch_size)

    if training_args.gradient_checkpointing:
        model.gradient_checkpointing_enable()

    accelerator = Accelerator(fp16=training_args.fp16)
    model, optimizer, dataloader = accelerator.prepare(model, adam_bnb_optim, dataloader)

    model.train()
    for step, batch in enumerate(dataloader, start=1):
        loss = model(**batch).loss
        loss = loss / training_args.gradient_accumulation_steps
        accelerator.backward(loss)
        if step % training_args.gradient_accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()


Multiple GPUs and parallelism
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* é‡‡ç”¨å¤šç§æŠ€æœ¯æ¥å®ç°å¹¶è¡Œæ€§ï¼Œä¾‹å¦‚æ•°æ®å¹¶è¡Œæ€§ã€å¼ é‡å¹¶è¡Œæ€§å’Œç®¡é“å¹¶è¡Œæ€§ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ²¡æœ‰ä¸€ç§ä¸‡èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œæœ€ä½³è®¾ç½®å–å†³äºæ‚¨æ‰€ä½¿ç”¨çš„ç‰¹å®šç¡¬ä»¶é…ç½®ã€‚

Scalability strategy::

    1. Parallelization strategy for a single Node / multi-GPU setup
        Case 1: Your model fits onto a single GPU
            DDP - Distributed DataParallel
            Zero Redundancy Optimizer (ZeRO): https://arxiv.org/abs/1910.02054
        Case 2: Your model doesnâ€™t fit onto a single GPU:
            PipelineParallel (PP)
            ZeRO
            TensorParallel (TP)
        Case 3: Largest layer of your model does not fit onto a single GPU
            If you are not using ZeRO, you have to use TensorParallel (TP), because PipelineParallel (PP) alone wonâ€™t be sufficient to accommodate the large layer.
            If you are using ZeRO, additionally adopt techniques from the Methods and tools for efficient training on a single GPU.

    2. Parallelization strategy for a multi Node / multi-GPU setup
        When you have fast inter-node connectivity (e.g., NVLINK or NVSwitch) consider using one of these options:
            ZeRO - as it requires close to no modifications to the model
            A combination of PipelineParallel(PP) with TensorParallel(TP) and DataParallel(DP)
        When you have slow inter-node connectivity and still low on GPU memory:
            Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP), and ZeRO.




Fully Sharded Data Parallel
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* `Fully Sharded Data Parallel (FSDP) <https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/>`_ : å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ (FSDP)æ˜¯ä¸€ç§æ•°æ®å¹¶è¡Œæ–¹æ³•ï¼Œå¯å°†æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¯ç”¨ GPUï¼ˆä¹Ÿç§°ä¸ºå·¥ä½œçº¿ç¨‹æˆ–ç­‰çº§ï¼‰çš„æ•°é‡ä¸Šã€‚
* ä¸DistributedDataParallelï¼ˆDDPï¼‰ä¸åŒï¼ŒFSDPé€šè¿‡åœ¨æ¯ä¸ªGPUä¸Šè¿›è¡Œæ¨¡å‹åˆ†ç‰‡è€Œéå®Œæ•´å¤åˆ¶ï¼Œä»è€Œé™ä½äº†å†…å­˜ä½¿ç”¨ã€‚è¿™æé«˜äº† GPU å†…å­˜æ•ˆç‡ï¼Œå¹¶å…è®¸æ‚¨åœ¨æ›´å°‘çš„ GPU ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚
* Unlike DistributedDataParallel (DDP), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs.
* FSDP is integrated with the Accelerate






DeepSpeed
^^^^^^^^^

* DeepSpeedæ˜¯ä¸€ä¸ª PyTorch ä¼˜åŒ–åº“ï¼Œå¯æé«˜åˆ†å¸ƒå¼è®­ç»ƒçš„å†…å­˜æ•ˆç‡å’Œé€Ÿåº¦ã€‚å…¶æ ¸å¿ƒæ˜¯é›¶å†—ä½™ä¼˜åŒ–å™¨ï¼ˆZeROï¼‰ï¼Œå®ƒå¯ä»¥å¤§è§„æ¨¡è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚ 


Efficient Training on CPU
^^^^^^^^^^^^^^^^^^^^^^^^^

::

    training_args = TrainingArguments(
        output_dir=args.output_path,
    +   bf16=True,
    +   use_ipex=True,
    +   use_cpu=True,
        **kwargs
    )


Distributed CPU training
^^^^^^^^^^^^^^^^^^^^^^^^^

* åŸºäº PyTorch çš„ DDPæ”¯æŒè¿›è¡Œåˆ†å¸ƒå¼ CPU è®­ç»ƒ


Optimizing inference
--------------------

CPU inference
^^^^^^^^^^^^^

* é€šè¿‡ä¸€äº›ä¼˜åŒ–ï¼Œå¯ä»¥åœ¨ CPU ä¸Šé«˜æ•ˆè¿è¡Œå¤§å‹æ¨¡å‹æ¨ç†ã€‚
* å…¶ä¸­ä¸€ç§ä¼˜åŒ–æŠ€æœ¯æ¶‰åŠå°† PyTorch ä»£ç ç¼–è¯‘ä¸ºé€‚ç”¨äº C++ ç­‰é«˜æ€§èƒ½ç¯å¢ƒçš„ä¸­é—´æ ¼å¼ã€‚
* å¦ä¸€ç§æŠ€æœ¯å°†å¤šä¸ªæ“ä½œèåˆåˆ°ä¸€ä¸ªå†…æ ¸ä¸­ï¼Œä»¥å‡å°‘å•ç‹¬è¿è¡Œæ¯ä¸ªæ“ä½œçš„å¼€é”€ã€‚

ğŸ¤— Optimum
"""""""""""""

* ONNX Runtime (ORT) æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œé»˜è®¤åœ¨ CPU ä¸Šè¿è¡Œæ¨ç†ã€‚

.. code-block:: python

    from transformers import AutoTokenizer, pipeline
    from optimum.onnxruntime import ORTModelForQuestionAnswering

    model = ORTModelForQuestionAnswering.from_pretrained("optimum/roberta-base-squad2")
    tokenizer = AutoTokenizer.from_pretrained("deepset/roberta-base-squad2")

    onnx_qa = pipeline("question-answering", model=model, tokenizer=tokenizer)

    question = "What's my name?"
    context = "My name is Philipp and I live in Nuremberg."
    pred = onnx_qa(question, context)




GPU inference
^^^^^^^^^^^^^

FlashAttention-2
""""""""""""""""

.. warning:: FlashAttention-2 æ˜¯å®éªŒæ€§çš„ï¼Œåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å¯èƒ½ä¼šå‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ã€‚

* FlashAttention-2æ˜¯æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„å®ç°ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾ç€åŠ é€Ÿæ¨ç†::

    - additionally parallelizing the attention computation over sequence length
    - partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them


* è¦å¯ç”¨ FlashAttention-2ï¼Œè¯·ä¼ é€’å‚æ•° attn_implementation="flash_attention_2" åˆ°from_pretrained() ï¼š

å®‰è£…::

    pip install flash-attn --no-build-isolation



BetterTransformer
"""""""""""""""""

BetterTransformer é€šè¿‡å…¶ fastpath æ‰§è¡ŒåŠ é€Ÿæ¨ç†ã€‚fastpath æ‰§è¡Œä¸­çš„ä¸¤ä¸ªä¼˜åŒ–æ˜¯::

    fusion, which combines multiple sequential operations into a single â€œkernelâ€ to reduce the number of computation steps
    skipping the inherent sparsity of padding tokens to avoid unnecessary computation with nested tensors


bitsandbytes
""""""""""""

* bitsandbytes æ˜¯ä¸€ä¸ªé‡åŒ–åº“ï¼Œæ”¯æŒ 4 ä½å’Œ 8 ä½é‡åŒ–ã€‚ä¸åŸç”Ÿå…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œé‡åŒ–å¯å‡å°æ¨¡å‹å¤§å°ï¼Œä»è€Œæ›´è½»æ¾åœ°å°†å¤§å‹æ¨¡å‹å®‰è£…åˆ°å†…å­˜æœ‰é™çš„ GPU ä¸Šã€‚
* åšå®¢- ä½¿ç”¨ Hugging Face Transformersã€Accelerate å’Œ BitsandBytes è¿›è¡Œå¤§è§„æ¨¡å˜å‹å™¨çš„ 8 ä½çŸ©é˜µä¹˜æ³•ç®€ä»‹: https://huggingface.co/blog/hf-bitsandbytes-integration

::

    pip install bitsandbytes>=0.39.0 accelerate>=0.20.0




Others
------

Optimize inference using torch.compile()
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* æœ¬èŠ‚æ—¨åœ¨ä¸ºğŸ¤— Transformers ä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„torch.compile()å¼•å…¥çš„æ¨ç†åŠ é€Ÿæä¾›åŸºå‡†ã€‚

* æ ¹æ®æ¨¡å‹å’Œ GPUï¼Œ torch.compile()åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯å®ç°é«˜è¾¾ 30% çš„åŠ é€Ÿ

ç¤ºä¾‹::

    from transformers import AutoModelForImageClassification

    model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to("cuda")
    + model = torch.compile(model)





Conceptual guides
=================

Glossary
--------


DataParallel (DP)
^^^^^^^^^^^^^^^^^

* ã€åŸå§‹ã€‘Parallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance receiving a distinct data slice. The processing is done in parallel and all setups are synchronized at the end of each training step.
* ã€å®šä¹‰ã€‘Data Parallelismï¼ˆæ•°æ®å¹¶è¡Œï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„å¹¶è¡Œè®­ç»ƒæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºå¤š GPU çš„**è®­ç»ƒ**åœºæ™¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡åœ¨å¤šä¸ª GPU ä¸Šå¤åˆ¶ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ï¼Œå¹¶å°†è¾“å…¥æ•°æ®åˆ’åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„éƒ¨åˆ†ï¼Œä½¿å¾—æ¯ä¸ª GPU å¤„ç†ä¸åŒçš„æ•°æ®å­é›†ï¼Œä»è€Œå¹¶è¡Œæ‰§è¡Œè®¡ç®—ä»»åŠ¡ã€‚æ¯ä¸ª GPU å¤„ç†è‡ªå·±çš„æ•°æ®å—ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œæœ€ç»ˆæ‰€æœ‰ GPU çš„æ¢¯åº¦ä¼šè¿›è¡Œæ±‡æ€»ï¼ˆåŒæ­¥ï¼‰ï¼Œå¹¶æ›´æ–°æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚
* ã€æ ¸å¿ƒæ€æƒ³ã€‘å°†æ•°æ®åˆ†ç‰‡ï¼Œæ¨¡å‹å¤åˆ¶ï¼Œåœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œè®¡ç®—ã€‚

    * æ¨¡å‹å¤åˆ¶ï¼šæ¯ä¸ª GPU éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªå®Œå…¨ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ã€‚æ‰€æœ‰çš„ GPU éƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ç»“æ„å’Œæƒé‡è¿›è¡Œè®¡ç®—ã€‚
    * æ•°æ®åˆ’åˆ†ï¼šè®­ç»ƒæ•°æ®è¢«åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†ï¼Œæ¯ä¸ª GPU å¤„ç†ä¸åŒçš„å­é›†ï¼Œå®Œæˆå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚
    * æ¢¯åº¦åŒæ­¥ï¼šåœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹è®¡ç®—å®Œæ¢¯åº¦åï¼Œæ‰€æœ‰ GPU ä¹‹é—´ä¼šé€šè¿‡é€šä¿¡æœºåˆ¶å°†æ¢¯åº¦è¿›è¡Œæ±‡æ€»æˆ–å¹³å‡ï¼Œç„¶åæ›´æ–°æ¯ä¸ªæ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚è¿™ä¿è¯äº†æ‰€æœ‰ GPU çš„æ¨¡å‹åœ¨æ¯æ¬¡è®­ç»ƒæ­¥éª¤ç»“æŸåä¿æŒåŒæ­¥ã€‚

* ã€å·¥ä½œåŸç†ã€‘

    * æ•°æ®åˆ’åˆ†ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 1024 æ¡æ ·æœ¬çš„æ‰¹æ¬¡ï¼ˆbatchï¼‰ï¼Œå¹¶ä¸”ä½ æœ‰ 4 ä¸ª GPUã€‚Data Parallelism ä¼šå°†è¿™ 1024 æ¡æ ·æœ¬åˆ’åˆ†æˆ 4 ä¸ªå­é›†ï¼ˆæ¯ä¸ªå­é›† 256 æ¡æ ·æœ¬ï¼‰ï¼Œå¹¶åˆ†é…åˆ°ä¸åŒçš„ GPU ä¸Šã€‚
    * æ¨¡å‹å¤åˆ¶ï¼šæ¯ä¸ª GPU ä¸Šéƒ½ä¼šæœ‰ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ã€‚è¿™äº›å‰¯æœ¬ä¼šåˆå§‹åŒ–ä¸ºç›¸åŒçš„æƒé‡ï¼Œå¹¶ä¸”åœ¨æ¯ä¸€æ­¥è®­ç»ƒä¸­ï¼Œå®ƒä»¬çš„è®¡ç®—éƒ½æ˜¯åŒæ­¥çš„ã€‚
    * å¹¶è¡Œè®¡ç®—ï¼šæ¯ä¸ª GPU ç‹¬ç«‹åœ°å¤„ç†è‡ªå·±åˆ†é…åˆ°çš„æ•°æ®å­é›†ï¼Œæ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆforward passï¼‰å’Œåå‘ä¼ æ’­ï¼ˆbackward passï¼‰ã€‚è¿™ä¸€éƒ¨åˆ†è®¡ç®—æ˜¯å¹¶è¡Œè¿›è¡Œçš„ï¼Œæ¯ä¸ª GPU çš„è®¡ç®—äº’ä¸å¹²æ‰°ã€‚
    * æ¢¯åº¦åŒæ­¥ï¼šå½“æ¯ä¸ª GPU è®¡ç®—å®Œåå‘ä¼ æ’­å¹¶å¾—åˆ°æ¢¯åº¦åï¼Œæ‰€æœ‰ GPU ä¼šè¿›è¡Œæ¢¯åº¦åŒæ­¥ã€‚è¿™æ„å‘³ç€å„ GPU ä¹‹é—´ä¼šé€šè¿‡ç½‘ç»œé€šä¿¡ï¼Œå°†å®ƒä»¬å„è‡ªè®¡ç®—çš„æ¢¯åº¦æ±‡æ€»ï¼ˆé€šå¸¸æ˜¯å–å¹³å‡ï¼‰ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ä¸€è‡´æ›´æ–°ã€‚
    * å‚æ•°æ›´æ–°ï¼šæ¢¯åº¦åŒæ­¥å®Œæˆåï¼Œæ¯ä¸ª GPU ä¼šæ›´æ–°è‡ªå·±æ¨¡å‹çš„å‚æ•°ã€‚è¿™äº›å‚æ•°ä¼šé€šè¿‡æ±‡æ€»åçš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ï¼Œä»è€Œä½¿å¾—æ‰€æœ‰ GPU ä¸Šçš„æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸåä¿æŒç›¸åŒçš„æƒé‡ã€‚

* ã€ä¼˜åŠ¿ã€‘

    * è®¡ç®—åŠ é€Ÿï¼šé€šè¿‡å°†å¤§æ‰¹é‡æ•°æ®åˆ†æˆå¤šä¸ªå°å—ï¼Œå¹¶è¡Œå¤„ç†ä¸åŒçš„éƒ¨åˆ†ï¼Œå¯ä»¥æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚ç†è®ºä¸Šï¼Œä½¿ç”¨ N ä¸ª GPU è¿›è¡Œ Data Parallelismï¼Œå¯ä»¥å®ç°è¿‘ä¼¼ N å€çš„åŠ é€Ÿæ•ˆæœï¼ˆå—é™äºé€šä¿¡å¼€é”€å’Œè´Ÿè½½å‡è¡¡ï¼‰ã€‚
    * æ˜“äºå®ç°ï¼šç›¸æ¯”å…¶ä»–å¹¶è¡ŒæŠ€æœ¯ï¼ˆå¦‚ Tensor Parallelism æˆ– Pipeline Parallelismï¼‰ï¼ŒData Parallelism çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå› ä¸ºåªéœ€è¦å¤åˆ¶æ¨¡å‹ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†å’Œæ¢¯åº¦åŒæ­¥ã€‚
    * æ‰©å±•æ€§ï¼šData Parallelism å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°å¤šä¸ª GPUï¼Œç”šè‡³å¤šä¸ªæœºå™¨ï¼ˆé€šè¿‡åˆ†å¸ƒå¼è®­ç»ƒï¼‰ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†ã€‚

* ã€åŠ£åŠ¿ã€‘

    * æ˜¾å­˜å‹åŠ›ï¼šæ¯ä¸ª GPU ä¸Šéƒ½éœ€è¦å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å‚æ•°ä¼šè¢«å¤šæ¬¡å¤åˆ¶ã€‚å¦‚æœæ¨¡å‹éå¸¸å¤§ï¼ˆä¾‹å¦‚ GPT-3 è¿™æ ·çš„æ¨¡å‹ï¼‰ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚
    * é€šä¿¡å¼€é”€ï¼šåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸæ—¶ï¼Œæ‰€æœ‰ GPU éœ€è¦åŒæ­¥æ¢¯åº¦ã€‚éšç€ GPU æ•°é‡çš„å¢åŠ ï¼Œé€šä¿¡å¼€é”€ä¼šé€æ¸å¢åŠ ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªæœºå™¨ä¹‹é—´è¿›è¡ŒåŒæ­¥æ—¶ï¼Œç½‘ç»œé€šä¿¡å¯èƒ½ä¼šæˆä¸ºç“¶é¢ˆã€‚
    * è´Ÿè½½å‡è¡¡ï¼šå¦‚æœæ•°æ®åˆ’åˆ†ä¸å‡åŒ€ï¼ŒæŸäº› GPU å¯èƒ½éœ€è¦å¤„ç†è¾ƒé‡çš„å·¥ä½œï¼Œè€Œå…¶ä»– GPU åˆ™å¯èƒ½å¤„äºé—²ç½®çŠ¶æ€ï¼Œè¿™ä¼šå½±å“å¹¶è¡Œæ•ˆç‡ã€‚


* ã€æ€»ç»“ã€‘Data Parallelism æ˜¯ä¸€ç§å°†æ¨¡å‹å‰¯æœ¬åˆ†é…åˆ°å¤šä¸ª GPU å¹¶è¡Œå¤„ç†ä¸åŒæ•°æ®å­é›†çš„è®­ç»ƒæŠ€æœ¯ã€‚é€šè¿‡åœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œå¤„ç†ï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯é€‚ç”¨äºå¤§å‹æ•°æ®é›†çš„å¤„ç†åœºæ™¯ã€‚è™½ç„¶å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†æ˜¾å­˜æ¶ˆè€—å’Œé€šä¿¡å¼€é”€æ˜¯ Data Parallelism é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚




PipelineParallel (PP)
^^^^^^^^^^^^^^^^^^^^^

* ã€åŸå§‹æ–‡æ¡£ã€‘Parallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and working on a small chunk of the batch. Learn more about how PipelineParallel works here.
* ã€å®šä¹‰ã€‘Pipeline Parallelismï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„å¹¶è¡ŒæŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºè®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†æ¨¡å‹æŒ‰å±‚çº§è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶å°†è¿™äº›åˆ’åˆ†åçš„éƒ¨åˆ†åˆ†é…åˆ°ä¸åŒçš„ GPU ä¸Šï¼Œä»è€Œåœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œå¤„ç†æ¨¡å‹çš„è®¡ç®—ä»»åŠ¡ã€‚æ¯ä¸ª GPU åªè´Ÿè´£æ‰§è¡Œæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆå³æŸäº›ç‰¹å®šçš„å±‚ï¼‰ã€‚è¿™ç§åˆ’åˆ†æ–¹å¼è¢«ç§°ä¸ºçºµå‘åˆ‡åˆ†ï¼ˆvertical splitï¼‰ï¼Œç›¸å¯¹ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œï¼ˆdata parallelismï¼‰ï¼Œå®ƒä¸æ˜¯åœ¨ä¸åŒè®¾å¤‡ä¸Šå¤„ç†ç›¸åŒçš„æ¨¡å‹ï¼Œè€Œæ˜¯å°†æ¨¡å‹æœ¬èº«æ‹†åˆ†å¼€æ¥ã€‚
* ã€å·¥ä½œåŸç†ã€‘

    * 1.æ¨¡å‹åˆ’åˆ†ï¼ˆåˆ†å±‚ï¼‰ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ª 12 å±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½ å¯ä»¥å°†å‰ 4 å±‚æ”¾åœ¨ GPU 1 ä¸Šï¼Œæ¥ä¸‹æ¥çš„ 4 å±‚æ”¾åœ¨ GPU 2 ä¸Šï¼Œæœ€å 4 å±‚æ”¾åœ¨ GPU 3 ä¸Šã€‚æ¯ä¸ª GPU åªå­˜å‚¨å’Œè®¡ç®—æ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚
    * 2.æ‰¹é‡å¤„ç†ï¼šPipeline Parallelism é€šå¸¸ä¸æ‰¹å¤„ç†ï¼ˆbatch processingï¼‰ç»“åˆä½¿ç”¨ã€‚å‡è®¾è¾“å…¥çš„ batch æ˜¯ 128 æ¡æ ·æœ¬ï¼šGPU 1 å¤„ç†å‰ 4 å±‚æ—¶ï¼Œå®ƒä¼šå¤„ç†ç¬¬ä¸€å°å—æ•°æ®ï¼ˆæ¯”å¦‚ 64 æ¡æ ·æœ¬ï¼‰ï¼Œç„¶åå°†è¿™äº›æ ·æœ¬çš„è¾“å‡ºä¼ é€’ç»™ GPU 2ã€‚å½“ GPU 2 å¼€å§‹å¤„ç†è¿™äº›æ ·æœ¬æ—¶ï¼ŒGPU 1 å¯ä»¥å¼€å§‹å¤„ç† batch ä¸­çš„ä¸‹ä¸€å°å—æ•°æ®ã€‚è¿™æ ·ï¼Œå¤šä¸ª GPU èƒ½å¤Ÿå¹¶è¡Œå·¥ä½œï¼Œåƒæµæ°´çº¿ä¸€æ ·å¤„ç†æ•°æ®ï¼Œè¿™å°±æ˜¯æµæ°´çº¿å¹¶è¡Œçš„åç§°æ¥æºã€‚
    * 3. æµæ°´çº¿æœºåˆ¶ï¼šå„ä¸ª GPU å¹¶ä¸æ˜¯å®Œå…¨ç‹¬ç«‹å·¥ä½œçš„ï¼Œè€Œæ˜¯æŒ‰é¡ºåºå¤„ç†æ•°æ®ã€‚æ¨¡å‹çš„æ¯ä¸€éƒ¨åˆ†ï¼ˆå±‚ï¼‰ä¾èµ–äºå‰ä¸€éƒ¨åˆ†çš„è¾“å‡ºã€‚è™½ç„¶å„ä¸ª GPU æ˜¯å¹¶è¡Œçš„ï¼Œä½†å®ƒä»¬å·¥ä½œåœ¨åŒä¸€æ¡æµæ°´çº¿ä¸Šï¼šå½“ GPU 1 å¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®å—æ—¶ï¼ŒGPU 2 å¤„äºç©ºé—²çŠ¶æ€ï¼›å½“ GPU 1 å¤„ç†å®Œç¬¬ä¸€ä¸ªæ•°æ®å—å¹¶ä¼ é€’ç»™ GPU 2 æ—¶ï¼ŒGPU 2 å¼€å§‹å¤„ç†ç¬¬ä¸€å—æ•°æ®ï¼ŒåŒæ—¶ GPU 1 å¯ä»¥å¤„ç†ç¬¬äºŒå—æ•°æ®ï¼›å¦‚æ­¤å¾ªç¯ï¼Œç›´åˆ°æ•´ä¸ª batch è¢«å¤„ç†å®Œæ¯•ã€‚

* ã€å¥½å¤„ã€‘

    * èŠ‚çœæ˜¾å­˜ï¼šå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œå•ä¸ª GPU å¯èƒ½æ— æ³•ä¸€æ¬¡æ€§å®¹çº³æ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å±‚ã€‚é€šè¿‡å°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œæ¯ä¸ª GPU åªå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œæ˜¾è‘—å‡å°‘äº†å•ä¸ª GPU çš„æ˜¾å­˜å‹åŠ›ã€‚
    * å¹¶è¡Œæ•ˆç‡ï¼šPipeline Parallelism é€šè¿‡è®©ä¸åŒçš„ GPU åŒæ—¶å¤„ç†ä¸åŒçš„æ•°æ®å—ï¼Œå¢åŠ äº†è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡éœ€è¦ä¸€å®šçš„é€šä¿¡å’ŒåŒæ­¥ï¼Œä½†ç›¸æ¯”äºåœ¨å•ä¸ª GPU ä¸Šè¿è¡Œå®Œæ•´æ¨¡å‹ï¼Œæµæ°´çº¿å¹¶è¡Œå¯ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

* ã€æŒ‘æˆ˜ã€‘

    * é€šä¿¡å¼€é”€ï¼šç”±äºä¸åŒ GPU ä¹‹é—´éœ€è¦ç›¸äº’ä¼ é€’æ•°æ®ï¼ˆå³å‰ä¸€å±‚çš„è¾“å‡ºéœ€è¦ä¼ é€’åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼‰ï¼ŒGPU ä¹‹é—´çš„é€šä¿¡å¸¦æ¥äº†ä¸€å®šçš„å¼€é”€ï¼Œç‰¹åˆ«æ˜¯å½“ GPU æ•°é‡è¾ƒå¤šæ—¶ï¼Œè¿™ç§å¼€é”€å¯èƒ½ä¼šå˜å¾—æ˜¾è‘—ã€‚
    * å»¶è¿Ÿï¼šæµæ°´çº¿å¹¶è¡Œä¼šæœ‰ä¸€å®šçš„å¯åŠ¨å»¶è¿Ÿï¼ˆå³å‰ä¸€ä¸ªè®¾å¤‡å¿…é¡»å…ˆå¤„ç†å®Œéƒ¨åˆ†æ•°æ®åï¼Œæ‰èƒ½å°†æ•°æ®ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡ï¼‰ã€‚å¯¹äºå° batch sizeï¼Œè¿™ç§å»¶è¿Ÿä¼šæ›´åŠ æ˜æ˜¾ã€‚
    * è´Ÿè½½å‡è¡¡ï¼šæ¨¡å‹å„å±‚çš„è®¡ç®—å¤æ‚åº¦ä¸åŒï¼ŒæŸäº›å±‚å¯èƒ½éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚å¦‚æœæ¯ä¸ª GPU å¤„ç†çš„å±‚æ•°ç›¸åŒï¼Œä½†è®¡ç®—é‡ä¸åŒï¼Œå°±ä¼šå¯¼è‡´æŸäº› GPU å·¥ä½œè´Ÿè½½é‡ï¼Œå¦ä¸€äº› GPU é—²ç½®ï¼Œè¿™ç§è´Ÿè½½ä¸å‡è¡¡ä¹Ÿä¼šå½±å“å¹¶è¡Œæ•ˆç‡ã€‚

* ã€æ€»ç»“ã€‘Pipeline Parallelism æ˜¯ä¸€ç§é€šè¿‡å°†æ¨¡å‹çºµå‘æ‹†åˆ†ï¼ˆæŒ‰å±‚åˆ’åˆ†ï¼‰å¹¶åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šå¤„ç†çš„å¹¶è¡ŒæŠ€æœ¯ã€‚æ¯ä¸ª GPU è´Ÿè´£è®¡ç®—æ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ï¼Œå¹¶ä¸”å„ GPU åƒæµæ°´çº¿ä¸€æ ·å¤„ç†æ‰¹é‡æ•°æ®ï¼Œè¿™æ—¢èƒ½å‡å°‘å•ä¸ª GPU çš„æ˜¾å­˜æ¶ˆè€—ï¼Œåˆèƒ½é€šè¿‡å¹¶è¡Œå¤„ç†åŠ é€Ÿè®¡ç®—ã€‚ä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†é€šä¿¡å¼€é”€å’Œè´Ÿè½½å‡è¡¡ç­‰æŒ‘æˆ˜ã€‚


Tensor Parallelism (TP)
^^^^^^^^^^^^^^^^^^^^^^^

* ã€åŸå§‹ã€‘Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets processed separately and in parallel on different GPUs and the results are synced at the end of the processing step. This is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level. Learn more about Tensor Parallelism here.

* ã€å®šä¹‰ã€‘Tensor Parallelismï¼ˆå¼ é‡å¹¶è¡Œï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºå°†æ¨¡å‹çš„å¼ é‡åˆ‡åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶å°†è¿™äº›éƒ¨åˆ†åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚ä¸ Pipeline Parallelism ä¸åŒï¼ŒTensor Parallelism ä¸å°†æ¨¡å‹æŒ‰å±‚çº§åˆ’åˆ†ï¼Œè€Œæ˜¯å°†æ¯ä¸ªå¼ é‡ï¼ˆå¦‚æƒé‡çŸ©é˜µæˆ–è¾“å…¥æ•°æ®ï¼‰æ°´å¹³åˆ‡åˆ†ï¼ˆhorizontally splitï¼‰ï¼Œå› æ­¤ä¹Ÿè¢«ç§°ä¸ºæ°´å¹³å¹¶è¡Œï¼ˆhorizontal parallelismï¼‰ã€‚
* ã€æ ¸å¿ƒæ€æƒ³ã€‘å°†æ¨¡å‹çš„å¼ é‡ï¼ˆåŒ…æ‹¬å‚æ•°ã€æ¿€æ´»å€¼ç­‰ï¼‰åˆ‡åˆ†ä¸ºå¤šä¸ªå—ï¼ˆshardsï¼‰ï¼Œå¹¶å°†è¿™äº›å—åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚è¿™æ ·å¯ä»¥å‡å°‘æ¯ä¸ª GPU çš„è®¡ç®—è´Ÿè½½å’Œæ˜¾å­˜å‹åŠ›ï¼ŒåŒæ—¶åŠ é€Ÿè®­ç»ƒã€‚
* ã€å·¥ä½œåŸç†ã€‘

    * å¼ é‡åˆ‡åˆ†ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ªå¼ é‡ï¼ˆä¾‹å¦‚æƒé‡çŸ©é˜µï¼‰å¤§å°ä¸º (1024, 1024)ï¼Œè€Œä½ æœ‰ 4 ä¸ª GPUã€‚ä½ å¯ä»¥å°†è¿™ä¸ªå¼ é‡æ°´å¹³åˆ‡åˆ†æˆ 4 ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†çš„å¤§å°ä¸º (256, 1024)ï¼Œåˆ†åˆ«æ”¾ç½®åœ¨ 4 ä¸ªä¸åŒçš„ GPU ä¸Šã€‚è¿™æ ·ï¼Œå¼ é‡çš„ä¸åŒéƒ¨åˆ†ä¼šåˆ†åˆ«åœ¨ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¤„ç†ã€‚
    * å¹¶è¡Œè®¡ç®—ï¼šæ¯ä¸ª GPU å¤„ç†å¼ é‡çš„ä¸åŒéƒ¨åˆ†ï¼Œå¹¶è¿›è¡Œç‹¬ç«‹çš„è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª GPU ä¼šå¤„ç†å…¶åˆ†é…åˆ°çš„å¼ é‡éƒ¨åˆ†ã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¹Ÿåœ¨å„è‡ªçš„ GPU ä¸Šè®¡ç®—ã€‚
    * ç»“æœåŒæ­¥ï¼šåœ¨æ¯ä¸ªè®¡ç®—æ­¥éª¤ï¼ˆå¦‚å‰å‘ä¼ æ’­æˆ–åå‘ä¼ æ’­ï¼‰ç»“æŸæ—¶ï¼Œå„ä¸ª GPU ä¼šå°†å®ƒä»¬çš„éƒ¨åˆ†ç»“æœè¿›è¡ŒåŒæ­¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ›´æ–°æ—¶æ‰€æœ‰å¼ é‡éƒ¨åˆ†çš„è®¡ç®—ç»“æœèƒ½å¤Ÿæ±‡æ€»ã€‚åŒæ­¥çš„è¿‡ç¨‹æ˜¯é€šè¿‡é€šä¿¡æœºåˆ¶å®Œæˆçš„ï¼Œé€šå¸¸ä½¿ç”¨åˆ†å¸ƒå¼æ¡†æ¶ï¼ˆå¦‚ NVIDIA çš„ NCCL åº“ï¼‰æ¥é«˜æ•ˆä¼ é€’æ•°æ®ã€‚

* ã€ä¼˜åŠ¿ã€‘

    * å‡å°‘æ˜¾å­˜å ç”¨ï¼šé€šè¿‡å°†å¤§çš„å¼ é‡åˆ†æˆå¤šä¸ªå°å—ï¼Œæ¯ä¸ª GPU åªéœ€è¦å­˜å‚¨å’Œè®¡ç®—è‡ªå·±è´Ÿè´£çš„å¼ é‡éƒ¨åˆ†ï¼Œæ˜¾è‘—å‡å°‘äº†å•ä¸ª GPU çš„æ˜¾å­˜æ¶ˆè€—ã€‚è¿™åœ¨å¤„ç†å¤§å‹æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰æ—¶éå¸¸é‡è¦ï¼Œå› ä¸ºå•ä¸ª GPU æ— æ³•å®¹çº³æ•´ä¸ªæ¨¡å‹çš„æƒé‡ã€‚
    * åŠ é€Ÿè®¡ç®—ï¼šç”±äºæ¯ä¸ª GPU åªè´Ÿè´£ä¸€éƒ¨åˆ†å¼ é‡ï¼Œè®¡ç®—å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œè¿›è¡Œï¼Œç†è®ºä¸Šå¯ä»¥çº¿æ€§åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚

* ã€æŒ‘æˆ˜ã€‘

    * é€šä¿¡å¼€é”€ï¼šæ¯ä¸ªè®¡ç®—æ­¥éª¤ç»“æŸæ—¶ï¼Œå„ä¸ª GPU éœ€è¦åŒæ­¥ç»“æœã€‚å¯¹äºè¾ƒå¤§çš„æ¨¡å‹å’Œè¾ƒé¢‘ç¹çš„åŒæ­¥æ“ä½œï¼Œè¿™ä¼šå¯¼è‡´æ˜¾è‘—çš„é€šä¿¡å¼€é”€ï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚
    * è´Ÿè½½å‡è¡¡ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒæŸäº›å¼ é‡å¯èƒ½è¾ƒå°ï¼Œåˆ‡åˆ†åä¸åŒçš„ GPU ä¸Šè®¡ç®—é‡å¯èƒ½ä¸å‡è¡¡ï¼Œå¯¼è‡´æŸäº› GPU è®¡ç®—è¾ƒæ…¢ï¼Œè¿›è€Œæ‹–æ…¢æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚
    * å®ç°å¤æ‚åº¦ï¼šç›¸æ¯” Data Parallelismï¼ŒTensor Parallelism çš„å®ç°æ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºæ¶‰åŠåˆ°å¼ é‡çš„åˆ‡åˆ†ã€åˆ†é…ã€å¹¶è¡Œè®¡ç®—å’ŒåŒæ­¥ç­‰å¤šä¸ªæ­¥éª¤ã€‚

* ã€æ€»ç»“ã€‘Tensor Parallelism æ˜¯ä¸€ç§é€šè¿‡æ°´å¹³åˆ‡åˆ†æ¨¡å‹å¼ é‡æ¥åˆ†é…åˆ°å¤šä¸ª GPU è¿›è¡Œå¹¶è¡Œè®¡ç®—çš„æŠ€æœ¯ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜æ¶ˆè€—ï¼Œå°¤å…¶é€‚åˆå¤„ç†éå¸¸å¤§çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡åœ¨ä¸åŒ GPU ä¸Šå¹¶è¡Œå¤„ç†å¼ é‡çš„ä¸åŒéƒ¨åˆ†æ¥åŠ é€Ÿè®¡ç®—ï¼Œå¹¶åœ¨æ¯ä¸ªè®¡ç®—æ­¥éª¤åé€šè¿‡é€šä¿¡æœºåˆ¶åŒæ­¥ç»“æœã€‚ä¸å…¶ä»–å¹¶è¡ŒæŠ€æœ¯ç›¸æ¯”ï¼ŒTensor Parallelism åœ¨å¤„ç†è¶…å¤§æ¨¡å‹æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†ä¹Ÿé¢ä¸´é€šä¿¡å¼€é”€å’Œå®ç°å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚


Tensor Parallelism vs. Data Parallelism vs. Pipeline Parallelism
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* Tensor Parallelismï¼šå¼ é‡ï¼ˆå¦‚æƒé‡çŸ©é˜µï¼‰è¢«åˆ‡åˆ†æˆå¤šä¸ªå°å—ï¼Œåˆ†å¸ƒåœ¨ä¸åŒçš„ GPU ä¸Šå¹¶è¡Œè®¡ç®—ã€‚é€‚ç”¨äºéå¸¸å¤§çš„å¼ é‡ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ˜¾å­˜å‹åŠ›ã€‚
* Data Parallelismï¼šç›¸åŒçš„æ¨¡å‹å‰¯æœ¬è¿è¡Œåœ¨æ¯ä¸ª GPU ä¸Šï¼Œä½†æ¯ä¸ª GPU å¤„ç†ä¸åŒçš„è¾“å…¥æ•°æ®ã€‚åœ¨æ¯ä¸ª GPU è®¡ç®—å®Œæ¢¯åº¦åï¼Œæ¢¯åº¦ä¼šè¿›è¡Œå¹³å‡å¹¶æ›´æ–°æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚ä¼˜ç‚¹æ˜¯å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†æ˜¾å­˜å‹åŠ›ä¾ç„¶å¾ˆå¤§ï¼Œå› ä¸ºæ¯ä¸ª GPU éƒ½éœ€è¦å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€‚
* Pipeline Parallelismï¼šæ¨¡å‹æŒ‰å±‚çº§åˆ‡åˆ†ï¼Œä¸åŒçš„å±‚åˆ†é…åˆ°ä¸åŒçš„ GPUï¼Œå¤šä¸ª GPU ä»¥æµæ°´çº¿çš„æ–¹å¼å¤„ç†æ‰¹æ¬¡æ•°æ®ã€‚é€‚åˆéå¸¸æ·±çš„ç½‘ç»œï¼Œä½†éœ€è¦è§£å†³æµæ°´çº¿å¯åŠ¨å’ŒåŒæ­¥çš„é—®é¢˜ã€‚





Zero Redundancy Optimizer (ZeRO)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ã€åŸå§‹ã€‘Parallelism technique which performs sharding of the tensors somewhat similar to TensorParallel, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesnâ€™t need to be modified. This method also supports various offloading techniques to compensate for limited GPU memory. Learn more about ZeRO here.
* ã€å®šä¹‰ã€‘Zero Redundancy Optimizer (ZeRO) æ˜¯ç”± DeepSpeed æ¡†æ¶æå‡ºçš„ä¸€ç§åˆ†å¸ƒå¼ä¼˜åŒ–ç­–ç•¥ã€‚æ˜¯ä¸€ç§ä¼˜åŒ–å™¨å¹¶è¡ŒæŠ€æœ¯ï¼Œæ—¨åœ¨å¤§å¹…å‡å°‘æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒä¸­çš„å†…å­˜ä½¿ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤š GPU ç¯å¢ƒä¸‹ã€‚å®ƒé€šè¿‡ **åˆ†ç‰‡ï¼ˆshardingï¼‰ç®¡ç†** æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæå¤§åœ°æé«˜äº† GPU çš„å†…å­˜æ•ˆç‡ï¼Œä»è€Œå…è®¸æ›´å¤§è§„æ¨¡çš„æ¨¡å‹åœ¨æœ‰é™çš„ GPU èµ„æºä¸Šè¿›è¡Œè®­ç»ƒã€‚ZeRO æ˜¯ä¸€ç§ä¸ Tensor Parallelism ç›¸ä¼¼çš„æŠ€æœ¯ï¼Œä½†æœ‰ä¸€äº›ç‹¬ç‰¹çš„ç‰¹æ€§ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›ä¸€æ­¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨ã€‚
* ã€æ ¸å¿ƒæ€æƒ³ã€‘åªåœ¨è®¡ç®—éœ€è¦æ—¶æ‰å°†å¼ é‡é‡æ–°æ„å»ºï¼Œè€Œåœ¨è®¡ç®—å®Œæˆåå°†å…¶å†æ¬¡åˆ†ç‰‡åˆ°å„ä¸ª GPUã€‚å› æ­¤ï¼Œæ•´ä¸ªæ¨¡å‹çš„å¼ é‡ä¸éœ€è¦ä¸€ç›´é©»ç•™åœ¨å•ä¸ª GPU ä¸Šï¼Œä»è€Œæœ‰æ•ˆå‡å°‘äº†æ˜¾å­˜çš„å ç”¨ã€‚
* ã€ä¸‰ä¸ªé˜¶æ®µã€‘

    * Stage 1ï¼šOptimizer State Shardingï¼ˆä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼˜åŒ–å™¨ä¼šç»´æŠ¤å¾ˆå¤šçŠ¶æ€ä¿¡æ¯ï¼ˆä¾‹å¦‚åŠ¨é‡ã€äºŒé˜¶æ¢¯åº¦ç­‰ï¼‰ï¼Œè¿™äº›çŠ¶æ€ä¼šæ¶ˆè€—å¤§é‡çš„å†…å­˜ã€‚åœ¨ Stage 1ï¼ŒZeRO å°†è¿™äº›ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°ä¸åŒçš„ GPU ä¸Šï¼Œæ¯ä¸ª GPU åªç»´æŠ¤ä¸€éƒ¨åˆ†ä¼˜åŒ–å™¨çŠ¶æ€ã€‚å› æ­¤ï¼Œå†…å­˜æ¶ˆè€—ä»æ‰€æœ‰ GPU éƒ½éœ€è¦å­˜å‚¨å®Œæ•´çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå˜æˆæ¯ä¸ª GPU åªå­˜å‚¨ä¸€éƒ¨åˆ†ã€‚
    * Stage 2ï¼šGradient Shardingï¼ˆæ¢¯åº¦åˆ†ç‰‡ï¼‰æ¢¯åº¦é€šå¸¸åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­è®¡ç®—å¹¶å­˜å‚¨ã€‚Stage 2 å°†è¿™äº›æ¢¯åº¦ä¹Ÿè¿›è¡Œåˆ†ç‰‡ï¼Œå„ä¸ª GPU åªå­˜å‚¨ä¸€éƒ¨åˆ†çš„æ¢¯åº¦ã€‚è¿™æ ·ï¼Œåå‘ä¼ æ’­ä¸­çš„æ¢¯åº¦è®¡ç®—ä»ç„¶æ˜¯å¹¶è¡Œçš„ï¼Œä½†å­˜å‚¨çš„å†…å­˜æ˜¾è‘—å‡å°‘ã€‚
    * Stage 3ï¼šParameter Shardingï¼ˆå‚æ•°åˆ†ç‰‡ï¼‰åœ¨ Stage 3ï¼Œæ¨¡å‹çš„å‚æ•°æœ¬èº«ä¹Ÿè¢«åˆ†ç‰‡ã€‚æ¯ä¸ª GPU åªå­˜å‚¨æ¨¡å‹å‚æ•°çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¨¡å‹çš„å‰¯æœ¬ã€‚åœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°åªä¼šåœ¨éœ€è¦è®¡ç®—çš„æ—¶åˆ»è¢«é‡æ„å’Œä½¿ç”¨ï¼Œä¹‹åå†é‡æ–°åˆ†ç‰‡å’ŒåŒæ­¥ã€‚

* ã€åŸç†ã€‘åœ¨ ZeRO ä¸­ï¼Œæ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ä¸å†è¢«æ¯ä¸ª GPU å®Œæ•´åœ°å¤åˆ¶ï¼Œè€Œæ˜¯è¢«åˆ’åˆ†æˆå¤šä¸ªå—ï¼ˆshardsï¼‰ï¼Œåˆ†å¸ƒåœ¨ä¸åŒçš„ GPU ä¸Šã€‚å½“æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­æˆ–è€…æ›´æ–°å‚æ•°æ—¶ï¼š

    * åœ¨éœ€è¦è®¡ç®—çš„æ—¶å€™ï¼ŒZeRO ä¼šå°†åˆ†ç‰‡çš„å¼ é‡é‡æ„ï¼ˆreconstructï¼‰æˆå®Œæ•´çš„å¼ é‡ï¼Œè¿›è¡Œè®¡ç®—ã€‚
    * è®¡ç®—ç»“æŸåï¼Œå¼ é‡ä¼šå†æ¬¡è¢«åˆ‡åˆ†å¹¶åˆ†é…å›å„ä¸ª GPUï¼Œè¿™æ ·æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ¯ä¸ª GPU åªéœ€è¦å¤„ç†ä¸€éƒ¨åˆ†çš„å¼ é‡ã€‚

* ZeRO ä¸ Tensor Parallelism çš„åŒºåˆ«ï¼šTensor Parallelism æ˜¯åœ¨å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­å°†å¼ é‡æ°´å¹³åˆ‡åˆ†ï¼Œä¸åŒ GPU å¹¶è¡Œå¤„ç†å¼ é‡çš„ä¸åŒéƒ¨åˆ†ï¼Œç„¶ååŒæ­¥è®¡ç®—ç»“æœã€‚ZeRO é€šè¿‡åˆ†ç‰‡æ¨¡å‹å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œä½¿å¾—æ¯ä¸ª GPU ä¸éœ€è¦åŒæ—¶å­˜å‚¨æ•´ä¸ªæ¨¡å‹çš„å‰¯æœ¬ã€‚åœ¨è®¡ç®—æ—¶ï¼ŒZeRO åŠ¨æ€é‡æ„å¼ é‡ï¼Œè®¡ç®—å®Œåå†è¿›è¡Œåˆ†ç‰‡ï¼Œè€Œä¸æ˜¯ä¸€å¼€å§‹å°±å°†å¼ é‡æ°´å¹³åˆ‡åˆ†å¹¶å›ºå®šä¸‹æ¥ã€‚

* ã€ä¼˜åŠ¿ã€‘æ— éœ€å¯¹æ¨¡å‹ç»“æ„è¿›è¡Œä¿®æ”¹ï¼Œè€Œåªéœ€è¦è°ƒæ•´å¼ é‡çš„å­˜å‚¨å’Œå¤„ç†æ–¹å¼ï¼Œå› æ­¤æ¨¡å‹æœ¬èº«ä¸éœ€è¦ç‰¹åˆ«ä¸º ZeRO è¿›è¡Œé‡å†™ã€‚ä¸ä»…é€šè¿‡åˆ†ç‰‡å‡å°‘ GPU å†…å­˜çš„å ç”¨ï¼Œè¿˜æ”¯æŒoffloading æŠ€æœ¯ï¼Œè¿™æ„å‘³ç€å¯ä»¥å°†éƒ¨åˆ†è®¡ç®—æˆ–æ•°æ®ä» GPU å¸è½½åˆ° CPUï¼Œç”šè‡³ NVMe å­˜å‚¨ï¼ˆç¡¬ç›˜ï¼‰ï¼Œé€šè¿‡è¿™ç§æŠ€æœ¯ï¼Œå³ä½¿ GPU å†…å­˜éå¸¸æœ‰é™ï¼Œä¹Ÿèƒ½æ”¯æŒå¤§æ¨¡å‹è®­ç»ƒï¼š

* ã€æ€»ç»“ã€‘Zero Redundancy Optimizer (ZeRO) æ˜¯ä¸€ç§é«˜æ•ˆçš„å¹¶è¡Œä¼˜åŒ–æŠ€æœ¯ï¼Œå®ƒé€šè¿‡ **åˆ†ç‰‡ï¼ˆshardingï¼‰** æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå°†è¿™äº›ä¿¡æ¯åˆ†å¸ƒåœ¨å¤šä¸ª GPU ä¸Šï¼Œä»è€Œæå¤§åœ°å‡å°‘äº†å•ä¸ª GPU çš„å†…å­˜å ç”¨ã€‚å®ƒèƒ½å¤Ÿæ”¯æŒè®­ç»ƒéå¸¸å¤§çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„è®¡ç®—æ•ˆç‡ã€‚ä¸ Tensor Parallelism ä¸åŒï¼ŒZeRO å…è®¸åŠ¨æ€é‡æ„å¼ é‡ï¼Œä¸”ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¿®æ”¹ã€‚æ­¤å¤–ï¼ŒZeRO è¿˜æ”¯æŒå°†è®¡ç®—å’Œå­˜å‚¨å¸è½½åˆ° CPU æˆ–ç¡¬ç›˜ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–æœ‰é™èµ„æºä¸Šçš„æ¨¡å‹è®­ç»ƒã€‚


Fully Sharded Data Parallel
^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. note:: å‚è§ä¸Šé¢ç›¸å…³ç« èŠ‚



Optimizing LLMs for Speed and Memory
------------------------------------

effective techniques for efficient LLM deployment::

    1. Lower Precision
        é™ä½ç²¾åº¦ï¼ˆå³8 ä½å’Œ 4 ä½ï¼‰è¿è¡Œå¯ä»¥å®ç°è®¡ç®—ä¼˜åŠ¿ï¼Œè€Œä¸ä¼šæ˜¾ç€é™ä½æ¨¡å‹æ€§èƒ½
    2. Flash Attention
        æ³¨æ„åŠ›ç®—æ³•çš„ä¸€ç§å˜ä½“ï¼Œå®ƒä¸ä»…æä¾›äº†ä¸€ç§æ›´èŠ‚çœå†…å­˜çš„æ–¹æ³•ï¼Œè€Œä¸”ç”±äºä¼˜åŒ–äº† GPU å†…å­˜åˆ©ç”¨ç‡è€Œå®ç°äº†æ•ˆç‡çš„æé«˜
    3. Architectural Innovations
        æ¨¡å‹æ¶æ„ä¸­æœ€é‡è¦çš„è¿›æ­¥æœ‰: 
            Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).

1. Lower Precision
^^^^^^^^^^^^^^^^^^

æ‰€æœ‰é‡åŒ–æŠ€æœ¯çš„å·¥ä½œåŸç†::

    1. Quantize all weights to the target precision
        å°†æ‰€æœ‰æƒé‡é‡åŒ–åˆ°ç›®æ ‡ç²¾åº¦
    2. Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision
        åŠ è½½é‡åŒ–æƒé‡ï¼Œå¹¶ä»¥ bfloat16 ç²¾åº¦ä¼ é€’å‘é‡çš„è¾“å…¥åºåˆ—
    3. Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision
        å°†æƒé‡åŠ¨æ€åé‡åŒ–ä¸º bfloat16ï¼Œä»¥ä½¿ç”¨ bfloat16 ç²¾åº¦çš„è¾“å…¥å‘é‡æ‰§è¡Œè®¡ç®—

.. note:: ä½¿ç”¨é‡åŒ–æƒé‡æ—¶ï¼Œå…ˆæŠŠæƒé‡åé‡åŒ–ä¸º bfloat16ï¼Œè¾“å…¥åºåˆ—è¿˜æ˜¯ bfloat16ï¼Œè®¡ç®—ä¸¤è€…ä¹˜ç§¯ã€‚æ‰€ä»¥ï¼Œæ¨ç†æ—¶é—´é€šå¸¸ä¸ä¼šå‡å°‘ï¼Œåè€Œä¼šå¢åŠ ã€‚æ€»ä¹‹ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œæ¨¡å‹é‡åŒ–ä»¥å‡†ç¡®æ€§å’Œåœ¨æŸäº›æƒ…å†µä¸‹ç‰ºç‰²æ¨ç†æ—¶é—´ä¸ºä»£ä»·æé«˜å†…å­˜æ•ˆç‡ã€‚

2. Flash Attention
^^^^^^^^^^^^^^^^^^

* å¯¹äºå¤§å‹è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œé»˜è®¤çš„è‡ªæ³¨æ„åŠ›ç®—æ³•å¾ˆå¿«å°±ä¼šå˜å¾—éå¸¸æ˜‚è´µçš„å†…å­˜æ¶ˆè€—ã€‚
* é€šè¿‡è·Ÿè¸ª softmax å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®å¹¶ä½¿ç”¨ä¸€äº›æ™ºèƒ½æ•°å­¦ï¼Œä¸é»˜è®¤çš„è‡ªæ³¨æ„åŠ›å±‚ç›¸æ¯”ï¼ŒFlash Attention æä¾›äº†ç›¸åŒçš„æ•°å€¼è¾“å‡ºï¼Œè€Œå†…å­˜æˆæœ¬ä»…éšæ—¶é—´çº¿æ€§å¢åŠ  N
* è€Œä¸”ä¸é»˜è®¤æ³¨æ„åŠ›ç›¸æ¯”ï¼Œé—ªå­˜æ³¨æ„åŠ›çš„æ¨ç†é€Ÿåº¦è¦å¿«å¾—å¤šï¼Œè¿™æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæ˜¾ç€å‡å°‘å¯¹ GPU (VRAM) è¾ƒæ…¢ã€é«˜å¸¦å®½å†…å­˜çš„éœ€æ±‚ï¼Œè€Œæ˜¯ä¸“æ³¨äºæ›´å¿«çš„ç‰‡ä¸Šå†…å­˜ (SRAM) ã€‚
* å®é™…ä¸Šï¼Œç›®å‰ç»å¯¹æ²¡æœ‰ç†ç”±ä¸ä½¿ç”¨ Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚è¯¥ç®—æ³•åœ¨æ•°å­¦ä¸Šç»™å‡ºç›¸åŒçš„è¾“å‡ºï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ä¸”å†…å­˜æ•ˆç‡æ›´é«˜ã€‚


3. Architectural Innovations
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

3.1 Improving positional embeddings of LLMs
"""""""""""""""""""""""""""""""""""""""""""

* ä½ç½®åµŒå…¥ï¼ˆpositional embeddingsï¼‰ æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œè´Ÿè´£å¸®åŠ©æ¨¡å‹ç†è§£æ–‡æœ¬åºåˆ—ä¸­ä¸åŒ token ä¹‹é—´çš„é¡ºåºå…³ç³»
* ã€èƒŒæ™¯ï¼šä½ç½®åµŒå…¥çš„å¿…è¦æ€§ã€‘**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ä¸­çš„ Softmax(QK^T) æ“ä½œå°†æ¯ä¸ª token ä¸åºåˆ—ä¸­çš„å…¶ä»– token è¿›è¡Œå…³è”ï¼Œä½†é»˜è®¤æƒ…å†µä¸‹å®ƒæ— æ³•ç†è§£ token ä¹‹é—´çš„ç›¸å¯¹é¡ºåºã€‚æ²¡æœ‰ä½ç½®åµŒå…¥çš„æ¨¡å‹éš¾ä»¥åŒºåˆ†ä¸åŒçš„è¾“å…¥é¡ºåºã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ† â€œHello I love youâ€ å’Œ â€œYou love I helloâ€ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ²¡æœ‰ä½ç½®ä¿¡æ¯çš„æƒ…å†µä¸‹çœ‹èµ·æ¥æ˜¯ç­‰ä»·çš„ã€‚
* å› æ­¤ï¼Œä½ç½®åµŒå…¥ï¼ˆpositional embeddingsï¼‰è¢«å¼•å…¥ï¼Œç”¨æ¥ç¼–ç æ¯ä¸ª token åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†è¾“å…¥æ–‡æœ¬çš„é¡ºåºã€‚
* ä¼ ç»Ÿæ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šå›ºå®šä½ç½®åµŒå…¥ï¼ˆsinusoidal embeddingsï¼‰ å’Œ å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼ˆlearned positional embeddingsï¼‰ã€‚
* ã€é—®é¢˜ï¼šä¼ ç»Ÿä½ç½®åµŒå…¥çš„å±€é™æ€§ã€‘1.å¯¹é•¿æ–‡æœ¬è¡¨ç°è¾ƒå·®ï¼šç»å¯¹ä½ç½®åµŒå…¥ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ç¼–ç ï¼ˆä¾‹å¦‚ 0 åˆ° N çš„ä½ç½®ç¼–å·ï¼‰ï¼Œä½†å¯¹äºé•¿æ–‡æœ¬ï¼Œæ¨¡å‹éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ token ä¹‹é—´çš„è¿œè·ç¦»å…³ç³»ã€‚2.å›ºå®šè¾“å…¥é•¿åº¦é—®é¢˜ï¼šå¦‚æœä½¿ç”¨å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ¨¡å‹åªèƒ½å¤„ç†ä¸è®­ç»ƒæ—¶é•¿åº¦ç›¸åŒçš„è¾“å…¥ã€‚å¦‚æœè¾“å…¥é•¿åº¦è¶…å‡ºè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ï¼Œæ¨¡å‹æ— æ³•å¾ˆå¥½åœ°è¿›è¡Œæ¨æ–­ã€‚
* ã€è§£å†³ï¼šç›¸å¯¹ä½ç½®åµŒå…¥ï¼šRoPE å’Œ ALiBiã€‘ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆrelative positional embeddingsï¼‰ï¼Œè¿™ç§æ–¹æ³•ä¸å†ä¸ºæ¯ä¸ªä½ç½®åˆ†é…ç»å¯¹å€¼ï¼Œè€Œæ˜¯å…³æ³¨ token ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ã€‚ä¸¤ç§æµè¡Œçš„ç›¸å¯¹ä½ç½®åµŒå…¥æ–¹æ³•æ˜¯ **RoPEï¼ˆRotary Position Embeddingï¼‰** å’Œ **ALiBiï¼ˆAttention with Linear Biasesï¼‰**ã€‚å®ƒä»¬é€šè¿‡ä¿®æ”¹è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ QK^T çŸ©é˜µæ¥å¼•å…¥ä½ç½®ä¿¡æ¯ã€‚




3.2 The key-value cache
"""""""""""""""""""""""

* è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†é•¿åºåˆ—æˆ–ç”Ÿæˆå¤§é‡æ–‡æœ¬çš„åœºæ™¯ã€‚
* ã€èƒŒæ™¯ï¼šè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆã€‘åœ¨ LLM ä¸­ï¼Œè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆé€šè¿‡é€æ­¥ç”Ÿæˆä¸‹ä¸€ä¸ª token æ¥å®Œæˆã€‚æ¯æ¬¡è¾“å…¥å…ˆå‰ç”Ÿæˆçš„ token åºåˆ—ï¼Œç„¶åæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œå¹¶å°†å…¶åŠ å…¥è¾“å…¥åºåˆ—ä¸­ï¼Œå¦‚æ­¤å¾ªç¯ç›´åˆ°ç”Ÿæˆç»“æŸã€‚åœ¨è¿™ç§é€æ­¥ç”Ÿæˆçš„è¿‡ç¨‹ä¸­ï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œæ¯æ¬¡éƒ½éœ€è¦å¯¹æ•´ä¸ªåºåˆ—é‡æ–°è®¡ç®— ``QK^T`` çŸ©é˜µï¼Œè¿›è€Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚è¿™æ„å‘³ç€æ¯ä¸€æ­¥çš„è®¡ç®—å¤æ‚åº¦ä¼šéšç€åºåˆ—é•¿åº¦çš„å¢åŠ è€Œå¢å¤§ã€‚
* ã€é—®é¢˜ï¼šé‡å¤è®¡ç®—çš„æ•ˆç‡ä½ä¸‹ã€‘å¯¹äºè‡ªå›å½’ç”Ÿæˆæ¥è¯´ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰ä¹‹å‰çš„ token çš„ key å’Œ valueï¼Œå°½ç®¡è¿™äº›å€¼åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­å·²ç»è®¡ç®—è¿‡äº†ã€‚é‡å¤è®¡ç®—è¿™äº›ä¸å¿…è¦çš„ key-value å¯¹ä¼šå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆé•¿åºåˆ—æ—¶æ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦å’Œæ˜¾å­˜å ç”¨ã€‚
* ã€è§£å†³æ–¹æ¡ˆï¼šKey-Value Cacheã€‘é€šè¿‡ç¼“å­˜æ¯ä¸€å±‚çš„ key å’Œ value å‘é‡ï¼ˆå³ K å’Œ Vï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚
* ã€åŸç†ã€‘**QK^T çŸ©é˜µä¼˜åŒ–**ï¼šåœ¨æ ‡å‡† Transformer æ¨¡å‹ä¸­ï¼ŒQK^T çŸ©é˜µæ˜¯é€šè¿‡å°†æ¯ä¸ª token çš„ query å‘é‡ï¼ˆQï¼‰ä¸æ‰€æœ‰ key å‘é‡ï¼ˆKï¼‰è¿›è¡Œç‚¹ç§¯è®¡ç®—å¾—å‡ºçš„ã€‚ç„¶è€Œï¼Œå¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œæˆ‘ä»¬æ¯æ¬¡åªéœ€è¦ä¸ºæ–°å¢çš„ token è®¡ç®— query ï¼ˆq_cï¼‰ä¸ä¹‹å‰ç¼“å­˜çš„ keyï¼ˆKï¼‰è¿›è¡Œç›¸ä¹˜ï¼Œè€Œä¸éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰çš„ key å’Œ valueã€‚**ç¼“å­˜æœºåˆ¶**ï¼šåœ¨æ¯ä¸€æ­¥ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå°†ä¹‹å‰çš„ key-value å¯¹ä¿å­˜åœ¨ç¼“å­˜ä¸­ï¼Œä¸‹ä¸€æ­¥ç”Ÿæˆæ—¶åªéœ€è®¡ç®—å½“å‰æ–° token çš„ queryï¼Œç„¶åä¸ç¼“å­˜ä¸­çš„ key è¿›è¡Œè®¡ç®—ã€‚è¿™æ ·é¿å…äº†é‡å¤è®¡ç®—æ•´ä¸ªåºåˆ—çš„ key-value å¯¹ã€‚


3.2.1 Multi-round conversation
++++++++++++++++++++++++++++++

* é”®å€¼ç¼“å­˜å¯¹äºèŠå¤©ç­‰éœ€è¦å¤šæ¬¡è‡ªåŠ¨å›å½’è§£ç çš„åº”ç”¨ç¨‹åºç‰¹åˆ«æœ‰ç”¨


ç¤ºä¾‹::

    User: How many people live in France?
    Assistant: Roughly 75 million people live in France
    User: And how many are in Germany?
    Assistant: Germany has ca. 81 million inhabitants

* åœ¨æ­¤èŠå¤©ä¸­ï¼Œ LLMè¿è¡Œè‡ªå›å½’è§£ç ä¸¤æ¬¡ï¼š
* ç¬¬ä¸€æ¬¡ï¼Œé”®å€¼ç¼“å­˜ä¸ºç©ºï¼Œè¾“å…¥æç¤ºä¸º "User: How many people live in France?" æ¨¡å‹è‡ªåŠ¨å›å½’ç”Ÿæˆæ–‡æœ¬ "Roughly 75 million people live in France" åŒæ—¶åœ¨æ¯ä¸ªè§£ç æ­¥éª¤å¢åŠ é”®å€¼ç¼“å­˜ã€‚
* ç¬¬äºŒæ¬¡è¾“å…¥æç¤ºæ˜¯ "User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many in Germany?" ã€‚ç”±äºç¼“å­˜ï¼Œå‰ä¸¤ä¸ªå¥å­çš„æ‰€æœ‰é”®å€¼å‘é‡éƒ½å·²ç»è®¡ç®—å‡ºæ¥ã€‚å› æ­¤è¾“å…¥æç¤ºä»…åŒ…å« "User: And how many in Germany?" ã€‚åœ¨å¤„ç†ç¼©çŸ­çš„è¾“å…¥æç¤ºæ—¶ï¼Œå…¶è®¡ç®—å‡ºçš„é”®å€¼å‘é‡è¢«è¿æ¥åˆ°ç¬¬ä¸€æ¬¡è§£ç çš„é”®å€¼ç¼“å­˜ã€‚ç¬¬äºŒæ¬¡å›ç­” "Germany has ca. 81 million inhabitants" ç„¶åä½¿ç”¨ç”±ç¼–ç çš„é”®å€¼å‘é‡ç»„æˆçš„é”®å€¼ç¼“å­˜è‡ªåŠ¨ç”Ÿæˆ "User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many are in Germany?" ã€‚




.. code-block:: python

    # Generation as usual
    prompt = system_prompt + "Question: Please write a function in Python that transforms bytes to Giga bytes.\n\nAnswer: Here"
    model_inputs = tokenizer(prompt, return_tensors='pt')
    generation_output = model.generate(**model_inputs, max_new_tokens=60, return_dict_in_generate=True)
    decoded_output = tokenizer.batch_decode(generation_output.sequences)[0]

    # Piping the returned `past_key_values` to speed up the next conversation round
    prompt = decoded_output + "\nQuestion: How can I modify the function above to return Mega bytes instead?\n\nAnswer: Here"
    model_inputs = tokenizer(prompt, return_tensors='pt')
    generation_output = model.generate(
      **model_inputs,
      past_key_values=generation_output.past_key_values,    # â‡ï¸
      max_new_tokens=60,
      return_dict_in_generate=True
    )
    tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]





3.2.2 Multi-Query-Attention (MQA)
+++++++++++++++++++++++++++++++++

* ã€å®šä¹‰ã€‘ä¸€ç§ä¼˜åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘key-value ç¼“å­˜çš„å†…å­˜å ç”¨å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç”± Noam Shazeer åœ¨è®ºæ–‡ "Fast Transformer Decoding: One Write-Head is All You Need" ä¸­æå‡ºï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯å…±äº« key-value æŠ•å½±ï¼Œä»è€Œå¤§å¹…é™ä½å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰çš„å†…å­˜å¼€é”€ã€‚
* ã€èƒŒæ™¯ï¼šä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜ç“¶é¢ˆã€‘åœ¨ä¼ ç»Ÿçš„**å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰**ä¸­ï¼Œæ¨¡å‹ä¼šä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆattention headï¼‰è®¡ç®—ç‹¬ç«‹çš„ key-value å¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œn ä¸ªæ³¨æ„åŠ›å¤´æ„å‘³ç€éœ€è¦è®¡ç®—å¹¶å­˜å‚¨ n ç»„ key-value å‘é‡ï¼Œé€šå¸¸ä¼šæ˜¾è‘—å¢åŠ å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œé€šå¸¸æœ‰ 20 åˆ° 100 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå› æ­¤å½“ç¼“å­˜è¿™äº› key-value å¯¹æ—¶ï¼Œå†…å­˜æ¶ˆè€—éå¸¸é«˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæˆ–å¤šè½®å¯¹è¯ä¸­ã€‚
* ã€æ ¸å¿ƒæ€æƒ³ã€‘å…³é”®åœ¨äºï¼šæ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ª key-value æŠ•å½±ï¼Œå³æ¯ä¸ªå¤´ä»ç„¶æ‹¥æœ‰ç‹¬ç«‹çš„ queryï¼ˆæŸ¥è¯¢å‘é‡ï¼‰ï¼Œä½† key å’Œ value å‘é‡åœ¨æ‰€æœ‰æ³¨æ„åŠ›å¤´ä¸­æ˜¯ç›¸åŒçš„ã€‚è¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨å’Œè®¡ç®—çš„å¼€é”€ï¼Œè€Œä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚
* ã€åº”ç”¨ã€‘MQA æŠ€æœ¯å·²ç»è¢«è®¸å¤šä¸»æµçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰€é‡‡ç”¨ï¼ŒåŒ…æ‹¬ï¼šFalconï¼ŒPaLMï¼ŒMPTï¼ŒBLOOM



3.2.3 Grouped-Query-Attention (GQA)
+++++++++++++++++++++++++++++++++++

* ã€å®šä¹‰ã€‘ç”± Google ç ”ç©¶å‘˜ Ainslie ç­‰äººåœ¨è®ºæ–‡ä¸­æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒæ—¨åœ¨è§£å†³ Multi-Query Attention (MQA) å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†å†…å­˜å’Œè®¡ç®—æ•ˆç‡çš„æå‡ã€‚ç›¸æ¯” MQAï¼ŒGQA æä¾›äº†ä¸€ä¸ªæ›´åŠ æŠ˜ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå‡å°‘å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚
* ã€èƒŒæ™¯ï¼šMQA çš„å±€é™æ€§ã€‘Multi-Query Attention (MQA) é€šè¿‡ä¸ºæ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ª key-value æŠ•å½±ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ï¼Œå°¤å…¶åœ¨è‡ªå›å½’ç”Ÿæˆæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦å¹¶é™ä½äº†æ˜¾å­˜å ç”¨ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç° MQA çš„è¿™ç§æç«¯å…±äº«æœºåˆ¶ä¼šå¸¦æ¥ä¸€å®šçš„æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºä¸åŒçš„ query å¤´æ— æ³•å†ç‹¬ç«‹å­¦ä¹ ä¸ key-value çš„å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´æ³¨æ„åŠ›æœºåˆ¶çš„çµæ´»æ€§å—é™ã€‚
* ã€æ ¸å¿ƒæ€æƒ³ã€‘Grouped-Query Attention (GQA) æå‡ºäº†ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆï¼šå‡å°‘æ³¨æ„åŠ›å¤´çš„ query æŠ•å½±æ•°é‡ï¼Œä½†ä¸å°†å…¶å®Œå…¨åˆå¹¶ä¸ºä¸€ä¸ªã€‚å…·ä½“æ¥è¯´ï¼ŒGQA æå‡ºå°†æ³¨æ„åŠ›å¤´åˆ†ç»„ï¼Œå¤šä¸ªå¤´å…±äº«ä¸€ç»„ key-value æŠ•å½±ï¼Œè€Œä¸æ˜¯æ¯ä¸ªå¤´éƒ½ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æŠ•å½±ï¼Œä¹Ÿä¸æ˜¯åƒ MQA é‚£æ ·æ‰€æœ‰å¤´å…±äº«åŒä¸€ç»„æŠ•å½±ã€‚





API
===


Main Classes
------------

Auto Classes
^^^^^^^^^^^^^^

::

    AutoConfig
    AutoTokenizer
    AutoFeatureExtractor
    AutoImageProcessor
    AutoProcessor

Generic model classes::

    AutoModel

Generic pretraining classes::

    AutoModelForPreTraining





Natural Language Processing::

    AutoModelForCausalLM
    AutoModelForMaskedLM
    AutoModelForMaskGeneration
    AutoModelForSeq2SeqLM
    AutoModelForSequenceClassification
    AutoModelForMultipleChoice
    AutoModelForNextSentencePrediction
    AutoModelForTokenClassification
    AutoModelForQuestionAnswering
    AutoModelForTextEncoding

Computer vision::

    AutoModelForDepthEstimation
    AutoModelForImageClassification
    AutoModelForVideoClassification
    AutoModelForKeypointDetection
    AutoModelForMaskedImageModeling
    AutoModelForObjectDetection
    AutoModelForImageSegmentation
    AutoModelForImageToImage
    AutoModelForSemanticSegmentation
    AutoModelForInstanceSegmentation
    AutoModelForUniversalSegmentation
    AutoModelForZeroShotImageClassification
    AutoModelForZeroShotObjectDetection


Audio::

    AutoModelForAudioClassification
    AutoModelForAudioFrameClassification
    AutoModelForCTC
    AutoModelForSpeechSeq2Seq
    AutoModelForAudioXVector
    AutoModelForTextToSpectrogram
    AutoModelForTextToWaveform

Multimodal::

    AutoModelForTableQuestionAnswering
    AutoModelForDocumentQuestionAnswering
    AutoModelForVisualQuestionAnswering
    AutoModelForVision2Seq

Backbone
^^^^^^^^

* backbone(ä¸»å¹²)æ˜¯ä¸€ç§ç”¨äºä¸ºæ›´é«˜çº§åˆ«çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆä¾‹å¦‚å¯¹è±¡æ£€æµ‹å’Œå›¾åƒåˆ†ç±»ï¼‰è¿›è¡Œç‰¹å¾æå–çš„æ¨¡å‹ã€‚
* Transformers æä¾›äº†ä¸€ä¸ª ``AutoBackbone`` ç±»ï¼Œç”¨äºæ ¹æ®é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡åˆå§‹åŒ– Transformers ä¸»å¹²
* ä¸¤ä¸ªå®ç”¨ç¨‹åºç±»::

    AutoBackbone

    BackboneMixin
    BackboneConfigMixin

    TimmBackbone
    TimmBackboneConfig

Callbacks
^^^^^^^^^

* Callbacks æ˜¯å¯ä»¥åœ¨ PyTorch Trainerä¸­è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„å¯¹è±¡ï¼ˆè¯¥åŠŸèƒ½å°šæœªåœ¨ TensorFlow ä¸­å®ç°ï¼‰ï¼Œå¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€ï¼ˆç”¨äºè¿›åº¦æŠ¥å‘Šã€ç™»å½• TensorBoard æˆ–å…¶ä»– ML å¹³å°......ï¼‰å¹¶åšå‡ºå†³ç­–ï¼ˆå¦‚æå‰åœæ­¢ï¼‰ã€‚


Configuration
^^^^^^^^^^^^^

base class::

    PretrainedConfig

é€šç”¨å±æ€§æœ‰::

    hidden_size
    num_attention_heads
    num_hidden_layers

    æ–‡æœ¬æ¨¡å‹è¿›ä¸€æ­¥å®ç°ï¼š vocab_size

Data Collator
^^^^^^^^^^^^^

* Data collators æ˜¯é€šè¿‡ä½¿ç”¨æ•°æ®é›†å…ƒç´ åˆ—è¡¨ä½œä¸ºè¾“å…¥æ¥å½¢æˆæ‰¹æ¬¡çš„å¯¹è±¡ã€‚è¿™äº›å…ƒç´ ä¸train_datasetæˆ–eval_datasetçš„å…ƒç´ å…·æœ‰ç›¸åŒçš„ç±»å‹ã€‚

::

    DefaultDataCollator
    DataCollatorWithPadding
    DataCollatorForTokenClassification
    DataCollatorForSeq2Seq
    DataCollatorForLanguageModeling
    DataCollatorForWholeWordMask
    DataCollatorForPermutationLanguageModeling
    DataCollatorWithFlattening

Logging
^^^^^^^

* é»˜è®¤æ˜¯WARNING

ä¿®æ”¹ä»£ç ::

    import transformers
    transformers.logging.set_verbosity_info()

    ç¯å¢ƒå˜é‡ TRANSFORMERS_VERBOSITY:
    TRANSFORMERS_VERBOSITY=error ./myprogram.py

    ç¯å¢ƒå˜é‡æ¥ç¦ç”¨ä¸€äº›warnings TRANSFORMERS_NO_ADVISORY_WARNINGS
    TRANSFORMERS_NO_ADVISORY_WARNINGS=1 ./myprogram.py


Models
^^^^^^

åŸºç±»::

    PreTrainedModel ã€ TFPreTrainedModelå’ŒFlaxPreTrainedModel

    ModuleUtilsMixin
    PushToHubMixin

Text Generation
^^^^^^^^^^^^^^^

* æ¯ä¸ªæ¡†æ¶éƒ½æœ‰ä¸€ä¸ªç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨å„è‡ªçš„ ``GenerationMixin`` ç±»ä¸­å®ç°ï¼š

::

    GenerationConfig
        WatermarkingConfig

    GenerationMixin


.. code-block:: python

    from transformers import GenerationConfig

    # Download configuration from huggingface.co and cache.
    generation_config = GenerationConfig.from_pretrained("openai-community/gpt2")

    # E.g. config was saved using *save_pretrained('./test/saved_model/')*
    generation_config.save_pretrained("./test/saved_model/")
    generation_config = GenerationConfig.from_pretrained("./test/saved_model/")

    # You can also specify configuration names to your generation configuration file
    generation_config.save_pretrained("./test/saved_model/", config_file_name="my_configuration.json")
    generation_config = GenerationConfig.from_pretrained("./test/saved_model/", "my_configuration.json")

    # If you'd like to try a minor variation to an existing configuration, you can also pass generation
    # arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored
    generation_config, unused_kwargs = GenerationConfig.from_pretrained(
        "openai-community/gpt2", top_k=1, foo=False, do_sample=True, return_unused_kwargs=True
    )
    generation_config.top_k  # 1

    unused_kwargs   # {'foo': False}



ONNX
^^^^

ä¸‰ä¸ªæŠ½è±¡ç±»::

    We provide three abstract classes that you should inherit from, depending on the type of model architecture you wish to export:

    OnnxConfig
    OnnxConfigWithPast
    OnnxSeq2SeqConfigWithPast

ONNX Features::

    Each ONNX configuration is associated with a set of features that enable you to export models for different types of topologies or tasks.

    FeaturesManager


Optimization
^^^^^^^^^^^^

The .optimization module provides::

    an optimizer with weight decay fixed that can be used to fine-tuned models, and
        ä¸€ä¸ªå›ºå®šæƒé‡è¡°å‡çš„ä¼˜åŒ–å™¨ï¼Œå¯ç”¨äºå¾®è°ƒæ¨¡å‹
    several schedules in the form of schedule objects that inherit from `_LRSchedule`:
    a gradient accumulation class to accumulate the gradients of multiple batches
        ä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯ç±»ï¼Œç”¨äºç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦

::

    AdamW
        Implements Adam algorithm with weight decay fix 
    AdaFactor
        can be used as a drop in replacement for Adam original fairseq code

    Schedules
        SchedulerType

Model outputs
^^^^^^^^^^^^^

* æ‰€æœ‰æ¨¡å‹çš„è¾“å‡ºéƒ½æ˜¯ModelOutputå­ç±»çš„å®ä¾‹ã€‚
* All models have outputs that are instances of subclasses of ModelOutput. Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.

.. code-block:: python

    from transformers import BertTokenizer, BertForSequenceClassification
    import torch

    tokenizer = BertTokenizer.from_pretrained("google-bert/bert-base-uncased")
    model = BertForSequenceClassification.from_pretrained("google-bert/bert-base-uncased")

    inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
    outputs = model(**inputs, labels=labels)
    # outputså¯¹è±¡æ˜¯ä¸€ä¸ªSequenceClassifierOutput


::

    ModelOutput
    BaseModelOutput
    BaseModelOutputWithPooling
    BaseModelOutputWithCrossAttentions
    BaseModelOutputWithPoolingAndCrossAttentions
    BaseModelOutputWithPast
    BaseModelOutputWithPastAndCrossAttentions

    Seq2SeqModelOutput

    CausalLMOutput
    CausalLMOutputWithCrossAttentions
    CausalLMOutputWithPast

    MaskedLMOutput
    Seq2SeqLMOutput
    NextSentencePredictorOutput

    SequenceClassifierOutput
    Seq2SeqSequenceClassifierOutput

    MultipleChoiceModelOutput

    TokenClassifierOutput

    QuestionAnsweringModelOutput
    Seq2SeqQuestionAnsweringModelOutput
    Seq2SeqSpectrogramOutput
    SemanticSegmenterOutput

    ImageClassifierOutput
    ImageClassifierOutputWithNoAttention

    DepthEstimatorOutput
    Wav2Vec2BaseModelOutput
    XVectorOutput

    Seq2SeqTSModelOutput
    Seq2SeqTSPredictionOutput
    SampleTSPredictionOutput


Pipelines
^^^^^^^^^

::

    pipe = pipeline("text-classification")
    pipe("This restaurant is awesome")

Processors
^^^^^^^^^^


Multi-modal processors
""""""""""""""""""""""

* ä»»ä½•å¤šæ¨¡æ€æ¨¡å‹éƒ½éœ€è¦ä¸€ä¸ªå¯¹è±¡æ¥ç¼–ç æˆ–è§£ç å°†å¤šç§æ¨¡æ€(modalities)ï¼ˆæ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘ï¼‰åˆ†ç»„çš„æ•°æ®ã€‚
* è¿™æ˜¯ç”±ç§°ä¸º ``processors`` çš„å¯¹è±¡æ¥å¤„ç†çš„ï¼Œå®ƒå°†ä¸¤ä¸ªæˆ–å¤šä¸ªå¤„ç†å¯¹è±¡(processing objects)ç»„åˆåœ¨ä¸€èµ·ï¼Œä¾‹å¦‚åˆ†è¯å™¨ï¼ˆç”¨äºæ–‡æœ¬æ¨¡æ€ï¼‰ã€å›¾åƒå¤„ç†å™¨ï¼ˆç”¨äºè§†è§‰ï¼‰å’Œç‰¹å¾æå–å™¨ï¼ˆç”¨äºéŸ³é¢‘ï¼‰ã€‚

::

    ProcessorMixin

Quantization
^^^^^^^^^^^^

* é‡åŒ–æŠ€æœ¯é€šè¿‡ä½¿ç”¨ 8 ä½æ•´æ•° (int8) ç­‰è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹è¡¨ç¤ºæƒé‡å’Œæ¿€æ´»æ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚è¿™å¯ä»¥åŠ è½½é€šå¸¸æ— æ³•è£…å…¥å†…å­˜çš„æ›´å¤§æ¨¡å‹ï¼Œå¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚ Transformers æ”¯æŒ AWQ å’Œ GPTQ é‡åŒ–ç®—æ³•ï¼Œå¹¶ä¸”æ”¯æŒ 8 ä½å’Œ 4 ä½é‡åŒ–ï¼ˆbitsandbytesï¼‰ã€‚

::

    QuantoConfig
    AqlmConfig
    AwqConfig
    EetqConfig
    GPTQConfig
    BitsAndBytesConfig
    HfQuantizer
    HqqConfig
    FbgemmFp8Config
    CompressedTensorsConfig
    TorchAoConfig



Tokenizer
^^^^^^^^^

::

    PreTrainedTokenizer
    PreTrainedTokenizerFast

    BatchEncoding


Trainer
^^^^^^^

::

    Trainer
    Seq2SeqTrainer

    TrainingArguments
    Seq2SeqTrainingArguments



Models
------


Text models
^^^^^^^^^^^

Qwen2
"""""

* Qwen2 is the new model series of large language models from the Qwen team.

::

    Qwen2Config
    Qwen2Tokenizer
    Qwen2TokenizerFast
    Qwen2Model
    Qwen2ForCausalLM
    Qwen2ForSequenceClassification
    Qwen2ForTokenClassification


Qwen2_VL
""""""""

::

    Qwen2VLConfig
    Qwen2VLImageProcessor
    Qwen2VLProcessor
    Qwen2VLModel
    Qwen2VLForConditionalGeneration


CPM
"""

* The CPM model was proposed in CPM: `A Large-scale Generative Chinese Pre-trained Language Model <https://arxiv.org/abs/2012.00413>`_






DPR
"""

* Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. 
* It was introduced in Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.
* ç›¸å…³è®ºæ–‡: Dense Passage Retrieval for Open-Domain Question Answering(https://arxiv.org/abs/2004.04906)

* å¼€æ”¾åŸŸé—®ç­”ä¾èµ–äºé«˜æ•ˆçš„æ®µè½æ£€ç´¢æ¥é€‰æ‹©å€™é€‰ä¸Šä¸‹æ–‡ï¼Œå…¶ä¸­ä¼ ç»Ÿçš„ç¨€ç–å‘é‡ç©ºé—´æ¨¡å‹ï¼ˆå¦‚ TF-IDF æˆ– BM25ï¼‰æ˜¯äº‹å®ä¸Šçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜æ£€ç´¢å¯ä»¥å•ç‹¬ä½¿ç”¨å¯†é›†è¡¨ç¤ºå®é™…å®ç°ï¼Œå…¶ä¸­åµŒå…¥æ˜¯é€šè¿‡ä¸€ä¸ªç®€å•çš„åŒç¼–ç å™¨æ¡†æ¶ä»å°‘é‡é—®é¢˜å’Œæ®µè½ä¸­å­¦ä¹ çš„ã€‚åœ¨å¹¿æ³›çš„å¼€æ”¾åŸŸ QA æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„å¯†é›†æ£€ç´¢å™¨åœ¨å‰ 20 åä¼ ä»£æ£€ç´¢å‡†ç¡®ç‡æ–¹é¢æ¯”å¼ºå¤§çš„ Lucene-BM25 ç³»ç»Ÿé«˜å‡º 9%-19%ï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬çš„ç«¯åˆ°ç«¯ QA ç³»ç»Ÿåœ¨å¤šä¸ªå¼€æ”¾åŸŸ QA åŸºå‡†ä¸Šå»ºç«‹æ–°çš„æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚
* Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.


DPR consists in three models::

    1. Question encoder: encode questions as vectors
    2. Context encoder: encode contexts as vectors
    3. Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score
        (high if the inferred span actually answers the question).



* Dense Passage Retrieval (DPR) æ˜¯ä¸€ç§ç”¨äºä¿¡æ¯æ£€ç´¢çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶é€‚åˆå›ç­”å¼€æ”¾é¢†åŸŸçš„é—®é¢˜ã€‚DPRé€šè¿‡åŒå¡”ç»“æ„ï¼ˆdual encoderï¼‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ¥å®ç°æ–‡æœ¬çš„å‘é‡åŒ–è¡¨ç¤ºï¼Œä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªæŸ¥è¯¢ç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æ¡£ç¼–ç å™¨ã€‚è¿™ä¸¤ä¸ªç¼–ç å™¨é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹æˆ–å…¶ä»–Transformeræ¨¡å‹ï¼Œåˆ†åˆ«å¯¹ç”¨æˆ·æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå›ºå®šç»´åº¦çš„å¯†é›†å‘é‡ï¼ˆdense vectorï¼‰ã€‚
* é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒWikipedia passagesï¼‰ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ è®­ç»ƒï¼ŒDPRå¯ä»¥åœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æ¡£æ£€ç´¢ï¼‰ä¸­å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„æ–‡æœ¬æ£€ç´¢ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç¨€ç–å‘é‡æ£€ç´¢ï¼ˆå¦‚TF-IDFæˆ–BM25ï¼‰ï¼ŒDPRçš„å¯†é›†è¡¨ç¤ºå¯ä»¥æ›´å¥½åœ°æ•æ‰è¯æ±‡çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› æ­¤åœ¨éœ€è¦æ›´ç²¾å‡†è¯­ä¹‰åŒ¹é…çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚

* DPRçš„å·¥ä½œåŸç†å¦‚ä¸‹::

    1. æŸ¥è¯¢ç¼–ç ï¼šå°†ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚é—®é¢˜æˆ–å…³é”®è¯ï¼‰è½¬åŒ–ä¸ºå¯†é›†å‘é‡ã€‚
    2. æ–‡æ¡£ç¼–ç ï¼šå°†å€™é€‰æ–‡æ¡£åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£è½¬åŒ–ä¸ºå¯†é›†å‘é‡ã€‚
    3. ç›¸ä¼¼åº¦è®¡ç®—ï¼šåˆ©ç”¨å‘é‡ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚å†…ç§¯æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä»è€Œé€‰å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ã€‚


Related Models::

    dpr-reader-single-nq-base
    dpr-ctx_encoder-single-nq-base
    dpr-question_encoder-single-nq-base

    dpr-ctx_encoder-multiset-base
    dpr-question_encoder-multiset-base
    dpr-reader-multiset-base

    è¯´æ˜:
        single-nq-base: é€‚åˆä¸“æ³¨äºNatural Questionsï¼ˆNQï¼‰æ•°æ®é›†çš„é—®é¢˜å›ç­”ï¼Œé€šå¸¸åœ¨ç‰¹å®šé¢†åŸŸçš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³
        multiset-base: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå…·å¤‡è·¨é¢†åŸŸé—®ç­”çš„èƒ½åŠ›ï¼Œæ›´åŠ é€šç”¨

* dpr-reader: ç”¨äºç­”æ¡ˆæŠ½å–çš„é˜…è¯»å™¨æ¨¡å‹ã€‚å®ƒåœ¨DPRçš„æ£€ç´¢åå¤„ç†é˜¶æ®µä½¿ç”¨ï¼Œä»¥ä»é€‰å®šçš„å€™é€‰æ®µè½ä¸­æ‰¾åˆ°å…·ä½“çš„ç­”æ¡ˆã€‚è¿™ç§æ¨¡å‹åŸºäºBERTæˆ–ç±»ä¼¼çš„Transformeræ¶æ„ï¼Œç»è¿‡è®­ç»ƒå¯ä»¥ä»å€™é€‰æ–‡æœ¬ä¸­æå–å‡ºæœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç‰‡æ®µã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒä¾èµ–äºæŸ¥è¯¢ç¼–ç å™¨å’Œæ–‡æ¡£ç¼–ç å™¨ç­›é€‰å‡ºçš„å€™é€‰æ®µè½ä½œä¸ºè¾“å…¥ã€‚
* dpr-ctx_encoder: ç”¨äºå°†æ–‡æ¡£æˆ–æ®µè½ç¼–ç ä¸ºå¯†é›†å‘é‡çš„æ–‡æ¡£ï¼ˆæˆ–ä¸Šä¸‹æ–‡ï¼‰ç¼–ç å™¨ã€‚å…¶ä½œç”¨æ˜¯å°†å¤§é‡å€™é€‰æ–‡æ¡£æˆ–æ®µè½è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å¯†é›†å‘é‡è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿå¯ä»¥åˆ©ç”¨å‘é‡æ£€ç´¢æ–¹æ³•ï¼ˆå¦‚å†…ç§¯æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å¤§è§„æ¨¡æ–‡æ¡£åº“ä¸­å¿«é€Ÿæ£€ç´¢å‡ºä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
* dpr-question_encoder: ç”¨äºå°†ç”¨æˆ·é—®é¢˜ç¼–ç ä¸ºå¯†é›†å‘é‡çš„æŸ¥è¯¢ç¼–ç å™¨ã€‚è¿™ä¸ªæ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå¯†é›†å‘é‡ï¼Œä»¥ä¾¿ä¸æ–‡æ¡£ç¼–ç å™¨ç”Ÿæˆçš„æ–‡æ¡£å‘é‡è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„å€™é€‰æ–‡æ¡£ã€‚


DPR æ¨¡å‹çš„æµç¨‹::

    Step 1: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜é€šè¿‡ dpr-question_encoder ç¼–ç ä¸ºå¯†é›†å‘é‡ã€‚
    Step 2: å€™é€‰æ–‡æ¡£é€šè¿‡ dpr-ctx_encoder ç¼–ç ä¸ºå¯†é›†å‘é‡ã€‚
    Step 3: é€šè¿‡è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆé€šå¸¸æ˜¯å†…ç§¯ï¼‰ï¼Œæ£€ç´¢åˆ°ç›¸å…³åº¦æœ€é«˜çš„å€™é€‰æ–‡æ¡£ã€‚
    Step 4: å°†è¿™äº›å€™é€‰æ–‡æ¡£äº¤ç»™ dpr-readerï¼Œç”±å…¶ä»ä¸­æŠ½å–å…·ä½“ç­”æ¡ˆç‰‡æ®µã€‚






å…¶ä»–-fromGPT
============


Transformer çš„ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬::

    1. Attention Blockã€
        æ³¨æ„åŠ›æ¨¡å—ï¼Œæœ€æ ¸å¿ƒçš„æ¨¡å—
    2. Feed-Forward Network (FFN)
        åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼Œæ³¨æ„åŠ›æ¨¡å—åé¢æ˜¯ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ç½‘ç»œ
        FFN å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹æ“ä½œï¼Œä¸»è¦è´Ÿè´£éçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
        FFN çš„å‚æ•°è§„æ¨¡é€šå¸¸å¾ˆå¤§ï¼Œå› æ­¤ä¹Ÿæ˜¯è®¡ç®—å¼€é”€çš„ä¸€ä¸ªä¸»è¦æ¥æºã€‚
    3. Embedding Layer
        Embedding å±‚ç”¨äºå°†è¾“å…¥çš„ç¦»æ•£æ ‡è®°ï¼ˆtokensï¼‰è½¬åŒ–ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºã€‚
        åœ¨ NLP æ¨¡å‹ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºé€šå¸¸éƒ½æœ‰ embedding å±‚ï¼Œåˆ†åˆ«å°†è¾“å…¥æ ‡è®°è½¬ä¸ºå‘é‡å’Œå°†è¾“å‡ºå‘é‡è½¬å›æ ‡è®°ã€‚
        å¯¹äº NLP æ¨¡å‹ï¼Œè¯æ±‡è¡¨çš„å¤§å°ä¼šå½±å“ embedding å±‚çš„å‚æ•°è§„æ¨¡ã€‚
    4. Positional Encoding
        Transformer ä¸å…·å¤‡å†…åœ¨çš„åºåˆ—ä½ç½®æ„ŸçŸ¥ï¼Œå› æ­¤éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰æ¥è¡¨ç¤ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„ä¿¡æ¯ã€‚
        ä½ç½®ç¼–ç å¯ä»¥æ˜¯å›ºå®šçš„ï¼ˆä¾‹å¦‚é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆï¼‰æˆ–æ˜¯å¯è®­ç»ƒçš„å‘é‡ã€‚
        è™½ç„¶ä½ç½®ç¼–ç ä¸åŒ…å«å¾ˆå¤šå‚æ•°ï¼Œä½†å®ƒåœ¨ Transformer ä¸­è‡³å…³é‡è¦ï¼Œå¸®åŠ©æ¨¡å‹åŒºåˆ†åºåˆ—ä¸­çš„ä½ç½®ã€‚
    5. Layer Normalization
        æ¯ä¸ª Transformer å±‚é€šå¸¸åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰æ­¥éª¤ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½ã€‚
        è™½ç„¶ Layer Normalization çš„å‚æ•°å¾ˆå°‘ï¼Œä½†å¯¹æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½æœ‰è¾ƒå¤§å½±å“ã€‚
    6. Residual Connections
        åœ¨æ¯ä¸ªæ¨¡å—ï¼ˆå¦‚ Attention å’Œ FFNï¼‰ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ˆresidual connectionsï¼‰æ¥è¿æ¥è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚
        æ®‹å·®è¿æ¥æœ¬èº«æ²¡æœ‰å‚æ•°ï¼Œä½†åœ¨è®¡ç®—å›¾ä¸­å¢åŠ äº†å¯¹è¾“å…¥çš„ç›´æ¥å¼•ç”¨ã€‚






























