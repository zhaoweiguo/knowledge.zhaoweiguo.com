LLM评估
#######





AgentBench
==========


* https://github.com/THUDM/AgentBench
* A Comprehensive Benchmark to Evaluate LLMs as Agents (ICLR'24)




SacreBLEU
=========

* 用于计算机器翻译质量的评估指标的 Python 库，专门用于计算 BLEU（Bilingual Evaluation Understudy）分数。
* BLEU 是机器翻译和其他自然语言生成任务中最常用的自动评估指标之一。
* SacreBLEU 提供了一种标准化的方式来计算 BLEU 分数，保证不同实验之间的一致性，避免了不同工具使用不同 BLEU 计算方法导致的分数不一致问题。



jiwer
=====


* 用于评估自动语音识别 (ASR) 系统输出的 Python 库，主要通过计算文本序列之间的差异来衡量 ASR 系统的性能。该库可以计算多种评估指标，如字错误率 (Word Error Rate, WER)、句子错误率 (Sentence Error Rate, SER)、字符错误率 (Character Error Rate, CER) 等。这些指标通常用于衡量 ASR 输出与参考文本之间的匹配程度。























