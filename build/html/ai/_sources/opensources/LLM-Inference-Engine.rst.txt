LLM Inference Engines
#####################


大型语言模型 (LLM) 引擎::

    llama.cpp: 专门为 LLM 开发的开源引擎，旨在提供高性能和效率。
    TensorRT-LLM: 基于 NVIDIA TensorRT 的开源引擎，可利用 GPU 加速 LLM 推理。

通用推理引擎::

    Triton: 支持多种模型格式和硬件平台的开源推理引擎。
    ONNX Runtime: 支持多种模型格式和硬件平台的开源推理引擎。




vLLM
====

* 参见具体的条目

llama.cpp
=========

* 参见具体的条目

Triton
======


* NVIDIA 开发的开源推理服务器，旨在为多模型、多框架的深度学习和机器学习应用提供高效的推理服务。它能够在云端、数据中心和边缘设备上高效地运行，支持多种部署和扩展方案。
* https://developer.nvidia.com/triton-inference-server
* 开发文档: https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/index.html




主要特点
--------

* 多框架支持：Triton 支持多种深度学习框架，包括 TensorFlow、PyTorch、ONNX、TensorRT 等。它可以运行不同框架的模型，使得在生产环境中混合使用不同框架的模型变得容易。
* 多模型部署：Triton 支持在同一个推理服务器上同时运行多个模型。这使得在一个服务器上部署不同任务的模型成为可能，并且支持动态加载和卸载模型。
* 高性能：Triton 旨在提供高性能的推理服务，支持 GPU 加速和 CPU 计算。它使用多种优化策略来提高模型的推理效率，适合处理高负载和高并发的场景。
* 灵活的部署选项：Triton 可以在 Kubernetes 等容器化环境中运行，支持弹性扩展和自动化部署。同时，它支持 RESTful 和 gRPC 接口，使得与其他服务的集成变得简单。
* 监控和可观察性：Triton 提供丰富的监控和可观察性功能，支持 Prometheus、Grafana 等工具。通过这些功能，您可以监控模型的性能、资源使用情况等。



ONNX_Runtime
============

ONNX Runtime 是一个高性能的推理引擎，旨在加速 ONNX（Open Neural Network Exchange）模型的推理和执行。它是由微软开发的，作为开源项目在 GitHub 上发布。以下是 ONNX Runtime 的主要特点和用途：

主要特点
--------

* 高性能：ONNX Runtime 被设计成高效的推理引擎，旨在提供快速和低延迟的模型推理。它使用多种优化技术来确保模型在运行时表现出最佳性能。
* 跨平台：ONNX Runtime 支持多种操作系统和硬件架构，包括 Linux、Windows、macOS、以及各种硬件加速器，如 NVIDIA GPU、AMD GPU、英特尔硬件、ARM 等。
* 多框架支持：它支持 ONNX 模型，这意味着您可以将来自不同深度学习框架（如 PyTorch、TensorFlow、MXNet、Caffe2 等）的模型转换为 ONNX 格式，然后在 ONNX Runtime 中运行。
* 灵活性：ONNX Runtime 提供多种编程语言的接口，如 Python、C++、C#、Java 等。它支持灵活的集成，可以用于桌面应用、服务器、移动应用和边缘设备。
* 优化策略：ONNX Runtime 支持多种优化策略，如图形优化、内存优化、运行时调优等，以提高模型的运行效率。


TensorRT-LLM
============

* TensorRT-LLM 是 NVIDIA 推出的专门用于大型语言模型（LLM）加速和优化的框架。该框架基于 NVIDIA 的 TensorRT 技术，旨在提供在 GPU 上高效推理的解决方案。
* https://github.com/NVIDIA/TensorRT-LLM
* https://github.com/NVIDIA/TensorRT
* 开发文档: https://nvidia.github.io/TensorRT-LLM/


主要特点
--------

* 高性能推理：TensorRT-LLM 利用 NVIDIA 的 TensorRT 技术，可以在 NVIDIA GPU 上高效运行和推理大型语言模型。通过使用 GPU 的并行计算能力，它可以显著加速模型的推理过程。
* 支持多种 LLM：TensorRT-LLM 适用于多种大型语言模型，包括流行的 GPT、BERT、T5、LLaMA 等。这使得它能够满足各种 NLP（自然语言处理）应用的需求。
* 优化和精简：该框架提供了一系列优化策略，可以通过量化、融合、自动调优等方式来减少计算量，从而提高性能和降低资源消耗。
* 可定制性：TensorRT-LLM 允许用户根据自己的需求定制优化策略，包括分片、编译、调优等，以适应特定模型和工作负载。
* 易于部署：TensorRT-LLM 与 NVIDIA Triton Inference Server 集成，方便在生产环境中部署。它还支持各种部署方案，包括云端和边缘设备。













































































