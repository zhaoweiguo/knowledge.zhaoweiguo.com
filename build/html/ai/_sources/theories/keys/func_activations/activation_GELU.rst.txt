激活函数-GELU
#############

* GELU (Gaussian Error Linear Unit) 
* 近似于 ReLU，在神经网络中表现良好，但具有更加平滑的导数，在某些情况下能够提供更好的性能。
* CDF: cumulative distribution function of the standard Gaussian distribution(标准正态分布的累积分布函数)
* 公式：

.. math:: 

    \text{GELU}(x) = x \cdot \Phi(x) \\


其中 :math:`\Phi (x)` 是标准正态分布的累积分布函数(CDF)

.. math::

    \Phi (x) = \frac{1}{2} \left( 1+\text{erf} \left( \frac{x}{\sqrt{2}} \right) \right)


简化版::

    GELU(x) ≈ 0.5 ⋅ x ⋅ (1 + tanh[√((2/π)) ⋅ (x + 0.044715 ⋅ x^3)]

    erf(x)：误差函数（Error Function），是统计学中用于描述正态分布的函数

.. math::

    \text{GELU}(x) \approx 0.5 \cdot x \cdot \left(1+ \tan \left(\sqrt{\frac{2}{\pi}} \cdot (x+0.044715 \cdot x^3 \right) \right)


.. figure:: https://img.zhaoweiguo.com/uPic/2024/11/a1RWsJ.png

    The x-axis shows the function inputs and the y-axis shows the function outputs.


其他
====

* SwiGLU (Sigmoid-Weighted Linear Unit)





