# K-Means聚类算法


## K-Means聚类算法简介

### 一、核心思想

K-Means是一种**无监督学习**算法，用于对未标记的数据进行聚类。它的目标非常直观：将数据集中的 `n` 个样本划分为 `k` 个簇，使得**同一簇内的样本彼此相似，而不同簇的样本差异较大**。

这里的“相似”通常用**距离**来衡量，最常用的是欧几里得距离。算法通过迭代的方式，不断优化簇内样本到其中心点的距离之和，最终达到一个稳定的聚类结果。

### 二、工作流程（步骤）

K-Means算法遵循一个简单而有效的迭代过程：

1.  **初始化中心点**
    *   随机选择 `k` 个数据点作为初始的**聚类中心**（质心）。`k` 的值需要用户事先指定。

2.  **分配数据点**
    *   计算每一个数据点到 `k` 个质心的距离。
    *   将每个数据点**分配**到距离它**最近**的那个质心所在的簇。

3.  **更新中心点**
    *   对于第一步分配形成的 `k` 个簇，重新计算每个簇的质心。
    *   新的质心就是该簇中所有数据点的**平均值**（均值，这也是“K-Means”名字的由来）。

4.  **迭代与收敛**
    *   **重复步骤2和步骤3**，直到满足终止条件。常见的终止条件有：
        *   质心的位置不再发生明显变化（即移动距离小于某个阈值）。
        *   簇的分配情况不再改变。
        *   达到了预设的最大迭代次数。

整个过程可以直观地理解为下图所展示的迭代过程：

```mermaid
flowchart TD
    A[开始: 随机初始化K个质心] --> B[将每个点分配到最近的质心<br>形成K个簇]
    B --> C[重新计算每个簇的质心<br>（取平均值）]
    C --> D{质心位置是否变化?}
    D -- 是 --> B
    D -- 否 --> E[输出聚类结果]
```

### 三、关键概念与优缺点

**优点：**
*   **简单高效**：原理和实现都非常简单，计算速度快，对于大型数据集也有较好的伸缩性。
*   **结果可解释性强**：聚类结果通常很直观，每个簇可以用其质心来代表。

**缺点：**
*   **需要预先指定K值**：`k` 的选择通常是一个难题，需要依赖经验或使用“肘部法则”等方法辅助判断。
*   **对初始值敏感**：不同的初始质心可能会导致完全不同的聚类结果。常用**K-Means++** 算法来优化初始点的选择。
*   **对异常值敏感**：极端值或噪声点会对均值计算产生很大影响。
*   **只能发现球状簇**：它基于距离度量，对于非球形、复杂形状的簇识别效果不佳。
*   **不适合离散分类**：均值计算要求数据最好是连续数值型的。

### 四、如何选择K值？

由于K值需要人为指定，常用的选择方法有：

*   **肘部法则**：计算不同K值对应的聚类损失（通常是所有样本到其质心的距离平方和，称为**SSE**）。随着K增大，SSE会减小。当K增加到真实簇数时，SSE的下降幅度会骤减，然后随着K继续增大而平稳下降。这个转折点对应的K值就是建议值，形如手肘，故名“肘部法则”。

    

*   **轮廓系数**：结合了聚类的凝聚度和分离度，用于评估聚类的效果。轮廓系数的取值范围为[-1, 1]，值越大表示聚类效果越好。可以对不同的K值分别计算平均轮廓系数，从而选择最佳的K。

### 五、应用场景

K-Means算法因其简单高效，被广泛应用于各个领域：

*   **客户细分**：根据用户的购买行为、 demographics 等特征对客户进行分群，制定精准营销策略。
*   **图像压缩**：将图像中颜色相似的像素点聚类，用簇的质心颜色代替簇内所有颜色，减少颜色数量，从而实现压缩。
*   **文档分类**：对文本文档进行聚类，发现主题或类别。
*   **异常检测**：远离任何簇质心的点可以被识别为异常点。
*   **特征学习**：作为其他复杂算法的预处理步骤。

---

## 总结

K-Means是聚类分析中最基础、最常用的算法之一。它通过**迭代计算样本点与质心的距离并不断更新质心**来完成聚类任务。虽然存在对K值选择、初始值和异常值敏感等局限性，但其**简单性、高效性和可解释性**使其成为数据挖掘和机器学习领域不可或缺的工具。在实际应用中，常与K-Means++初始化、肘部法则等技巧结合使用，以获得更优的聚类效果。





