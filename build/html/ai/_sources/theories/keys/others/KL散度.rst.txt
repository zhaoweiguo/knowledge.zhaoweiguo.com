Kullback-Leibler 散度
#####################

* Kullback-Leibler (KL) 散度，又称为相对熵，是一种衡量两个概率分布之间差异的非对称度量
* KL散度用于计算一个概率分布 𝑃 和另一个概率分布 𝑄 之间的信息差异，反映了从 𝑄 的角度来看，使用 𝑃 分布进行建模的“成本”。

* KL散度的数学表达式如下：

.. math::

    D_{\mathrm{KL}}(P \| Q)=\sum_{x} P(x) \log \left(\frac{P(x)}{Q(x)}\right)

* 其中：
    * 𝑃(𝑥) 是实际的分布，通常被视为“真实”的概率分布
    * 𝑄(𝑥) 是预测的分布或假设的分布
* KL散度的值始终是非负的，即 :math:`𝐷_{KL}(𝑃∣∣𝑄)≥0` ，并且只有当 𝑃(𝑥)=𝑄(𝑥)x 对于所有 𝑥 都成立时，KL散度为零。



KL散度的性质
=============

* 非对称性：KL散度是非对称的，即 :math:`𝐷_{KL}(𝑃∣∣𝑄)≠𝐷_{KL}(𝑄∣∣𝑃)`
* 信息量度：KL散度衡量了使用 𝑄 来近似 𝑃 时损失的信息量。越大意味着 𝑄 分布和 𝑃 分布之间的差距越大。
* 在优化中的应用：KL散度广泛应用于机器学习，尤其是在变分推理、生成对抗网络（GANs）和信息论中。































