Weight Tying
############

* Weight Tying（权重共享）是一种优化深度学习模型参数的方法，常用于语言模型（如Transformer和RNN）中，特别是在词嵌入层（Embedding Layer）和输出层之间。



定义
====

* 在语言模型中，通常有以下两个关键层：

    * Embedding Layer：将输入的词汇索引映射为一个连续的、高维向量表示。
    * Output Layer：将模型的最终表示映射回词汇表的概率分布，用于预测下一个词。


* 这两层会有独立的权重矩阵：

    * Embedding Layer 权重矩阵（大小为 V × d）：词汇表大小为 𝑉，嵌入维度为 𝑑。
    * Output Layer 权重矩阵（大小为 d × V）：从隐藏层映射到词汇表。

.. note:: Weight Tying 的核心思想是：将 Embedding Layer 和 Output Layer 的权重矩阵设置为相同，即共享权重。



为什么使用 Weight Tying
=======================

* 减少模型参数：原本需要两个独立的矩阵（一个大小为 𝑉×𝑑，另一个为 𝑑×𝑉），而 Weight Tying 将它们合并成一个矩阵 𝑊（大小仍为 𝑉×𝑑），从而显著减少参数量，尤其在大词汇表的情况下。

* 提升模型表现：输入词向量和输出预测之间共享表示，使模型在学习过程中保持一致性。Embedding 表示的质量更高，因为它同时服务于输入编码和输出解码的任务。

* 更高效的计算：Weight Tying 减少了冗余的矩阵存储需求，使内存利用率更高。



实际运作方式
============

* 输入的嵌入矩阵是 :math:`𝑊_{𝑒𝑚𝑏𝑒𝑑}∈𝑅^{𝑉×𝑑}`
* 输出层的权重矩阵为 :math:`𝑊_{𝑜𝑢𝑡𝑝𝑢𝑡}∈𝑅^{𝑑×𝑉}`

* 使用 Weight Tying 后： :math:`𝑊_{𝑜𝑢𝑡𝑝𝑢𝑡}=𝑊_{𝑒𝑚𝑏𝑒𝑑}^⊤`

* 即：
    * Embedding 阶段： 输入索引 𝑥 映射为嵌入 :math:`𝑒_𝑥=𝑊_{𝑒𝑚𝑏𝑒𝑑}⋅𝑜𝑛𝑒\_ℎ𝑜𝑡(𝑥)`
    * 输出阶段： 使用 :math:`𝑒_𝑥` 和 :math:`𝑊_{𝑒𝑚𝑏𝑒𝑑}^⊤` 来计算词汇概率分布。





应用场景
========

* Transformer 模型：Weight Tying 通常用于 Transformer 的嵌入层和解码器的 Softmax 输出层之间。
* 语言模型（RNN/LSTM）：如经典的 GPT 和语言建模任务中，也普遍采用 Weight Tying。































