AdamW
#####

* AdamW 是一种优化算法，是 Adam（Adaptive Moment Estimation）优化算法的改进版。它在机器学习和深度学习中广泛应用，尤其适合处理具有大规模参数的模型（如深度神经网络）。AdamW 的主要改进在于引入了**权重衰减（Weight Decay）** 的正确实现，从而提升了模型的泛化能力。




Adam 和权重衰减的问题
=====================

* 在深度学习中，**权重衰减（Weight Decay）** 是一种正则化方法，用于防止模型过拟合。传统做法是在目标函数的损失项中加上权重 𝐿2 正则化项：

.. math::

    𝐿=Loss(𝜃)+\frac{𝜆}{2}∥𝜃∥^2_2 ：


* 其中：
* Loss(𝜃)：原始损失函数。
* :math:`∥𝜃∥^2_2` ：权重的平方和，鼓励模型的参数保持较小。
* 𝜆：正则化系数（也称权重衰减系数）。


公式
====

* AdamW（Adam + Weight Decay）通过将权重衰减从梯度更新中分离，提供了更准确的正则化实现。具体来说，它将权重衰减直接作用于参数，而不是作为损失梯度的一部分。



.. math::

    \begin{array}{l}
    一阶动量（均值）的更新：𝑚_𝑡=𝛽_1𝑚_{𝑡−1}+(1−𝛽_1)𝑔_𝑡 \\
    二阶动量（方差）的更新：𝑣_𝑡=𝛽_2𝑣_{𝑡−1}+(1−𝛽_2)𝑔_𝑡^2 \\
    偏差校正： 为了修正前期动量的偏差（初始值为 0），对 𝑚_𝑡 和 𝑣_𝑡 做偏差校正： \\
    \hat{𝑚}_𝑡=\frac{𝑚_𝑡}{1−𝛽_1^𝑡},\hat{𝑣}_𝑡=\frac{𝑣_𝑡}{1−𝛽_2^𝑡} \\
    参数更新：𝜃_{𝑡+1}=𝜃_𝑡−𝜂⋅\frac{\hat{𝑚}_𝑡}{\sqrt{\hat{𝑣}_𝑡}+𝜖}−η⋅λ⋅θ_t
    \end{array}

* ϵ 是一个小值（如 10^−8 ），用于防止除零
* :math:`𝑔_𝑡` ：当前梯度。
* :math:`𝑚_𝑡` ：梯度的一阶动量（均值）。
* :math:`𝑣_𝑡` ：梯度的二阶动量（方差）。
* :math:`\hat{𝑚}_𝑡,\hat{𝑣}_𝑡` ：偏差校正后的 :math:`𝑚_𝑡` 和 :math:`𝑣_𝑡` 。
* :math:`𝜂`：学习率。
* :math:`−η⋅λ⋅θ_t` 是权重衰减的实现


与Adam的区别
============

核心区别::

    正则化分离：权重衰减直接施加到参数上，而不是通过修改梯度。
    动量校正保持不变：权重衰减不会干扰 Adam 的动量和自适应学习率计算。


优缺点
======

优点::

    更准确的权重衰减：避免了传统 Adam 中的正则化失效问题。
    提升泛化性能：分离权重衰减后，优化效果更贴近理论设计，尤其在训练深层模型时，能更好地抑制过拟合。
    适配性强：保留了 Adam 的所有优点，如对稀疏梯度的处理和高效的学习率调整能力。


.. note:: AdamW 的优化机制是深度学习优化算法中的里程碑之一，其在实际训练中的效果已经被多个模型验证，尤其在 Transformer 系列模型中得到了广泛应用。



















