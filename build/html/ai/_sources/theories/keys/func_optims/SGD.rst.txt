SGD随机梯度下降
###############

* 随机梯度下降(SGD, Stochastic Gradient Descent)是一种优化算法，用于最小化目标函数（如损失函数）。它是梯度下降（Gradient Descent）的变体，与批量梯度下降不同，SGD 每次仅使用一个样本计算梯度并更新参数，因此计算效率高，但更新过程具有一定的随机性。




核心思想
========

* 在每次迭代中，随机选择一个样本（或一个小批量的样本，称为 Mini-batch）。
* 计算该样本的梯度。
* 使用该梯度更新参数。


.. note:: 与批量梯度下降相比，SGD 在每次更新时引入了噪声（因为只依赖于一个样本的梯度），这使得 SGD 在某些情况下能够跳出局部最优解。


公式
====

.. math::

    \begin{array}{l}
    \theta_{t+1}=\theta_{t}-\eta \nabla_{\theta} L\left(\theta_{t} ; x_{i}, y_{i}\right) \\
    θ_{t+1} =θ_t −η∇_θ L(θ_t; x_i, y_i) 
    \end{array}

* 其中:
* θ：模型参数
    * :math:`θ_t` ：当前模型参数
    * :math:`θ_{t+1}` ：更新后的模型参数
    * t： 为迭代次数
* :math:`∇_θ L(θ_t; x_i, y_i)` : 为损失函数关于模型参数的梯度
    * :math:`x_{i}, y_{i}` 为一个样本
    * i 为样本索引
    * :math:`L(\theta)` 为损失函数
* η：学习率




优缺点
======


优点::

    计算高效：
        每次只计算一个样本的梯度，计算成本远低于批量梯度下降，特别适合大数据集。
    跳出局部最优：
        由于更新中引入了随机性，SGD 在某些情况下能够跳出局部最优解。
    在线学习：
        SGD 能够直接处理流式数据，每次只需要加载一个样本。


缺点::

    更新不稳定：
        每次使用一个样本，导致梯度更新的波动较大，可能使损失函数来回波动而不收敛。
    收敛速度较慢：
        虽然能跳出局部最优，但可能需要更长的时间才能达到收敛点。
    对学习率敏感：
        学习率的选择至关重要，过大可能导致发散，过小则收敛速度变慢








































