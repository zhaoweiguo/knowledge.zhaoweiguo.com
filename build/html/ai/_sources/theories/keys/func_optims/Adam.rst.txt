Adam
####

* Adam（Adaptive Moment Estimation）自适应矩估计
* Adam优化器在2015年由Kingma和Ba在论文《Adam: A Method for Stochastic Optimization》中提出。它结合了动态学习率和动态momentum，以解决梯度问题。在许多实践中，Adam优化器表现出比传统优化方法更好的性能。
* 能够为每个参数动态调整学习率，因而表现出色。



核心思想
========

* Adam 同时维护了参数梯度的一阶动量（均值）和二阶动量（方差），并使用这两个信息动态调整学习率。

    * 一阶动量（均值）： 梯度的指数加权平均，用于平滑梯度的变化。
    * 二阶动量（方差）： 梯度平方的指数加权平均，用于衡量梯度的变化幅度。

这种机制使 Adam 能够在训练过程中::

    加速收敛
    在稀疏梯度或梯度变化剧烈的场景中表现稳定



公式
====

.. math::

    \begin{array}{l}
    一阶动量（均值）的更新：𝑚_𝑡=𝛽_1𝑚_{𝑡−1}+(1−𝛽_1)𝑔_𝑡 \\
    二阶动量（方差）的更新：𝑣_𝑡=𝛽_2𝑣_{𝑡−1}+(1−𝛽_2)𝑔_𝑡^2 \\
    偏差校正： 为了修正前期动量的偏差（初始值为 0），对 𝑚_𝑡 和 𝑣_𝑡 做偏差校正： \\
    \hat{𝑚}_𝑡=\frac{𝑚_𝑡}{1−𝛽_1^𝑡},\hat{𝑣}_𝑡=\frac{𝑣_𝑡}{1−𝛽_2^𝑡} \\
    参数更新：𝜃_{𝑡+1}=𝜃_𝑡−𝜂⋅\frac{\hat{𝑚}_𝑡}{\sqrt{\hat{𝑣}_𝑡}+𝜖}
    \end{array}

* ϵ 是一个小值（如 10^−8 ），用于防止除零
* :math:`𝑔_𝑡` ：当前梯度。
* :math:`𝑚_𝑡` ：梯度的一阶动量（均值）。
* :math:`𝑣_𝑡` ：梯度的二阶动量（方差）。
* :math:`\hat{𝑚}_𝑡,\hat{𝑣}_𝑡` ：偏差校正后的 :math:`𝑚_𝑡` 和 :math:`𝑣_𝑡` 。
* :math:`𝜂`：学习率。



优缺点
======

优点::

    自适应学习率：
        根据一阶和二阶动量动态调整每个参数的学习率，无需手动调节。

    快速收敛：
        在高维参数空间中，Adam 能快速找到较优解。

    对稀疏梯度友好：
        在 NLP 和推荐系统等场景中，Adam 对稀疏梯度的处理非常高效。

    易于实现：
        只需少量代码即可实现，且超参数（如 𝛽_1,𝛽_2,𝜖）对最终结果影响较小。




缺点::

    欠收敛：
        在某些问题上，Adam 可能会陷入局部最优解或无法充分收敛。

    泛化能力不足：
        相比 SGD（随机梯度下降），Adam 的正则化效果较差，可能导致泛化性能下降。

    超参数敏感性：
        尽管默认参数适用于大多数场景，但特定任务可能需要微调。




