GELU
####

* GELU (Gaussian Error Linear Unit) 
* 近似于 ReLU，在神经网络中表现良好，但具有更加平滑的导数，在某些情况下能够提供更好的性能。
* 公式：

.. math:: 

    \text{GELU}(x) = x \cdot \Phi(x)
    \\
    \Phi (x) 是标准正态分布的累积分布函数











