

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7.4.2. vLLM官方文档 &mdash; 新溪-gordon V2025.07 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="7.5. llama.cpp框架" href="../llama.cpp.html" />
    <link rel="prev" title="7.4.1. 常用" href="normal.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/ml.html">1.3. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/deep_learning.html">1.5. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/normal.html">1.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/history.html">1.5.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/algorithm.html">1.7. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/tool.html">1.8. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/question.html">1.9. 常见问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. 机器人领域</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../theories/tmp.html">2.1. 临时</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/ReAct.html">2.1.1. ReAct框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Reflection.html">2.1.2. Reflection反思</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/math.html">2.1.3. 数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/bag-of-words.html">2.1.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/word2vec.html">2.1.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/doc2vec.html">2.1.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/FastText.html">2.1.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/LDA.html">2.1.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/overfitting-underfitting.html">2.1.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/RAG.html">2.1.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Agent.html">2.1.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/LLM.html">2.1.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/RL.html">2.1.13. RL-强化学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/prompt_engineering.html">2.1.14. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/finetune.html">2.1.15. LLM调优(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Workflow.html">2.1.16. Workflow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-推理模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. 模型量化(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Quantizations/normal.html">3.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Quantizations/GGUF.html">3.4.2. GGUF 文件</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/fileformat.html">3.5. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/normal.html">3.5.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/GGML.html">3.5.2. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/openai.html">3.6. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/normal.html">3.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/prompt.html">3.7. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_chinese.html">3.7.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/skill.html">3.7.3. 示例</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/Android.html">3.8. Android版LLM相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/normal.html">3.8.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Android版部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../RAG.html">4. RAG相关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/preprocess.html">5.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/normal.html">5.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/NER.html">5.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/normal.html">5.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/seq-label.html">5.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/history.html">5.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/summary.html">5.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/summarys/normal.html">5.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">6. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Image.html">6.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Video.html">6.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/normal.html">6.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/magic.html">6.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/display.html">6.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/normal.html">6.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/Ndarray.html">6.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/function.html">6.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/normal.html">6.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_subset.html">6.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_analysis.html">6.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_sql.html">6.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_default_value.html">6.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_multi_index.html">6.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/practice.html">6.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_input_output.html">6.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/normal.html">6.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/install.html">6.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/example.html">6.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pylab.html">6.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/SciPys/normal.html">6.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/normal.html">6.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/supervised.html">6.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/unsupervised.html">6.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/normal.html">6.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/example.html">6.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/struct.html">6.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Seaborns/normal.html">6.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/jieba.html">6.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/gensim.html">6.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/normal.html">6.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/LAC.html">6.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../framework.html">7. 学习框架</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../normal.html">7.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/normal.html">7.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/nn.html">7.2.2. nn模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../huggingface.html">7.3. huggingface</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/normal.html">7.3.1. 常用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/Transformers.html">7.3.2. Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/Transformers/Transformers.html">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/Transformers/Transformers_V4.45.2.html">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/Tokenizers_V0.13.3.html">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../huggingfaces/collect.html">7.3.7. 收集</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/collects/model.html">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../huggingfaces/collects/blog_decoding-methods.html">博文: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../vLLM.html">7.4. vLLM</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="normal.html">7.4.1. 常用</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">7.4.2. vLLM官方文档</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../llama.cpp.html">7.5. llama.cpp框架</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../llama.cpps/normal.html">7.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mxnet.html">7.7. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/ndarray.html">7.7.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/gluon.html">7.7.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/autograd.html">7.7.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Keras/normal.html">7.9.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Keras/demo.html">7.9.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../other.html">7.10. 其他</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../website.html">8. 关键网站</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/ArXiv.html">8.3. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/video.html">8.4. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/normal.html">8.5. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-inference-accelerate.html">10.9. LLM推理加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Evaluate.html">10.10. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/platform.html">10.11. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese.html">11.2. 中文数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese_image.html">11.3. 中文图片相关数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/huggingface.html">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/website.html">11.5. 数据集相关网站</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model.html">12. 常见模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">13. 图形&amp;计算加速技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cudas/normal.html">13.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Evaluate.html">14. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/normal.html">14.1. 通用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E4%BC%A0%E7%BB%9FAI.html">15. 传统AI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../framework.html"><span class="section-number">7. </span>学习框架</a> &raquo;</li>
        
          <li><a href="../vLLM.html"><span class="section-number">7.4. </span>vLLM</a> &raquo;</li>
        
      <li><span class="section-number">7.4.2. </span>vLLM官方文档</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/frameworks/vLLMs/vLLM_doc.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">7.4.2. vLLM官方文档</a><ul>
<li><a class="reference internal" href="#getting-started">Getting Started</a><ul>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#offline-batched-inference">Offline Batched Inference</a></li>
<li><a class="reference internal" href="#openai-compatible-server">OpenAI-Compatible Server</a><ul>
<li><a class="reference internal" href="#openai-completions-api-with-vllm">OpenAI Completions API with vLLM</a></li>
<li><a class="reference internal" href="#openai-chat-completions-api-with-vllm">OpenAI Chat Completions API with vLLM</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#debugging-tips">Debugging Tips</a><ul>
<li><a class="reference internal" href="#enable-more-logging">Enable more logging</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#serving">Serving</a><ul>
<li><a class="reference internal" href="#id2">OpenAI Compatible Server</a><ul>
<li><a class="reference internal" href="#supported-apis">Supported APIs</a></li>
<li><a class="reference internal" href="#chat-template">Chat Template</a></li>
<li><a class="reference internal" href="#cli-reference">CLI Reference</a><ul>
<li><a class="reference internal" href="#configuration-file">Configuration file</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#deploying-with-docker">Deploying with Docker</a><ul>
<li><a class="reference internal" href="#use-vllms-official-docker-image">Use vLLM’s Official Docker Image</a></li>
<li><a class="reference internal" href="#building-vllms-docker-image-from-source">Building vLLM’s Docker Image from Source</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deploying-with-kubernetes">Deploying with Kubernetes</a></li>
<li><a class="reference internal" href="#distributed-inference-and-serving">Distributed Inference and Serving</a><ul>
<li><a class="reference internal" href="#how-to-decide-the-distributed-inference-strategy">How to decide the distributed inference strategy?</a></li>
<li><a class="reference internal" href="#details-for-distributed-inference-and-serving">Details for Distributed Inference and Serving</a></li>
<li><a class="reference internal" href="#multi-node-inference-and-serving">Multi-Node Inference and Serving</a></li>
</ul>
</li>
<li><a class="reference internal" href="#production-metrics">Production Metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#models">Models</a><ul>
<li><a class="reference internal" href="#loading-a-model">Loading a Model</a><ul>
<li><a class="reference internal" href="#huggingface-hub">HuggingFace Hub</a></li>
<li><a class="reference internal" href="#modelscope">ModelScope</a></li>
<li><a class="reference internal" href="#list-of-models">List of Models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generative-models">Generative Models</a><ul>
<li><a class="reference internal" href="#offline-inference">Offline Inference</a><ul>
<li><a class="reference internal" href="#llm-generate">LLM.generate</a></li>
<li><a class="reference internal" href="#llm-beam-search">LLM.beam_search</a></li>
<li><a class="reference internal" href="#llm-chat">LLM.chat</a></li>
</ul>
</li>
<li><a class="reference internal" href="#online-inference">Online Inference</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pooling-models">Pooling Models</a></li>
<li><a class="reference internal" href="#adding-a-new-model">Adding a New Model</a></li>
<li><a class="reference internal" href="#enabling-multimodal-inputs">Enabling Multimodal Inputs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#usage">Usage</a><ul>
<li><a class="reference internal" href="#lora-adapters">LoRA Adapters</a><ul>
<li><a class="reference internal" href="#serving-lora-adapters">Serving LoRA Adapters</a></li>
<li><a class="reference internal" href="#dynamically-serving-lora-adapters">Dynamically serving LoRA Adapters</a></li>
<li><a class="reference internal" href="#new-format-for-lora-modules">New format for –lora-modules</a></li>
<li><a class="reference internal" href="#lora-model-lineage-in-model-card">Lora model lineage in model card</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal-inputs">Multimodal Inputs</a><ul>
<li><a class="reference internal" href="#image">Image</a></li>
<li><a class="reference internal" href="#video">Video</a></li>
<li><a class="reference internal" href="#audio">Audio</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tool-calling">Tool Calling</a><ul>
<li><a class="reference internal" href="#id3">Quickstart</a></li>
</ul>
</li>
<li><a class="reference internal" href="#structured-outputs">Structured Outputs</a></li>
<li><a class="reference internal" href="#speculative-decoding">Speculative decoding</a></li>
<li><a class="reference internal" href="#performance-and-tuning">Performance and Tuning</a></li>
<li><a class="reference internal" href="#environment-variables">Environment Variables</a></li>
<li><a class="reference internal" href="#usage-stats-collection">Usage Stats Collection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#design">Design</a><ul>
<li><a class="reference internal" href="#architecture-overview">Architecture Overview</a><ul>
<li><a class="reference internal" href="#entrypoints">Entrypoints</a><ul>
<li><a class="reference internal" href="#llm-class">LLM Class</a></li>
<li><a class="reference internal" href="#openai-compatible-api-server">OpenAI-compatible API server</a></li>
</ul>
</li>
<li><a class="reference internal" href="#llm-engine">LLM Engine</a><ul>
<li><a class="reference internal" href="#llmengine">LLMEngine</a></li>
<li><a class="reference internal" href="#asyncllmengine">AsyncLLMEngine</a></li>
</ul>
</li>
<li><a class="reference internal" href="#worker">Worker</a></li>
<li><a class="reference internal" href="#model-runner">Model Runner</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#class-hierarchy">Class Hierarchy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#input-processing">Input Processing</a><ul>
<li><a class="reference internal" href="#input-processing-pipeline">Input Processing Pipeline</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vllm-paged-attention">vLLM Paged Attention</a><ul>
<li><a class="reference internal" href="#inputs">Inputs</a></li>
<li><a class="reference internal" href="#concepts">Concepts</a></li>
<li><a class="reference internal" href="#query">Query</a></li>
<li><a class="reference internal" href="#key">Key</a></li>
<li><a class="reference internal" href="#qk">QK</a></li>
<li><a class="reference internal" href="#other">Other</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="vllm">
<h1><span class="section-number">7.4.2. </span>vLLM官方文档<a class="headerlink" href="#vllm" title="此标题的永久链接">¶</a></h1>
<ul class="simple">
<li><p>最新文档: <a class="reference external" href="https://docs.vllm.ai/en/stable/">https://docs.vllm.ai/en/stable/</a></p></li>
<li><p>当前文档: <a class="reference external" href="https://docs.vllm.ai/en/v0.6.6/index.html">https://docs.vllm.ai/en/v0.6.6/index.html</a></p></li>
<li><p>GitHub: <a class="reference external" href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a></p></li>
</ul>
<section id="getting-started">
<h2>Getting Started<a class="headerlink" href="#getting-started" title="此标题的永久链接">¶</a></h2>
<section id="quickstart">
<h3>Quickstart<a class="headerlink" href="#quickstart" title="此标题的永久链接">¶</a></h3>
<p>Prerequisites:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OS: Linux
Python: 3.9 – 3.12
GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, H100, etc.)
</pre></div>
</div>
<section id="offline-batched-inference">
<h4>Offline Batched Inference<a class="headerlink" href="#offline-batched-inference" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The president of the United States is&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The capital of France is&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The future of AI is&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="openai-compatible-server">
<h4>OpenAI-Compatible Server<a class="headerlink" href="#openai-compatible-server" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vllm serve Qwen/Qwen2.5-1.5B-Instruct

$ curl http://localhost:8000/v1/models
</pre></div>
</div>
<section id="openai-completions-api-with-vllm">
<h5>OpenAI Completions API with vLLM<a class="headerlink" href="#openai-completions-api-with-vllm" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">completions</span> \
    <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> \
    <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;</span><span class="p">,</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;San Francisco is a&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">0</span>
    <span class="p">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
<p>python版本:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Modify OpenAI&#39;s API key and API base to use vLLM&#39;s API server.</span>
<span class="n">openai_api_key</span> <span class="o">=</span> <span class="s2">&quot;EMPTY&quot;</span>
<span class="n">openai_api_base</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">openai_api_base</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;</span><span class="p">,</span>
                                      <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;San Francisco is a&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Completion result:&quot;</span><span class="p">,</span> <span class="n">completion</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="openai-chat-completions-api-with-vllm">
<h5>OpenAI Chat Completions API with vLLM<a class="headerlink" href="#openai-chat-completions-api-with-vllm" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">chat</span><span class="o">/</span><span class="n">completions</span> \
    <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> \
    <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;</span><span class="p">,</span>
        <span class="s2">&quot;messages&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Who won the world series in 2020?&quot;</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="c1"># Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.</span>
<span class="n">openai_api_key</span> <span class="o">=</span> <span class="s2">&quot;EMPTY&quot;</span>
<span class="n">openai_api_base</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">openai_api_key</span><span class="p">,</span>
    <span class="n">base_url</span><span class="o">=</span><span class="n">openai_api_base</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">chat_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-1.5B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke.&quot;</span><span class="p">},</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Chat response:&quot;</span><span class="p">,</span> <span class="n">chat_response</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="debugging-tips">
<h3>Debugging Tips<a class="headerlink" href="#debugging-tips" title="此标题的永久链接">¶</a></h3>
<section id="enable-more-logging">
<h4>Enable more logging<a class="headerlink" href="#enable-more-logging" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">VLLM_LOGGING_LEVEL</span><span class="o">=</span><span class="n">DEBUG</span>
<span class="n">export</span> <span class="n">CUDA_LAUNCH_BLOCKING</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">to</span> <span class="n">identify</span> <span class="n">which</span> <span class="n">CUDA</span> <span class="n">kernel</span> <span class="ow">is</span> <span class="n">causing</span> <span class="n">the</span> <span class="n">problem</span><span class="o">.</span>
<span class="n">export</span> <span class="n">NCCL_DEBUG</span><span class="o">=</span><span class="n">TRACE</span>
    <span class="n">to</span> <span class="n">turn</span> <span class="n">on</span> <span class="n">more</span> <span class="n">logging</span> <span class="k">for</span> <span class="n">NCCL</span><span class="o">.</span>
<span class="n">export</span> <span class="n">VLLM_TRACE_FUNCTION</span><span class="o">=</span><span class="mi">1</span>
    <span class="n">to</span> <span class="n">record</span> <span class="nb">all</span> <span class="n">function</span> <span class="n">calls</span> <span class="k">for</span> <span class="n">inspection</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">log</span> <span class="n">files</span> <span class="n">to</span> <span class="n">tell</span> <span class="n">which</span> <span class="n">function</span> <span class="n">crashes</span> <span class="ow">or</span> <span class="n">hangs</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="serving">
<h2>Serving<a class="headerlink" href="#serving" title="此标题的永久链接">¶</a></h2>
<section id="id2">
<h3>OpenAI Compatible Server<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="n">NousResearch</span><span class="o">/</span><span class="n">Meta</span><span class="o">-</span><span class="n">Llama</span><span class="o">-</span><span class="mi">3</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> <span class="o">--</span><span class="n">dtype</span> <span class="n">auto</span> <span class="o">--</span><span class="n">api</span><span class="o">-</span><span class="n">key</span> <span class="n">token</span><span class="o">-</span><span class="n">abc123</span>
</pre></div>
</div>
<p>normal format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">}</span>
</pre></div>
</div>
<section id="supported-apis">
<h4>Supported APIs<a class="headerlink" href="#supported-apis" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>OpenAI APIs:</dt><dd><ul>
<li><p>Completions API (/v1/completions)</p></li>
<li><p>Chat Completions API (/v1/chat/completions)</p></li>
<li><p>Embeddings API (/v1/embeddings)</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>custom APIs</dt><dd><ul>
<li><p>Tokenizer API (/tokenize, /detokenize)</p></li>
<li><p>Pooling API (/pooling)</p></li>
<li><p>Score API (/score)</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="chat-template">
<h4>Chat Template<a class="headerlink" href="#chat-template" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span> <span class="o">--</span><span class="n">chat</span><span class="o">-</span><span class="n">template</span> <span class="o">./</span><span class="n">path</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">chat</span><span class="o">-</span><span class="n">template</span><span class="o">.</span><span class="n">jinja</span>
</pre></div>
</div>
<p>OpenAI spec accept a new format which specifies both a type and a text field.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Classify this sentiment: vLLM is wonderful!&quot;</span><span class="p">}]}</span>
</pre></div>
</div>
</section>
<section id="cli-reference">
<h4>CLI Reference<a class="headerlink" href="#cli-reference" title="此标题的永久链接">¶</a></h4>
<section id="configuration-file">
<h5>Configuration file<a class="headerlink" href="#configuration-file" title="此标题的永久链接">¶</a></h5>
<p>config.yaml:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">host</span><span class="p">:</span> <span class="s2">&quot;127.0.0.1&quot;</span>
<span class="n">port</span><span class="p">:</span> <span class="mi">6379</span>
<span class="n">uvicorn</span><span class="o">-</span><span class="n">log</span><span class="o">-</span><span class="n">level</span><span class="p">:</span> <span class="s2">&quot;info&quot;</span>
</pre></div>
</div>
<p>指定 yaml 文件:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="n">SOME_MODEL</span> <span class="o">--</span><span class="n">config</span> <span class="n">config</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="deploying-with-docker">
<h3>Deploying with Docker<a class="headerlink" href="#deploying-with-docker" title="此标题的永久链接">¶</a></h3>
<section id="use-vllms-official-docker-image">
<h4>Use vLLM’s Official Docker Image<a class="headerlink" href="#use-vllms-official-docker-image" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">run</span> <span class="o">--</span><span class="n">runtime</span> <span class="n">nvidia</span> <span class="o">--</span><span class="n">gpus</span> <span class="nb">all</span> \
    <span class="o">-</span><span class="n">v</span> <span class="o">~/.</span><span class="n">cache</span><span class="o">/</span><span class="n">huggingface</span><span class="p">:</span><span class="o">/</span><span class="n">root</span><span class="o">/.</span><span class="n">cache</span><span class="o">/</span><span class="n">huggingface</span> \
    <span class="o">--</span><span class="n">env</span> <span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=&lt;secret&gt;&quot;</span> \
    <span class="o">-</span><span class="n">p</span> <span class="mi">8000</span><span class="p">:</span><span class="mi">8000</span> \
    <span class="o">--</span><span class="n">ipc</span><span class="o">=</span><span class="n">host</span> \
    <span class="n">vllm</span><span class="o">/</span><span class="n">vllm</span><span class="o">-</span><span class="n">openai</span><span class="p">:</span><span class="n">latest</span> \
    <span class="o">--</span><span class="n">model</span> <span class="n">mistralai</span><span class="o">/</span><span class="n">Mistral</span><span class="o">-</span><span class="mi">7</span><span class="n">B</span><span class="o">-</span><span class="n">v0</span><span class="mf">.1</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>You can either use the ipc=host flag or –shm-size flag to allow the container to access the host’s shared memory. vLLM uses PyTorch, which uses shared memory to share data between processes under the hood, particularly for tensor parallel inference.</p>
</div>
</section>
<section id="building-vllms-docker-image-from-source">
<h4>Building vLLM’s Docker Image from Source<a class="headerlink" href="#building-vllms-docker-image-from-source" title="此标题的永久链接">¶</a></h4>
<p>optionally specifies: –build-arg max_jobs=8 –build-arg nvcc_threads=2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOCKER_BUILDKIT</span><span class="o">=</span><span class="mi">1</span> <span class="n">docker</span> <span class="n">build</span> <span class="o">.</span> <span class="o">--</span><span class="n">target</span> <span class="n">vllm</span><span class="o">-</span><span class="n">openai</span> <span class="o">--</span><span class="n">tag</span> <span class="n">vllm</span><span class="o">/</span><span class="n">vllm</span><span class="o">-</span><span class="n">openai</span>
</pre></div>
</div>
</section>
</section>
<section id="deploying-with-kubernetes">
<h3>Deploying with Kubernetes<a class="headerlink" href="#deploying-with-kubernetes" title="此标题的永久链接">¶</a></h3>
</section>
<section id="distributed-inference-and-serving">
<h3>Distributed Inference and Serving<a class="headerlink" href="#distributed-inference-and-serving" title="此标题的永久链接">¶</a></h3>
<section id="how-to-decide-the-distributed-inference-strategy">
<h4>How to decide the distributed inference strategy?<a class="headerlink" href="#how-to-decide-the-distributed-inference-strategy" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Single GPU (no distributed inference): If your model fits in a single GPU, you probably don’t need to use distributed inference. Just use the single GPU to run the inference.</p></li>
<li><p>Single-Node Multi-GPU (tensor parallel inference): If your model is too large to fit in a single GPU, but it can fit in a single node with multiple GPUs, you can use tensor parallelism. The tensor parallel size is the number of GPUs you want to use. For example, if you have 4 GPUs in a single node, you can set the tensor parallel size to 4.</p></li>
<li><p>Multi-Node Multi-GPU (tensor parallel plus pipeline parallel inference): If your model is too large to fit in a single node, you can use tensor parallel together with pipeline parallelism. The tensor parallel size is the number of GPUs you want to use in each node, and the pipeline parallel size is the number of nodes you want to use. For example, if you have 16 GPUs in 2 nodes (8GPUs per node), you can set the tensor parallel size to 8 and the pipeline parallel size to 2.</p></li>
</ul>
</section>
<section id="details-for-distributed-inference-and-serving">
<h4>Details for Distributed Inference and Serving<a class="headerlink" href="#details-for-distributed-inference-and-serving" title="此标题的永久链接">¶</a></h4>
<p>run API server on 4 GPUs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="n">facebook</span><span class="o">/</span><span class="n">opt</span><span class="o">-</span><span class="mi">13</span><span class="n">b</span> <span class="o">--</span><span class="n">tensor</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">4</span>
</pre></div>
</div>
<p>run API server on 8 GPUs with pipeline parallelism and tensor parallelism:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="n">gpt2</span> \
    <span class="o">--</span><span class="n">tensor</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">4</span> \
    <span class="o">--</span><span class="n">pipeline</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
<section id="multi-node-inference-and-serving">
<h4>Multi-Node Inference and Serving<a class="headerlink" href="#multi-node-inference-and-serving" title="此标题的永久链接">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>It is important to make sure the execution environment is the same on all nodes, including the model path, the Python environment. The recommended way is to use docker images to ensure the same environment, and hide the heterogeneity of the host machines via mapping them into the same docker configuration.</p>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Administrative privileges are needed when to access GPU performance counters when running profiling and tracing tools.</p>
</div>
<p>Pick a node as the head node, and run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">run_cluster</span><span class="o">.</span><span class="n">sh</span> \
                  <span class="n">vllm</span><span class="o">/</span><span class="n">vllm</span><span class="o">-</span><span class="n">openai</span> \
                  <span class="o">&lt;</span><span class="n">ip_of_head_node</span><span class="o">&gt;</span> \
                  <span class="o">--</span><span class="n">head</span> \
                  <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">the</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="ow">in</span><span class="o">/</span><span class="n">this</span><span class="o">/</span><span class="n">node</span>
</pre></div>
</div>
<p>On the rest of the worker nodes, run the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bash</span> <span class="n">run_cluster</span><span class="o">.</span><span class="n">sh</span> \
                  <span class="n">vllm</span><span class="o">/</span><span class="n">vllm</span><span class="o">-</span><span class="n">openai</span> \
                  <span class="o">&lt;</span><span class="n">ip_of_head_node</span><span class="o">&gt;</span> \
                  <span class="o">--</span><span class="n">worker</span> \
                  <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">the</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="ow">in</span><span class="o">/</span><span class="n">this</span><span class="o">/</span><span class="n">node</span>
</pre></div>
</div>
<p>进入容器:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">docker</span> <span class="n">exec</span> <span class="o">-</span><span class="n">it</span> <span class="n">node</span> <span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">bash</span>
</pre></div>
</div>
<p>像在一个node上一样执行(set the tensor parallel size to the number of GPUs in each node, and the pipeline parallel size to the number of nodes):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">the</span><span class="o">/</span><span class="n">model</span><span class="o">/</span><span class="ow">in</span><span class="o">/</span><span class="n">the</span><span class="o">/</span><span class="n">container</span> \
    <span class="o">--</span><span class="n">tensor</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">8</span> \
    <span class="o">--</span><span class="n">pipeline</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">2</span>
</pre></div>
</div>
</section>
</section>
<section id="production-metrics">
<h3>Production Metrics<a class="headerlink" href="#production-metrics" title="此标题的永久链接">¶</a></h3>
<p>get the latest metrics from the server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ curl http://0.0.0.0:8000/metrics
vllm:iteration_tokens_total_sum{model_name=&quot;unsloth/Llama-3.2-1B-Instruct&quot;} 0.0
vllm:iteration_tokens_total_bucket{le=&quot;1.0&quot;,model_name=&quot;unsloth/Llama-3.2-1B-Instruct&quot;} 3.0
...
</pre></div>
</div>
</section>
</section>
<section id="models">
<h2>Models<a class="headerlink" href="#models" title="此标题的永久链接">¶</a></h2>
<section id="loading-a-model">
<h3>Loading a Model<a class="headerlink" href="#loading-a-model" title="此标题的永久链接">¶</a></h3>
<section id="huggingface-hub">
<h4>HuggingFace Hub<a class="headerlink" href="#huggingface-hub" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>default</p></li>
</ul>
<p>The easiest way to check if your model is really supported at runtime:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span>

<span class="c1"># For generative models (task=generate) only</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=...</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;generate&quot;</span><span class="p">)</span>  <span class="c1"># Name or path of your model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># For pooling models (task={embed,classify,reward,score}) only</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=...</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;embed&quot;</span><span class="p">)</span>  <span class="c1"># Name or path of your model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>If vLLM successfully returns text (for generative models) or hidden states (for pooling models), it indicates that your model is supported.</p></li>
</ul>
</section>
<section id="modelscope">
<h4>ModelScope<a class="headerlink" href="#modelscope" title="此标题的永久链接">¶</a></h4>
<p>环境变量:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ export VLLM_USE_MODELSCOPE=True
</pre></div>
</div>
<p>And use with trust_remote_code=True:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=...</span><span class="p">,</span> <span class="n">revision</span><span class="o">=...</span><span class="p">,</span> <span class="n">task</span><span class="o">=...</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># For generative models (task=generate) only</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># For pooling models (task={embed,classify,reward,score}) only</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="list-of-models">
<h4>List of Models<a class="headerlink" href="#list-of-models" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>当前支持的模型(文本模型、多模态模型)</p></li>
<li><p><a class="reference external" href="https://docs.vllm.ai/en/stable/models/supported_models.html">https://docs.vllm.ai/en/stable/models/supported_models.html</a></p></li>
</ul>
</section>
</section>
<section id="generative-models">
<h3>Generative Models<a class="headerlink" href="#generative-models" title="此标题的永久链接">¶</a></h3>
<section id="offline-inference">
<h4>Offline Inference<a class="headerlink" href="#offline-inference" title="此标题的永久链接">¶</a></h4>
<section id="llm-generate">
<h5>LLM.generate<a class="headerlink" href="#llm-generate" title="此标题的永久链接">¶</a></h5>
<p>基本:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>use greedy sampling by setting temperature=0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="llm-beam-search">
<h5>LLM.beam_search<a class="headerlink" href="#llm-beam-search" title="此标题的永久链接">¶</a></h5>
<p>search using 5 beams and output at most 50 tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">BeamSearchParams</span><span class="p">(</span><span class="n">beam_width</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="llm-chat">
<h5>LLM.chat<a class="headerlink" href="#llm-chat" title="此标题的永久链接">¶</a></h5>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>In general, only instruction-tuned models have a chat template. Base models may perform poorly as they are not trained to respond to the chat conversation.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">)</span>
<span class="n">conversation</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello! How can I assist you today?&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write an essay about the importance of higher education.&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">]</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">conversation</span><span class="p">)</span>

<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If the model doesn’t have a chat template or you want to specify another one, you can explicitly pass a chat template:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm.entrypoints.chat_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_chat_template</span>

<span class="c1"># You can find a list of existing chat templates under `examples/`</span>
<span class="n">custom_template</span> <span class="o">=</span> <span class="n">load_chat_template</span><span class="p">(</span><span class="n">chat_template</span><span class="o">=</span><span class="s2">&quot;&lt;path_to_template&gt;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loaded chat template:&quot;</span><span class="p">,</span> <span class="n">custom_template</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">conversation</span><span class="p">,</span> <span class="n">chat_template</span><span class="o">=</span><span class="n">custom_template</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="online-inference">
<h4>Online Inference<a class="headerlink" href="#online-inference" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
</section>
<section id="pooling-models">
<h3>Pooling Models<a class="headerlink" href="#pooling-models" title="此标题的永久链接">¶</a></h3>
<p>LLM.encode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2.5-Math-RM-72B&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">output</span><span class="p">,)</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">data</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Data: </span><span class="si">{</span><span class="n">data</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>LLM.embed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;intfloat/e5-mistral-7b-instruct&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;embed&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">output</span><span class="p">,)</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>

<span class="n">embeds</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">embedding</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embeddings: </span><span class="si">{</span><span class="n">embeds</span><span class="si">!r}</span><span class="s2"> (size=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeds</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>LLM.classify:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;jason9693/Qwen2.5-1.5B-apeach&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;classify&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">output</span><span class="p">,)</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">)</span>

<span class="n">probs</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">probs</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Class Probabilities: </span><span class="si">{</span><span class="n">probs</span><span class="si">!r}</span><span class="s2"> (size=</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>LLM.score:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;BAAI/bge-reranker-v2-m3&quot;</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;score&quot;</span><span class="p">)</span>
<span class="p">(</span><span class="n">output</span><span class="p">,)</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="s2">&quot;What is the capital of France?&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The capital of Brazil is Brasilia.&quot;</span><span class="p">)</span>

<span class="n">score</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">score</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Score: </span><span class="si">{</span><span class="n">score</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="adding-a-new-model">
<h3>Adding a New Model<a class="headerlink" href="#adding-a-new-model" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>指定如何自己适配一个新的模型</p></li>
<li><p><a class="reference external" href="https://docs.vllm.ai/en/stable/models/adding_model.html">https://docs.vllm.ai/en/stable/models/adding_model.html</a></p></li>
</ul>
</section>
<section id="enabling-multimodal-inputs">
<h3>Enabling Multimodal Inputs<a class="headerlink" href="#enabling-multimodal-inputs" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>vllm 默认不支持多模态，需要按本节完成配置</p></li>
<li><p><a class="reference external" href="https://docs.vllm.ai/en/stable/models/enabling_multimodal_inputs.html">https://docs.vllm.ai/en/stable/models/enabling_multimodal_inputs.html</a></p></li>
</ul>
</section>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="此标题的永久链接">¶</a></h2>
<section id="lora-adapters">
<h3>LoRA Adapters<a class="headerlink" href="#lora-adapters" title="此标题的永久链接">¶</a></h3>
<p>instantiate the base model and pass in the enable_lora=True flag:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">vllm.lora.request</span><span class="w"> </span><span class="kn">import</span> <span class="n">LoRARequest</span>

<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span> <span class="n">enable_lora</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>submit the prompts and call llm.generate with the lora_request parameter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="n">snapshot_download</span>
<span class="n">sql_lora_path</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;yard1/llama-2-7b-sql-lora-test&quot;</span><span class="p">)</span>

<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
    <span class="n">stop</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[/assistant]&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
     <span class="s2">&quot;[user] Write a SQL query to answer the question based on the table schema.</span><span class="se">\n\n</span><span class="s2"> context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)</span><span class="se">\n\n</span><span class="s2"> question: Name the ICAO for lilongwe international airport [/user] [assistant]&quot;</span><span class="p">,</span>
     <span class="s2">&quot;[user] Write a SQL query to answer the question based on the table schema.</span><span class="se">\n\n</span><span class="s2"> context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)</span><span class="se">\n\n</span><span class="s2"> question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># LoRARequest 参数</span>
<span class="c1">#   1. a human identifiable name</span>
<span class="c1">#   2. a globally unique ID</span>
<span class="c1">#   3. path to the LoRA adapter</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompts</span><span class="p">,</span>
    <span class="n">sampling_params</span><span class="p">,</span>
    <span class="n">lora_request</span><span class="o">=</span><span class="n">LoRARequest</span><span class="p">(</span><span class="s2">&quot;sql_adapter&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sql_lora_path</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="serving-lora-adapters">
<h4>Serving LoRA Adapters<a class="headerlink" href="#serving-lora-adapters" title="此标题的永久链接">¶</a></h4>
<p>specify each LoRA module when we kickoff the server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>vllm serve meta-llama/Llama-2-7b-hf \
    --enable-lora \
    --lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/
</pre></div>
</div>
<p>请求:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">completions</span> \
    <span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> \
    <span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;sql-lora&quot;</span><span class="p">,</span>
        <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;San Francisco is a&quot;</span><span class="p">,</span>
        <span class="s2">&quot;max_tokens&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mi">0</span>
    <span class="p">}</span><span class="s1">&#39; | jq</span>
</pre></div>
</div>
</section>
<section id="dynamically-serving-lora-adapters">
<h4>Dynamically serving LoRA Adapters<a class="headerlink" href="#dynamically-serving-lora-adapters" title="此标题的永久链接">¶</a></h4>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>Note: Enabling this feature in production environments is risky as user may participate model adapter management.</p>
</div>
<p>To enable dynamic LoRA loading and unloading:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">VLLM_ALLOW_RUNTIME_LORA_UPDATING</span><span class="o">=</span><span class="kc">True</span>
</pre></div>
</div>
<p>request to load a LoRA adapter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">load_lora_adapter</span> \
<span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> \
<span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{</span>
    <span class="s2">&quot;lora_name&quot;</span><span class="p">:</span> <span class="s2">&quot;sql_adapter&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lora_path&quot;</span><span class="p">:</span> <span class="s2">&quot;/path/to/sql-lora-adapter&quot;</span>
<span class="p">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
<p>request to unload a LoRA adapter:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">X</span> <span class="n">POST</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">unload_lora_adapter</span> \
<span class="o">-</span><span class="n">H</span> <span class="s2">&quot;Content-Type: application/json&quot;</span> \
<span class="o">-</span><span class="n">d</span> <span class="s1">&#39;{</span>
    <span class="s2">&quot;lora_name&quot;</span><span class="p">:</span> <span class="s2">&quot;sql_adapter&quot;</span>
<span class="p">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
</section>
<section id="new-format-for-lora-modules">
<h4>New format for –lora-modules<a class="headerlink" href="#new-format-for-lora-modules" title="此标题的永久链接">¶</a></h4>
<p>provide LoRA modules via key-value pair:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>--lora-modules sql-lora=$HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/
</pre></div>
</div>
<p>specify a <code class="docutils literal notranslate"><span class="pre">base_model_name</span></code> alongside the name and path using JSON format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">lora</span><span class="o">-</span><span class="n">modules</span> <span class="s1">&#39;{&quot;name&quot;: &quot;sql-lora&quot;, &quot;path&quot;: &quot;/path/to/lora&quot;, &quot;base_model_name&quot;: &quot;meta-llama/Llama-2-7b&quot;}&#39;</span>
</pre></div>
</div>
</section>
<section id="lora-model-lineage-in-model-card">
<h4>Lora model lineage in model card<a class="headerlink" href="#lora-model-lineage-in-model-card" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">8000</span><span class="o">/</span><span class="n">v1</span><span class="o">/</span><span class="n">models</span>
<span class="p">{</span>
    <span class="s2">&quot;object&quot;</span><span class="p">:</span> <span class="s2">&quot;list&quot;</span><span class="p">,</span>
    <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span>
        <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="p">,</span>
        <span class="s2">&quot;object&quot;</span><span class="p">:</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;created&quot;</span><span class="p">:</span> <span class="mi">1715644056</span><span class="p">,</span>
        <span class="s2">&quot;owned_by&quot;</span><span class="p">:</span> <span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;root&quot;</span><span class="p">:</span> <span class="s2">&quot;~/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-hf/snapshots/01c7f73d771dfac7d292323805ebc428287df4f9/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parent&quot;</span><span class="p">:</span> <span class="n">null</span><span class="p">,</span>
        <span class="s2">&quot;permission&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
            <span class="o">.....</span>
            <span class="p">}</span>
        <span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
        <span class="s2">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;sql-lora&quot;</span><span class="p">,</span>
        <span class="s2">&quot;object&quot;</span><span class="p">:</span> <span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;created&quot;</span><span class="p">:</span> <span class="mi">1715644056</span><span class="p">,</span>
        <span class="s2">&quot;owned_by&quot;</span><span class="p">:</span> <span class="s2">&quot;vllm&quot;</span><span class="p">,</span>
        <span class="s2">&quot;root&quot;</span><span class="p">:</span> <span class="s2">&quot;~/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parent&quot;</span><span class="p">:</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mi">2</span><span class="o">-</span><span class="mi">7</span><span class="n">b</span><span class="o">-</span><span class="n">hf</span><span class="p">,</span>
        <span class="s2">&quot;permission&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
            <span class="o">....</span>
            <span class="p">}</span>
        <span class="p">]</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="multimodal-inputs">
<h3>Multimodal Inputs<a class="headerlink" href="#multimodal-inputs" title="此标题的永久链接">¶</a></h3>
<section id="image">
<h4>Image<a class="headerlink" href="#image" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llava-hf/llava-1.5-7b-hf&quot;</span><span class="p">)</span>

<span class="c1"># Refer to the HuggingFace repo for the correct format to use</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;USER: &lt;image&gt;</span><span class="se">\n</span><span class="s2">What is the content of this image?</span><span class="se">\n</span><span class="s2">ASSISTANT:&quot;</span>

<span class="c1"># Load the image using PIL.Image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># Single prompt inference</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">({</span>
    <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
    <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image</span><span class="p">},</span>
<span class="p">})</span>

<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>

<span class="c1"># Batch inference</span>
<span class="n">image_1</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">image_2</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;USER: &lt;image&gt;</span><span class="se">\n</span><span class="s2">What is the content of this image?</span><span class="se">\n</span><span class="s2">ASSISTANT:&quot;</span><span class="p">,</span>
            <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image_1</span><span class="p">},</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;USER: &lt;image&gt;</span><span class="se">\n</span><span class="s2">What&#39;s the color of this image?</span><span class="se">\n</span><span class="s2">ASSISTANT:&quot;</span><span class="p">,</span>
            <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="n">image_2</span><span class="p">},</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
<p>multiple images:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;microsoft/Phi-3.5-vision-instruct&quot;</span><span class="p">,</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># Required to load Phi-3.5-vision</span>
    <span class="n">max_model_len</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span>  <span class="c1"># Otherwise, it may not fit in smaller GPUs</span>
    <span class="n">limit_mm_per_prompt</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>  <span class="c1"># The maximum number to accept</span>
<span class="p">)</span>

<span class="c1"># Refer to the HuggingFace repo for the correct format to use</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;|user|&gt;</span><span class="se">\n</span><span class="s2">&lt;|image_1|&gt;</span><span class="se">\n</span><span class="s2">&lt;|image_2|&gt;</span><span class="se">\n</span><span class="s2">What is the content of each image?&lt;|end|&gt;</span><span class="se">\n</span><span class="s2">&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s2">&quot;</span>

<span class="c1"># Load the images using PIL.Image</span>
<span class="n">image1</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">image2</span> <span class="o">=</span> <span class="n">PIL</span><span class="o">.</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">({</span>
    <span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
    <span class="s2">&quot;multi_modal_data&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">image1</span><span class="p">,</span> <span class="n">image2</span><span class="p">]</span>
    <span class="p">},</span>
<span class="p">})</span>

<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
<p>Multi-image input can be extended to perform video captioning:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Specify the maximum number of frames per video to be 4. This can be changed.</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2-VL-2B-Instruct&quot;</span><span class="p">,</span> <span class="n">limit_mm_per_prompt</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;image&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">})</span>

<span class="c1"># Create the request payload.</span>
<span class="n">video_frames</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># load your video making sure it only has the number of frames specified earlier.</span>
<span class="n">message</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
    <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;Describe this set of frames. Consider the frames to be a part of the same video.&quot;</span><span class="p">},</span>
    <span class="p">],</span>
<span class="p">}</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">video_frames</span><span class="p">)):</span>
    <span class="n">base64_image</span> <span class="o">=</span> <span class="n">encode_image</span><span class="p">(</span><span class="n">video_frames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="c1"># base64 encoding.</span>
    <span class="n">new_image</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;image_url&quot;</span><span class="p">,</span> <span class="s2">&quot;image_url&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data:image/jpeg;base64,</span><span class="si">{</span><span class="n">base64_image</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}}</span>
    <span class="n">message</span><span class="p">[</span><span class="s2">&quot;content&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_image</span><span class="p">)</span>

<span class="c1"># Perform inference and log output.</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">chat</span><span class="p">([</span><span class="n">message</span><span class="p">])</span>

<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">o</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">generated_text</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="video">
<h4>Video<a class="headerlink" href="#video" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>examples/offline_inference_vision_language.py</p></li>
</ul>
</section>
<section id="audio">
<h4>Audio<a class="headerlink" href="#audio" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>examples/offline_inference_audio_language.py</p></li>
</ul>
</section>
</section>
<section id="tool-calling">
<h3>Tool Calling<a class="headerlink" href="#tool-calling" title="此标题的永久链接">¶</a></h3>
<section id="id3">
<h4>Quickstart<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h4>
<p>use the llama3 tool calling chat template:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vllm</span> <span class="n">serve</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">--</span><span class="n">enable</span><span class="o">-</span><span class="n">auto</span><span class="o">-</span><span class="n">tool</span><span class="o">-</span><span class="n">choice</span> \
    <span class="o">--</span><span class="n">tool</span><span class="o">-</span><span class="n">call</span><span class="o">-</span><span class="n">parser</span> <span class="n">llama3_json</span> \
    <span class="o">--</span><span class="n">chat</span><span class="o">-</span><span class="n">template</span> <span class="n">examples</span><span class="o">/</span><span class="n">tool_chat_template_llama3_json</span><span class="o">.</span><span class="n">jinja</span>
</pre></div>
</div>
<p>request to the model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;dummy&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_weather</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">unit</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Getting the weather for </span><span class="si">{</span><span class="n">location</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">...&quot;</span>
<span class="n">tool_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;get_weather&quot;</span><span class="p">:</span> <span class="n">get_weather</span><span class="p">}</span>

<span class="n">tools</span> <span class="o">=</span> <span class="p">[{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;function&quot;</span><span class="p">,</span>
    <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Get the current weather in a given location&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
            <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;City and state, e.g., &#39;San Francisco, CA&#39;&quot;</span><span class="p">},</span>
                <span class="s2">&quot;unit&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;enum&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;celsius&quot;</span><span class="p">,</span> <span class="s2">&quot;fahrenheit&quot;</span><span class="p">]}</span>
            <span class="p">},</span>
            <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">,</span> <span class="s2">&quot;unit&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather like in San Francisco?&quot;</span><span class="p">}],</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>

<span class="n">tool_call</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">function</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Function called: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Arguments: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">arguments</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result: </span><span class="si">{</span><span class="n">get_weather</span><span class="p">(</span><span class="o">**</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tool_call</span><span class="o">.</span><span class="n">arguments</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>输出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Function</span> <span class="n">called</span><span class="p">:</span> <span class="n">get_weather</span>
<span class="n">Arguments</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="s2">&quot;San Francisco, CA&quot;</span><span class="p">,</span> <span class="s2">&quot;unit&quot;</span><span class="p">:</span> <span class="s2">&quot;fahrenheit&quot;</span><span class="p">}</span>
<span class="n">Result</span><span class="p">:</span> <span class="n">Getting</span> <span class="n">the</span> <span class="n">weather</span> <span class="k">for</span> <span class="n">San</span> <span class="n">Francisco</span><span class="p">,</span> <span class="n">CA</span> <span class="ow">in</span> <span class="n">fahrenheit</span><span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="structured-outputs">
<h3>Structured Outputs<a class="headerlink" href="#structured-outputs" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>vLLM 支持使用 outline 或 lm-format-enforcer 作为引导式解码的后端来生成结构化输出</p></li>
</ul>
</section>
<section id="speculative-decoding">
<h3>Speculative decoding<a class="headerlink" href="#speculative-decoding" title="此标题的永久链接">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>请注意，vLLM 中的推测解码尚未优化，通常不会减少所有提示数据集或采样参数的令牌间延迟。优化工作参见 <a class="reference external" href="https://github.com/vllm-project/vllm/issues/4630#">这儿</a></p>
</div>
</section>
<section id="performance-and-tuning">
<h3>Performance and Tuning<a class="headerlink" href="#performance-and-tuning" title="此标题的永久链接">¶</a></h3>
<ul>
<li><p>KV 缓存空间不足以处理所有批处理请求。vLLM 可以抢占请求，为其他请求释放 KV 缓存空间。</p></li>
<li><p>当足够的 KV 缓存空间再次可用时，将重新计算被抢占的请求。发生这种情况时，将打印以下警告:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">WARNING</span> <span class="mi">05</span><span class="o">-</span><span class="mi">09</span> <span class="mi">00</span><span class="p">:</span><span class="mi">49</span><span class="p">:</span><span class="mi">33</span> <span class="n">scheduler</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">1057</span> <span class="n">Sequence</span> <span class="n">group</span> <span class="mi">0</span> <span class="ow">is</span> <span class="n">preempted</span> <span class="n">by</span> <span class="n">PreemptionMode</span><span class="o">.</span><span class="n">SWAP</span> <span class="n">mode</span> <span class="n">because</span> <span class="n">there</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">enough</span> <span class="n">KV</span> <span class="n">cache</span> <span class="n">space</span><span class="o">.</span>
<span class="n">This</span> <span class="n">can</span> <span class="n">affect</span> <span class="n">the</span> <span class="n">end</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">end</span> <span class="n">performance</span><span class="o">.</span>
<span class="n">Increase</span> <span class="n">gpu_memory_utilization</span> <span class="ow">or</span> <span class="n">tensor_parallel_size</span> <span class="n">to</span> <span class="n">provide</span> <span class="n">more</span> <span class="n">KV</span> <span class="n">cache</span> <span class="n">memory</span><span class="o">.</span> <span class="n">total_cumulative_preemption_cnt</span><span class="o">=</span><span class="mi">1</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="environment-variables">
<h3>Environment Variables<a class="headerlink" href="#environment-variables" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>vLLM 使用的所有环境变量都以 <code class="docutils literal notranslate"><span class="pre">VLLM_</span></code> 为前缀</p></li>
</ul>
</section>
<section id="usage-stats-collection">
<h3>Usage Stats Collection<a class="headerlink" href="#usage-stats-collection" title="此标题的永久链接">¶</a></h3>
<ul>
<li><p>预览收集的数据:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tail</span> <span class="o">~/.</span><span class="n">config</span><span class="o">/</span><span class="n">vllm</span><span class="o">/</span><span class="n">usage_stats</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</li>
</ul>
<p>可以通过设置 <code class="docutils literal notranslate"><span class="pre">VLLM_NO_USAGE_STATS</span></code> 或 <code class="docutils literal notranslate"><span class="pre">DO_NOT_TRACK</span></code> 环境变量，或者通过创建 <code class="docutils literal notranslate"><span class="pre">~/.config/vllm/do_not_track</span></code> 文件来选择退出使用情况统计信息收集:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Any of the following methods can disable usage stats collection</span>
<span class="n">export</span> <span class="n">VLLM_NO_USAGE_STATS</span><span class="o">=</span><span class="mi">1</span>
<span class="n">export</span> <span class="n">DO_NOT_TRACK</span><span class="o">=</span><span class="mi">1</span>
<span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">~/.</span><span class="n">config</span><span class="o">/</span><span class="n">vllm</span> <span class="o">&amp;&amp;</span> <span class="n">touch</span> <span class="o">~/.</span><span class="n">config</span><span class="o">/</span><span class="n">vllm</span><span class="o">/</span><span class="n">do_not_track</span>
</pre></div>
</div>
</section>
</section>
<section id="design">
<h2>Design<a class="headerlink" href="#design" title="此标题的永久链接">¶</a></h2>
<section id="architecture-overview">
<h3>Architecture Overview<a class="headerlink" href="#architecture-overview" title="此标题的永久链接">¶</a></h3>
<section id="entrypoints">
<h4>Entrypoints<a class="headerlink" href="#entrypoints" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id6">
<img alt="https://img.zhaoweiguo.com/uPic/2025/01/skO5cr.png" src="https://img.zhaoweiguo.com/uPic/2025/01/skO5cr.png" />
<figcaption>
<p><span class="caption-text">entrypoints for interacting with the system.</span><a class="headerlink" href="#id6" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="llm-class">
<h5>LLM Class<a class="headerlink" href="#llm-class" title="此标题的永久链接">¶</a></h5>
<p>the primary Python interface for doing offline inference</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">vllm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="c1"># Define a list of input prompts</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The capital of France is&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The largest ocean is&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="c1"># Define sampling parameters</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Initialize the LLM engine with the OPT-125M model</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/opt-125m&quot;</span><span class="p">)</span>

<span class="c1"># Generate outputs for the input prompts</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>

<span class="c1"># Print the generated outputs</span>
<span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">prompt</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">!r}</span><span class="s2">, Generated text: </span><span class="si">{</span><span class="n">generated_text</span><span class="si">!r}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="openai-compatible-api-server">
<h5>OpenAI-compatible API server<a class="headerlink" href="#openai-compatible-api-server" title="此标题的永久链接">¶</a></h5>
<p>The second primary interface to vLLM is via its OpenAI-compatible API server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. started using the vllm serve command.</span>
<span class="n">vllm</span> <span class="n">serve</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span>

<span class="c1"># 2. used directly</span>
<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">vllm</span><span class="o">.</span><span class="n">entrypoints</span><span class="o">.</span><span class="n">openai</span><span class="o">.</span><span class="n">api_server</span> <span class="o">--</span><span class="n">model</span> <span class="o">&lt;</span><span class="n">model</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
</section>
<section id="llm-engine">
<h4>LLM Engine<a class="headerlink" href="#llm-engine" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id7">
<img alt="https://img.zhaoweiguo.com/uPic/2025/01/NiYtIY.png" src="https://img.zhaoweiguo.com/uPic/2025/01/NiYtIY.png" />
<figcaption>
<p><span class="caption-text">The <code class="docutils literal notranslate"><span class="pre">LLMEngine</span></code> and <code class="docutils literal notranslate"><span class="pre">AsyncLLMEngine</span></code> classes are central to the functioning of the vLLM system, handling model inference and asynchronous request processing.</span><a class="headerlink" href="#id7" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="llmengine">
<h5>LLMEngine<a class="headerlink" href="#llmengine" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>The LLMEngine class is the core component of the vLLM engine.</p></li>
<li><p>It is responsible for receiving requests from clients and generating outputs from the model.</p></li>
<li><p>The LLMEngine includes input processing, model execution (possibly distributed across multiple hosts and/or GPUs), scheduling, and output processing</p></li>
<li><dl class="simple">
<dt>主要功能</dt><dd><ul>
<li><p>Input Processing: Handles tokenization of input text using the specified tokenizer.</p></li>
<li><p>Scheduling: Chooses which requests are processed in each step.</p></li>
<li><p>Model Execution: Manages the execution of the language model, including distributed execution across multiple GPUs.</p></li>
<li><p>Output Processing: Processes the outputs generated by the model, decoding the token IDs from a language model into human-readable text.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="asyncllmengine">
<h5>AsyncLLMEngine<a class="headerlink" href="#asyncllmengine" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>The AsyncLLMEngine class is an asynchronous wrapper for the LLMEngine class.</p></li>
<li><p>It uses asyncio to create a background loop that continuously processes incoming requests.</p></li>
<li><p>The AsyncLLMEngine is designed for online serving, where it can handle multiple concurrent requests and stream outputs to clients.</p></li>
</ul>
</section>
</section>
<section id="worker">
<h4>Worker<a class="headerlink" href="#worker" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>A worker is a process that runs the model inference.</p></li>
<li><p>vLLM follows the common practice of using one process to control one accelerator device, such as GPUs.</p></li>
<li><p>For example, if we use tensor parallelism of size 2 and pipeline parallelism of size 2, we will have 4 workers in total.</p></li>
<li><p>Workers are identified by their <code class="docutils literal notranslate"><span class="pre">rank</span></code> and <code class="docutils literal notranslate"><span class="pre">local_rank</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rank</span></code> is used for global orchestration, while <code class="docutils literal notranslate"><span class="pre">local_rank</span></code> is mainly used for assigning the accelerator device and accessing local resources such as the file system and shared memory.</p></li>
</ul>
</section>
<section id="model-runner">
<h4>Model Runner<a class="headerlink" href="#model-runner" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Every worker has one model runner object, responsible for loading and running the model.</p></li>
<li><p>Much of the model execution logic resides here, such as preparing input tensors and capturing cudagraphs.</p></li>
</ul>
</section>
<section id="model">
<h4>Model<a class="headerlink" href="#model" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Every model runner object has one model object, which is the actual <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> instance.</p></li>
</ul>
</section>
<section id="class-hierarchy">
<h4>Class Hierarchy<a class="headerlink" href="#class-hierarchy" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id8">
<img alt="https://img.zhaoweiguo.com/uPic/2025/01/3wmnPt.png" src="https://img.zhaoweiguo.com/uPic/2025/01/3wmnPt.png" />
<figcaption>
<p><span class="caption-text">class hierarchy of vLLM</span><a class="headerlink" href="#id8" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="input-processing">
<h3>Input Processing<a class="headerlink" href="#input-processing" title="此标题的永久链接">¶</a></h3>
<section id="input-processing-pipeline">
<h4>Input Processing Pipeline<a class="headerlink" href="#input-processing-pipeline" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Input data is passed to <code class="docutils literal notranslate"><span class="pre">LLMEngine</span></code> (or <code class="docutils literal notranslate"><span class="pre">AsyncLLMEngine</span></code> ).</p></li>
<li><p>Tokenize the data if necessary.</p></li>
<li><dl class="simple">
<dt>Process the inputs using <code class="docutils literal notranslate"><span class="pre">INPUT_REGISTRY.process_input</span></code></dt><dd><ul>
<li><p>For example, add placeholder tokens to reserve KV cache for multi-modal embeddings.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Send the processed inputs to <code class="docutils literal notranslate"><span class="pre">ExecutorBase</span></code></p></li>
<li><p>Distribute the inputs via <code class="docutils literal notranslate"><span class="pre">WorkerBase</span></code> to <code class="docutils literal notranslate"><span class="pre">ModelRunnerBase</span></code></p></li>
<li><dl class="simple">
<dt>If the data contains multi-modal data, convert it into keyword arguments using MULTIMODAL_REGISTRY.map_input.</dt><dd><ul>
<li><p>For example, convert a PIL.Image.Image input to its pixel values for a vision model.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
</section>
<section id="vllm-paged-attention">
<h3>vLLM Paged Attention<a class="headerlink" href="#vllm-paged-attention" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>refer to vLLM paged attention block as “block”, while refer to GPU thread block as “thread block”</p></li>
</ul>
<section id="inputs">
<h4>Inputs<a class="headerlink" href="#inputs" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">template</span><span class="o">&lt;</span>
<span class="n">typename</span> <span class="n">scalar_t</span><span class="p">,</span>
<span class="nb">int</span> <span class="n">HEAD_SIZE</span><span class="p">,</span>
<span class="nb">int</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
<span class="nb">int</span> <span class="n">NUM_THREADS</span><span class="p">,</span>
<span class="nb">int</span> <span class="n">PARTITION_SIZE</span> <span class="o">=</span> <span class="mi">0</span><span class="o">&gt;</span>
<span class="n">__device__</span> <span class="n">void</span> <span class="n">paged_attention_kernel</span><span class="p">(</span>
<span class="o">...</span> <span class="o">//</span> <span class="n">Other</span> <span class="n">side</span> <span class="n">args</span><span class="o">.</span>
<span class="n">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">out</span><span class="p">,</span>       <span class="o">//</span> <span class="p">[</span><span class="n">num_seqs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">max_num_partitions</span><span class="p">,</span> <span class="n">head_size</span><span class="p">]</span>
<span class="n">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">q</span><span class="p">,</span>         <span class="o">//</span> <span class="p">[</span><span class="n">num_seqs</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">]</span>
<span class="n">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">k_cache</span><span class="p">,</span>   <span class="o">//</span> <span class="p">[</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="o">/</span><span class="n">x</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span>
<span class="n">const</span> <span class="n">scalar_t</span><span class="o">*</span> <span class="n">__restrict__</span> <span class="n">v_cache</span><span class="p">,</span>   <span class="o">//</span> <span class="p">[</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">num_kv_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">]</span>
<span class="o">...</span> <span class="o">//</span> <span class="n">Other</span> <span class="n">side</span> <span class="n">args</span><span class="o">.</span>
<span class="p">)</span>
</pre></div>
</div>
<p>三个最重要的参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">q</span><span class="p">:</span>          <span class="n">全局内存上需要的查询</span>
<span class="n">k_cache</span><span class="p">:</span>    <span class="n">键</span>
<span class="n">v_cache</span><span class="p">:</span>    <span class="n">值</span>
</pre></div>
</div>
<p>在编译期间确定的模板参数列表:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>scalar_t    表示查询、键和值数据元素的数据类型，例如 FP16
HEAD_SIZE   表示每个头中的元素数量
BLOCK_SIZE  指的是每个区块中的token数量
NUM_THREADS 表示每个线程块中的线程数
PARTITION_SIZE 表示张量并行 GPU 的数量（为简单起见，我们假设这是 0 并且张量并行被禁用）
</pre></div>
</div>
</section>
<section id="concepts">
<h4>Concepts<a class="headerlink" href="#concepts" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>Sequence:</dt><dd><ul>
<li><p>A sequence represents a client request.</p></li>
<li><p>q 指向的数据的形状为 [num_seqs, num_heads, head_size] 。</p></li>
<li><p>这表示 q 指向了总共 num_seqs 条查询序列数据。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Context:</dt><dd><ul>
<li><p>The context consists of the generated tokens from the sequence.</p></li>
<li><p>例如， [“What”, “is”, “your”] 是 context token，输入查询token是 “name” 。该模型可能会生成token “?”</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Vec:</dt><dd><ul>
<li><p>The vec is a list of elements that are fetched and calculated together.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Thread group:</dt><dd><ul>
<li><p>The thread group is a small group of threads(THREAD_GROUP_SIZE) that fetches and calculates one query token and one key token at a time.</p></li>
<li><dl class="simple">
<dt>示例：</dt><dd><ul>
<li><p>如果线程组包含 2 个线程，头大小（head_size）为 8：</p></li>
<li><p>线程 0 处理索引 0, 2, 4, 6 的查询和键数据。</p></li>
<li><p>线程 1 处理索引 1, 3, 5, 7 的数据。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Block:</dt><dd><ul>
<li><p>The key and value cache data in vLLM are split into blocks.</p></li>
<li><dl class="simple">
<dt>示例：</dt><dd><ul>
<li><p>如果块大小为 16，头大小为 128：</p></li>
<li><p>一个块可以存储 16 × 128 = 2048 个元素。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Warp(线程束):</dt><dd><ul>
<li><p>A warp is a group of 32 threads(WARP_SIZE) that execute simultaneously on a stream multiprocessor (SM).</p></li>
<li><p>职责：每个线程束一次处理一个查询令牌与一个块的键令牌之间的计算。</p></li>
<li><dl class="simple">
<dt>示例：</dt><dd><ul>
<li><p>如果有 4 个线程束和 6 个块：</p></li>
<li><p>线程束 0 处理第 0 和第 4 块。</p></li>
<li><p>线程束 1 处理第 1 和第 5 块。</p></li>
<li><p>线程束 2 处理第 2 块。</p></li>
<li><p>线程束 3 处理第 3 块。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Thread block:</dt><dd><ul>
<li><p>A thread block is a group of threads(NUM_THREADS) that can access the same shared memory.</p></li>
<li><p>结构：每个线程块包含多个线程束（NUM_WARPS）。</p></li>
<li><p>职责：每个线程块处理一个查询令牌与整个上下文的键令牌之间的计算。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Grid:</dt><dd><ul>
<li><p>A grid is a collection of thread blocks and defines the shape of the collection.</p></li>
<li><p>In this kernel, the shape is (num_heads, num_seqs, max_num_partitions).</p></li>
<li><p>职责：每个线程块只处理一个头、一个序列和一个分区的计算。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="query">
<h4>Query<a class="headerlink" href="#query" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>线程组的职责：每个线程组（thread group）负责获取一个查询令牌的数据。</p></li>
<li><p>线程的职责：每个线程只处理查询令牌数据的一部分。</p></li>
<li><dl class="simple">
<dt>线程束内的数据复用：</dt><dd><ul>
<li><p>每个线程组会提取相同的查询令牌数据。</p></li>
<li><p>但不同线程组会将查询令牌数据与不同的键令牌数据相乘。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>q_ptr：指向查询数据的指针</p></li>
<li><p>q_vecs：存储在共享内存中的查询向量</p></li>
<li><dl class="simple">
<dt>总结</dt><dd><ul>
<li><p>如何通过指针 q_ptr 从全局内存中提取查询数据。</p></li>
<li><p>如何利用共享内存（q_vecs）存储和分配查询数据。</p></li>
<li><p>通过内存合并技术优化数据访问的性能。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="key">
<h4>Key<a class="headerlink" href="#key" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>与 Query 的区别：</dt><dd><ul>
<li><p>每个线程组在一次内核（kernel）运行中只处理一个查询令牌。</p></li>
<li><p>但对于键数据，每个线程组会在多次迭代中处理多个键令牌。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>k_ptr：指向键数据的指针</p></li>
<li><p>k_vecs：存储在寄存器中的键向量</p></li>
<li><dl class="simple">
<dt>总结</dt><dd><ul>
<li><p>这段话的核心是解释键数据在多线程中的读取和存储策略：</p></li>
<li><p>k_ptr 指向的动态性：在不同迭代中，指针会指向不同的键令牌数据。</p></li>
<li><p>寄存器存储优化：键数据存储在寄存器中，减少访问延迟。</p></li>
<li><p>内存合并优化：通过相邻线程的协同工作，最大化内存访问效率。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="qk">
<h4>QK<a class="headerlink" href="#qk" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>查询和键向量的点积计算流程。</p></li>
<li><p>如何通过跨线程组的归约得到完整的点积结果。</p></li>
<li><p>利用共享内存和寄存器存储优化性能，同时减少内存访问开销。</p></li>
</ul>
</section>
<section id="other">
<h4>Other<a class="headerlink" href="#other" title="此标题的永久链接">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>需要的时候再学吧</p>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../llama.cpp.html" class="btn btn-neutral float-right" title="7.5. llama.cpp框架" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="normal.html" class="btn btn-neutral" title="7.4.1. 常用" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>