

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   å¢åŠ å¯¹markdownä¸­å…¬å¼çš„æ”¯æŒ -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   å¢åŠ å¯¹markdownä¸­å…¬å¼çš„æ”¯æŒ -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7.3.3. Tokenizers &mdash; æ–°æºª-gordon V2025.07 æ–‡æ¡£</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="ç´¢å¼•" href="../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../search.html" />
    <link rel="next" title="7.3.4. PEFT" href="PEFT.html" />
    <link rel="prev" title="Transformers 4.45.2" href="Transformers/Transformers_V4.45.2.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- è¯„è®ºæ’ä»¶ gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- è¯„è®ºæ’ä»¶ gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> æ–°æºª-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../normal.html">1. å¸¸ç”¨</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../normals/normal.html">1.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/ml.html">1.3. æœºå™¨å­¦ä¹ machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/deep_learning.html">1.5. æ·±åº¦å­¦ä¹ </a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/normal.html">1.5.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/history.html">1.5.2. å†å²</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/algorithm.html">1.7. ç›¸å…³ç®—æ³•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/tool.html">1.8. å·¥å…·</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/question.html">1.9. å¸¸è§é—®é¢˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. æœºå™¨äººé¢†åŸŸ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../theory.html">2. ç†è®º</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../theories/tmp.html">2.1. ä¸´æ—¶</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/ReAct.html">2.1.1. ReActæ¡†æ¶</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Reflection.html">2.1.2. Reflectionåæ€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/math.html">2.1.3. æ•°å­¦</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/bag-of-words.html">2.1.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/word2vec.html">2.1.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/doc2vec.html">2.1.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/FastText.html">2.1.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/LDA.html">2.1.8. LDA-Latent Dirichlet Allocation(æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/overfitting-underfitting.html">2.1.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/RAG.html">2.1.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Agent.html">2.1.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/LLM.html">2.1.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/RL.html">2.1.13. RL-å¼ºåŒ–å­¦ä¹ </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/prompt_engineering.html">2.1.14. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/finetune.html">2.1.15. LLMè°ƒä¼˜(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../theories/tmps/Workflow.html">2.1.16. Workflow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">3. å¤§æ¨¡å‹</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/normal.html">3.1. å¸¸ç”¨</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/normal.html">3.1.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/package.html">3.1.2. ä¾èµ–å®‰è£…</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/encoder.html">3.1.3. ç¼–ç -è§£ç å™¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/usage.html">3.1.4. ä½¿ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/tmp.html">3.1.5. ä¸´æ—¶</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/model.html">3.2. è‘—åæ¨¡å‹</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-æ¨ç†æ¨¡å‹</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/finetune.html">3.3. è°ƒä¼˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. æ¨¡å‹é‡åŒ–(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Quantizations/normal.html">3.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Quantizations/GGUF.html">3.4.2. GGUF æ–‡ä»¶</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/fileformat.html">3.5. æ–‡ä»¶æ ¼å¼</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/normal.html">3.5.1. é€šç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/GGML.html">3.5.2. GGMLç³»åˆ—æ–‡ä»¶æ ¼å¼</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/normal.html">å¸¸ç”¨</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/openai.html">3.6. å•†ä¸šé¡¹ç›®</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/normal.html">3.6.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/prompt.html">3.7. Prompt æç¤ºè¯</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_chinese.html">3.7.1. ä¸­æ–‡</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/skill.html">3.7.3. ç¤ºä¾‹</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/Android.html">3.8. Androidç‰ˆLLMç›¸å…³</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/normal.html">3.8.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Androidç‰ˆéƒ¨ç½²</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../RAG.html">4. RAGç›¸å…³</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/normal.html">5.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/preprocess.html">5.2. é¢„å¤„ç†</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/normal.html">5.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. å…³é”®è¯æå–</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. åˆ†è¯</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. æƒ…æ„Ÿåˆ†æ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. æ–‡æœ¬è¡¨ç¤º</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. æ³¨æ„åŠ›æœºåˆ¶</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. è¯­è¨€æ¨¡å‹</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/NER.html">5.3. NER-å‘½åå®ä½“è¯†åˆ«</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/normal.html">5.3.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/seq-label.html">5.3.2. åºåˆ—æ ‡æ³¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/history.html">5.3.4. å†å²</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/summary.html">5.4. æ€»ç»“-æ‘˜è¦</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/summarys/normal.html">5.4.1. é€šç”¨</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">6. å‡½æ•°åº“</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/normal.html">6.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Image.html">6.2. Imageå›¾åƒå¤„ç†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Video.html">6.3. Videoè§†é¢‘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/normal.html">6.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/magic.html">6.4.2. é­”æ³•å‘½ä»¤ </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/display.html">6.4.3. displayå‡½æ•°</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/normal.html">6.6.1. é€šç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/Ndarray.html">6.6.2. Ndarray å¯¹è±¡</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/function.html">6.6.3. é€šç”¨å‡½æ•°</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/normal.html">6.7.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_subset.html">6.7.2. å®ä¾‹-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_analysis.html">6.7.3. å®ä¾‹-ç»Ÿè®¡åˆ†æ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_sql.html">6.7.4. åˆ©ç”¨pandaså®ç°SQLæ“ä½œ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_default_value.html">6.7.5. å®ä¾‹-ç¼ºå¤±å€¼çš„å¤„ç†</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_multi_index.html">6.7.6. å¤šå±‚ç´¢å¼•çš„ä½¿ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/practice.html">6.7.7. å®è·µ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Pandas/practices/practice_2012ObamaElect.html">å®è·µ-2012å¹´å¥¥å·´é©¬æ€»ç»Ÿè¿ä»»é€‰ä¸¾</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_input_output.html">6.7.8. API-è¾“å…¥è¾“å‡º</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/normal.html">6.8.1. åŸºæœ¬</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/install.html">6.8.2. å®‰è£…</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/example.html">6.8.5. å®ä¾‹</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/plot.html">æŠ˜çº¿å›¾plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/bar.html">æ¡å½¢å›¾bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/hist.html">ç›´æ–¹å›¾hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/scatter.html">æ•£ç‚¹å›¾scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/stackplot.html">é¢ç§¯å›¾stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/pie.html">é¥¼å›¾pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/box.html">ç®±å‹å›¾box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/multi.html">å¤šå›¾åˆå¹¶multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pylab.html">6.8.6. pylabå­åŒ…</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/SciPys/normal.html">6.9.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/normal.html">6.10.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/supervised.html">6.10.2. ç›‘ç£å­¦ä¹ </a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/sklearns/superviseds/glm.html">å¹¿ä¹‰çº¿æ€§æ¨¡å‹</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/unsupervised.html">6.10.3. æ— ç›‘ç£å­¦ä¹ </a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/normal.html">6.12.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/example.html">6.12.2. å®ä¾‹</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/struct.html">6.12.3. ä»£ç ç±»ç»“æ„</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Seaborns/normal.html">6.13.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/jieba.html">6.14. jiebaä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/gensim.html">6.15. gensim: æ–‡æœ¬ä¸»é¢˜å»ºæ¨¡å’Œç›¸ä¼¼æ€§åˆ†æ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/normal.html">6.15.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/LAC.html">6.16. LAC-ç™¾åº¦è¯æ³•åˆ†æå·¥å…·</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../framework.html">7. å­¦ä¹ æ¡†æ¶</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../normal.html">7.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/normal.html">7.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/nn.html">7.2.2. nnæ¨¡å—</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../huggingface.html">7.3. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="normal.html">7.3.1. å¸¸ç”¨</a><ul>
<li class="toctree-l4"><a class="reference internal" href="normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Transformers.html">7.3.2. Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Transformers/Transformers.html">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="Transformers/Transformers_V4.45.2.html">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="collect.html">7.3.7. æ”¶é›†</a><ul>
<li class="toctree-l4"><a class="reference internal" href="collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/model.html">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/blog_decoding-methods.html">åšæ–‡: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../vLLM.html">7.4. vLLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../vLLMs/normal.html">7.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../vLLMs/vLLM_doc.html">7.4.2. vLLMå®˜æ–¹æ–‡æ¡£</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../llama.cpp.html">7.5. llama.cppæ¡†æ¶</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../llama.cpps/normal.html">7.5.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../mxnet.html">7.7. mxnetåº“</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/ndarray.html">7.7.1. ndæ¨¡å—</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/gluon.html">7.7.2. gluonæ¨¡å—</a></li>
<li class="toctree-l3"><a class="reference internal" href="../mxnets/autograd.html">7.7.3. autogradæ¨¡å—</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Keras/normal.html">7.9.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Keras/demo.html">7.9.2. å®ä¾‹</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/binary_classification.html">äºŒåˆ†ç±»é—®é¢˜</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/multiclass_classification.html">å¤šåˆ†ç±»é—®é¢˜</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Keras/demos/regression.html">å›å½’é—®é¢˜</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../other.html">7.10. å…¶ä»–</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../website.html">8. å…³é”®ç½‘ç«™</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/ArXiv.html">8.3. ArXiv å­¦æœ¯è®ºæ–‡é¢„å°æœ¬å¹³å°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/video.html">8.4. è§†é¢‘ç›¸å…³</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../websites/normal.html">8.5. é€šç”¨</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../practice.html">9. å®è·µ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/OCRs/normal.html">9.1.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/normal.html">9.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../opensource.html">10. å¼€æºé¡¹ç›®</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/normal.html">10.3. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/ui.html">10.4. UIç•Œé¢</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/finetune.html">10.5. è°ƒä¼˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/search.html">10.6. æœç´¢</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Tool.html">10.8. æ¨¡å‹æ¨ç†å¹³å°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-inference-accelerate.html">10.9. LLMæ¨ç†åŠ é€Ÿ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Evaluate.html">10.10. LLMè¯„ä¼°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/platform.html">10.11. AIå¹³å°</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset.html">11. æ•°æ®é›†</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/normal.html">11.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese.html">11.2. ä¸­æ–‡æ•°æ®é›†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese_image.html">11.3. ä¸­æ–‡å›¾ç‰‡ç›¸å…³æ•°æ®é›†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/huggingface.html">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/website.html">11.5. æ•°æ®é›†ç›¸å…³ç½‘ç«™</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../model.html">12. å¸¸è§æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">13. å›¾å½¢&amp;è®¡ç®—åŠ é€ŸæŠ€æœ¯</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cudas/normal.html">13.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Evaluate.html">14. Evaluateè¯„æµ‹</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/normal.html">14.1. é€šç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E4%BC%A0%E7%BB%9FAI.html">15. ä¼ ç»ŸAI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">æ–°æºª-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../framework.html"><span class="section-number">7. </span>å­¦ä¹ æ¡†æ¶</a> &raquo;</li>
        
          <li><a href="../huggingface.html"><span class="section-number">7.3. </span>huggingface</a> &raquo;</li>
        
      <li><span class="section-number">7.3.3. </span>Tokenizers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/frameworks/huggingfaces/Tokenizers_V0.13.3.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">7.3.3. Tokenizers</a><ul>
<li><a class="reference internal" href="#getting-started">Getting started</a><ul>
<li><a class="reference internal" href="#quicktour">Quicktour</a><ul>
<li><a class="reference internal" href="#build-a-tokenizer-from-scratch">Build a tokenizer from scratch</a></li>
<li><a class="reference internal" href="#pretrained">Pretrained</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-tokenization-pipeline">The tokenization pipeline</a><ul>
<li><a class="reference internal" href="#normalization">Normalization</a></li>
<li><a class="reference internal" href="#pre-tokenization">Pre-Tokenization</a></li>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#post-processing">Post-Processing</a></li>
<li><a class="reference internal" href="#all-together-a-bert-tokenizer-from-scratch">All together: a BERT tokenizer from scratch</a></li>
<li><a class="reference internal" href="#decoding">Decoding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#components">Components</a><ul>
<li><a class="reference internal" href="#normalizers">Normalizers</a></li>
<li><a class="reference internal" href="#pre-tokenizers">Pre-tokenizers</a></li>
<li><a class="reference internal" href="#models">Models</a><ul>
<li><a class="reference internal" href="#wordlevel">WordLevel</a></li>
<li><a class="reference internal" href="#bpe">BPE</a></li>
<li><a class="reference internal" href="#wordpiece">WordPiece</a></li>
<li><a class="reference internal" href="#unigram">Unigram</a></li>
<li><a class="reference internal" href="#id2">æ€»ç»“å¯¹æ¯”</a></li>
</ul>
</li>
<li><a class="reference internal" href="#post-processors">Post-Processors</a></li>
<li><a class="reference internal" href="#decoders">Decoders</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api">API</a><ul>
<li><a class="reference internal" href="#id3">Normalizers</a></li>
<li><a class="reference internal" href="#trainers">Trainers</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">ä¸»é¡µ</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">æ¨¡å—ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">æœç´¢é¡µé¢</span></a></p></td>
</tr>
</tbody>
</table>
<section id="tokenizers">
<h1><span class="section-number">7.3.3. </span>Tokenizers<a class="headerlink" href="#tokenizers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h1>
<ul class="simple">
<li><p>From: <a class="reference external" href="https://huggingface.co/docs/tokenizers/index">https://huggingface.co/docs/tokenizers/index</a></p></li>
</ul>
<section id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="quicktour">
<h3>Quicktour<a class="headerlink" href="#quicktour" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="build-a-tokenizer-from-scratch">
<h4>Build a tokenizer from scratch<a class="headerlink" href="#build-a-tokenizer-from-scratch" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>Training the tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">BPE</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>

<span class="c1"># train our tokenizer on the wikitext files:</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BpeTrainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">])</span>

<span class="c1"># è‹±æ–‡ç­‰å¢åŠ å¯¹ç©ºæ ¼çš„åˆ†æ ¼(å¦åˆ™åƒit isè¿™ç§ä¼šå› ä¸ºå¸¸å‡ºç°è¢«è®¤è¯†æ˜¯ä¸€ä¸ªtoken)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>

<span class="c1"># train</span>
<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;data/wikitext-103-raw/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.raw&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;data/tokenizer-wiki.json&quot;</span><span class="p">)</span>

<span class="c1"># é‡æ–°åŠ è½½:</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="o">.</span><span class="n">from_file</span><span class="p">(</span><span class="s2">&quot;data/tokenizer-wiki.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Using the tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, y&#39;all! How are you ğŸ˜ ?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#39;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="c1"># [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">offsets</span><span class="p">[</span><span class="mi">9</span><span class="p">])</span>
<span class="c1"># (26, 27)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="s2">&quot;Hello, y&#39;all! How are you ğŸ˜ ?&quot;</span>
<span class="n">sentence</span><span class="p">[</span><span class="mi">26</span><span class="p">:</span><span class="mi">27</span><span class="p">]</span>
<span class="c1"># &quot;ğŸ˜&quot;</span>
</pre></div>
</div>
<p>Post-processing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>tokenizer.token_to_id(&quot;[SEP]&quot;)
# 2

from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single=&quot;[CLS] $A [SEP]&quot;,
    pair=&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;,
    special_tokens=[
        (&quot;[CLS]&quot;, tokenizer.token_to_id(&quot;[CLS]&quot;)),
        (&quot;[SEP]&quot;, tokenizer.token_to_id(&quot;[SEP]&quot;)),
    ],
)
# è¯´æ˜
1. æŒ‡å®šå•å¥å­çš„æ¨¡æ¿ï¼šæ ¼å¼æ˜¯ â€œ[CLS] $A [SEP]â€ï¼Œå…¶ä¸­ $A ä»£è¡¨æˆ‘ä»¬çš„å¥å­
2. æŒ‡å®šå¥å­å¯¹çš„æ¨¡æ¿ï¼šæ ¼å¼æ˜¯ â€œ[CLS] $A [SEP] $B [SEP]â€ï¼Œå…¶ä¸­ $A ä»£è¡¨ç¬¬ä¸€ä¸ªå¥å­ï¼Œ$B ä»£è¡¨ç¬¬äºŒä¸ªå¥å­

# å•å¥å­ç¤ºä¾‹
output = tokenizer.encode(&quot;Hello, y&#39;all! How are you ğŸ˜ ?&quot;)
print(output.tokens)
# [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#39;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]

# å¥å­å¯¹ç¤ºä¾‹
output = tokenizer.encode(&quot;Hello, y&#39;all!&quot;, &quot;How are you ğŸ˜ ?&quot;)
print(output.tokens)
# [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#39;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]
print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
</pre></div>
</div>
<p>Encoding multiple sentences in a batch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># process your texts by batches</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">([</span><span class="s2">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span> <span class="s2">&quot;How are you ğŸ˜ ?&quot;</span><span class="p">])</span>

<span class="c1"># batch of sentences pairs</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">(</span>
    <span class="p">[[</span><span class="s2">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span> <span class="s2">&quot;How are you ğŸ˜ ?&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Hello to you too!&quot;</span><span class="p">,</span> <span class="s2">&quot;I&#39;m fine, thank you!&quot;</span><span class="p">]]</span>
<span class="p">)</span>

<span class="c1"># automatically pad the outputs to the longest sentence</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">enable_padding</span><span class="p">(</span><span class="n">pad_id</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;[PAD]&quot;</span><span class="p">)</span> <span class="c1"># åˆ†è¯å™¨ä¸ºå¡«å……æ ‡è®° [PAD] é¢„è®¾çš„å”¯ä¸€æ•´æ•° IDã€‚å½“åˆ†è¯å™¨é‡åˆ° [PAD] æ—¶ï¼Œè‡ªåŠ¨ä¼šå°†å…¶è½¬æ¢ä¸º 3</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_batch</span><span class="p">([</span><span class="s2">&quot;Hello, y&#39;all!&quot;</span><span class="p">,</span> <span class="s2">&quot;How are you ğŸ˜ ?&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">attention_mask</span><span class="p">)</span>
<span class="c1"># [1, 1, 1, 1, 1, 1, 1, 0]</span>
</pre></div>
</div>
</section>
<section id="pretrained">
<h4>Pretrained<a class="headerlink" href="#pretrained" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>Using a pretrained tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Importing a pretrained tokenizer from legacy vocabulary files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertWordPieceTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertWordPieceTokenizer</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased-vocab.txt&quot;</span><span class="p">,</span> <span class="n">lowercase</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="the-tokenization-pipeline">
<h3>The tokenization pipeline<a class="headerlink" href="#the-tokenization-pipeline" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<p>When calling <code class="docutils literal notranslate"><span class="pre">Tokenizer.encode</span></code> or <code class="docutils literal notranslate"><span class="pre">Tokenizer.encode_batch</span></code>, the <code class="docutils literal notranslate"><span class="pre">input</span> <span class="pre">text(s)</span></code> go through the following pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">normalization</span>
<span class="n">pre</span><span class="o">-</span><span class="n">tokenization</span>
<span class="n">model</span>
<span class="n">post</span><span class="o">-</span><span class="n">processing</span>
</pre></div>
</div>
<section id="normalization">
<h4>Normalization<a class="headerlink" href="#normalization" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æ ‡å‡†åŒ–æ˜¯å¯¹åŸå§‹å­—ç¬¦ä¸²åº”ç”¨çš„ä¸€ç»„æ“ä½œï¼Œä»¥ä½¿å…¶ä¸é‚£ä¹ˆéšæœºæˆ–â€œæ›´å¹²å‡€â€ã€‚</p></li>
<li><p>å¸¸è§æ“ä½œåŒ…æ‹¬å»é™¤ç©ºæ ¼ã€åˆ é™¤é‡éŸ³å­—ç¬¦æˆ–å°å†™æ‰€æœ‰æ–‡æœ¬ã€‚</p></li>
<li><p>æ¯ä¸ªæ ‡å‡†åŒ–æ“ä½œåœ¨ ğŸ¤— Tokenizers åº“ä¸­éƒ½ç”±Normalizerè¡¨ç¤ºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨normalizers.Sequenceç»„åˆå…¶ä¸­çš„å¤šä¸ªæ“ä½œ</p></li>
</ul>
<p>åº”ç”¨ <strong>NFD Unicode æ ‡å‡†åŒ–</strong> å¹¶ <strong>åˆ é™¤é‡éŸ³ç¬¦å·</strong> çš„æ ‡å‡†åŒ–å™¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">normalizers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.normalizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">StripAccents</span>
<span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>
</pre></div>
</div>
<p>ä½¿ç”¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">normalizer</span><span class="o">.</span><span class="n">normalize_str</span><span class="p">(</span><span class="s2">&quot;HÃ©llÃ² hÃ´w are Ã¼?&quot;</span><span class="p">)</span>
<span class="c1"># &quot;Hello how are u?&quot;</span>
</pre></div>
</div>
<p>æ›´æ”¹ç›¸åº”çš„å±æ€§æ¥è‡ªå®šä¹‰å…¶è§„èŒƒåŒ–å™¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizer</span>
</pre></div>
</div>
</section>
<section id="pre-tokenization">
<h4>Pre-Tokenization<a class="headerlink" href="#pre-tokenization" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é¢„æ ‡è®°åŒ–æ˜¯å°†æ–‡æœ¬åˆ†å‰²æˆæ›´å°çš„å¯¹è±¡çš„è¡Œä¸ºï¼Œè¿™äº›å¯¹è±¡ä¸ºè®­ç»ƒç»“æŸæ—¶çš„æ ‡è®°ç»™å‡ºäº†ä¸Šé™(upper bound)ã€‚</p></li>
<li><p>A good way to think of this is that the pre-tokenizer will split your text into â€œwordsâ€ and then, your final tokens will be parts of those words.</p></li>
</ul>
<p>é¢„æ ‡è®°è¾“å…¥çš„ä¸€ç§ç®€å•æ–¹æ³•æ˜¯æ ¹æ®ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†å‰²:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>
<span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
<span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="s2">&quot;Hello! How are you? I&#39;m fine, thank you.&quot;</span><span class="p">)</span>
<span class="c1"># [(&quot;Hello&quot;, (0, 5)), (&quot;!&quot;, (5, 6)), (&quot;How&quot;, (7, 10)), (&quot;are&quot;, (11, 14)), (&quot;you&quot;, (15, 18)),</span>
<span class="c1">#  (&quot;?&quot;, (18, 19)), (&quot;I&quot;, (20, 21)), (&quot;&#39;&quot;, (21, 22)), (&#39;m&#39;, (22, 23)), (&quot;fine&quot;, (24, 28)),</span>
<span class="c1">#  (&quot;,&quot;, (28, 29)), (&quot;thank&quot;, (30, 35)), (&quot;you&quot;, (36, 39)), (&quot;.&quot;, (39, 40))]</span>
</pre></div>
</div>
<p>å¯ä»¥å°†ä»»ä½•PreTokenizerç»„åˆåœ¨ä¸€èµ·:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># å°†æŒ‰ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·å’Œæ•°å­—è¿›è¡Œåˆ†å‰²ï¼Œå°†æ•°å­—åˆ†éš”æˆå„ä¸ªæ•°å­—</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">pre_tokenizers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Digits</span>
<span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">Whitespace</span><span class="p">(),</span> <span class="n">Digits</span><span class="p">(</span><span class="n">individual_digits</span><span class="o">=</span><span class="kc">True</span><span class="p">)])</span>
<span class="n">pre_tokenizer</span><span class="o">.</span><span class="n">pre_tokenize_str</span><span class="p">(</span><span class="s2">&quot;Call 911!&quot;</span><span class="p">)</span>
<span class="c1"># [(&quot;Call&quot;, (0, 4)), (&quot;9&quot;, (5, 6)), (&quot;1&quot;, (6, 7)), (&quot;1&quot;, (7, 8)), (&quot;!&quot;, (8, 9))]</span>
</pre></div>
</div>
<p>æ›´æ”¹ç›¸åº”çš„å±æ€§æ¥è‡ªå®šä¹‰Tokenizerçš„é¢„åˆ†è¯å™¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">pre_tokenizer</span>
</pre></div>
</div>
</section>
<section id="model">
<h4>Model<a class="headerlink" href="#model" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>ä¸€æ—¦è¾“å…¥æ–‡æœ¬è¢«æ ‡å‡†åŒ–å’Œé¢„æ ‡è®°åŒ–ï¼Œ Tokenizerå°±ä¼šåœ¨é¢„æ ‡è®°ä¸Šåº”ç”¨æ¨¡å‹ã€‚</p></li>
<li><p>æ¨¡å‹çš„ä½œç”¨æ˜¯ä½¿ç”¨å®ƒå­¦åˆ°çš„è§„åˆ™å°†ä½ çš„â€œå•è¯â€åˆ†å‰²æˆæ ‡è®°ã€‚å®ƒè¿˜è´Ÿè´£å°†è¿™äº›æ ‡è®°æ˜ å°„åˆ°æ¨¡å‹è¯æ±‡è¡¨ä¸­ç›¸åº”çš„ IDã€‚</p></li>
<li><p>This model is passed along when intializing the Tokenizer</p></li>
</ul>
<p>Tokenizers åº“æ”¯æŒ:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span>
<span class="n">models</span><span class="o">.</span><span class="n">Unigram</span>
<span class="n">models</span><span class="o">.</span><span class="n">WordLevel</span>
<span class="n">models</span><span class="o">.</span><span class="n">WordPiece</span>
</pre></div>
</div>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tokenizeråˆä½¿åŒ–æ—¶å°±ä¼ é€’</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="post-processing">
<h4>Post-Processing<a class="headerlink" href="#post-processing" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>åå¤„ç†æ˜¯æœ€åä¸€æ­¥ï¼Œç”¨äºåœ¨è¿”å›Encodingä¹‹å‰å¯¹å…¶æ‰§è¡Œä»»ä½•å…¶ä»–è½¬æ¢ï¼Œä¾‹å¦‚æ·»åŠ æ½œåœ¨çš„ç‰¹æ®Šæ ‡è®°ã€‚</p></li>
<li><p>é€šè¿‡è®¾ç½®ç›¸åº”çš„å±æ€§æ¥è‡ªå®šä¹‰Tokenizerçš„åå¤„ç†å™¨</p></li>
</ul>
<p>ç¤ºä¾‹-é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿›è¡Œåå¤„ç†ä»¥ä½¿è¾“å…¥é€‚åˆ BERT æ¨¡å‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateProcessing</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>
    <span class="n">single</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span>
    <span class="n">pair</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>ä¸é¢„åˆ†è¯å™¨æˆ–æ ‡å‡†åŒ–å™¨ä¸åŒï¼Œæ— éœ€åœ¨æ›´æ”¹åå¤„ç†å™¨åé‡æ–°è®­ç»ƒåˆ†è¯å™¨</p></li>
</ul>
</section>
<section id="all-together-a-bert-tokenizer-from-scratch">
<h4>All together: a BERT tokenizer from scratch<a class="headerlink" href="#all-together-a-bert-tokenizer-from-scratch" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>BERT ä¾èµ–äº WordPieceï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨æ­¤æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªæ–°çš„Tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordPiece</span>
<span class="n">bert_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">WordPiece</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>BERT é€šè¿‡åˆ é™¤é‡éŸ³ç¬¦å·å’Œå°å†™å­—æ¯æ¥é¢„å¤„ç†æ–‡æœ¬:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">normalizers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.normalizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">NFD</span><span class="p">,</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">StripAccents</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">normalizers</span><span class="o">.</span><span class="n">Sequence</span><span class="p">([</span><span class="n">NFD</span><span class="p">(),</span> <span class="n">Lowercase</span><span class="p">(),</span> <span class="n">StripAccents</span><span class="p">()])</span>
</pre></div>
</div>
<p>æ ¹æ®ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·è¿›è¡Œåˆ†å‰²:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
</pre></div>
</div>
<p>uses the template:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.processors</span><span class="w"> </span><span class="kn">import</span> <span class="n">TemplateProcessing</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">post_processor</span> <span class="o">=</span> <span class="n">TemplateProcessing</span><span class="p">(</span>
    <span class="n">single</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP]&quot;</span><span class="p">,</span>
    <span class="n">pair</span><span class="o">=</span><span class="s2">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
    <span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>use this tokenizer and train on it on wikitext:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">WordPieceTrainer</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">WordPieceTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">])</span>
<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;data/wikitext-103-raw/wiki.</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">.raw&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;valid&quot;</span><span class="p">]]</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;data/bert-wiki.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="decoding">
<h4>Decoding<a class="headerlink" href="#decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul>
<li><p>é™¤äº†å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ä¹‹å¤–ï¼Œ Tokenizerè¿˜å…·æœ‰ç”¨äºè§£ç çš„ APIï¼Œå³å°†æ¨¡å‹ç”Ÿæˆçš„ ID è½¬æ¢å›æ–‡æœ¬:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Tokenizer</span><span class="o">.</span><span class="n">decode</span> <span class="p">(</span><span class="k">for</span> <span class="n">one</span> <span class="n">predicted</span> <span class="n">text</span><span class="p">)</span>
<span class="n">Tokenizer</span><span class="o">.</span><span class="n">decode_batch</span> <span class="p">(</span><span class="k">for</span> <span class="n">a</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>decoderé¦–å…ˆå°† ID è½¬æ¢å›æ ‡è®°ï¼ˆä½¿ç”¨æ ‡è®°å™¨çš„è¯æ±‡è¡¨ï¼‰å¹¶åˆ é™¤æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œç„¶åå°†è¿™äº›æ ‡è®°ä¸ç©ºæ ¼è¿æ¥èµ·æ¥:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Hello, y&#39;all! How are you ğŸ˜ ?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="c1"># [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">27253</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">5097</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7961</span><span class="p">,</span> <span class="mi">5112</span><span class="p">,</span> <span class="mi">6218</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="c1"># &quot;Hello , y &#39; all ! How are you ?&quot;</span>
</pre></div>
</div>
<p>å¦‚æœæ‚¨ä½¿ç”¨çš„æ¨¡å‹æ·»åŠ äº†ç‰¹æ®Šå­—ç¬¦æ¥è¡¨ç¤ºç»™å®šâ€œå•è¯â€çš„å­æ ‡è®°ï¼ˆä¾‹å¦‚ WordPiece ä¸­çš„â€##â€ ï¼‰:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;Welcome to the ğŸ¤— Tokenizers library.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">tokens</span><span class="p">)</span>
<span class="c1"># [&quot;[CLS]&quot;, &quot;welcome&quot;, &quot;to&quot;, &quot;the&quot;, &quot;[UNK]&quot;, &quot;tok&quot;, &quot;##eni&quot;, &quot;##zer&quot;, &quot;##s&quot;, &quot;library&quot;, &quot;.&quot;, &quot;[SEP]&quot;]</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="c1"># &quot;welcome to the tok ##eni ##zer ##s library .&quot;</span>
</pre></div>
</div>
<p>è¿™ç§æƒ…å†µéœ€è¦è‡ªå®šä¹‰decoderä»¥æ­£ç¡®å¤„ç†å®ƒä»¬:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">decoders</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoders</span><span class="o">.</span><span class="n">WordPiece</span><span class="p">()</span>
<span class="n">bert_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="c1"># &quot;welcome to the tokenizers library.&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="components">
<h3>Components<a class="headerlink" href="#components" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="normalizers">
<h4>Normalizers<a class="headerlink" href="#normalizers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>å¸¸ç”¨çš„Normalizeræ–¹æ³•:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>NFD
NFKD
NFC
NFKC
Lowercase       =&gt; Input: HELLO á½ˆÎ”Î¥Î£Î£Î•ÎÎ£  Output: helloá½€Î´Ï…ÏƒÏƒÎµÏÏ‚`
Strip           =&gt; Input: &quot;  hi  &quot; Output: &quot;hi&quot;
StripAccents    =&gt; Input: Ã© Ouput: e
Replace         =&gt; Replace(&quot;a&quot;, &quot;e&quot;) will behave like this: Input: &quot;banana&quot; Ouput: &quot;benene&quot;
BertNormalizer
    =&gt; Provides an implementation of the Normalizer used in the original BERT
        clean_text
        handle_chinese_chars
        strip_accents
        lowercase
Sequence
    =&gt; Composes multiple normalizers that will run in the provided order
        Sequence([NFKC(), Lowercase()])
</pre></div>
</div>
</section>
<section id="pre-tokenizers">
<h4>Pre-tokenizers<a class="headerlink" href="#pre-tokenizers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>The PreTokenizer takes care of splitting the input according to a set of rules.</p></li>
</ul>
<p>algorithms:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>ByteLevel
    Splits on whitespaces while remapping all the bytes to a set of visible characters.
    introduced by OpenAI with GPT-2
Whitespace          =&gt; Input: &quot;Hello there!&quot; Output: &quot;Hello&quot;, &quot;there!&quot;
    Splits on word boundaries (using the following regular expression: `\w+|[^\w\s]+`
WhitespaceSplit
    Splits on any whitespace character
Punctuation         =&gt; Input: &quot;Hello?&quot; Ouput: &quot;Hello&quot;, &quot;?&quot;
Metaspace           =&gt; Input: &quot;Hello there&quot; Ouput: &quot;Hello&quot;, &quot;â–there&quot;
    Splits on whitespaces and replaces them with a special char â€œâ–â€ (U+2581)
CharDelimiterSplit  =&gt; Example with x: Input: &quot;Helloxthere&quot; Ouput: &quot;Hello&quot;, &quot;there&quot;
Digits              =&gt; Input: &quot;Hello123there&quot; Output: &quot;Hello&quot;, &quot;123&quot;, &quot;there&quot;
        Splits the numbers from any other characters.
Split
    æœ‰ä¸‹é¢3ä¸ªå‚æ•°:
        1. `pattern` should be either a custom string or regexp.
        2. `behavior` should be one of:
            removed
            isolated
            merged_with_previous
            merged_with_next
            contiguous
        3. `invert` should be a boolean flag.
    ç¤ºä¾‹:
        pattern = , behavior = &quot;isolated&quot;, invert = False:
        Input: &quot;Hello, how are you?&quot;
        Output: &quot;Hello,&quot;, &quot; &quot;, &quot;how&quot;, &quot; &quot;, &quot;are&quot;, &quot; &quot;, &quot;you?&quot;
Sequence
    ç¤ºä¾‹:
        Sequence([Punctuation(), WhitespaceSplit()])
</pre></div>
</div>
</section>
<section id="models">
<h4>Models<a class="headerlink" href="#models" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>åˆ†è¯:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">models</span><span class="o">.</span><span class="n">BPE</span>          <span class="o">=&gt;</span> <span class="n">Byte</span><span class="o">-</span><span class="n">Pair</span><span class="o">-</span><span class="n">Encoding</span>
<span class="n">models</span><span class="o">.</span><span class="n">Unigram</span>      <span class="o">=&gt;</span>
<span class="n">models</span><span class="o">.</span><span class="n">WordLevel</span>    <span class="o">=&gt;</span>
<span class="n">models</span><span class="o">.</span><span class="n">WordPiece</span>    <span class="o">=&gt;</span>
</pre></div>
</div>
<section id="wordlevel">
<h5>WordLevel<a class="headerlink" href="#wordlevel" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç®€ä»‹ï¼šWordLevel æ˜¯æœ€åŸºç¡€çš„åˆ†è¯æ–¹æ³•ï¼Œä¹Ÿæ˜¯æˆ‘ä»¬é€šå¸¸æ‰€è¯´çš„â€œè¯çº§åˆ«åˆ†è¯â€ã€‚å®ƒå°†æ¯ä¸ªå®Œæ•´çš„è¯æ˜ å°„åˆ°ä¸€ä¸ªå”¯ä¸€çš„IDï¼Œè€Œä¸å¯¹è¯è¿›ä¸€æ­¥åˆ†è§£ã€‚</p></li>
<li><p>ä¼˜ç‚¹ï¼šç®€å•ç›´è§‚ï¼Œæ˜“äºç†è§£å’Œå®ç°ã€‚åªéœ€è¦ä¸€ä¸ªå•è¯-IDæ˜ å°„è¡¨ã€‚</p></li>
<li><p>ç¼ºç‚¹ï¼šéœ€è¦éå¸¸å¤§çš„è¯æ±‡è¡¨æ¥è¦†ç›–æ‰€æœ‰å¯èƒ½å‡ºç°çš„å•è¯ï¼Œå¯¼è‡´æ¨¡å‹ä½“ç§¯å¤§ã€‚è€Œä¸”åœ¨å¤„ç†æœªè§è¿‡çš„å•è¯ï¼ˆout-of-vocabulary, OOVï¼‰æ—¶å¾ˆå¯èƒ½ä¼šå‡ºç°â€œ[UNK]â€ï¼ˆæœªçŸ¥è¯ï¼‰ã€‚</p></li>
<li><p>é€‚ç”¨åœºæ™¯ï¼šé€‚ç”¨äºè¯æ±‡é‡è¾ƒå°çš„ä»»åŠ¡ï¼Œæˆ–å¯¹è¯æ±‡é‡æ²¡æœ‰ä¸¥æ ¼è¦æ±‚çš„ç®€å•åº”ç”¨ã€‚</p></li>
</ul>
</section>
<section id="bpe">
<h5>BPE<a class="headerlink" href="#bpe" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç®€ä»‹ï¼šBPE æ˜¯ä¸€ç§æµè¡Œçš„å­è¯åˆ†è¯ç®—æ³•ã€‚å®ƒä»å­—ç¬¦çº§åˆ«å¼€å§‹ï¼Œé€šè¿‡åˆå¹¶åœ¨è¯­æ–™ä¸­æœ€å¸¸å‡ºç°çš„å­—ç¬¦å¯¹é€æ­¥åˆ›å»ºæ–°çš„å­è¯ï¼ˆtokensï¼‰ã€‚è¿™ç§åˆå¹¶æ“ä½œæ˜¯è¿­ä»£è¿›è¡Œçš„ï¼Œä»¥æ„å»ºæ›´é•¿çš„å­è¯ã€‚</p></li>
<li><p>ä¼˜ç‚¹ï¼šBPE å¯ä»¥å¤„ç†æœªè§è¿‡çš„å•è¯ï¼Œå› ä¸ºå®ƒèƒ½å¤Ÿå°†ç”Ÿè¯åˆ†è§£ä¸ºå­è¯å¹¶è¿›è¡Œç»„åˆã€‚å› æ­¤å®ƒçš„è¯æ±‡è¡¨å¯ä»¥ç›¸å¯¹è¾ƒå°ã€‚</p></li>
<li><p>ç¼ºç‚¹ï¼šBPE çš„åˆ†è¯æ˜¯åŸºäºé¢‘ç‡ç»Ÿè®¡çš„ï¼Œå› æ­¤åˆ†è¯ç»“æœæ˜¯å›ºå®šçš„ï¼Œä¸å…·å¤‡åŠ¨æ€æ€§å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€‚</p></li>
<li><p>é€‚ç”¨åœºæ™¯ï¼šå¹¿æ³›ç”¨äº GPT-2 ç­‰å­è¯æ¨¡å‹ï¼Œé€‚åˆè¯æ±‡ä¸°å¯Œçš„è¯­è¨€ä»¥åŠå¸Œæœ›å‡å°‘è¯æ±‡è¡¨å¤§å°çš„åœºæ™¯ã€‚</p></li>
</ul>
</section>
<section id="wordpiece">
<h5>WordPiece<a class="headerlink" href="#wordpiece" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç®€ä»‹ï¼šWordPiece æ˜¯ä¸€ç§ä¸ BPE ç›¸ä¼¼çš„å­è¯åˆ†è¯ç®—æ³•ï¼Œä¸»è¦ç”± Google åœ¨ BERT æ¨¡å‹ä¸­ä½¿ç”¨ã€‚å®ƒçš„åˆ†è¯ç­–ç•¥æ˜¯è´ªå©ªçš„ï¼Œå³ä¼˜å…ˆå°è¯•ç”Ÿæˆæœ€é•¿çš„å­è¯ã€‚å¯¹äºæ²¡æœ‰å®Œæ•´è¯æ±‡çš„å•è¯ï¼ŒWordPiece ä¼šå°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªå­è¯ï¼Œå¹¶åœ¨å•è¯å†…éƒ¨ä½¿ç”¨ ## å‰ç¼€æ ‡è¯†åç»­å­è¯ã€‚</p></li>
<li><p>ä¼˜ç‚¹ï¼šèƒ½æœ‰æ•ˆå¤„ç†æœªè§è¿‡çš„å•è¯ï¼Œé€šè¿‡å°†æœªçŸ¥å•è¯æ‹†åˆ†æˆå¤šä¸ªå­è¯ä»¥è¦†ç›–æ›´å¤šå¯èƒ½çš„ç»„åˆã€‚è¾ƒå°‘å‡ºç°â€œ[UNK]â€æ ‡è®°ã€‚</p></li>
<li><p>ç¼ºç‚¹ï¼šä¾èµ–è¯­æ–™çš„è®­ç»ƒï¼Œéœ€è¦æ›´å¤šçš„æ—¶é—´å’Œè®¡ç®—èµ„æºã€‚</p></li>
<li><p>é€‚ç”¨åœºæ™¯ï¼šç”¨äº BERT åŠå…¶è¡ç”Ÿæ¨¡å‹ã€‚é€‚åˆè¾ƒé•¿æ–‡æœ¬æˆ–å¸Œæœ›é€šè¿‡åˆ†è¯ç®—æ³•è·å¾—ç¨³å®šæ•ˆæœçš„æƒ…å†µã€‚</p></li>
</ul>
</section>
<section id="unigram">
<h5>Unigram<a class="headerlink" href="#unigram" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç®€ä»‹ï¼šUnigram æ˜¯å¦ä¸€ç§å­è¯åˆ†è¯ç®—æ³•ï¼Œä¸ BPE å’Œ WordPiece ä¸åŒï¼ŒUnigram åŸºäºæ¦‚ç‡æ¨¡å‹æ¥é€‰æ‹©æœ€ä¼˜çš„åˆ†è¯ç»„åˆã€‚å®ƒä¼šä¸ºä¸€ä¸ªå¥å­è®¡ç®—å¤šç§åˆ†è¯æ–¹å¼ï¼Œé€‰æ‹©å…¶ä¸­æ¦‚ç‡æœ€é«˜çš„ç»„åˆã€‚</p></li>
<li><p>ä¼˜ç‚¹ï¼šUnigram ä¸ä¾èµ–å›ºå®šè§„åˆ™ï¼Œè€Œæ˜¯åŸºäºæ¦‚ç‡åŠ¨æ€é€‰æ‹©æœ€ä¼˜åˆ†è¯ï¼Œå…·æœ‰ä¸€å®šçš„çµæ´»æ€§å’Œä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€‚èƒ½å¤Ÿæœ‰æ•ˆåœ°å‹ç¼©è¯æ±‡è¡¨å¹¶å‡å°‘ OOV é—®é¢˜ã€‚</p></li>
<li><p>ç¼ºç‚¹ï¼šç›¸æ¯”å…¶ä»–ç®—æ³•æ›´ä¸ºå¤æ‚ï¼Œå¯èƒ½è®¡ç®—é‡è¾ƒå¤§ã€‚</p></li>
<li><p>é€‚ç”¨åœºæ™¯ï¼šåº”ç”¨äº XLNet å’Œ SentencePiece ç­‰æ¨¡å‹ï¼Œé€‚åˆéœ€è¦çµæ´»åˆ†è¯çš„è¯­è¨€æ¨¡å‹åº”ç”¨ã€‚</p></li>
</ul>
</section>
<section id="id2">
<h5>æ€»ç»“å¯¹æ¯”<a class="headerlink" href="#id2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>ç®—æ³•</p></th>
<th class="head"><p>åˆ†è¯æ–¹å¼</p></th>
<th class="head"><p>è¯æ±‡è¡¨å¤§å°</p></th>
<th class="head"><p>æœªçŸ¥è¯å¤„ç†</p></th>
<th class="head"><p>ä¸»è¦åº”ç”¨æ¨¡å‹</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>WordLevel</p></td>
<td><p>ç›´æ¥æŒ‰è¯æ˜ å°„ID</p></td>
<td><p>éå¸¸å¤§</p></td>
<td><p>ä½¿ç”¨â€œ[UNK]â€</p></td>
<td><p>ç®€å•æ–‡æœ¬åˆ†æä»»åŠ¡</p></td>
</tr>
<tr class="row-odd"><td><p>BPE</p></td>
<td><p>å­è¯åˆå¹¶ï¼Œé¢‘ç‡ç»Ÿè®¡</p></td>
<td><p>ä¸­ç­‰</p></td>
<td><p>å­è¯ç»„åˆ</p></td>
<td><p>GPT-2 ç­‰æ¨¡å‹</p></td>
</tr>
<tr class="row-even"><td><p>WordPiece</p></td>
<td><p>è´ªå©ªåŒ¹é…ï¼Œè¯é¦–â€œ##â€æ ‡è®°</p></td>
<td><p>ä¸­ç­‰</p></td>
<td><p>å­è¯ç»„åˆ</p></td>
<td><p>BERT åŠå…¶å˜ä½“</p></td>
</tr>
<tr class="row-odd"><td><p>Unigram</p></td>
<td><p>æ¦‚ç‡æ¨¡å‹ï¼Œæœ€ä¼˜ç»„åˆ</p></td>
<td><p>ä¸­ç­‰</p></td>
<td><p>åŠ¨æ€é€‰æ‹©</p></td>
<td><p>SentencePiece, XLNet</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="post-processors">
<h4>Post-Processors<a class="headerlink" href="#post-processors" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>TemplateProcessing</p></li>
<li><p>å…·ä½“å‚è§ä¸Šé¢</p></li>
</ul>
</section>
<section id="decoders">
<h4>Decoders<a class="headerlink" href="#decoders" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ByteLevel</span>
<span class="n">Metaspace</span>
<span class="n">WordPiece</span>

<span class="n">BPEDecoder</span>
<span class="n">CTC</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="id3">
<h3>Normalizers<a class="headerlink" href="#id3" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BertNormalizer</span>
<span class="n">Lowercase</span>
</pre></div>
</div>
</section>
<section id="trainers">
<h3>Trainers<a class="headerlink" href="#trainers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BpeTrainer</span>
<span class="n">UnigramTrainer</span>
<span class="n">WordLevelTrainer</span>
<span class="n">WordPieceTrainer</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">ä¸»é¡µ</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">æ¨¡å—ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">æœç´¢é¡µé¢</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="PEFT.html" class="btn btn-neutral float-right" title="7.3.4. PEFT" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Transformers/Transformers_V4.45.2.html" class="btn btn-neutral" title="Transformers 4.45.2" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, æ–°æºª-gordon.

    </p>
  </div>
  <div>å¤‡æ¡ˆå· <a href="http://www.beian.miit.gov.cn">äº¬ICPå¤‡16018553å·</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>