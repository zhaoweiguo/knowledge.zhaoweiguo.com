

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


<!-- start added 2025-08-06   增加对mermaid图的支持 -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
<script>
document.addEventListener('DOMContentLoaded', function() {
    mermaid.initialize({
        startOnLoad: true,
        theme: 'default',
        flowchart: { useMaxWidth: true }
    });
});
</script>
<!--  end added 2025-08-06   增加对mermaid图的支持 -->




  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>model &mdash; 新溪-gordon V2025.10 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="博文: decoding methods of LLM with transformers" href="blog_decoding-methods.html" />
    <link rel="prev" title="resources" href="resources.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script src="../../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.10
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/ml.html">1.3. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/deep_learning.html">1.5. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/normal.html">1.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/history.html">1.5.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/algorithm.html">1.7. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/tool.html">1.8. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/question.html">1.9. 常见问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. 机器人领域</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/key.html">2.1. 关键定义</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Recommenders/CF%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4.html">2.1.1. 协同过滤（Collaborative Filtering, CF）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Recommenders/MF%E6%A6%82%E7%8E%87%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html">2.1.2. MF(Matrix Factorization，矩阵分解)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Recommenders/PMF%E6%A6%82%E7%8E%87%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3.html">2.1.3. PMF（Probabilistic Matrix Factorization，概率矩阵分解）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Recommenders/Two-TowerModels%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B.html">2.1.4. Two-Tower Models（双塔模型）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Parallelism/normal.html">2.1.5. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Parallelism/PipelineParallelism.html">2.1.6. Pipeline Parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/Parallelism/TensorParallesim.html">2.1.7. Tensor Parallesim</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/activation_Sigmoid.html">2.1.8. 激活函数-Sigmoid</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/activation_RELU.html">2.1.9. 激活函数-ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/activation_Leaky-ReLU.html">2.1.10. 激活函数-Leaky ReLU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/activation_Tanh.html">2.1.11. 激活函数-Tanh</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/activation_GELU.html">2.1.12. 激活函数-GELU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/normalization_L1.html">2.1.13. 归一化-L1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/normalization_L2.html">2.1.14. 归一化-L2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/probabilistic_Softmax.html">2.1.15. 概率分布-Softmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/probabilistic_logSoftmax.html">2.1.16. 概率分布-logsoftmax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_activations/probabilistic_Sparsemax.html">2.1.17. 概率分布-Sparsemax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/classify_cross_entropy.html">2.1.18. 损失函数-分类-cross-entropy(交叉熵)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/classify_NLL.html">2.1.19. 损失函数-分类-负对数似然损失NLL Loss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/classify_log.html">2.1.20. 损失函数-分类-对数损失(Log Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/classify_kl.html">2.1.21. 损失函数-分类-KL 散度(KL Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/regression_MSE.html">2.1.22. 损失函数-回归-均方误差(MSE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/regression_MAE.html">2.1.23. 损失函数-回归-平均绝对误差(MAE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/regression_Huber.html">2.1.24. 损失函数-回归-Huber 损失</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/regression_log_cosh.html">2.1.25. 损失函数-回归-对数余弦损失(Log-Cosh Loss)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_loss/%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F-L2%E6%AD%A3%E5%88%99%E5%8C%96.html">2.1.26. 权重衰减(L2正则化)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/GD.html">2.1.27. GD(梯度下降)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/SGD.html">2.1.28. SGD随机梯度下降</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/RMSprop.html">2.1.29. RMSprop</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/Adam.html">2.1.30. Adam</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/AdamW.html">2.1.31. AdamW</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/func_optims/Momentum.html">2.1.32. Momentum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/ners/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">2.1.33. HMM-隐马尔可夫模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/ners/WWM-%E5%85%A8%E8%AF%8DMask.html">2.1.34. WWM-Whole Word Masking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/ners/CRF-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html">2.1.35. CRF-条件随机场</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dls/ANN.html">2.1.36. ANN(NN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dls/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">2.1.37. 深度神经网络(Deep Neural Network, DNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dls/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">2.1.38. 卷积神经网络(Convolutional Neural Network, CNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dls/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91.html">2.1.39. RNN: 循环神经网(Recurrent Neural Network, RNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dls/LSTM-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86.html">2.1.40. LSTM: 长短时记忆(Long Short Term Memory, LSTM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/propagation.html">2.1.41. 前向/反向传播</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/LinearLayer.html">2.1.42. Linear Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/FFN.html">2.1.43. Feedforward Network-前馈网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/LayerNorm.html">2.1.44. LayerNorm(层归一化)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/WeightTying.html">2.1.45. Weight Tying</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/GreedyDecoding.html">2.1.46. Greedy Decoding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/ImageGrounding.html">2.1.47. Image Grounding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/dl_theorys/Perplexity.html">2.1.48. Perplexity(PPL)困惑度</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/ManhattanWorld%E6%9B%BC%E5%93%88%E9%A1%BF%E4%B8%96%E7%95%8C.html">2.1.49. Manhattan World(曼哈顿世界)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/HoughTransform%E9%9C%8D%E5%A4%AB%E5%8F%98%E6%8D%A2.html">2.1.50. Hough Transform（霍夫变换）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/PolarCoordinateSystem%E6%9E%81%E5%9D%90%E6%A0%87%E8%A1%A8%E7%A4%BA%E6%B3%95.html">2.1.51. 极坐标表示法(Polar Coordinate System)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/GaussianSphere%E9%AB%98%E6%96%AF%E7%90%83.html">2.1.52. Gaussian Sphere（高斯球）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/edge_direction%E8%BE%B9%E7%BC%98%E6%96%B9%E5%90%91.html">2.1.53. 边缘方向 Edge Direction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/3Ds/NormalVector%E6%B3%95%E5%90%91%E9%87%8F.html">2.1.54. NormalVector法向量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/AllReduce.html">2.1.55. AllReduce</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/BPE.html">2.1.56. BPE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/Embedding%E6%A8%A1%E5%9E%8B.html">2.1.57. Embedding 模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/K-Means%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95.html">2.1.58. K-Means聚类算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/LLM.html">2.1.59. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/deeplearning.html">2.1.60. 深度学习相关</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/other.html">2.1.61. 其他</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8Bvs%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B.html">2.1.62. 判别式模型vs生成式模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%A9%BA%E9%97%B4.html">2.1.63. 欧几里得空间(Euclidean space)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/keys/others/%E7%9F%A2%E9%87%8F%E5%8C%96%E8%AE%A1%E7%AE%97.html">2.1.64. 矢量化计算(Vectorize calculations)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/tmp.html">2.2. 临时</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/ReAct.html">2.2.1. ReAct框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Reflection.html">2.2.2. Reflection反思</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/math.html">2.2.3. 数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/bag-of-words.html">2.2.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/word2vec.html">2.2.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/doc2vec.html">2.2.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/FastText.html">2.2.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LDA.html">2.2.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/overfitting-underfitting.html">2.2.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RAG.html">2.2.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Agent.html">2.2.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LLM.html">2.2.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RL.html">2.2.13. RL-强化学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/prompt_engineering.html">2.2.14. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/finetune.html">2.2.15. LLM调优(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Workflow.html">2.2.16. Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/0normal.html">2.2.17. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-推理模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. 模型量化(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/normal.html">3.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/GGUF.html">3.4.2. GGUF 文件</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/fileformat.html">3.5. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/normal.html">3.5.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/GGML.html">3.5.2. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/openai.html">3.6. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/normal.html">3.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/prompt.html">3.7. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_chinese.html">3.7.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/skill.html">3.7.3. 示例</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Android.html">3.8. Android版LLM相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/normal.html">3.8.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Android版部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../RAG.html">4. RAG相关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/preprocess.html">5.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/normal.html">5.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/NER.html">5.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/normal.html">5.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/seq-label.html">5.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/history.html">5.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/summary.html">5.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/summarys/normal.html">5.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">6. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Image.html">6.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Video.html">6.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/normal.html">6.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/magic.html">6.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/display.html">6.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/normal.html">6.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/Ndarray.html">6.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/function.html">6.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/normal.html">6.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_subset.html">6.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_analysis.html">6.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_sql.html">6.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_default_value.html">6.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_multi_index.html">6.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/practice.html">6.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_input_output.html">6.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/normal.html">6.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/install.html">6.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/example.html">6.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pylab.html">6.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/SciPys/normal.html">6.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/normal.html">6.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/supervised.html">6.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/unsupervised.html">6.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/normal.html">6.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/example.html">6.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/struct.html">6.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Seaborns/normal.html">6.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/jieba.html">6.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/gensim.html">6.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/normal.html">6.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/LAC.html">6.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../framework.html">7. 学习框架</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../normal.html">7.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/normal.html">7.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/nn.html">7.2.2. nn模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../huggingface.html">7.3. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../normal.html">7.3.1. 常用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Transformers.html">7.3.2. Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Transformers/Transformers.html">Transformers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Transformers/Transformers_V4.45.2.html">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Tokenizers_V0.13.3.html">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../collect.html">7.3.7. 收集</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="resources.html">resources</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="blog_decoding-methods.html">博文: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../vLLM.html">7.4. vLLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/normal.html">7.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/vLLM_doc.html">7.4.2. vLLM官方文档</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../llama.cpp.html">7.5. llama.cpp框架</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/normal.html">7.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mxnet.html">7.7. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/ndarray.html">7.7.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/gluon.html">7.7.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/autograd.html">7.7.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/normal.html">7.9.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/demo.html">7.9.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html">7.10. 其他</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../website.html">8. 关键网站</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/ArXiv.html">8.3. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/video.html">8.4. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/normal.html">8.5. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-inference-accelerate.html">10.9. LLM推理加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Evaluate.html">10.10. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/platform.html">10.11. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese.html">11.2. 中文数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese_image.html">11.3. 中文图片相关数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/huggingface.html">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/website.html">11.5. 数据集相关网站</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model.html">12. 常见模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">13. 图形&amp;计算加速技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/normal.html">13.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Evaluate.html">14. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/normal.html">14.1. 通用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E4%BC%A0%E7%BB%9FAI.html">15. 传统AI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../framework.html"><span class="section-number">7. </span>学习框架</a> &raquo;</li>
        
          <li><a href="../../huggingface.html"><span class="section-number">7.3. </span>huggingface</a> &raquo;</li>
        
          <li><a href="../collect.html"><span class="section-number">7.3.7. </span>收集</a> &raquo;</li>
        
      <li>model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/frameworks/huggingfaces/collects/model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">model</a><ul>
<li><a class="reference internal" href="#multilingual-models">Multilingual models</a><ul>
<li><a class="reference internal" href="#xlm">XLM</a><ul>
<li><a class="reference internal" href="#xlm-clm-enfr-1024">xlm-clm-enfr-1024</a></li>
<li><a class="reference internal" href="#xlm-mlm-ende-1024">xlm-mlm-ende-1024</a></li>
</ul>
</li>
<li><a class="reference internal" href="#xlm-roberta">XLM-RoBERTa</a></li>
<li><a class="reference internal" href="#m2m100">M2M100</a><ul>
<li><a class="reference internal" href="#facebook-m2m100-418m">facebook/m2m100_418M</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#wav2vec2">wav2vec2</a><ul>
<li><a class="reference internal" href="#facebook-wav2vec2-base">facebook/wav2vec2-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-to-speech">Text to speech</a><ul>
<li><a class="reference internal" href="#suno-bark">suno/bark</a></li>
<li><a class="reference internal" href="#whisper">Whisper</a></li>
</ul>
</li>
<li><a class="reference internal" href="#imageclassification">ImageClassification</a><ul>
<li><a class="reference internal" href="#vit">ViT</a></li>
<li><a class="reference internal" href="#google-vit-base-patch16-224-in21k">google/vit-base-patch16-224-in21k</a></li>
</ul>
</li>
<li><a class="reference internal" href="#semanticsegmentation">SemanticSegmentation</a><ul>
<li><a class="reference internal" href="#nvidia-mit-b0">nvidia/mit-b0</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zeroshotobjectdetection">ZeroShotObjectDetection</a><ul>
<li><a class="reference internal" href="#owl-vit">Owl-ViT</a></li>
<li><a class="reference internal" href="#google-owlvit-base-patch32">google/owlvit-base-patch32</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zeroshotimageclassification">ZeroShotImageClassification</a><ul>
<li><a class="reference internal" href="#openai-clip-vit-large-patch14">openai/clip-vit-large-patch14</a></li>
</ul>
</li>
<li><a class="reference internal" href="#objectdetection">ObjectDetection</a><ul>
<li><a class="reference internal" href="#detr">DETR</a></li>
<li><a class="reference internal" href="#facebook-detr-resnet-50">facebook/detr-resnet-50</a></li>
<li><a class="reference internal" href="#vinvino02-glpn-nyu">vinvino02/glpn-nyu</a></li>
<li><a class="reference internal" href="#microsoft-git-base">microsoft/git-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#videomae">VideoMAE</a><ul>
<li><a class="reference internal" href="#mcg-nju-videomae-base">MCG-NJU/videomae-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal">MULTIMODAL</a><ul>
<li><a class="reference internal" href="#microsoft-layoutlmv2-base-uncased">microsoft/layoutlmv2-base-uncased</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="model">
<h1>model<a class="headerlink" href="#model" title="此标题的永久链接">¶</a></h1>
<section id="multilingual-models">
<h2>Multilingual models<a class="headerlink" href="#multilingual-models" title="此标题的永久链接">¶</a></h2>
<section id="xlm">
<h3>XLM<a class="headerlink" href="#xlm" title="此标题的永久链接">¶</a></h3>
<section id="xlm-clm-enfr-1024">
<h4>xlm-clm-enfr-1024<a class="headerlink" href="#xlm-clm-enfr-1024" title="此标题的永久链接">¶</a></h4>
<p>xlm-clm-enfr-1024是一个多语言预训练的语言模型,具有以下特征:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于XLM(Facebook的跨语言预训练语言模型),支持100种语言。
- 预训练使用英语和法语语料,可以处理英语和法语文本。
- 输入为文本序列,输出对应的词向量表示。
- 模型大小为1GB,包含1024个隐层单元。
- 提供了XLMTokenizer用于文本tokenize,以及XLMWithLMHeadModel作为模型实现。
- 可以进行Masked语言建模(MLM)、Next Sentence Prediction(NSP)等下游任务fine-tuning。
- 预训练目标是联合 masked language modeling 和翻译语言建模。
- 采用SentencePiece进行词元化,支持词汇表共享。
- 支持PyTorch框架,可以灵活集成。
- 预训练质量高,是英语和法语领域的通用语言表示模型。
</pre></div>
</div>
<p>总之,xlm-clm-enfr-1024是一个高质量的跨语言预训练模型,可以作为英法两语区域NLP任务的强大预训练语言模型基线。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">XLMTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlm-clm-enfr-1024&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XLMWithLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlm-clm-enfr-1024&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>XLM with language embeddings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">ende</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">German</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">enfr</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">French</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">enro</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">Romanian</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">xnli15</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">XNLI</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">tlm</span><span class="o">-</span><span class="n">xnli15</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span> <span class="o">+</span> <span class="n">translation</span><span class="p">,</span> <span class="n">XNLI</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">clm</span><span class="o">-</span><span class="n">enfr</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Causal</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">French</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">clm</span><span class="o">-</span><span class="n">ende</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Causal</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">German</span><span class="p">)</span>
</pre></div>
</div>
<p>XLM without language embeddings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="mi">17</span><span class="o">-</span><span class="mi">1280</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">17</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="mi">100</span><span class="o">-</span><span class="mi">1280</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xlm-mlm-ende-1024">
<h4>xlm-mlm-ende-1024<a class="headerlink" href="#xlm-mlm-ende-1024" title="此标题的永久链接">¶</a></h4>
<p>xlm-mlm-ende-1024是一个多语言Mask语言模型(MLM),主要功能和特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 该模型由Facebook AI研究院在2019年提出,支持100种语言。
- 采用Transformer架构,并在编码器部分进行多语言预训练。
- 预训练目标是masked language modeling (MLM),即预测被遮挡的语言词汇。
- 支持1024个Transformer块,参数量约为570亿。
- 训练数据包含100种语言的维基百科文本数据,总计约250GB。
- 支持跨语言的表示学习,一个模型就可以处理100种语言。
- 通过在大规模多语言数据集上预训练,获得了强大的语言理解能力。
- xlm-mlm-ende-1024是一个通用的多语言语义表示模型,可应用于下游跨语言自然语言处理任务中。
- 开源发布后受到广泛关注,被视为多语言预训练模型的重要进展。
</pre></div>
</div>
</section>
</section>
<section id="xlm-roberta">
<h3>XLM-RoBERTa<a class="headerlink" href="#xlm-roberta" title="此标题的永久链接">¶</a></h3>
<p>The following XLM-RoBERTa models can be used for multilingual tasks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">roberta</span><span class="o">-</span><span class="n">base</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">roberta</span><span class="o">-</span><span class="n">large</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="m2m100">
<h3>M2M100<a class="headerlink" href="#m2m100" title="此标题的永久链接">¶</a></h3>
<p>The following M2M100 models can be used for multilingual translation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">facebook</span><span class="o">/</span><span class="n">m2m100_418M</span> <span class="p">(</span><span class="n">Translation</span><span class="p">)</span>
<span class="n">facebook</span><span class="o">/</span><span class="n">m2m100_1</span><span class="mf">.2</span><span class="n">B</span> <span class="p">(</span><span class="n">Translation</span><span class="p">)</span>
</pre></div>
</div>
<section id="facebook-m2m100-418m">
<h4>facebook/m2m100_418M<a class="headerlink" href="#facebook-m2m100-418m" title="此标题的永久链接">¶</a></h4>
<p>facebook/m2m100_418M 是一个大型的多语言对多语言机器翻译模型，由 Facebook AI 研究院训练并开源。</p>
<p>主要特点:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>采用 Transformer 架构，基于 Fairseq 代码库实现。
训练数据包括 100 种语言对，总计 418 万对句子。覆盖语言非常广泛。
支持直接端到端多语言翻译，不需要中继语言。
性能较强，在 WMT、Flores 翻译测试集上表现良好。
模型大小为 1.1GB (半精度),inference 速度快。
开源且无需许可，可以自由使用在商业产品中。
预训练模型可直接下载使用，简单方便。
支持添加自定义词典来改进翻译质量。
可在 CPU、GPU 等硬件上部署，适合不同场景。
</pre></div>
</div>
<p>整体来说，这是一个非常强大且实用的多语言机器翻译模型，值得推荐使用。它的开源特性也使其易于集成到各种产品和服务中。</p>
<p>m2m100_418M是Facebook在2022年开源的一个多模态大模型,主要特征是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 采用Transformer作为模型架构,包含文本编码器和视觉编码器。
- 文本编码器采用T5结构,视觉编码器采用Vision Transformer结构。
- 模型参数量达到418亿,使用了大量训练数据,包括4000万张图像和1700万段文本。
- 训练目标是进行视觉问答,即根据图片内容来回答自然语言问题。
- 模型支持100种语言,可以进行跨语言的视觉问答。
- 开源的模型包括英文、德文、法文、意大利文、日文、韩文、简体中文、繁体中文等版本。
- m2m100展示了视觉与语言的多模态预训练的强大能力,是一种统一的多语言多模态模型。
- 该模型仍然有进一步提升的空间,后续工作将会在模型规模、训练数据、泛化能力等方面进行改进。
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">M2M100Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/m2m100_418M&quot;</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;zh&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">M2M100ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/m2m100_418M&quot;</span><span class="p">)</span>

<span class="n">chinese_text</span> <span class="o">=</span> <span class="s2">&quot;不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒.&quot;</span>
<span class="n">encoded_zh</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">chinese_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_zh</span><span class="p">,</span> <span class="n">forced_bos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_lang_id</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">))</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="wav2vec2">
<h2>wav2vec2<a class="headerlink" href="#wav2vec2" title="此标题的永久链接">¶</a></h2>
<section id="facebook-wav2vec2-base">
<h3>facebook/wav2vec2-base<a class="headerlink" href="#facebook-wav2vec2-base" title="此标题的永久链接">¶</a></h3>
<p>facebook/wav2vec2-base是一个由Facebook AI研究院训练的音频表示模型,属于Wav2Vec2系列。该模型的主要特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 无监督预训练:在大规模语音数据上通过自监督学习获得语音表示,没有标签数据。
- 基于 transformers:模型结构基于transformers,包含时间和频域的卷积层,多层自注意力机制。
- 输出语音表示:模型可以直接输出语音的表征向量表示,包含丰富的语义信息。
- 小模型大小:相比BERT系列,wav2vec2-base模型大小更小,只有95M参数。
- 应用广泛:预训练表示可 Fine-tune 在各种下游语音任务,包括语音识别、音频分类等。
- 性能优异:在多个公开语音数据集上表现优于传统MFCC特征。
- 易于使用:提供了方便的API,可以快速应用到项目中。
</pre></div>
</div>
<p>总体来说,wav2vec2-base是一个非常强大的语音表示模型,具有预训练的优势,可以广泛地应用到语音领域的任务中,是音频领域很有价值的预训练模型之一。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For Audio Classification</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForAudioClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>


<span class="c1"># For ASR</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCTC</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="text-to-speech">
<h2>Text to speech<a class="headerlink" href="#text-to-speech" title="此标题的永久链接">¶</a></h2>
<section id="suno-bark">
<h3>suno/bark<a class="headerlink" href="#suno-bark" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/suno-ai/bark">https://github.com/suno-ai/bark</a></p></li>
</ul>
<p>suno/bark是一个语音合成模型,具有以下特征:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于Facebook的BARK模型,可以生成高质量的英语语音。BARK采用流式设计,可以快速生成语音。
- 预训练模型suno/bark已在LibriTTS英语语料库上进行fine-tune,可以生成自然的英语语音。
- 输入为英文文本,输出为16kHz采样率的单声道语音波形。
- 模型大小仅有87MB,非常轻量级,适合在各种环境下部署。
- 支持PyTorch和TensorFlow两种框架,可以灵活地集成到不同的项目中。
- 提供了AutoProcessor和BarkModel两个类,可以通过简单的API进行语音合成。
- 预训练模型质量较高,合成语音清晰流畅,语调自然。
-  Open-source社区维护,后续会继续优化和完善模型。
</pre></div>
</div>
<p>总体来说,suno/bark是一个非常理想的英语TTS模型选择,可直接用于产品中,同时也为研究人员提供了一个优秀的语音合成基线。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;suno/bark&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BarkModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;suno/bark&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="whisper">
<h3>Whisper<a class="headerlink" href="#whisper" title="此标题的永久链接">¶</a></h3>
<p>Whisper 是一个面向文本的非常大规模的神经网络模型，由 OpenAI 于 2022 年 9 月开源。其主要特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>模型规模巨大，包含了 1000 亿参数，是目前公开的最大的参数量模型之一。
基于 Transformers 架构，采用了类似 GPT 的编码器 - 解码器结构。
可以直接对原始语音进行识别转录，也可生成人类语音。
训练数据集包含了 660,000 小时 (合 75 年) 的语音数据，涵盖多种语言。
采用了一些技术来控制生成内容的安全性。
拥有语音识别、语音合成、文字转语音、语音翻译等多种应用潜力。
目前还有一定局限，如音素错误、语法错误、生成可信信息等。
对未来 voice AI 技术产生重大影响，代表语音 AI 发展新方向。
研究引发了对大模型安全性、伦理规范等方面的广泛关注和讨论。
</pre></div>
</div>
<p>总体来说，Whisper 是一个划时代的大模型，预示着语音 AI 进入新的阶段，但也给社会带来新的挑战。</p>
</section>
</section>
<section id="imageclassification">
<h2>ImageClassification<a class="headerlink" href="#imageclassification" title="此标题的永久链接">¶</a></h2>
<section id="vit">
<h3>ViT<a class="headerlink" href="#vit" title="此标题的永久链接">¶</a></h3>
<p>Google发布的一个VISION Transformer(ViT)模型。</p>
</section>
<section id="google-vit-base-patch16-224-in21k">
<h3>google/vit-base-patch16-224-in21k<a class="headerlink" href="#google-vit-base-patch16-224-in21k" title="此标题的永久链接">¶</a></h3>
<p>这是一个在大规模图像数据集ImageNet-21k上预训练的Transformer模型,用于计算机视觉领域。其主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于Transformer:利用自注意力机制Capture全局信息。
- Patch分割:输入图片被分割为16x16的patches,每个patch生成一个向量。
- 大模型规模:Base模型包含86M参数。
- 高精度:top-1准确率可达83%。
- 预训练数据:使用了包含1400万张图像、21741分类的ImageNet-21k数据集。
- 易fine-tune:可以在下游任务中加入新的头部进行迁移Fine-tuning。
- 占用内存小:比 CNN 模型更高效。
</pre></div>
</div>
<p>这个ViT模型展示了Transformer在计算机视觉领域也可以取得非常强大的效果。可以用来进行图像分类、对象检测等任务。在下游任务中只需要加入新的分类头,就可以进行端到端的Fine-tuning,做出非常高精度的图像分类器。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/vit-base-patch16-224-in21k&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
     <span class="n">checkpoint</span><span class="p">,</span>
     <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span>
     <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
     <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="semanticsegmentation">
<h2>SemanticSegmentation<a class="headerlink" href="#semanticsegmentation" title="此标题的永久链接">¶</a></h2>
<section id="nvidia-mit-b0">
<h3>nvidia/mit-b0<a class="headerlink" href="#nvidia-mit-b0" title="此标题的永久链接">¶</a></h3>
<p>nvidia/mit-b0是一个针对语义分割预训练的Transformer模型,由Nvidia基于Megatron框架训练获得。其关键特征为:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 模型结构:基于Vision Transformer,使用分割注意力机制。
- 预训练任务:在ADE20K数据集上进行像素级预测进行预训练。
- 模型规模:基础版本mit-b0包含32M参数。
- 输入分辨率:高512,宽512。
- 速度快:基础版本可达到每秒处理8幅图像。
- 性能卓越:在多个语义分割数据集上达到或超越CNN模型的效果。
- 易于fine-tune:可在downstream任务中直接fine-tune。
- 可扩展性强:提供了小型到超大型的系列模型。
</pre></div>
</div>
<p>nvidia/mit-b0展示了Transformer模型在语义分割领域也可以达到极强的效果,甚至超过现有CNN模型。它可以作为通用的语义分割预训练模型,然后fine-tune到各种下游分割任务中,实现快速高效的语义分割。整体来说,该模型为Transformer在计算机视觉更广阔的应用提供了有力案例。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;nvidia/mit-b0&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">reduce_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span> <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="zeroshotobjectdetection">
<h2>ZeroShotObjectDetection<a class="headerlink" href="#zeroshotobjectdetection" title="此标题的永久链接">¶</a></h2>
<section id="owl-vit">
<h3>Owl-ViT<a class="headerlink" href="#owl-vit" title="此标题的永久链接">¶</a></h3>
<p>Owl-ViT是一种改进的Vision Transformer (ViT) 模型架构,主要特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 提出了一种称为本地分组自注意力(Locally Grouped Self-Attention)的机制,对ViT的自注意力机制进行改进,使其对小尺寸图像也更有效。
- 将图像划分成较小的非重叠patch,然后在每个局部邻域内学习patch之间的关系。
- 引入了多尺度结构,不同级别的注意力模块负责不同的视野范围,既保留全局信息,也学习局部细节。
- 通过引入残差连接,缓解了自注意力模块堆叠导致的过拟合问题。
- 相比ViT,Owl-ViT的参数量和计算量更小,训练速度更快。
- 在多个视觉任务上性能优于ViT,尤其在小样本和低分辨率图像上。
</pre></div>
</div>
<p>总体来说,Owl-ViT对ViT进行了改进,使其更适合计算资源受限而数据不足的实际场景。它比ViT参数量减小60%以上,计算量减小70%以上,但性能优于ViT。Owl-ViT是一个轻量高效的ViT改进版本。</p>
</section>
<section id="google-owlvit-base-patch32">
<h3>google/owlvit-base-patch32<a class="headerlink" href="#google-owlvit-base-patch32" title="此标题的永久链接">¶</a></h3>
<p>google/owlvit-base-patch32是一个基于Owl-ViT模型架构的图像分类预训练模型,主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 发布者:Google。
- 模型结构:Owl-ViT-B,采用Vision Transformer的网络架构。
- 输入:32x32大小的图像patch,形状为[3, 32, 32]。
- 预训练数据集:JFT-300M数据集,包含3亿张图像。
- 预训练任务:图像分类,可以区分3000个类别。
- 模型参数量:8600万。
- 应用:适用于迁移学习,可以初始化下游任务的backbone。在图像分类、目标检测等任务上有很好的表现。
- 权重文件大小:336MB。
- 推理速度快,是实际部署的理想选择。
</pre></div>
</div>
<p>Owl-ViT改进了ViT的局部注意力机制,使其对小尺寸图像也适用。google/owlvit-base-patch32是一个计算成本低但效果强劲的图像分类预训练模型,适合在各种视觉任务中使用。它是一个高效实用的选择。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/owlvit-base-patch32&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForZeroShotObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;zero-shot-object-detection&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="zeroshotimageclassification">
<h2>ZeroShotImageClassification<a class="headerlink" href="#zeroshotimageclassification" title="此标题的永久链接">¶</a></h2>
<section id="openai-clip-vit-large-patch14">
<h3>openai/clip-vit-large-patch14<a class="headerlink" href="#openai-clip-vit-large-patch14" title="此标题的永久链接">¶</a></h3>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai/clip-vit-large-patch14&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForZeroShotImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;zero-shot-image-classification&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="objectdetection">
<h2>ObjectDetection<a class="headerlink" href="#objectdetection" title="此标题的永久链接">¶</a></h2>
<section id="detr">
<h3>DETR<a class="headerlink" href="#detr" title="此标题的永久链接">¶</a></h3>
<p>DETR(End-to-End Object Detection with Transformers)是Facebook AI研究院在2020年提出的使用Transformers进行端到端目标检测的新颖模型。其主要特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">整体网络结构端到端</span><span class="p">,</span><span class="n">无需NMS后处理</span>
<span class="o">-</span> <span class="n">利用Transformer直接预测目标类别和坐标</span>
<span class="o">-</span> <span class="n">训练过程更稳定</span><span class="p">,</span><span class="n">从零训练也可收敛</span>
</pre></div>
</div>
</section>
<section id="facebook-detr-resnet-50">
<h3>facebook/detr-resnet-50<a class="headerlink" href="#facebook-detr-resnet-50" title="此标题的永久链接">¶</a></h3>
<p>facebook/detr-resnet-50是一个基于DETR架构的目标检测预训练模型。该模型使用ResNet-50作为Backbone,在COCO数据集上预训练,主要参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">Backbone</span><span class="p">:</span> <span class="n">ResNet</span><span class="o">-</span><span class="mi">50</span>
<span class="o">-</span> <span class="n">Transformer</span> <span class="n">Encoder包含6层</span>
<span class="o">-</span> <span class="mi">80</span><span class="n">类对象检测</span>
<span class="o">-</span> <span class="n">输入图像尺寸为800</span>
</pre></div>
</div>
<p>facebook/detr-resnet-50展示了Transformer模型在目标检测任务上的有效性,端到端 learns 到了检测的能力。效果竞争力强,是目标检测领域的重要低设计模型之一。</p>
<p>facebook/detr-resnet-50是一个基于transformers的目标检测模型,其应用场景主要包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. 通用目标检测:
该模型在COCO等通用目标检测数据集上效果优异,可以应用到需要检测日常物体的各种场景中,如视频监控、自动驾驶、图像搜索等。
2. 特定目标检测:
模型可以进行fine-tuning,适应特定目标检测任务,如质检、医疗等领域的特定物体检测。
3. 小数据集学习:
相比传统方法,DETR对数据量要求不高,适合小数据集的目标检测任务。
4. 端到端学习:
DETR通过端到端学习预测目标框,无需后处理,使得部署简单。适用于对系统流程敏感的应用。
5. 强化学习:
DETR的学习过程稳定,适合和强化学习算法相结合,实现模仿学习等功能。
6. 前沿研究:
DETR开辟了transformer在目标检测领域的新方向,可以进行各种改进的学术研究。
</pre></div>
</div>
<p>总之,facebook/detr-resnet-50在多种实际目标检测应用中都展示了较强的泛化能力,是目前较为先进的检测模型之一。其端到端学习方式也启发了后续一系列detection transformer的提出。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;facebook/detr-resnet-50&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
    <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
    <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="vinvino02-glpn-nyu">
<h3>vinvino02/glpn-nyu<a class="headerlink" href="#vinvino02-glpn-nyu" title="此标题的永久链接">¶</a></h3>
<p>vinvino02/glpn-nyu是一个针对深度估计(monocular depth estimation)任务进行预训练的模型,主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 该模型基于GLPN架构,通过encoder-decoder结构进行单眼图像深度预测。
- 数据集方面,该模型是在NYU Depth V2数据集上预训练的,这是一个包含各种室内场景的RGB-D数据集。
- 输入是标准分辨率的RGB图像,输出是与输入分辨率相同的深度图。
- 模型主要组件包括:ResNet50作为encoder,U-Net作为decoder,还使用了红外线辅助训练。
- 在NYU Depth V2测试集上可以达到leading performance,mean relative error为0.131。
- 相比其他深度学习方法,该模型可以更好地学习纹理细节,recover更丰富的场景结构信息。
- 预训练模型大小约为47MB,可以快速加载使用或在自定义数据集上fine-tune。
- 可通过PyTorch Hub快速加载使用。
</pre></div>
</div>
<p>总而言之,vinvino02/glpn-nyu是一个轻量而高效的深度估计预训练模型,尤其适合在室内场景中进行单眼深度预测或相关下游任务的fine-tuning,值得尝试使用。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;vinvino02/glpn-nyu&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">depth_estimator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;depth-estimation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="microsoft-git-base">
<h3>microsoft/git-base<a class="headerlink" href="#microsoft-git-base" title="此标题的永久链接">¶</a></h3>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;microsoft/git-base&quot;</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="videomae">
<h2>VideoMAE<a class="headerlink" href="#videomae" title="此标题的永久链接">¶</a></h2>
<section id="mcg-nju-videomae-base">
<h3>MCG-NJU/videomae-base<a class="headerlink" href="#mcg-nju-videomae-base" title="此标题的永久链接">¶</a></h3>
<p>MCG-NJU/videomae-base是一个基于VideoMAE的视频分类预训练模型。VideoMAE是由MCG-NJU团队在2022年提出的用于视频理解的MASKED AUTOENCODER(MAE)模型。其特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于MAE框架,使用MASKING策略进行视频预训练。
- 输入是视频序列的打散帧(Shuffled Tokens)。
- 编码器采用TimeSformer结构。
- 可以在各类视频理解任务中进行微调使用。
- 先进的视频表示学习能力。
</pre></div>
</div>
<p>MCG-NJU/videomae-base是在大规模视频数据上预训练得到的VideoMAE基础模型,主要参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- TimeSformer编码器包含21个阶段,107M参数。
- 在8900万视频上预训练,总时长超过9000小时。
- 预训练任务是预测视频帧顺序。
</pre></div>
</div>
<p>该模型展示了Transformer模型在视频领域的强大建模能力。 fine-tune后效果显著,特别是在小数据集上的低样本学习。是近期较为先进的视频理解预训练模型之一。</p>
</section>
</section>
<section id="multimodal">
<h2>MULTIMODAL<a class="headerlink" href="#multimodal" title="此标题的永久链接">¶</a></h2>
<section id="microsoft-layoutlmv2-base-uncased">
<h3>microsoft/layoutlmv2-base-uncased<a class="headerlink" href="#microsoft-layoutlmv2-base-uncased" title="此标题的永久链接">¶</a></h3>
<p>模型 “microsoft/layoutlmv2-base-uncased” 是由微软开发的一种预训练语言模型，属于 LayoutLMv2 模型系列。这个模型的主要目标是处理文档布局和文本信息的结合，以便更好地支持文档分析、信息提取和相关任务。</p>
<p>与传统的自然语言处理模型不同，LayoutLMv2 关注的是将文本与其在页面中的布局信息相结合，从而使模型能够更好地理解和处理具有多个文本区域、表格、图像和其他排版元素的文档。这对于诸如表格数据提取、文档分类、命名实体识别等任务非常有用。</p>
<p>“microsoft/layoutlmv2-base-uncased” 模型是一个预训练模型，使用了无大小写区分的文本（uncased text）作为输入。这意味着它对于大小写不敏感，能够处理大小写不同的文本。通过预训练，模型学会了从文本和布局信息中抽取有关上下文、语义和结构的特征。</p>
<p>使用这个预训练模型，您可以通过微调（fine-tuning）适应特定的任务，如文本分类、命名实体识别、表格数据提取等。模型的预训练能力使其能够在不同的文档分析任务中表现出色，因为它已经学会了从文本和布局信息中获取有价值的信息。</p>
<p>总之，”microsoft/layoutlmv2-base-uncased” 模型是一种专注于处理带有布局信息的文本的预训练模型，可用于各种文档分析和信息提取任务。</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="blog_decoding-methods.html" class="btn btn-neutral float-right" title="博文: decoding methods of LLM with transformers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="resources.html" class="btn btn-neutral" title="resources" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'V2025.10',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>