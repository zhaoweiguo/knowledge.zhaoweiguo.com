

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   å¢åŠ å¯¹markdownä¸­å…¬å¼çš„æ”¯æŒ -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   å¢åŠ å¯¹markdownä¸­å…¬å¼çš„æ”¯æŒ -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Transformers 4.45.2 &mdash; æ–°æºª-gordon V2025.07 æ–‡æ¡£</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="index" title="ç´¢å¼•" href="../../../genindex.html" />
    <link rel="search" title="æœç´¢" href="../../../search.html" />
    <link rel="next" title="7.3.3. Tokenizers" href="../Tokenizers_V0.13.3.html" />
    <link rel="prev" title="Transformers" href="Transformers.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script src="../../../_static/js/jquery.min.js"></script>


<!-- è¯„è®ºæ’ä»¶ gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- è¯„è®ºæ’ä»¶ gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> æ–°æºª-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../normal.html">1. å¸¸ç”¨</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/normal.html">1.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/ml.html">1.3. æœºå™¨å­¦ä¹ machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/deep_learning.html">1.5. æ·±åº¦å­¦ä¹ </a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/normal.html">1.5.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/history.html">1.5.2. å†å²</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/algorithm.html">1.7. ç›¸å…³ç®—æ³•</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/tool.html">1.8. å·¥å…·</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/question.html">1.9. å¸¸è§é—®é¢˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. æœºå™¨äººé¢†åŸŸ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../theory.html">2. ç†è®º</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/tmp.html">2.1. ä¸´æ—¶</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/ReAct.html">2.1.1. ReActæ¡†æ¶</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Reflection.html">2.1.2. Reflectionåæ€</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/math.html">2.1.3. æ•°å­¦</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/bag-of-words.html">2.1.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/word2vec.html">2.1.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/doc2vec.html">2.1.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/FastText.html">2.1.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LDA.html">2.1.8. LDA-Latent Dirichlet Allocation(æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/overfitting-underfitting.html">2.1.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RAG.html">2.1.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Agent.html">2.1.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LLM.html">2.1.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RL.html">2.1.13. RL-å¼ºåŒ–å­¦ä¹ </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/prompt_engineering.html">2.1.14. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/finetune.html">2.1.15. LLMè°ƒä¼˜(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Workflow.html">2.1.16. Workflow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../LLM.html">3. å¤§æ¨¡å‹</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/normal.html">3.1. å¸¸ç”¨</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/normal.html">3.1.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/package.html">3.1.2. ä¾èµ–å®‰è£…</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/encoder.html">3.1.3. ç¼–ç -è§£ç å™¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/usage.html">3.1.4. ä½¿ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/tmp.html">3.1.5. ä¸´æ—¶</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/model.html">3.2. è‘—åæ¨¡å‹</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-æ¨ç†æ¨¡å‹</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/finetune.html">3.3. è°ƒä¼˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. æ¨¡å‹é‡åŒ–(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/normal.html">3.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/GGUF.html">3.4.2. GGUF æ–‡ä»¶</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/fileformat.html">3.5. æ–‡ä»¶æ ¼å¼</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/normal.html">3.5.1. é€šç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/GGML.html">3.5.2. GGMLç³»åˆ—æ–‡ä»¶æ ¼å¼</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/normal.html">å¸¸ç”¨</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/openai.html">3.6. å•†ä¸šé¡¹ç›®</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/normal.html">3.6.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/prompt.html">3.7. Prompt æç¤ºè¯</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_chinese.html">3.7.1. ä¸­æ–‡</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/skill.html">3.7.3. ç¤ºä¾‹</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Android.html">3.8. Androidç‰ˆLLMç›¸å…³</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/normal.html">3.8.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Androidç‰ˆéƒ¨ç½²</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../RAG.html">4. RAGç›¸å…³</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/normal.html">5.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/preprocess.html">5.2. é¢„å¤„ç†</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/normal.html">5.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. å…³é”®è¯æå–</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. åˆ†è¯</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. æƒ…æ„Ÿåˆ†æ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. æ–‡æœ¬è¡¨ç¤º</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. æ³¨æ„åŠ›æœºåˆ¶</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. è¯­è¨€æ¨¡å‹</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/NER.html">5.3. NER-å‘½åå®ä½“è¯†åˆ«</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/normal.html">5.3.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/seq-label.html">5.3.2. åºåˆ—æ ‡æ³¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/history.html">5.3.4. å†å²</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/summary.html">5.4. æ€»ç»“-æ‘˜è¦</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/summarys/normal.html">5.4.1. é€šç”¨</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">6. å‡½æ•°åº“</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/normal.html">6.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Image.html">6.2. Imageå›¾åƒå¤„ç†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Video.html">6.3. Videoè§†é¢‘</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/normal.html">6.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/magic.html">6.4.2. é­”æ³•å‘½ä»¤ </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/display.html">6.4.3. displayå‡½æ•°</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/normal.html">6.6.1. é€šç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/Ndarray.html">6.6.2. Ndarray å¯¹è±¡</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/function.html">6.6.3. é€šç”¨å‡½æ•°</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/normal.html">6.7.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_subset.html">6.7.2. å®ä¾‹-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_analysis.html">6.7.3. å®ä¾‹-ç»Ÿè®¡åˆ†æ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_sql.html">6.7.4. åˆ©ç”¨pandaså®ç°SQLæ“ä½œ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_default_value.html">6.7.5. å®ä¾‹-ç¼ºå¤±å€¼çš„å¤„ç†</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_multi_index.html">6.7.6. å¤šå±‚ç´¢å¼•çš„ä½¿ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/practice.html">6.7.7. å®è·µ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Pandas/practices/practice_2012ObamaElect.html">å®è·µ-2012å¹´å¥¥å·´é©¬æ€»ç»Ÿè¿ä»»é€‰ä¸¾</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_input_output.html">6.7.8. API-è¾“å…¥è¾“å‡º</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/normal.html">6.8.1. åŸºæœ¬</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/install.html">6.8.2. å®‰è£…</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/example.html">6.8.5. å®ä¾‹</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/plot.html">æŠ˜çº¿å›¾plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/bar.html">æ¡å½¢å›¾bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/hist.html">ç›´æ–¹å›¾hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/scatter.html">æ•£ç‚¹å›¾scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/stackplot.html">é¢ç§¯å›¾stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/pie.html">é¥¼å›¾pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/box.html">ç®±å‹å›¾box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/multi.html">å¤šå›¾åˆå¹¶multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pylab.html">6.8.6. pylabå­åŒ…</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/SciPys/normal.html">6.9.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/normal.html">6.10.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/supervised.html">6.10.2. ç›‘ç£å­¦ä¹ </a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/sklearns/superviseds/glm.html">å¹¿ä¹‰çº¿æ€§æ¨¡å‹</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/unsupervised.html">6.10.3. æ— ç›‘ç£å­¦ä¹ </a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/normal.html">6.12.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/example.html">6.12.2. å®ä¾‹</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/struct.html">6.12.3. ä»£ç ç±»ç»“æ„</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Seaborns/normal.html">6.13.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/jieba.html">6.14. jiebaä¸­æ–‡åˆ†è¯</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/gensim.html">6.15. gensim: æ–‡æœ¬ä¸»é¢˜å»ºæ¨¡å’Œç›¸ä¼¼æ€§åˆ†æ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/normal.html">6.15.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/LAC.html">6.16. LAC-ç™¾åº¦è¯æ³•åˆ†æå·¥å…·</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../framework.html">7. å­¦ä¹ æ¡†æ¶</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../normal.html">7.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/normal.html">7.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/nn.html">7.2.2. nnæ¨¡å—</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../huggingface.html">7.3. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../normal.html">7.3.1. å¸¸ç”¨</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="../Transformers.html">7.3.2. Transformers</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="Transformers.html">Transformers</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Tokenizers_V0.13.3.html">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../collect.html">7.3.7. æ”¶é›†</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../collects/model.html">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../collects/blog_decoding-methods.html">åšæ–‡: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../vLLM.html">7.4. vLLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/normal.html">7.4.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/vLLM_doc.html">7.4.2. vLLMå®˜æ–¹æ–‡æ¡£</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../llama.cpp.html">7.5. llama.cppæ¡†æ¶</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/normal.html">7.5.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mxnet.html">7.7. mxnetåº“</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/ndarray.html">7.7.1. ndæ¨¡å—</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/gluon.html">7.7.2. gluonæ¨¡å—</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/autograd.html">7.7.3. autogradæ¨¡å—</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/normal.html">7.9.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/demo.html">7.9.2. å®ä¾‹</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/binary_classification.html">äºŒåˆ†ç±»é—®é¢˜</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/multiclass_classification.html">å¤šåˆ†ç±»é—®é¢˜</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/regression.html">å›å½’é—®é¢˜</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html">7.10. å…¶ä»–</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../website.html">8. å…³é”®ç½‘ç«™</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/ArXiv.html">8.3. ArXiv å­¦æœ¯è®ºæ–‡é¢„å°æœ¬å¹³å°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/video.html">8.4. è§†é¢‘ç›¸å…³</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/normal.html">8.5. é€šç”¨</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../practice.html">9. å®è·µ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/OCRs/normal.html">9.1.1. å¸¸ç”¨</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/normal.html">9.2.1. å¸¸ç”¨</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../opensource.html">10. å¼€æºé¡¹ç›®</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/normal.html">10.3. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/ui.html">10.4. UIç•Œé¢</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/finetune.html">10.5. è°ƒä¼˜</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/search.html">10.6. æœç´¢</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Tool.html">10.8. æ¨¡å‹æ¨ç†å¹³å°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-inference-accelerate.html">10.9. LLMæ¨ç†åŠ é€Ÿ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Evaluate.html">10.10. LLMè¯„ä¼°</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/platform.html">10.11. AIå¹³å°</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">11. æ•°æ®é›†</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/normal.html">11.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese.html">11.2. ä¸­æ–‡æ•°æ®é›†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese_image.html">11.3. ä¸­æ–‡å›¾ç‰‡ç›¸å…³æ•°æ®é›†</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/huggingface.html">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/website.html">11.5. æ•°æ®é›†ç›¸å…³ç½‘ç«™</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model.html">12. å¸¸è§æ¨¡å‹</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">13. å›¾å½¢&amp;è®¡ç®—åŠ é€ŸæŠ€æœ¯</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/normal.html">13.1. å¸¸ç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Evaluate.html">14. Evaluateè¯„æµ‹</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/normal.html">14.1. é€šç”¨</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E4%BC%A0%E7%BB%9FAI.html">15. ä¼ ç»ŸAI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">æ–°æºª-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../framework.html"><span class="section-number">7. </span>å­¦ä¹ æ¡†æ¶</a> &raquo;</li>
        
          <li><a href="../../huggingface.html"><span class="section-number">7.3. </span>huggingface</a> &raquo;</li>
        
          <li><a href="../Transformers.html"><span class="section-number">7.3.2. </span>Transformers</a> &raquo;</li>
        
      <li>Transformers 4.45.2</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/frameworks/huggingfaces/Transformers/Transformers_V4.45.2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">Transformers 4.45.2</a><ul>
<li><a class="reference internal" href="#tutorials">Tutorials</a><ul>
<li><a class="reference internal" href="#load-pretrained-instances-with-an-autoclass">Load pretrained instances with an AutoClass</a><ul>
<li><a class="reference internal" href="#autobackbone">AutoBackbone</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generation-with-llms">Generation with LLMs</a><ul>
<li><a class="reference internal" href="#generate-text">Generate text</a></li>
<li><a class="reference internal" href="#common-pitfalls">Common pitfalls</a><ul>
<li><a class="reference internal" href="#generated-output-is-too-short-long">Generated output is too short/long</a></li>
<li><a class="reference internal" href="#incorrect-generation-mode">Incorrect generation mode</a></li>
<li><a class="reference internal" href="#wrong-padding-side">Wrong padding side</a></li>
<li><a class="reference internal" href="#wrong-prompt">Wrong prompt</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#chatting-with-transformers">Chatting with Transformers</a><ul>
<li><a class="reference internal" href="#choosing-a-chat-model">Choosing a chat model</a></li>
<li><a class="reference internal" href="#id2">ç›¸å…³ä»£ç </a></li>
<li><a class="reference internal" href="#performance-memory-and-hardware">Performance, memory and hardware</a><ul>
<li><a class="reference internal" href="#memory-considerations">Memory considerations</a></li>
<li><a class="reference internal" href="#performance-considerations">Performance considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#task-guides">TASK GUIDES</a><ul>
<li><a class="reference internal" href="#computer-vision">COMPUTER VISION</a><ul>
<li><a class="reference internal" href="#image-to-image">Image-to-Image</a></li>
<li><a class="reference internal" href="#image-feature-extraction">Image Feature Extraction</a></li>
<li><a class="reference internal" href="#mask-generation">Mask Generation</a></li>
<li><a class="reference internal" href="#keypoint-detection">Keypoint Detection</a></li>
<li><a class="reference internal" href="#knowledge-distillation-for-computer-vision">Knowledge Distillation for Computer Vision</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal">MULTIMODAL</a></li>
<li><a class="reference internal" href="#generation">Generation</a><ul>
<li><a class="reference internal" href="#text-generation-strategies">Text generation strategies</a><ul>
<li><a class="reference internal" href="#default-text-generation-configuration">Default text generation configuration</a></li>
<li><a class="reference internal" href="#customize-text-generation">Customize text generation</a></li>
<li><a class="reference internal" href="#save-a-custom-decoding-strategy-with-your-model">Save a custom decoding strategy with your model</a></li>
<li><a class="reference internal" href="#streaming">Streaming</a></li>
<li><a class="reference internal" href="#watermarking">Watermarking</a></li>
<li><a class="reference internal" href="#decoding-strategies">Decoding strategies</a><ul>
<li><a class="reference internal" href="#greedy-search">Greedy Search</a></li>
<li><a class="reference internal" href="#contrastive-search">Contrastive search</a></li>
<li><a class="reference internal" href="#multinomial-sampling">Multinomial sampling</a></li>
<li><a class="reference internal" href="#beam-search-decoding">Beam-search decoding</a></li>
<li><a class="reference internal" href="#beam-search-multinomial-sampling">Beam-search multinomial sampling</a></li>
<li><a class="reference internal" href="#diverse-beam-search-decoding">Diverse beam search decoding</a></li>
<li><a class="reference internal" href="#speculative-decoding">Speculative Decoding</a></li>
<li><a class="reference internal" href="#universal-assisted-decoding">Universal Assisted Decoding</a></li>
<li><a class="reference internal" href="#dola-decoding">DoLa Decoding</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#best-practices-for-generation-with-cache">Best Practices for Generation with Cache</a><ul>
<li><a class="reference internal" href="#what-is-cache-and-why-we-should-care">What is Cache and why we should care</a></li>
<li><a class="reference internal" href="#generate-with-cache">Generate with Cache</a><ul>
<li><a class="reference internal" href="#quantized-cache">Quantized Cache</a></li>
<li><a class="reference internal" href="#offloaded-cache">Offloaded Cache</a></li>
<li><a class="reference internal" href="#static-cache">Static Cache</a></li>
<li><a class="reference internal" href="#offloaded-static-cache">Offloaded Static Cache</a></li>
<li><a class="reference internal" href="#sliding-window-cache">Sliding Window Cache</a></li>
<li><a class="reference internal" href="#sink-cache">Sink Cache</a></li>
<li><a class="reference internal" href="#encoder-decoder-cache">Encoder-Decoder Cache</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-specific-cache-classes">Model-specific Cache Classes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#prompting">Prompting</a><ul>
<li><a class="reference internal" href="#image-tasks-with-idefics">Image tasks with IDEFICS</a></li>
<li><a class="reference internal" href="#llm-prompting-guide">LLM prompting guide</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#developer-guides">Developer guides</a><ul>
<li><a class="reference internal" href="#use-fast-tokenizers-from-tokenizers">Use fast tokenizers from ğŸ¤— Tokenizers</a><ul>
<li><a class="reference internal" href="#loading-directly-from-the-tokenizer-object">Loading directly from the tokenizer object</a></li>
<li><a class="reference internal" href="#loading-from-a-json-file">Loading from a JSON file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#use-model-specific-apis">Use model-specific APIs</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#model">Model</a><ul>
<li><a class="reference internal" href="#model-heads">Model heads</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tokenizer">Tokenizer</a></li>
<li><a class="reference internal" href="#image-processor">Image processor</a></li>
<li><a class="reference internal" href="#backbone">Backbone</a></li>
<li><a class="reference internal" href="#feature-extractor">Feature extractor</a></li>
<li><a class="reference internal" href="#processor">Processor</a></li>
<li><a class="reference internal" href="#building-custom-models">Building custom models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#chat-templates">Chat Templates</a><ul>
<li><a class="reference internal" href="#introduce">Introduce</a></li>
<li><a class="reference internal" href="#how-do-i-use-chat-templates">How do I use chat templates</a></li>
<li><a class="reference internal" href="#id4">æ ¸å¿ƒå‚æ•°</a><ul>
<li><a class="reference internal" href="#add-generation-prompt">add_generation_prompt å‚æ•°</a></li>
<li><a class="reference internal" href="#continue-final-message">continue_final_message å‚æ•°</a></li>
<li><a class="reference internal" href="#tokenize">tokenize å‚æ•°</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-how-do-chat-templates-work">Advanced: How do chat templates work?</a></li>
<li><a class="reference internal" href="#advanced-adding-and-editing-chat-templates">Advanced: Adding and editing chat templates</a><ul>
<li><a class="reference internal" href="#how-do-i-create-a-chat-template">How do I create a chat template?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-template-writing-tips">Advanced: Template writing tips</a><ul>
<li><a class="reference internal" href="#trimming-whitespace">Trimming whitespace</a></li>
<li><a class="reference internal" href="#callable-functions">Callable functions</a></li>
<li><a class="reference internal" href="#compatibility-with-non-python-jinja">Compatibility with non-Python Jinja</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#trainer">Trainer</a></li>
<li><a class="reference internal" href="#export-to-onnx">Export to ONNX</a><ul>
<li><a class="reference internal" href="#exporting-a-transformers-model-to-onnx-with-cli">Exporting a ğŸ¤— Transformers model to ONNX with CLI</a></li>
<li><a class="reference internal" href="#exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime">Exporting a ğŸ¤— Transformers model to ONNX with optimum.onnxruntime</a></li>
<li><a class="reference internal" href="#exporting-a-model-with-transformers-onnx">Exporting a model with transformers.onnx</a></li>
</ul>
</li>
<li><a class="reference internal" href="#interoperability-with-gguf-files">Interoperability with GGUF files</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-methods">Quantization Methods</a><ul>
<li><a class="reference internal" href="#quantization">Quantization</a></li>
<li><a class="reference internal" href="#bitsandbytes">bitsandbytes</a></li>
<li><a class="reference internal" href="#gptq">GPTQ</a><ul>
<li><a class="reference internal" href="#exllama">ExLlama</a></li>
</ul>
</li>
<li><a class="reference internal" href="#awq">AWQ</a></li>
<li><a class="reference internal" href="#aqlm">AQLM</a></li>
<li><a class="reference internal" href="#quanto">Quanto</a></li>
<li><a class="reference internal" href="#eetq">EETQ</a></li>
<li><a class="reference internal" href="#hqq">HQQ</a></li>
<li><a class="reference internal" href="#fbgemm-fp8">FBGEMM FP8</a></li>
<li><a class="reference internal" href="#optimum">Optimum</a></li>
<li><a class="reference internal" href="#torchao">TorchAO</a></li>
<li><a class="reference internal" href="#compressed-tensors">Compressed Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-and-scalability">Performance and scalability</a><ul>
<li><a class="reference internal" href="#llm-inference-optimization">LLM inference optimization</a><ul>
<li><a class="reference internal" href="#static-kv-cache-and-torch-compile">Static kv-cache and torch.compile</a></li>
<li><a class="reference internal" href="#id5">Speculative decoding</a><ul>
<li><a class="reference internal" href="#prompt-lookup-decoding">Prompt lookup decoding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#attention-optimizations">Attention optimizations</a><ul>
<li><a class="reference internal" href="#flashattention-2">FlashAttention-2</a></li>
<li><a class="reference internal" href="#pytorch-scaled-dot-product-attention">PyTorch scaled dot product attention</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">Quantization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#efficient-training-techniques">Efficient training techniques</a><ul>
<li><a class="reference internal" href="#methods-and-tools-for-efficient-training-on-a-single-gpu">Methods and tools for efficient training on a single GPU</a><ul>
<li><a class="reference internal" href="#batch-size-choice">Batch size choice</a></li>
<li><a class="reference internal" href="#gradient-accumulation">Gradient Accumulation</a></li>
<li><a class="reference internal" href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li><a class="reference internal" href="#mixed-precision-training">Mixed precision training</a><ul>
<li><a class="reference internal" href="#fp16">fp16</a></li>
<li><a class="reference internal" href="#bf16">BF16</a></li>
<li><a class="reference internal" href="#tf32">TF32</a></li>
</ul>
</li>
<li><a class="reference internal" href="#flash-attention-2">Flash Attention 2</a></li>
<li><a class="reference internal" href="#optimizer-choice">Optimizer choice</a></li>
<li><a class="reference internal" href="#data-preloading">Data preloading</a></li>
<li><a class="reference internal" href="#deepspeed-zero">DeepSpeed ZeRO</a></li>
<li><a class="reference internal" href="#using-torch-compile">Using torch.compile</a></li>
<li><a class="reference internal" href="#using-peft">Using ğŸ¤— PEFT</a></li>
<li><a class="reference internal" href="#using-accelerate">Using ğŸ¤— Accelerate</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multiple-gpus-and-parallelism">Multiple GPUs and parallelism</a></li>
<li><a class="reference internal" href="#fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li>
<li><a class="reference internal" href="#deepspeed">DeepSpeed</a></li>
<li><a class="reference internal" href="#efficient-training-on-cpu">Efficient Training on CPU</a></li>
<li><a class="reference internal" href="#distributed-cpu-training">Distributed CPU training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizing-inference">Optimizing inference</a><ul>
<li><a class="reference internal" href="#cpu-inference">CPU inference</a><ul>
<li><a class="reference internal" href="#id10">ğŸ¤— Optimum</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu-inference">GPU inference</a><ul>
<li><a class="reference internal" href="#id11">FlashAttention-2</a></li>
<li><a class="reference internal" href="#bettertransformer">BetterTransformer</a></li>
<li><a class="reference internal" href="#id12">bitsandbytes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#others">Others</a><ul>
<li><a class="reference internal" href="#optimize-inference-using-torch-compile">Optimize inference using torch.compile()</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#conceptual-guides">Conceptual guides</a><ul>
<li><a class="reference internal" href="#glossary">Glossary</a><ul>
<li><a class="reference internal" href="#dataparallel-dp">DataParallel (DP)</a></li>
<li><a class="reference internal" href="#pipelineparallel-pp">PipelineParallel (PP)</a></li>
<li><a class="reference internal" href="#tensor-parallelism-tp">Tensor Parallelism (TP)</a></li>
<li><a class="reference internal" href="#tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism">Tensor Parallelism vs. Data Parallelism vs. Pipeline Parallelism</a></li>
<li><a class="reference internal" href="#zero-redundancy-optimizer-zero">Zero Redundancy Optimizer (ZeRO)</a></li>
<li><a class="reference internal" href="#id13">Fully Sharded Data Parallel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizing-llms-for-speed-and-memory">Optimizing LLMs for Speed and Memory</a><ul>
<li><a class="reference internal" href="#lower-precision">1. Lower Precision</a></li>
<li><a class="reference internal" href="#flash-attention">2. Flash Attention</a></li>
<li><a class="reference internal" href="#architectural-innovations">3. Architectural Innovations</a><ul>
<li><a class="reference internal" href="#improving-positional-embeddings-of-llms">3.1 Improving positional embeddings of LLMs</a></li>
<li><a class="reference internal" href="#the-key-value-cache">3.2 The key-value cache</a><ul>
<li><a class="reference internal" href="#multi-round-conversation">3.2.1 Multi-round conversation</a></li>
<li><a class="reference internal" href="#multi-query-attention-mqa">3.2.2 Multi-Query-Attention (MQA)</a></li>
<li><a class="reference internal" href="#grouped-query-attention-gqa">3.2.3 Grouped-Query-Attention (GQA)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api">API</a><ul>
<li><a class="reference internal" href="#main-classes">Main Classes</a><ul>
<li><a class="reference internal" href="#auto-classes">Auto Classes</a></li>
<li><a class="reference internal" href="#id14">Backbone</a></li>
<li><a class="reference internal" href="#callbacks">Callbacks</a></li>
<li><a class="reference internal" href="#id15">Configuration</a></li>
<li><a class="reference internal" href="#data-collator">Data Collator</a></li>
<li><a class="reference internal" href="#logging">Logging</a></li>
<li><a class="reference internal" href="#models">Models</a></li>
<li><a class="reference internal" href="#text-generation">Text Generation</a></li>
<li><a class="reference internal" href="#onnx">ONNX</a></li>
<li><a class="reference internal" href="#optimization">Optimization</a></li>
<li><a class="reference internal" href="#model-outputs">Model outputs</a></li>
<li><a class="reference internal" href="#pipelines">Pipelines</a></li>
<li><a class="reference internal" href="#processors">Processors</a><ul>
<li><a class="reference internal" href="#multi-modal-processors">Multi-modal processors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id16">Quantization</a></li>
<li><a class="reference internal" href="#id17">Tokenizer</a></li>
<li><a class="reference internal" href="#id18">Trainer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id19">Models</a><ul>
<li><a class="reference internal" href="#text-models">Text models</a><ul>
<li><a class="reference internal" href="#qwen2">Qwen2</a></li>
<li><a class="reference internal" href="#qwen2-vl">Qwen2_VL</a></li>
<li><a class="reference internal" href="#cpm">CPM</a></li>
<li><a class="reference internal" href="#dpr">DPR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#fromgpt">å…¶ä»–-fromGPT</a></li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">ä¸»é¡µ</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">æ¨¡å—ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">æœç´¢é¡µé¢</span></a></p></td>
</tr>
</tbody>
</table>
<section id="transformers-4-45-2">
<h1>Transformers 4.45.2<a class="headerlink" href="#transformers-4-45-2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h1>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>æœ¬æ–‡æ¡£æ˜¯è‡ª <code class="docutils literal notranslate"><span class="pre">v4.23.1</span></code> ç‰ˆæœ¬åˆ°æœ¬ç‰ˆæœ¬çš„å˜åŠ¨éƒ¨åˆ†</p>
</div>
<ul class="simple">
<li><p>from: <a class="reference external" href="https://huggingface.co/docs/transformers/v4.45.2/en/index">https://huggingface.co/docs/transformers/v4.45.2/en/index</a></p></li>
</ul>
<section id="tutorials">
<h2>Tutorials<a class="headerlink" href="#tutorials" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="load-pretrained-instances-with-an-autoclass">
<h3>Load pretrained instances with an AutoClass<a class="headerlink" href="#load-pretrained-instances-with-an-autoclass" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>å…¶ä»– AutoXXX å‚è§åŸå§‹transformersæ–‡æ¡£</p>
</div>
<section id="autobackbone">
<h4>AutoBackbone<a class="headerlink" href="#autobackbone" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<figure class="align-default" id="id21">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/KqpDhA.png" src="https://img.zhaoweiguo.com/uPic/2024/10/KqpDhA.png" />
<figcaption>
<p><span class="caption-text">A Swin backbone with multiple stages for outputting a feature map.</span><a class="headerlink" href="#id21" title="æ­¤å›¾åƒçš„æ°¸ä¹…é“¾æ¥">Â¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>AutoBackbone å…è®¸æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºä¸»å¹²ï¼Œä»ä¸»å¹²çš„ä¸åŒé˜¶æ®µè·å–ç‰¹å¾å›¾ã€‚</p></li>
<li><dl class="simple">
<dt>from_pretrained() å‡½æ•°æœ‰ä¸¤ä¸ªå‚æ•°:</dt><dd><ul>
<li><p>out_indices æ˜¯è¦ä»ä¸­è·å–ç‰¹å¾å›¾çš„å±‚çš„ç´¢å¼•</p></li>
<li><p>out_features æ˜¯è¦ä»ä¸­è·å–ç‰¹å¾å›¾çš„å›¾å±‚çš„åç§°</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<figure class="align-default" id="id22">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/8tHkPG.png" src="https://img.zhaoweiguo.com/uPic/2024/10/8tHkPG.png" />
<figcaption>
<p><span class="caption-text">A feature map from the first stage of the backbone. The patch partition refers to the model stem.</span><a class="headerlink" href="#id22" title="æ­¤å›¾åƒçš„æ°¸ä¹…é“¾æ¥">Â¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoBackbone</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/swin-tiny-patch4-window7-224&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoBackbone</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/swin-tiny-patch4-window7-224&quot;</span><span class="p">,</span> <span class="n">out_indices</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">feature_maps</span>

<span class="o">&gt;&gt;</span> <span class="nb">list</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="generation-with-llms">
<h3>Generation with LLMs<a class="headerlink" href="#generation-with-llms" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<p>å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="n">bitsandbytes</span><span class="o">&gt;=</span><span class="mf">0.39.0</span> <span class="o">-</span><span class="n">q</span>
</pre></div>
</div>
<section id="generate-text">
<h4>Generate text<a class="headerlink" href="#generate-text" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é’ˆå¯¹ <code class="docutils literal notranslate"><span class="pre">causal</span> <span class="pre">language</span> <span class="pre">modeling</span></code> è¿›è¡Œè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å°†ä¸€ç³»åˆ—æ–‡æœ¬æ ‡è®°ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒã€‚</p></li>
</ul>
<figure>
    <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/genLLM1.mov" width="620"></video>
    <figcaption style="display: block; margin-top: 10px; text-align: center;">ğŸ”µ"Forward pass of an LLM"</figcaption>
</figure><ul class="simple">
<li><p>ä»¥è¿­ä»£æ–¹å¼é‡å¤ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶ç”±æ¨¡å‹å†³å®šï¼Œæ¨¡å‹åº”è¯¥å­¦ä¹ ä½•æ—¶è¾“å‡ºåºåˆ—ç»“æŸ ï¼ˆEOSï¼‰ ä»¤ç‰Œã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œåˆ™å½“è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶ï¼Œç”Ÿæˆå°†åœæ­¢ã€‚</p></li>
</ul>
<figure>
    <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/gen_LLM2.mov" width="620"></video>
    <figcaption style="display: block; margin-top: 10px; text-align: center;">ğŸ”µ"Autoregressive generation iteratively selects the next token from a probability distribution to generate text"</figcaption>
</figure><p>åŠ è½½æ¨¡å‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    &quot;mistralai/Mistral-7B-v0.1&quot;, device_map=&quot;auto&quot;, load_in_4bit=True
)

è¯´æ˜:
device_map ç¡®ä¿å°†æ¨¡å‹ç§»åŠ¨åˆ°æ‚¨çš„ GPU
load_in_4bit åº”ç”¨ 4 ä½åŠ¨æ€é‡åŒ–ï¼Œå¤§å¹…é™ä½èµ„æºéœ€æ±‚
</pre></div>
</div>
<p>preprocess your text input with a tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A list of colors: red, blue&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>call the generate() method to returns the generated tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># &#39;A list of colors: red, blue, green, yellow, orange, purple, pink,&#39;</span>
</pre></div>
</div>
<p>æ‰¹å¤„ç†ï¼Œè¿™å°†ä»¥è¾ƒå°çš„å»¶è¿Ÿå’Œå†…å­˜æˆæœ¬å¤§å¤§æé«˜ååé‡:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;A list of colors: red, blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Portugal is&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># [&#39;A list of colors: red, blue, green, yellow, orange, purple, pink,&#39;,</span>
<span class="c1">#    &#39;Portugal is a country in southwestern Europe, on the Iber&#39;]</span>
</pre></div>
</div>
</section>
<section id="common-pitfalls">
<h4>Common pitfalls<a class="headerlink" href="#common-pitfalls" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>ç”Ÿæˆç­–ç•¥æœ‰å¾ˆå¤šï¼Œæœ‰æ—¶é»˜è®¤å€¼å¯èƒ½ä¸é€‚åˆæ‚¨çš„ä½¿ç”¨æ¡ˆä¾‹</p>
</div>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="generated-output-is-too-short-long">
<h5>Generated output is too short/long<a class="headerlink" href="#generated-output-is-too-short-long" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å¦‚æœæœªåœ¨ GenerationConfig æ–‡ä»¶ä¸­æŒ‡å®šï¼Œåˆ™ generate é»˜è®¤æœ€å¤šè¿”å› 20 ä¸ªä»¤ç‰Œ</p></li>
<li><p>å¼ºçƒˆå»ºè®®åœ¨ generate è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½® max_new_tokens ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„æœ€å¤§æ–°ä»¤ç‰Œæ•°ã€‚</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>LLMsï¼ˆæ›´å‡†ç¡®åœ°è¯´ï¼Œä»…è§£ç å™¨æ¨¡å‹ï¼‰ä¹Ÿä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚Keep in mind LLMs (more precisely, decoder-only models) also return the input prompt as part of the output.</p>
</div>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A sequence of numbers: 1, 2&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># By default, the output will contain up to 20 tokens</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º</span>
<span class="s1">&#39;A sequence of numbers: 1, 2, 3, 4, 5&#39;</span>


<span class="c1"># Setting `max_new_tokens` allows you to control the maximum length</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º</span>
<span class="s1">&#39;A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,&#39;</span>
</pre></div>
</div>
</section>
<section id="incorrect-generation-mode">
<h5>Incorrect generation mode<a class="headerlink" href="#incorrect-generation-mode" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨ GenerationConfig æ–‡ä»¶ä¸­æŒ‡å®šï¼Œå¦åˆ™ generate åœ¨æ¯æ¬¡è¿­ä»£ï¼ˆè´ªå©ªè§£ç ï¼‰æ—¶é€‰æ‹©æœ€å¯èƒ½çš„ tokenã€‚</p></li>
<li><p>æ ¹æ®æ‚¨çš„ä»»åŠ¡ï¼Œé€‰æ‹©ä¸åŒçš„æ–¹æ³•</p></li>
<li><p>åƒèŠå¤©æœºå™¨äººæˆ–å†™è®ºæ–‡è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡é€‚åˆæŒ‡å®š <code class="docutils literal notranslate"><span class="pre">do_sample=True</span></code></p></li>
<li><p>è€ŒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘ç­‰åŸºäºè¾“å…¥çš„ä»»åŠ¡å—ç›Šäºè´ªå©ªè§£ç ã€‚</p></li>
<li><p>ç›¸å…³åšå®¢æ–‡ç« : <a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seed for reproducibility -- you don&#39;t need this unless you want full reproducibility</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;I am a cat.&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># LLM + greedy decoding = repetitive, boring output</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º:</span>
<span class="s1">&#39;I am a cat. I am a cat. I am a cat. I am a cat&#39;</span>


<span class="c1"># With sampling, the output becomes more creative!</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º</span>
<span class="s1">&#39;I am a cat.  Specifically, I am an indoor-only cat.  I&#39;</span>
</pre></div>
</div>
</section>
<section id="wrong-padding-side">
<h5>Wrong padding side<a class="headerlink" href="#wrong-padding-side" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>LLMs are decoder-only architectures, meaning they continue to iterate on your input prompt.</p></li>
<li><p>If your inputs do not have the same length, they need to be padded.(ä¸‹é¢ç¤ºä¾‹é‡Œé¢çš„123å’ŒABCDEé•¿åº¦ä¸åŒ)</p></li>
<li><p>Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded.</p></li>
<li><p>Make sure you also donâ€™t forget to pass the attention mask to generate!</p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The tokenizer initialized above has right-padding active by default: the 1st sequence,</span>
<span class="c1"># which is shorter, has padding on the right side. Generation fails to capture the logic.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)</span>  <span class="c1"># é»˜è®¤æ˜¯right</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;1, 2, 3&quot;</span><span class="p">,</span> <span class="s2">&quot;A, B, C, D, E&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º</span>
<span class="c1"># &#39;1, 2, 33333333333&#39;</span>



<span class="c1"># With left-padding, it works as expected!</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;1, 2, 3&quot;</span><span class="p">,</span> <span class="s2">&quot;A, B, C, D, E&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># è¾“å‡º</span>
<span class="c1"># &#39;1, 2, 3, 4, 5, 6,&#39;</span>
</pre></div>
</div>
</section>
<section id="wrong-prompt">
<h5>Wrong prompt<a class="headerlink" href="#wrong-prompt" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>æŸäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦æŸç§è¾“å…¥æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚</p></li>
<li><p>æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦å°å¿ƒï¼Œè¯·å‚é˜… <code class="docutils literal notranslate"><span class="pre">Task</span> <span class="pre">Guides</span> <span class="pre">-&gt;</span> <span class="pre">Prompting</span> <span class="pre">-&gt;</span> <span class="pre">LLM</span> <span class="pre">prompting</span> <span class="pre">guide</span></code> ã€‚</p></li>
<li><p>ä¸‹é¢çœ‹ä¸€ä¸ª <code class="docutils literal notranslate"><span class="pre">chat</span> <span class="pre">templating</span></code> ä¾‹å­(ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">tokenizer.apply_chat_template()</span></code> å‡½æ•°)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;How many helicopters can a human eat in one sitting? Reply as a thug.&quot;&quot;&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">input_length</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">input_length</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># è¾“å‡º</span>
<span class="c1"># &quot;I&#39;m not a thug, but i can tell you that a human cannot eat&quot;</span>
<span class="c1"># è¯´æ˜</span>
<span class="c1"># Oh no, it did not follow our instruction to reply as a thug!</span>


<span class="c1"># write a better prompt and use the right template for this model</span>
<span class="c1"># (through `tokenizer.apply_chat_template`)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a friendly chatbot who always responds in the style of a thug&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;How many helicopters can a human eat in one sitting?&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">input_length</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">input_length</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># è¾“å‡º</span>
<span class="c1"># &#39;None, you thug. How bout you try to focus on more useful questions?&#39;</span>
<span class="c1"># è¯´æ˜</span>
<span class="c1"># As we can see, it followed a proper thug style ğŸ˜</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="chatting-with-transformers">
<h3>Chatting with Transformers<a class="headerlink" href="#chatting-with-transformers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="choosing-a-chat-model">
<h4>Choosing a chat model<a class="headerlink" href="#choosing-a-chat-model" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>â€œ8Bâ€æˆ–â€œ70Bâ€ã€‚è¿™æ˜¯æ¨¡å‹ä¸­çš„å‚æ•°æ•°ã€‚å¦‚æœæ²¡æœ‰é‡åŒ–ï¼Œæ¯ä¸ªå‚æ•°å¤§çº¦éœ€è¦ 2 å­—èŠ‚çš„å†…å­˜ã€‚è¿™æ„å‘³ç€å…·æœ‰ 80 äº¿ä¸ªå‚æ•°çš„â€œ8Bâ€æ¨¡å‹å°†éœ€è¦å¤§çº¦ 16GB çš„å†…å­˜æ¥é€‚åº”å‚æ•°ï¼Œå†åŠ ä¸Šä¸€äº›é¢å¤–çš„å…¶ä»–å¼€é”€ã€‚å®ƒéå¸¸é€‚åˆå…·æœ‰ 24GB æ˜¾å­˜</p></li>
<li><p>â€œMixed of Expertsâ€ æ¨¡å‹ã€‚è¿™äº›å¯èƒ½ä¼šä»¥ä¸åŒçš„æ–¹å¼åˆ—å‡ºå®ƒä»¬çš„å°ºå¯¸ï¼Œä¾‹å¦‚â€œ8x7Bâ€æˆ–â€œ141B-A35Bâ€ã€‚è¿™é‡Œçš„æ•°å­—æœ‰ç‚¹æ¨¡ç³Šï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸ºæ¨¡å‹åœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹å¤§çº¦æœ‰ 56 ï¼ˆ8x7ï¼‰ äº¿ä¸ªå‚æ•°ï¼Œåœ¨ç¬¬äºŒç§æƒ…å†µä¸‹æœ‰ 1410 äº¿ä¸ªå‚æ•°ã€‚</p></li>
</ul>
</section>
<section id="id2">
<h4>ç›¸å…³ä»£ç <a class="headerlink" href="#id2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Prepare the input as before</span>
<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hey, can you tell me any fun things to do in New York?&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># 1: Load the model and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">)</span>

<span class="c1"># 2: Apply the chat template</span>
<span class="n">formatted_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Formatted chat:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">formatted_chat</span><span class="p">)</span>

<span class="c1"># 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">formatted_chat</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Move the tokenized inputs to the same device the model is on (GPU/CPU)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized inputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="c1"># 4: Generate text from the model</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated tokens:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="c1"># 5: Decode the output back to a string</span>
<span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded output:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">decoded_output</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-memory-and-hardware">
<h4>Performance, memory and hardware<a class="headerlink" href="#performance-memory-and-hardware" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="memory-considerations">
<h5>Memory considerations<a class="headerlink" href="#memory-considerations" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å¤§å¤šæ•°ç°ä»£è¯­è¨€æ¨¡å‹éƒ½ä»¥â€œbfloat16â€ç²¾åº¦è¿›è¡Œè®­ç»ƒï¼Œæ¯ä¸ªå‚æ•°ä»…ä½¿ç”¨ 2 ä¸ªå­—èŠ‚ï¼Œè€Œä¸ä½¿ç”¨å  4 ä¸ªå­—èŠ‚çš„float32</p></li>
<li><p>ä½¿ç”¨ â€œquantizationâ€ å¯ä»¥é™ä½åˆ° 16 ä½ä»¥ä¸‹ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æŸå‹ç¼©æ¨¡å‹æƒé‡çš„æ–¹æ³•ã€‚è¿™å…è®¸å°†æ¯ä¸ªå‚æ•°å‹ç¼©åˆ° 8 ä½ã€4 ä½ç”šè‡³æ›´å°‘ã€‚</p></li>
</ul>
<p>é‡åŒ–:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># You can also try load_in_4bit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-considerations">
<h5>Performance considerations<a class="headerlink" href="#performance-considerations" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>èŠå¤©æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ç›¸å¯¹ä¸åŒï¼Œå› ä¸ºå®ƒçš„ç“¶é¢ˆæ˜¯ <strong>å†…å­˜å¸¦å®½</strong> è€Œä¸æ˜¯ <strong>è®¡ç®—èƒ½åŠ›</strong> ï¼Œå› ä¸ºå®ƒå¿…é¡»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ª token ä»å†…å­˜ä¸­è¯»å–æ¯ä¸€ä¸ªactive parameterã€‚è¿™æ„å‘³ç€æ‚¨æ¯ç§’å¯ä»¥ä»èŠå¤©æ¨¡å‹ç”Ÿæˆçš„ token æ•°é‡é€šå¸¸ä¸å®ƒè¿™ä¸ªè¡¨è¾¾å¼æˆæ­£æ¯”ï¼š<code class="docutils literal notranslate"><span class="pre">å†…å­˜æ€»å¸¦å®½é™¤ä»¥æ¨¡å‹çš„å¤§å°</span></code> ã€‚ä¸€ä¸ª8Bçš„æ¨¡å‹ï¼Œä»¥ bfloat16 ç²¾åº¦åŠ è½½æ—¶ï¼Œæ¨¡å‹å¤§å°ä¸º ~16GBã€‚è¿™æ„å‘³ç€å¿…é¡»ä¸ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ªä»¤ç‰Œä»å†…å­˜ä¸­è¯»å– 16GBã€‚</p></li>
<li><p>æ€»å†…å­˜å¸¦å®½ä»æ¶ˆè´¹ç±» CPU çš„ <code class="docutils literal notranslate"><span class="pre">20-100GB/ç§’</span></code> åˆ°æ¶ˆè´¹ç±» GPUã€Intel Xeonã€AMD Threadripper/Epyc æˆ–é«˜ç«¯ Apple Silicon ç­‰ä¸“ç”¨ CPU çš„ <code class="docutils literal notranslate"><span class="pre">200-900GB/ç§’</span></code> ä¸ç­‰ï¼Œæœ€åé«˜è¾¾ <code class="docutils literal notranslate"><span class="pre">2-3TB/ç§’</span></code> çš„æ•°æ®ä¸­å¿ƒ GPUï¼Œå¦‚ Nvidia A100 æˆ– H100ã€‚è¿™åº”è¯¥å¯ä»¥è®©æ‚¨å¾ˆå¥½åœ°äº†è§£è¿™äº›ä¸åŒç¡¬ä»¶ç±»å‹çš„ç”Ÿæˆé€Ÿåº¦ã€‚</p></li>
<li><p>assisted generationçš„å˜ä½“ï¼šä¹Ÿç§°ä¸º â€œæ¨æµ‹æ€§é‡‡æ ·(speculative sampling)â€ï¼Œé€šå¸¸ä½¿ç”¨è¾ƒå°çš„â€œè‰ç¨¿æ¨¡å‹(draft model)â€å°è¯•ä¸€æ¬¡çŒœæµ‹å¤šä¸ªæœªæ¥çš„ tokenï¼Œç„¶åç”¨èŠå¤©æ¨¡å‹ç¡®è®¤è¿™äº›generationsã€‚å¦‚æœé€šè¿‡èŠå¤©æ¨¡å‹éªŒè¯äº†çŒœæµ‹ç»“æœï¼Œåˆ™æ¯æ¬¡forward passå¯ä»¥ç”Ÿæˆå¤šä¸ª Tokenï¼Œå¤§å¤§ç¼“è§£äº†å¸¦å®½ç“¶é¢ˆï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚</p></li>
<li><p>MoE æ¨¡å‹ï¼šå‡ ç§æµè¡Œçš„èŠå¤©æ¨¡å‹ï¼Œå¦‚ Mixtralã€Qwen-MoE å’Œ DBRXï¼Œéƒ½æ˜¯ MoE æ¨¡å‹ã€‚åœ¨è¿™äº›æ¨¡å‹ä¸­ï¼Œå¹¶éæ¯ä¸ªå‚æ•°å¯¹äºç”Ÿæˆçš„æ¯ä¸ª Token éƒ½å¤„äºæ´»åŠ¨çŠ¶æ€ã€‚å› æ­¤ï¼ŒMoE æ¨¡å‹é€šå¸¸å…·æœ‰ä½å¾—å¤šçš„å†…å­˜å¸¦å®½è¦æ±‚ï¼Œå³ä½¿å®ƒä»¬çš„æ€»å¤§å°å¯èƒ½ç›¸å½“å¤§ã€‚å› æ­¤ï¼Œå®ƒä»¬å¯ä»¥æ¯”ç›¸åŒå¤§å°çš„æ™®é€š â€œå¯†é›†â€ æ¨¡å‹å¿«å‡ å€ã€‚ç„¶è€Œï¼Œåƒè¾…åŠ©ç”Ÿæˆ(assisted generation)è¿™æ ·çš„æŠ€æœ¯é€šå¸¸å¯¹è¿™äº›æ¨¡å‹æ— æ•ˆï¼Œå› ä¸ºæ¯ä¸ªæ–°çš„æ¨æµ‹ä»¤ç‰Œéƒ½ä¼šæœ‰æ›´å¤šçš„å‚æ•°å˜å¾—æ´»è·ƒï¼Œè¿™å°†æŠµæ¶ˆ MoE æ¶æ„æä¾›çš„å¸¦å®½å’Œé€Ÿåº¦ä¼˜åŠ¿ã€‚</p></li>
</ul>
</section>
</section>
</section>
</section>
<section id="task-guides">
<h2>TASK GUIDES<a class="headerlink" href="#task-guides" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="computer-vision">
<h3>COMPUTER VISION<a class="headerlink" href="#computer-vision" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="image-to-image">
<h4>Image-to-Image<a class="headerlink" href="#image-to-image" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>image enhancement (super resolution, low light enhancement, deraining and so on)
    å›¾åƒå¢å¼º(è¶…åˆ†è¾¨ç‡ã€å¼±å…‰å¢å¼ºã€å»æ±¡ç­‰)
image inpainting
    å›¾åƒä¿®å¤
</pre></div>
</div>
</section>
<section id="image-feature-extraction">
<h4>Image Feature Extraction<a class="headerlink" href="#image-feature-extraction" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="n">similarity</span>
    <span class="n">å›¾åƒç›¸ä¼¼åº¦</span>
<span class="n">image</span> <span class="n">retrieval</span>
    <span class="n">å›¾åƒæ£€ç´¢</span>
</pre></div>
</div>
<ul class="simple">
<li><p>remove the task-specific head (image classification, object detection etc) and get the features</p></li>
<li><p>These features are very useful on a higher level: edge detection, corner detection and so on.</p></li>
<li><p>They may also contain information about the real world (e.g. what a cat looks like) depending on how deep the model is.</p></li>
<li><p>Therefore, these outputs can be used to train new classifiers on a specific dataset.</p></li>
</ul>
</section>
<section id="mask-generation">
<h4>Mask Generation<a class="headerlink" href="#mask-generation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p><strong>Mask Generation</strong> ä¸ºå›¾åƒç”Ÿæˆè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰çš„æ©æ¨¡çš„ä»»åŠ¡ã€‚è¯¥ä»»åŠ¡ä¸å›¾åƒåˆ†å‰²éå¸¸ç›¸ä¼¼ï¼Œä½†å­˜åœ¨è®¸å¤šå·®å¼‚ã€‚å›¾åƒåˆ†å‰²æ¨¡å‹åœ¨æ ‡è®°æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå¹¶ä¸”ä»…é™äºå®ƒä»¬åœ¨è®­ç»ƒæœŸé—´çœ‹åˆ°çš„ç±»ï¼›ç»™å®šå›¾åƒï¼Œå®ƒä»¬è¿”å›ä¸€ç»„æ©ç å’Œç›¸åº”çš„ç±»ã€‚</p></li>
<li><p>Mask generation is the task of generating semantically meaningful masks for an image. This task is very similar to image segmentation, but many differences exist.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Image</span> <span class="pre">segmentation</span></code> models are trained on labeled datasets and are limited to the classes they have seen during training; they return a set of masks and corresponding classes, given an image.</p></li>
</ul>
<p>Mask generation models are trained on large amounts of data and operate in two modes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Prompting mode:
    æ¨¡å‹æ¥æ”¶å›¾åƒå’Œæç¤ºï¼Œå…¶ä¸­æç¤ºå¯ä»¥æ˜¯å¯¹è±¡å†…å›¾åƒä¸­çš„ 2D ç‚¹ä½ç½®ï¼ˆXY åæ ‡ï¼‰æˆ–å¯¹è±¡å‘¨å›´çš„è¾¹ç•Œæ¡†ã€‚
    åœ¨æç¤ºæ¨¡å¼ä¸‹ï¼Œæ¨¡å‹ä»…è¿”å›æç¤ºæ‰€æŒ‡å‘çš„å¯¹è±¡ä¸Šçš„mask
2. Segment Everything mode:
    ç»™å®šä¸€å¼ å›¾åƒï¼Œæ¨¡å‹ä¼šç”Ÿæˆå›¾åƒä¸­çš„æ¯ä¸ªè’™ç‰ˆã€‚
    ä¸ºæ­¤ï¼Œå°†ç”Ÿæˆä¸€ä¸ªç‚¹ç½‘æ ¼å¹¶å°†å…¶å åŠ åœ¨å›¾åƒä¸Šä»¥è¿›è¡Œæ¨ç†ã€‚
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/hvrrET.png" src="https://img.zhaoweiguo.com/uPic/2024/10/hvrrET.png" />
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Point</span> <span class="n">Prompting</span>
<span class="n">Box</span> <span class="n">Prompting</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>å…·ä½“ç»†çœ‹æ–‡æ¡£å§(æœ‰ç©ºè¿è¡Œä¸€ä¸‹ç›¸å…³ä»£ç å†ç»†åˆ†æå§)</p>
</div>
</section>
<section id="keypoint-detection">
<h4>Keypoint Detection<a class="headerlink" href="#keypoint-detection" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>å…³é”®ç‚¹æ£€æµ‹è¯†åˆ«å¹¶å®šä½å›¾åƒä¸­çš„ç‰¹å®šå…´è¶£ç‚¹ã€‚è¿™äº›å…³é”®ç‚¹ä¹Ÿç§°ä¸ºåœ°æ ‡ï¼Œä»£è¡¨å¯¹è±¡çš„æœ‰æ„ä¹‰çš„ç‰¹å¾ï¼Œä¾‹å¦‚é¢éƒ¨ç‰¹å¾æˆ–å¯¹è±¡éƒ¨åˆ†ã€‚</p></li>
<li><p>Keypoint detection identifies and locates specific points of interest within an image. These keypoints, also known as landmarks, represent meaningful features of objects, such as facial features(é¢éƒ¨ç‰¹å¾) or object parts(å¯¹è±¡éƒ¨ä½).</p></li>
</ul>
<p>These models take an image input and return the following outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Keypoints and Scores:
    å…´è¶£ç‚¹åŠå…¶ç½®ä¿¡åº¦åˆ†æ•°
    Points of interest and their confidence scores.
2. Descriptors:
    æ¯ä¸ªå…³é”®ç‚¹å‘¨å›´çš„å›¾åƒåŒºåŸŸçš„è¡¨ç¤ºå½¢å¼ï¼Œæ•è·å…¶çº¹ç†ã€æ¸å˜ã€æ–¹å‘å’Œå…¶ä»–å±æ€§
    A representation of the image region surrounding each keypoint,
        capturing its texture, gradient, orientation and other properties.
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/KEzcue.png" src="https://img.zhaoweiguo.com/uPic/2024/11/KEzcue.png" />
</figure>
</section>
<section id="knowledge-distillation-for-computer-vision">
<h4>Knowledge Distillation for Computer Vision<a class="headerlink" href="#knowledge-distillation-for-computer-vision" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student).</p></li>
</ul>
</section>
</section>
<section id="multimodal">
<h3>MULTIMODAL<a class="headerlink" href="#multimodal" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>Visual Question Answering: åŸºäºå›¾åƒå›ç­”å¼€æ”¾å¼é—®é¢˜çš„ä»»åŠ¡ã€‚æ”¯æŒæ­¤ä»»åŠ¡çš„æ¨¡å‹çš„è¾“å…¥é€šå¸¸æ˜¯å›¾åƒå’Œé—®é¢˜çš„ç»„åˆï¼Œè¾“å‡ºæ˜¯ç”¨è‡ªç„¶è¯­è¨€è¡¨è¾¾çš„ç­”æ¡ˆã€‚</p></li>
<li><p>Image-text-to-tex: ä¹Ÿç§°ä¸ºè§†è§‰è¯­è¨€æ¨¡å‹ (VLM: vision language models)ï¼Œæ˜¯é‡‡ç”¨å›¾åƒè¾“å…¥çš„è¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œä»è§†è§‰é—®ç­”(visual question answering)åˆ°å›¾åƒåˆ†å‰²(image segmentation)ã€‚æ­¤ä»»åŠ¡ä¸å›¾åƒåˆ°æ–‡æœ¬(image-to-text)æœ‰è®¸å¤šç›¸ä¼¼ä¹‹å¤„ï¼Œå¹¶ä¸”åœ¨ä¸€äº›ä½¿ç”¨åœºæ™¯ä¸Šæœ‰é‡å ï¼Œå¦‚ï¼šå›¾åƒå­—å¹•(image captioning)ã€‚å›¾åƒåˆ°æ–‡æœ¬(Image-to-text)æ¨¡å‹ä»…æ¥å—å›¾åƒè¾“å…¥å¹¶ä¸”é€šå¸¸å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œè€Œ VLM æ¥å—å¼€æ”¾å¼æ–‡æœ¬å’Œå›¾åƒè¾“å…¥ï¼Œå¹¶ä¸”æ˜¯æ›´é€šç”¨çš„æ¨¡å‹ã€‚</p></li>
<li><p>Video-text-to-text: ä¹Ÿç§°ä¸ºè§†é¢‘è¯­è¨€æ¨¡å‹(video language models)æˆ–å…·æœ‰è§†é¢‘è¾“å…¥çš„è§†è§‰è¯­è¨€æ¨¡å‹(vision language models with video input)ï¼Œæ˜¯é‡‡ç”¨è§†é¢‘è¾“å…¥çš„è¯­è¨€æ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥å¤„ç†å„ç§ä»»åŠ¡ï¼Œä»è§†é¢‘é—®ç­”(video question answering)åˆ°è§†é¢‘å­—å¹•(video captioning)ã€‚</p></li>
</ul>
</section>
<section id="generation">
<h3>Generation<a class="headerlink" href="#generation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="text-generation-strategies">
<h4>Text generation strategies<a class="headerlink" href="#text-generation-strategies" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy that the generate() method will use.</p></li>
<li><p>é€‰æ‹©è¾“å‡ºtokenæ¥ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ç§°ä¸º decoding ï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰generate()æ–¹æ³•å°†ä½¿ç”¨çš„è§£ç ç­–ç•¥(decoding strategy)</p></li>
<li><p>ä¿®æ”¹è§£ç ç­–ç•¥ä¸ä¼šæ”¹å˜ä»»ä½•å¯è®­ç»ƒå‚æ•°çš„å€¼ã€‚ä½†æ˜¯ï¼Œå®ƒä¼šå¯¹ç”Ÿæˆçš„è¾“å‡ºçš„è´¨é‡äº§ç”Ÿæ˜¾ç€å½±å“ã€‚</p></li>
</ul>
<section id="default-text-generation-configuration">
<h5>Default text generation configuration<a class="headerlink" href="#default-text-generation-configuration" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>å½“æ‚¨æ˜¾å¼åŠ è½½æ¨¡å‹æ—¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡ <code class="docutils literal notranslate"><span class="pre">model.generation_config</span></code> æ£€æŸ¥æ¨¡å‹é™„å¸¦çš„ç”Ÿæˆé…ç½®:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">generation_config</span>
<span class="go">GenerationConfig {</span>
<span class="go">  &quot;bos_token_id&quot;: 50256,</span>
<span class="go">  &quot;eos_token_id&quot;: 50256</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>æ‰“å°model.generation_configä»…æ˜¾ç¤ºä¸é»˜è®¤ç”Ÿæˆé…ç½®ä¸åŒçš„å€¼ï¼Œå¹¶ä¸”ä¸åˆ—å‡ºä»»ä½•é»˜è®¤å€¼ã€‚</p>
</div>
<p>default generation configuration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">prompt</span> <span class="n">to</span> <span class="n">a</span> <span class="n">maximum</span><span class="p">:</span> <span class="mi">20</span> <span class="n">token</span>
<span class="n">default</span> <span class="n">decoding</span> <span class="n">strategy</span> <span class="ow">is</span> <span class="n">greedy</span> <span class="n">search</span>
</pre></div>
</div>
</section>
<section id="customize-text-generation">
<h5>Customize text generation<a class="headerlink" href="#customize-text-generation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>é€šè¿‡å°†å‚æ•°åŠå…¶å€¼ç›´æ¥ä¼ é€’ç»™generateæ–¹æ³•æ¥è¦†ç›–ä»»ä½•generation_config:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>ç»å¸¸è°ƒæ•´çš„å‚æ•°:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. max_new_tokens:
    è¦ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°(the maximum number of tokens to generate)
    è¾“å‡ºåºåˆ—çš„å¤§å°ï¼Œä¸åŒ…æ‹¬è¾“å…¥prompt
2. num_beams:
    é€šè¿‡æŒ‡å®šå¤§äº 1 çš„æ³¢æŸæ•°é‡ï¼Œæ‚¨å¯ä»¥æœ‰æ•ˆåœ°ä»è´ªå©ªæœç´¢åˆ‡æ¢åˆ°æ³¢æŸæœç´¢
3. do_sample:
    å¦‚æœè®¾ç½®ä¸ºTrue ï¼Œæ­¤å‚æ•°å¯ç”¨è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚å¤šé¡¹å¼é‡‡æ ·ã€æ³¢æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€Top-K é‡‡æ ·å’Œ Top-p é‡‡æ ·ã€‚
4. num_return_sequences:
    æ¯ä¸ªè¾“å…¥è¿”å›çš„åºåˆ—å€™é€‰æ•°
    è¯¥é€‰é¡¹ä»…é€‚ç”¨äºæ”¯æŒå¤šä¸ªåºåˆ—å€™é€‰çš„è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚æ³¢æŸæœç´¢(beam_search)å’Œ é‡‡æ ·(sampling)
    è´ªå©ªæœç´¢(greedy_search)å’Œå¯¹æ¯”æœç´¢(contrastive_search)ç­‰è§£ç ç­–ç•¥è¿”å›å•ä¸ªè¾“å‡ºåºåˆ—
</pre></div>
</div>
</section>
<section id="save-a-custom-decoding-strategy-with-your-model">
<h5>Save a custom decoding strategy with your model<a class="headerlink" href="#save-a-custom-decoding-strategy-with-your-model" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>specific generation configuration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GenerationConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;my_account/my_model&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;my_account/my_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>å¦‚æœæ‚¨æƒ³ä¸ºå•ä¸ªæ¨¡å‹å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ç§ç”¨äºé€šè¿‡é‡‡æ ·ç”Ÿæˆåˆ›æ„æ–‡æœ¬ï¼Œä¸€ç§ç”¨äºé€šè¿‡é›†æŸæœç´¢è¿›è¡Œæ‘˜è¦ï¼‰æ—¶ä¼šå¾ˆæœ‰ç”¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ä½¿ç”¨GenerationConfig.save_pretrained()ä¸­çš„config_file_nameå‚æ•°å°†å¤šä¸ªç”Ÿæˆé…ç½®å­˜å‚¨åœ¨å•ä¸ªç›®å½•ä¸­ã€‚</span>
<span class="c1"># ä½¿ç”¨GenerationConfig.from_pretrained()å®ä¾‹åŒ–å®ƒä»¬</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GenerationConfig</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-small&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-small&quot;</span><span class="p">)</span>

<span class="n">translation_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
    <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">early_stopping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># è¯´æ˜ï¼šé€šè¿‡æŒ‡ä»¤é…ç½®æ–‡ä»¶åæŠŠç›¸å…³é…ç½®å†™å…¥åˆ°æŒ‡å®šæ–‡ä»¶å’Œä»æŒ‡å®šæ–‡ä»¶åŠ è½½</span>
<span class="n">translation_generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;/tmp&quot;</span><span class="p">,</span> <span class="s2">&quot;translation_generation_config.json&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;/tmp&quot;</span><span class="p">,</span> <span class="s2">&quot;translation_generation_config.json&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;translate English to French: Configuration files are easy to use!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="streaming">
<h5>Streaming<a class="headerlink" href="#streaming" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>æµåª’ä½“ç±»çš„ API ä»åœ¨å¼€å‘ä¸­ï¼Œå°†æ¥å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="n">tok</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tok</span><span class="p">([</span><span class="s2">&quot;An increasing sequence: one,&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span>

<span class="c1"># Despite returning the usual output, the streamer will also print the generated text to stdout.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># è¾“å‡º:</span>
<span class="n">An</span> <span class="n">increasing</span> <span class="n">sequence</span><span class="p">:</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">,</span> <span class="n">three</span><span class="p">,</span> <span class="n">four</span><span class="p">,</span> <span class="n">five</span><span class="p">,</span> <span class="n">six</span><span class="p">,</span> <span class="n">seven</span><span class="p">,</span> <span class="n">eight</span><span class="p">,</span> <span class="n">nine</span><span class="p">,</span> <span class="n">ten</span><span class="p">,</span> <span class="n">eleven</span><span class="p">,</span>
</pre></div>
</div>
</section>
<section id="watermarking">
<h5>Watermarking<a class="headerlink" href="#watermarking" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>è®ºæ–‡: On the Reliability of Watermarks for Large Language Models: <a class="reference external" href="https://arxiv.org/abs/2306.04634">https://arxiv.org/abs/2306.04634</a></p></li>
</ul>
</section>
<section id="decoding-strategies">
<h5>Decoding strategies<a class="headerlink" href="#decoding-strategies" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†: <a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></p></li>
<li><p>å½±å“æ¨¡å‹çš„generate()ç»“æœæœ‰2</p></li>
<li><ol class="arabic simple">
<li><p>è§£ç ç­–ç•¥(decoding strategies)ä¸»è¦åŸºäº Logits(ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒ)ï¼Œå› æ­¤é€‰æ‹©ä¸€ä¸ªå¥½çš„ Logitsæ“ä½œç­–ç•¥(logits manipulation strategy)å¯ä»¥å¤§æœ‰å¸®åŠ©ï¼</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>é™¤äº†é€‰æ‹©è§£ç ç­–ç•¥ä¹‹å¤–ï¼Œæ“ä½œé€»è¾‘(manipulating the logits)æ˜¯æ‚¨å¯ä»¥é‡‡å–çš„å¦ä¸€ä¸ªæ–¹æ³•ã€‚æµè¡Œçš„ logits æ“ä½œç­–ç•¥åŒ…æ‹¬top_p ã€ min_på’Œrepetition_penalty</p></li>
</ol>
</li>
</ul>
<section id="greedy-search">
<h6>Greedy Search<a class="headerlink" href="#greedy-search" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>é»˜è®¤ä½¿ç”¨è´ªå©ªæœç´¢è§£ç ï¼Œå› æ­¤æ‚¨ä¸å¿…ä¼ é€’ä»»ä½•å‚æ•°æ¥å¯ç”¨å®ƒã€‚</p></li>
<li><p>è¿™æ„å‘³ç€å‚æ•°num_beamsè®¾ç½®ä¸º 1 ä¸”do_sample=False ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;I look forward to&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert/distilgpt2&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># è¾“å‡º</span>
<span class="p">[</span><span class="s1">&#39;I look forward to seeing you all again!</span><span class="se">\n\n\n\n\n\n\n\n\n\n\n</span><span class="s1">&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="contrastive-search">
<h6>Contrastive search<a class="headerlink" href="#contrastive-search" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>è®ºæ–‡A Contrastive Framework for Neural Text Generation: <a class="reference external" href="https://arxiv.org/abs/2202.06417">https://arxiv.org/abs/2202.06417</a></p></li>
<li><p>å¯¹æ¯”æœç´¢çš„å·¥ä½œåŸç†: <a class="reference external" href="https://huggingface.co/blog/introducing-csearch">https://huggingface.co/blog/introducing-csearch</a></p></li>
<li><p>å¯ç”¨å’Œæ§åˆ¶å¯¹æ¯”æœç´¢è¡Œä¸ºçš„ä¸¤ä¸ªä¸»è¦å‚æ•°æ˜¯penalty_alphaå’Œtop_k</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-large&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Hugging Face Company is&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># [&#39;Hugging Face Company is a family owned and operated business. .... We look forward to hearing from you!&#39;]</span>
</pre></div>
</div>
</section>
<section id="multinomial-sampling">
<h6>Multinomial sampling<a class="headerlink" href="#multinomial-sampling" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ä¸æ€»æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°çš„è´ªå©ªæœç´¢ç›¸åï¼Œå¤šé¡¹å¼é‡‡æ ·ï¼ˆä¹Ÿç§°ä¸ºç¥–å…ˆé‡‡æ ·ï¼‰æ ¹æ®æ¨¡å‹ç»™å‡ºçš„æ•´ä¸ªè¯æ±‡è¡¨çš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚</p></li>
<li><p>æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„ä»¤ç‰Œéƒ½æœ‰è¢«é€‰æ‹©çš„æœºä¼šï¼Œä»è€Œé™ä½äº†é‡å¤çš„é£é™©ã€‚</p></li>
<li><p>è®¾ç½®do_sample=Trueå’Œnum_beams=1</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-large&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Today was an amazing day because&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="beam-search-decoding">
<h6>Beam-search decoding<a class="headerlink" href="#beam-search-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œæ³¢æŸæœç´¢è§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿ç•™å¤šä¸ªå‡è®¾ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—æ€»ä½“æ¦‚ç‡æœ€é«˜çš„å‡è®¾ã€‚</p></li>
<li><p>è¿™æ ·åšçš„ä¼˜ç‚¹æ˜¯å¯ä»¥è¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å§‹çš„é«˜æ¦‚ç‡åºåˆ—ï¼Œå¹¶ä¸”ä¼šè¢«è´ªå©ªæœç´¢å¿½ç•¥ã€‚</p></li>
</ul>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/XNPDom.png" src="https://img.zhaoweiguo.com/uPic/2024/10/XNPDom.png" />
</figure>
<ul class="simple">
<li><p>äº¤äº’å¼æ¼”ç¤º: <a class="reference external" href="https://huggingface.co/spaces/m-ric/beam_search_visualizer">https://huggingface.co/spaces/m-ric/beam_search_visualizer</a></p></li>
<li><p>è¦å¯ç”¨æ­¤è§£ç ç­–ç•¥ï¼Œè¯·æŒ‡å®šå¤§äº 1 çš„num_beams ï¼ˆä¹Ÿç§°ä¸ºè¦è·Ÿè¸ªçš„å‡è®¾æ•°ï¼‰</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;It is astonishing how one can&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-medium&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="beam-search-multinomial-sampling">
<h6>Beam-search multinomial sampling<a class="headerlink" href="#beam-search-multinomial-sampling" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>è¿™ç§è§£ç ç­–ç•¥å°†æ³¢æŸæœç´¢ä¸å¤šé¡¹å¼é‡‡æ ·ç›¸ç»“åˆã€‚</p></li>
<li><p>æŒ‡å®šnum_beamså¤§äº 1ï¼Œå¹¶è®¾ç½®do_sample=Trueæ‰èƒ½ä½¿ç”¨æ­¤è§£ç ç­–ç•¥ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: The house is wonderful.&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google-t5/t5-small&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="diverse-beam-search-decoding">
<h6>Diverse beam search decoding<a class="headerlink" href="#diverse-beam-search-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>å¤šæ ·åŒ–æ³¢æŸæœç´¢è§£ç ç­–ç•¥æ˜¯æ³¢æŸæœç´¢ç­–ç•¥çš„æ‰©å±•ï¼Œå…è®¸ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„æ³¢æŸåºåˆ—é›†ä»¥ä¾›é€‰æ‹©ã€‚</p></li>
<li><p>å·¥ä½œåŸç†ï¼Œè¯·å‚é˜…Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models: <a class="reference external" href="https://arxiv.org/pdf/1610.02424.pdf">https://arxiv.org/pdf/1610.02424.pdf</a></p></li>
<li><p>è¯¥æ–¹æ³•å…·æœ‰ä¸‰ä¸ªä¸»è¦å‚æ•°ï¼š <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> ã€ <code class="docutils literal notranslate"><span class="pre">num_beam_groups</span></code> å’Œ <code class="docutils literal notranslate"><span class="pre">diversity_penalty</span></code></p></li>
<li><p>å¤šæ ·æ€§æƒ©ç½šç¡®ä¿è¾“å‡ºåœ¨ç»„ä¹‹é—´æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç»„å†…ä½¿ç”¨æ³¢æŸæœç´¢ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/pegasus-xsum&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;The Permaculture Design Principles are a set of universal design principles &quot;</span>
    <span class="o">...</span>
    <span class="s2">&quot;efficient way possible.&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="speculative-decoding">
<h6>Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>æ¨æµ‹è§£ç ï¼ˆä¹Ÿç§°ä¸ºè¾…åŠ©è§£ç ï¼‰æ˜¯ä¸Šè¿°è§£ç ç­–ç•¥çš„ä¸€ç§ä¿®æ”¹ï¼Œå®ƒä½¿ç”¨è¾…åŠ©æ¨¡å‹ï¼ˆæœ€å¥½æ˜¯æ›´å°çš„æ¨¡å‹ï¼‰æ¥ç”Ÿæˆä¸€äº›å€™é€‰æ ‡è®°ã€‚</p></li>
<li><p>ç„¶åï¼Œä¸»æ¨¡å‹åœ¨ä¸€æ¬¡å‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰æ ‡è®°ï¼Œä»è€ŒåŠ å¿«è§£ç è¿‡ç¨‹ã€‚</p></li>
<li><p>å¦‚æœdo_sample=True ï¼Œåˆ™ä½¿ç”¨æ¨æµ‹è§£ç è®ºæ–‡ä¸­å¼•å…¥çš„å¸¦æœ‰é‡é‡‡æ ·çš„ä»¤ç‰ŒéªŒè¯ã€‚</p></li>
<li><p>è¾…åŠ©è§£ç å‡è®¾ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ç›¸åŒçš„åˆ†è¯å™¨ï¼Œå¦åˆ™ï¼Œè¯·å‚é˜…ä¸‹é¢çš„é€šç”¨è¾…åŠ©è§£ç ã€‚</p></li>
<li><p>ç›®å‰è¾…åŠ©è§£ç ä»…æ”¯æŒè´ªå©ªæœç´¢å’Œé‡‡æ ·ï¼Œè¾…åŠ©è§£ç ä¸æ”¯æŒæ‰¹é‡è¾“å…¥ã€‚</p></li>
<li><p>è¦äº†è§£æœ‰å…³è¾…åŠ©è§£ç çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹æ­¤åšå®¢æ–‡ç« : <a class="reference external" href="https://huggingface.co/blog/assisted-generation">https://huggingface.co/blog/assisted-generation</a></p></li>
<li><p>è¦å¯ç”¨è¾…åŠ©è§£ç ï¼Œè¯·ä½¿ç”¨æ¨¡å‹è®¾ç½®assistant_modelå‚æ•°ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Alice and Bob&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/pythia-1.4b-deduped&quot;</span>
<span class="n">assistant_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/pythia-160m-deduped&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="universal-assisted-decoding">
<h6>Universal Assisted Decoding<a class="headerlink" href="#universal-assisted-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>é€šç”¨è¾…åŠ©è§£ç  (UAD) æ·»åŠ äº†å¯¹å…·æœ‰ä¸åŒæ ‡è®°å™¨çš„ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹çš„æ”¯æŒã€‚</p></li>
<li><p>è¦ä½¿ç”¨å®ƒï¼Œåªéœ€ä½¿ç”¨tokenizerå’Œassistant_tokenizerå‚æ•°ä¼ é€’æ ‡è®°å™¨ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚</p></li>
<li><p>åœ¨å†…éƒ¨ï¼Œä¸»æ¨¡å‹è¾“å…¥æ ‡è®°è¢«é‡æ–°ç¼–ç ä¸ºè¾…åŠ©æ¨¡å‹æ ‡è®°ï¼Œç„¶ååœ¨è¾…åŠ©ç¼–ç ä¸­ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œè¿™äº›å€™é€‰æ ‡è®°åˆè¢«é‡æ–°ç¼–ç ä¸ºä¸»æ¨¡å‹å€™é€‰æ ‡è®°ã€‚ç„¶åéªŒè¯æŒ‰ç…§ä¸Šé¢çš„è§£é‡Šè¿›è¡Œã€‚é‡æ–°ç¼–ç æ­¥éª¤æ¶‰åŠå°†ä»¤ç‰Œ ID è§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸åŒçš„ä»¤ç‰Œç”Ÿæˆå™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ç”±äºé‡æ–°ç¼–ç ä»¤ç‰Œå¯èƒ½ä¼šå¯¼è‡´ä»¤ç‰ŒåŒ–å·®å¼‚ï¼Œå› æ­¤ UAD ä¼šæ‰¾åˆ°æºç¼–ç å’Œç›®æ ‡ç¼–ç ä¹‹é—´çš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼Œä»¥ç¡®ä¿æ–°ä»¤ç‰ŒåŒ…å«æ­£ç¡®çš„æç¤ºåç¼€ã€‚</p></li>
<li><p>å¦‚æœä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ä¸åŒçš„æ ‡è®°å™¨ï¼Œè¯·ä½¿ç”¨é€šç”¨è¾…åŠ©è§£ç ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Alice and Bob&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/gemma-2-9b&quot;</span>
<span class="n">assistant_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;double7/vicuna-68m&quot;</span>

<span class="n">assistant_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">assistant_tokenizer</span><span class="o">=</span><span class="n">assistant_tokenizer</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dola-decoding">
<h6>DoLa Decoding<a class="headerlink" href="#dola-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>Decoding by Contrasting Layers (DoLa) æ˜¯ä¸€ç§å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œæ—¨åœ¨æé«˜äº‹å®æ€§å¹¶å‡å°‘LLMsçš„å¹»è§‰</p></li>
<li><p>å¦‚ ICLR 2024 DoLa çš„è®ºæ–‡æ‰€è¿°ï¼šDecoding by Contrasting Layers Improves Factuality in Large Language Models: <a class="reference external" href="https://arxiv.org/abs/2309.03883">https://arxiv.org/abs/2309.03883</a></p></li>
<li><p>DoLa æ˜¯é€šè¿‡å¯¹æ¯”æœ€ç»ˆå±‚ä¸æ—©æœŸå±‚è·å¾—çš„ logits å·®å¼‚æ¥å®ç°çš„ï¼Œä»è€Œæ”¾å¤§äº†å˜å‹å™¨å±‚ç‰¹å®šéƒ¨åˆ†çš„äº‹å®çŸ¥è¯†ã€‚</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>æ›´è¯¦ç»†çš„å†ç»†çœ‹å—</p>
</div>
</section>
</section>
</section>
<section id="best-practices-for-generation-with-cache">
<h4>Best Practices for Generation with Cache<a class="headerlink" href="#best-practices-for-generation-with-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="what-is-cache-and-why-we-should-care">
<h5>What is Cache and why we should care<a class="headerlink" href="#what-is-cache-and-why-we-should-care" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>KV Cache</p></li>
<li><p>æ¨¡å‹ä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªtokenï¼Œå¹¶ä¸”æ¯ä¸ªæ–°é¢„æµ‹éƒ½å–å†³äºå…ˆå‰çš„ä¸Šä¸‹æ–‡</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># past_key_values is the key-value cache</span>
<span class="n">generated_tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">next_logits</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to_tuple</span><span class="p">()</span>
  <span class="n">next_logits</span> <span class="o">=</span> <span class="n">next_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
  <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shape of input_ids&quot;</span><span class="p">,</span> <span class="n">next_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of key-value cache&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]</span>
  <span class="n">generated_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">)</span>
<span class="n">generated_text</span>

<span class="c1"># è¾“å‡º</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">20</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">21</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">23</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">24</span>
<span class="p">[</span><span class="s1">&#39; Here&#39;</span><span class="p">,</span> <span class="s1">&#39; is&#39;</span><span class="p">,</span> <span class="s1">&#39; a&#39;</span><span class="p">,</span> <span class="s1">&#39; Python&#39;</span><span class="p">,</span> <span class="s1">&#39; function&#39;</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>è¿™æ„å‘³ç€ï¼Œè¦åœ¨ Generation ä¸­é¢„æµ‹ç¼–å·ä¸º 1000 çš„tokenï¼Œæ‚¨éœ€è¦æ¥è‡ªä¹‹å‰ 999 ä¸ªtokençš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä»¥ token å½¢å¼çš„çŸ©é˜µä¹˜æ³•è®¡ç®—ã€‚</p></li>
<li><p>ä½†æ˜¯è¦é¢„æµ‹ä»¤ç‰Œç¼–å· 1001ï¼Œæ‚¨è¿˜éœ€è¦å‰ 999 ä¸ªä»¤ç‰Œä¸­çš„ç›¸åŒä¿¡æ¯ï¼Œä»¥åŠä»¤ç‰Œç¼–å· 1000 ä¸­çš„é™„åŠ ä¿¡æ¯ã€‚</p></li>
<li><p>è¿™å°±æ˜¯ä½¿ç”¨é”®å€¼ç¼“å­˜(KV Cache)æ¥ä¼˜åŒ–é¡ºåºç”Ÿæˆè¿‡ç¨‹çš„åœ°æ–¹ï¼Œæ–¹æ³•æ˜¯å­˜å‚¨å…ˆå‰çš„è®¡ç®—ä»¥ä¾¿åœ¨åç»­ä¸­é‡ç”¨ä»¤ç‰Œï¼Œå› æ­¤ä¸éœ€è¦å†æ¬¡è®¡ç®—å®ƒä»¬ã€‚</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>è¯·æ³¨æ„ï¼Œç¼“å­˜åªèƒ½åœ¨æ¨ç†ä¸­ä½¿ç”¨ï¼Œå¹¶ä¸”åœ¨è®­ç»ƒæ—¶åº”ç¦ç”¨ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ„å¤–é”™è¯¯ã€‚</p>
</div>
</section>
<section id="generate-with-cache">
<h5>Generate with Cache<a class="headerlink" href="#generate-with-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ç¼“å­˜ç”Ÿæˆï¼Œå…¶ä¸­ã€œDynamicCacheç±»æ˜¯å¤§å¤šæ•°æ¨¡å‹çš„é»˜è®¤ç¼“å­˜ã€‚</p></li>
<li><p>å¦‚æœç”±äºæŸç§åŸå› æ‚¨ä¸æƒ³ä½¿ç”¨ç¼“å­˜ï¼Œåˆ™å¯ä»¥å°†use_cache=Falseä¼ é€’åˆ°generate()æ–¹æ³•ä¸­ã€‚</p></li>
<li><p>ç¼“å­˜ç±»å¯ä»¥åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨cache_implementationå‚æ•°è¿›è¡Œè®¾ç½®ã€‚</p></li>
</ul>
<section id="quantized-cache">
<h6>Quantized Cache<a class="headerlink" href="#quantized-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>é”®å’Œå€¼ç¼“å­˜ä¼šå ç”¨å¾ˆå¤§ä¸€éƒ¨åˆ†å†…å­˜ï¼Œæˆä¸ºé•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„ç“¶é¢ˆï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚</p></li>
<li><p>ä½¿ç”¨generate()æ—¶ä½¿ç”¨é‡åŒ–ç¼“å­˜å¯ä»¥æ˜¾ç€å‡å°‘å†…å­˜éœ€æ±‚ï¼Œä½†ä»£ä»·æ˜¯é€Ÿåº¦ã€‚</p></li>
<li><p>transformersä¸­çš„ <code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cacheé‡åŒ–</span></code> å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°æ­¤è®ºæ–‡å¯å‘: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache: <a class="reference external" href="https://arxiv.org/abs/2402.02750">https://arxiv.org/abs/2402.02750</a></p></li>
<li><p>å¦‚æœæ‚¨ä½¿ç”¨quantoåç«¯ï¼Œå»ºè®®å°†ç¼“å­˜é…ç½®ä¸­çš„axis-key/axis-valueå‚æ•°è®¾ç½®ä¸º0ï¼›å¦‚æœæ‚¨ä½¿ç”¨HQQåç«¯ï¼Œå»ºè®®å°†å…¶è®¾ç½®ä¸º1 ã€‚å¯¹äºå…¶ä»–é…ç½®å€¼ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;I like rock music because&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;quantized&quot;</span><span class="p">,</span> <span class="n">cache_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nbits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;backend&quot;</span><span class="p">:</span> <span class="s2">&quot;quanto&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="offloaded-cache">
<h6>Offloaded Cache<a class="headerlink" href="#offloaded-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ä¸ KV ç¼“å­˜é‡åŒ–ç±»ä¼¼ï¼Œ ~OffloadedCacheç­–ç•¥æ—¨åœ¨å‡å°‘ GPU VRAM ä½¿ç”¨ã€‚</p></li>
<li><p>å®ƒé€šè¿‡å°†å¤§å¤šæ•°å±‚çš„ KV ç¼“å­˜ç§»è‡³ CPU æ¥å®ç°è¿™ä¸€ç‚¹ã€‚</p></li>
<li><p>å½“æ¨¡å‹çš„forward()æ–¹æ³•è¿­ä»£å„å±‚æ—¶ï¼Œè¯¥ç­–ç•¥ä¼šåœ¨GPUä¸Šç»´æŠ¤å½“å‰å±‚ç¼“å­˜ã€‚åŒæ—¶ï¼Œå®ƒå¼‚æ­¥é¢„å–ä¸‹ä¸€å±‚ç¼“å­˜ï¼Œå¹¶å°†ä¸Šä¸€å±‚ç¼“å­˜å‘é€å› CPUã€‚</p></li>
<li><p>ä¸ KV ç¼“å­˜é‡åŒ–ä¸åŒï¼Œæ­¤ç­–ç•¥å§‹ç»ˆäº§ç”Ÿä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥ä½œä¸ºå®ƒçš„ç›´æ¥æ›¿ä»£å“æˆ–åå¤‡æ–¹æ¡ˆã€‚</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p><code class="docutils literal notranslate"><span class="pre">Cache</span> <span class="pre">offloading</span></code> éœ€è¦ GPUï¼Œå¹¶ä¸”å¯èƒ½æ¯” <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">KV</span> <span class="pre">cache</span></code> æ…¢ã€‚å¦‚æœæ‚¨é‡åˆ° CUDA å†…å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·ä½¿ç”¨å®ƒã€‚Cache offloading requires a GPU and can be slower than dynamic KV cache. Use it if you are getting CUDA out of memory errors.</p>
</div>
<ul class="simple">
<li><p>ç¤ºä¾‹-å¦‚ä½•ä½¿ç”¨ KV ç¼“å­˜å¸è½½ä½œä¸ºåå¤‡ç­–ç•¥</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="k">def</span><span class="w"> </span><span class="nf">resilient_generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">oom</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">OutOfMemoryError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;retrying with cache_implementation=&#39;offloaded&#39;&quot;</span><span class="p">)</span>
        <span class="n">oom</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">oom</span><span class="p">:</span>  <span class="c1"># å¦‚æœOOM,åˆ™å¯åŠ¨åå¤‡ç­–ç•¥</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;cache_implementation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;offloaded&quot;</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">ckpt</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckpt</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;okay &quot;</span><span class="o">*</span><span class="mi">1000</span> <span class="o">+</span> <span class="s2">&quot;Fun fact: The most&quot;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">beams</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;num_beam_groups&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;num_return_sequences&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;diversity_penalty&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">,</span> <span class="s2">&quot;early_stopping&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">resilient_generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">beams</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">responses</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">28</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="static-cache">
<h6>Static Cache<a class="headerlink" href="#static-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ç”±äºâ€œDynamicCacheâ€éšç€æ¯ä¸ªç”Ÿæˆæ­¥éª¤åŠ¨æ€å¢é•¿ï¼Œå› æ­¤å®ƒä¼šé˜»æ­¢æ‚¨åˆ©ç”¨ JIT ä¼˜åŒ–ã€‚</p></li>
<li><p>~StaticCacheä¸ºé”®å’Œå€¼é¢„å…ˆåˆ†é…ç‰¹å®šçš„æœ€å¤§å¤§å°ï¼Œå…è®¸æ‚¨ç”Ÿæˆæœ€å¤§é•¿åº¦è€Œæ— éœ€ä¿®æ”¹ç¼“å­˜å¤§å°ã€‚</p></li>
<li><p>æœ‰å…³é™æ€ç¼“å­˜å’Œ JIT ç¼–è¯‘çš„æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹StaticCache &amp; torchcompile: <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/llm_optims#static-kv-cache-and-torchcompile">https://huggingface.co/docs/transformers/main/en/llm_optims#static-kv-cache-and-torchcompile</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># simply pass the cache implementation=&quot;static&quot;</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;static&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="offloaded-static-cache">
<h6>Offloaded Static Cache<a class="headerlink" href="#offloaded-static-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># simply pass the cache implementation=&quot;static&quot;</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;offloaded_static&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="sliding-window-cache">
<h6>Sliding Window Cache<a class="headerlink" href="#sliding-window-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>æ³¨æ„ï¼Œæ‚¨åªèƒ½å°†æ­¤ç¼“å­˜ç”¨äºæ”¯æŒæ»‘åŠ¨çª—å£çš„æ¨¡å‹ï¼Œä¾‹å¦‚ Mistral æ¨¡å‹ã€‚</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">SinkCache</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Yesterday I was on a rock concert and.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># can be used by passing in cache implementation</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;sliding_window&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="sink-cache">
<h6>Sink Cache<a class="headerlink" href="#sink-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>è®ºæ–‡: Efficient Streaming Language Models with Attention Sinks: <a class="reference external" href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453</a></p></li>
<li><p>å…è®¸æ‚¨ç”Ÿæˆé•¿æ–‡æœ¬åºåˆ—ï¼ˆæ ¹æ®è®ºæ–‡â€œæ— é™é•¿åº¦â€ï¼‰ï¼Œæ— éœ€ä»»ä½•å¾®è°ƒã€‚è¿™æ˜¯é€šè¿‡æ™ºèƒ½å¤„ç†ä»¥å‰çš„é”®å’Œå€¼æ¥å®ç°çš„ï¼Œç‰¹åˆ«æ˜¯å®ƒä¿ç•™äº†åºåˆ—ä¸­çš„ä¸€äº›åˆå§‹æ ‡è®°ï¼Œç§°ä¸ºâ€œæ¥æ”¶å™¨æ ‡è®°â€ã€‚è¿™æ˜¯åŸºäºè¿™æ ·çš„è§‚å¯Ÿï¼šè¿™äº›åˆå§‹ä»¤ç‰Œåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸å¼•äº†å¾ˆå¤§ä¸€éƒ¨åˆ†æ³¨æ„åŠ›åˆ†æ•°ã€‚ â€œæ¥æ”¶å™¨ä»¤ç‰Œâ€ä¹‹åçš„ä»¤ç‰Œå°†åœ¨æ»‘åŠ¨çª—å£çš„åŸºç¡€ä¸Šè¢«ä¸¢å¼ƒï¼Œä»…ä¿ç•™æœ€æ–°çš„window_sizeä»¤ç‰Œã€‚é€šè¿‡å°†è¿™äº›åˆå§‹æ ‡è®°ä¿ç•™ä¸ºâ€œæ³¨æ„åŠ›æ± â€ï¼Œå³ä½¿åœ¨å¤„ç†å¾ˆé•¿çš„æ–‡æœ¬æ—¶ï¼Œæ¨¡å‹ä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼Œä»è€Œä¸¢å¼ƒå¤§éƒ¨åˆ†å…ˆå‰çš„çŸ¥è¯†ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">SinkCache</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This is a long story about unicorns, fairies and magic.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># get our cache, specify number of sink tokens and window size</span>
<span class="c1"># Note that window size already includes sink tokens, so has to be larger</span>
<span class="n">past_key_values</span> <span class="o">=</span> <span class="n">SinkCache</span><span class="p">(</span><span class="n">window_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_sink_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>ä¸å…¶ä»–ç¼“å­˜ç±»ä¸åŒï¼Œè¿™ä¸ªç¼“å­˜ç±»ä¸èƒ½é€šè¿‡æŒ‡ç¤ºcache_implementationæ¥ç›´æ¥ä½¿ç”¨ã€‚æ‚¨å¿…é¡»åœ¨è°ƒç”¨generate()ä¹‹å‰åˆå§‹åŒ–ç¼“å­˜</p>
</div>
</section>
<section id="encoder-decoder-cache">
<h6>Encoder-Decoder Cache<a class="headerlink" href="#encoder-decoder-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>~EncoderDecoderCacheæ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œæ—¨åœ¨å¤„ç†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ç¼“å­˜éœ€æ±‚ã€‚è¿™ç§ç¼“å­˜ç±»å‹æ˜¯ä¸“é—¨ä¸ºç®¡ç†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ç¼“å­˜è€Œæ„å»ºçš„ï¼Œç¡®ä¿å­˜å‚¨å’Œæ£€ç´¢è¿™äº›å¤æ‚æ¨¡å‹æ‰€éœ€çš„è¿‡å»çš„é”®/å€¼ã€‚</p></li>
</ul>
</section>
</section>
<section id="model-specific-cache-classes">
<h5>Model-specific Cache Classes<a class="headerlink" href="#model-specific-cache-classes" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>æœ‰äº›æ¨¡å‹éœ€è¦ä»¥ç‰¹å®šçš„æ–¹å¼å­˜å‚¨ä»¥å‰çš„é”®ã€å€¼æˆ–çŠ¶æ€ï¼Œå¹¶ä¸”ä¸èƒ½ä½¿ç”¨ä¸Šè¿°ç¼“å­˜ç±»ã€‚å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬æœ‰å‡ ä¸ªä¸“ä¸ºç‰¹å®šæ¨¡å‹è®¾è®¡çš„ä¸“ç”¨ç¼“å­˜ç±»ã€‚</p></li>
<li><p>ç¤ºä¾‹åŒ…æ‹¬ç”¨äºGemma2ç³»åˆ—æ¨¡å‹çš„~HybridCacheæˆ–ç”¨äºMambaæ¶æ„æ¨¡å‹çš„~MambaCache ã€‚</p></li>
</ul>
</section>
</section>
</section>
<section id="prompting">
<h3>Prompting<a class="headerlink" href="#prompting" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="image-tasks-with-idefics">
<h4>Image tasks with IDEFICS<a class="headerlink" href="#image-tasks-with-idefics" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>å¯¹äºæŠŠå›¾åƒå…ˆè½¬ä¸ºæ–‡æœ¬å†è¿›è¡Œåˆ†æçš„LLMï¼Œè¿™ç§å›¾åƒç±»çš„taskä¹Ÿå¯ä»¥åƒæ™®é€šçš„è¯­è¨€LLMä¸€æ ·ä½¿ç”¨prompt</p></li>
</ul>
</section>
<section id="llm-prompting-guide">
<h4>LLM prompting guide<a class="headerlink" href="#llm-prompting-guide" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>ç¼–ç å™¨-è§£ç å™¨å¼æ¨¡å‹é€šå¸¸ç”¨äºè¾“å‡ºä¸¥é‡ä¾èµ–è¾“å…¥çš„ç”Ÿæˆä»»åŠ¡ï¼Œä¾‹å¦‚ç¿»è¯‘å’Œæ‘˜è¦ã€‚ä»…è§£ç å™¨æ¨¡å‹ç”¨äºæ‰€æœ‰å…¶ä»–ç±»å‹çš„ç”Ÿæˆä»»åŠ¡ã€‚</p></li>
<li><p>å…·ä½“çœ‹promptç›¸å…³æ–‡æ¡£</p></li>
</ul>
</section>
</section>
</section>
<section id="developer-guides">
<h2>Developer guides<a class="headerlink" href="#developer-guides" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="use-fast-tokenizers-from-tokenizers">
<h3>Use fast tokenizers from ğŸ¤— Tokenizers<a class="headerlink" href="#use-fast-tokenizers-from-tokenizers" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>PreTrainedTokenizerï¼šè¿™æ˜¯ä¸€ä¸ªçº¯ Python å®ç°çš„åˆ†è¯å™¨åŸºç±»ï¼Œæ‰€æœ‰çš„åˆ†è¯å’Œç¼–ç æ“ä½œéƒ½æ˜¯é€šè¿‡ Python ä»£ç æ‰§è¡Œçš„ã€‚</p></li>
<li><p>PreTrainedTokenizerFastï¼šåŸºäº Rust ç¼–å†™çš„ ğŸ¤— Tokenizers åº“ï¼Œå®ç°äº†æ›´é«˜æ•ˆçš„åˆ†è¯ç®—æ³•ã€‚PreTrainedTokenizerFast é€šè¿‡ç»‘å®š Rust å®ç°ï¼Œæä¾›äº†æ›´å¿«çš„åˆ†è¯é€Ÿåº¦ã€‚</p></li>
<li><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰æ¨¡å‹çš„åˆ†è¯å™¨éƒ½æœ‰å¯¹åº”çš„ â€œFastâ€ å®ç°ï¼Œç‰¹åˆ«æ˜¯åŸºäº SentencePiece çš„åˆ†è¯å™¨ï¼ˆå¦‚ T5ã€ALBERTã€CamemBERTã€XLMRoBERTa å’Œ XLNet ç­‰æ¨¡å‹ï¼‰ç›®å‰å°šæ—  â€œFastâ€ ç‰ˆæœ¬å¯ç”¨</p></li>
<li><p>åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿåˆ†è¯å™¨(dummy tokenizer)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">BPE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BpeTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">])</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>

<span class="c1"># ä¿å­˜</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;tokenizer.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="loading-directly-from-the-tokenizer-object">
<h4>Loading directly from the tokenizer object<a class="headerlink" href="#loading-directly-from-the-tokenizer-object" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizerFast</span>
<span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">(</span><span class="n">tokenizer_object</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loading-from-a-json-file">
<h4>Loading from a JSON file<a class="headerlink" href="#loading-from-a-json-file" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizerFast</span>
<span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">(</span><span class="n">tokenizer_file</span><span class="o">=</span><span class="s2">&quot;tokenizer.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="use-model-specific-apis">
<h3>Use model-specific APIs<a class="headerlink" href="#use-model-specific-apis" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>Create a custom architecture</p></li>
<li><p>AutoClassè‡ªåŠ¨æ¨æ–­æ¨¡å‹æ¶æ„å¹¶ä¸‹è½½é¢„è®­ç»ƒçš„é…ç½®å’Œæƒé‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬ <strong>å»ºè®®</strong> ä½¿ç”¨AutoClassæ¥ç”Ÿæˆä¸æ£€æŸ¥ç‚¹æ— å…³çš„ä»£ç ã€‚</p></li>
<li><p>æœ¬èŠ‚ä¸»è¦äº†è§£å¦‚ä½•åˆ›å»ºä¸ä½¿ç”¨AutoClassè‡ªå®šä¹‰æ¨¡å‹</p></li>
</ul>
<section id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Configuration æŒ‡æ¨¡å‹çš„ç‰¹å®šå±æ€§ã€‚æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸åŒçš„å±æ€§</p></li>
<li><p>ç¤ºä¾‹ <code class="docutils literal notranslate"><span class="pre">DistilBertConfig</span></code> displays all the default attributes used to build a base <code class="docutils literal notranslate"><span class="pre">DistilBertModel</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="go">DistilBertConfig {</span>
<span class="go">  &quot;activation&quot;: &quot;gelu&quot;,</span>
<span class="go">  &quot;attention_dropout&quot;: 0.1,</span>
<span class="go">  &quot;dim&quot;: 768,</span>
<span class="go">  &quot;dropout&quot;: 0.1,</span>
<span class="go">  &quot;hidden_dim&quot;: 3072,</span>
<span class="go">  &quot;initializer_range&quot;: 0.02,</span>
<span class="go">  &quot;max_position_embeddings&quot;: 512,</span>
<span class="go">  &quot;model_type&quot;: &quot;distilbert&quot;,</span>
<span class="go">  &quot;n_heads&quot;: 12,</span>
<span class="go">  &quot;n_layers&quot;: 6,</span>
<span class="go">  &quot;pad_token_id&quot;: 0,</span>
<span class="go">  &quot;qa_dropout&quot;: 0.1,</span>
<span class="go">  &quot;seq_classif_dropout&quot;: 0.2,</span>
<span class="go">  &quot;sinusoidal_pos_embds&quot;: false,</span>
<span class="go">  &quot;transformers_version&quot;: &quot;4.16.2&quot;,</span>
<span class="go">  &quot;vocab_size&quot;: 30522</span>
<span class="go">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>æ‰€æœ‰å±æ€§å‡å¯å®šåˆ¶ï¼Œå¦‚ä¸‹ç¤ºä¾‹</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>
<span class="go">DistilBertConfig {</span>
<span class="go">  &quot;activation&quot;: &quot;relu&quot;,             # gelu-&gt;relu</span>
<span class="go">  &quot;attention_dropout&quot;: 0.4,         # 0.1-&gt;0.4</span>
<span class="go">  &quot;dim&quot;: 768,</span>
<span class="go">  &quot;dropout&quot;: 0.1,</span>
<span class="go">  &quot;hidden_dim&quot;: 3072,</span>
<span class="go">  &quot;initializer_range&quot;: 0.02,</span>
<span class="go">  &quot;max_position_embeddings&quot;: 512,</span>
<span class="go">  &quot;model_type&quot;: &quot;distilbert&quot;,</span>
<span class="go">  &quot;n_heads&quot;: 12,</span>
<span class="go">  &quot;n_layers&quot;: 6,</span>
<span class="go">  &quot;pad_token_id&quot;: 0,</span>
<span class="go">  &quot;qa_dropout&quot;: 0.1,</span>
<span class="go">  &quot;seq_classif_dropout&quot;: 0.2,</span>
<span class="go">  &quot;sinusoidal_pos_embds&quot;: false,</span>
<span class="go">  &quot;transformers_version&quot;: &quot;4.16.2&quot;,</span>
<span class="go">  &quot;vocab_size&quot;: 30522</span>
<span class="go">}</span>
</pre></div>
</div>
<p>ä¿å­˜&amp;åŠ è½½:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="c1"># ä¿å­˜</span>
<span class="n">my_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="o">=</span><span class="s2">&quot;./your_model_save_path&quot;</span><span class="p">)</span>
<span class="c1"># åŠ è½½</span>
<span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your_model_save_path/config.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model">
<h4>Model<a class="headerlink" href="#model" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>åŠ è½½:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­</span>
<span class="c1"># è¿™å°†åˆ›å»ºä¸€ä¸ªå…·æœ‰éšæœºå€¼è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹</span>
<span class="c1"># æ³¨æ„ï¼šåœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•æœ‰ç”¨çš„äº‹æƒ…</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertModel</span>
<span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your_model_save_path/config.json&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>

<span class="c1"># è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®çš„é¢„è®­ç»ƒæ¨¡å‹</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="c1"># ä½¿ç”¨è‡ªå·±çš„æ¨¡å‹é…ç½®å±æ€§</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">my_config</span><span class="p">)</span>
</pre></div>
</div>
<section id="model-heads">
<h5>Model heads<a class="headerlink" href="#model-heads" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>At this point, you have a base <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code> model which outputs the <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span></code> are passed as inputs to a <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">head</span></code> to produce the final output.</p></li>
<li><p>ğŸ¤— Transformers provides a different model head for each task as long as a model supports the task</p></li>
<li><p>(i.e., you canâ€™t use DistilBERT for a sequence-to-sequence task like translation).</p></li>
<li><p>ç¤ºä¾‹</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code> is a base <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code> model with a <code class="docutils literal notranslate"><span class="pre">sequence</span> <span class="pre">classification</span></code> head.</p></li>
<li><p>The sequence classification head is a linear layer on top of the <code class="docutils literal notranslate"><span class="pre">pooled</span> <span class="pre">outputs</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„ model headï¼Œå¯ä»¥è½»æ¾åœ°å°†æ­¤checkpointé‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚</p></li>
<li><p>å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œæ‚¨å°†ä½¿ç”¨ <code class="docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code> æ¨¡å‹å¤´(model head)ã€‚</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">question</span> <span class="pre">answering</span> <span class="pre">head</span></code> is similar to the <code class="docutils literal notranslate"><span class="pre">sequence</span> <span class="pre">classification</span> <span class="pre">head</span></code> except it is a linear layer on top of the <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span> <span class="pre">output</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="tokenizer">
<h4>Tokenizer<a class="headerlink" href="#tokenizer" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>PreTrainedTokenizer ï¼šåˆ†è¯å™¨çš„ Python å®ç°ã€‚</p></li>
<li><p>PreTrainedTokenizerFast ï¼šæ¥è‡ªæˆ‘ä»¬åŸºäº Rust çš„ğŸ¤— Tokenizeråº“çš„ tokenizerã€‚</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>å¹¶éæ¯ä¸ªæ¨¡å‹éƒ½æ”¯æŒå¿«é€Ÿåˆ†è¯å™¨ã€‚æŸ¥çœ‹æ­¤ <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/index#supported-frameworks">è¡¨</a> ä»¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦å…·æœ‰å¿«é€Ÿåˆ†è¯å™¨æ”¯æŒã€‚</p>
</div>
<p>å¦‚æœæ‚¨æƒ³è®­ç»ƒè‡ªå·±çš„åˆ†è¯å™¨ï¼Œåˆ™å¯ä»¥ä»è¯æ±‡è¡¨æ–‡ä»¶åˆ›å»ºä¸€ä¸ªåˆ†è¯å™¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertTokenizer</span>
<span class="n">my_tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="s2">&quot;my_vocab_file.txt&quot;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>åˆ›å»ºå…·æœ‰é¢„è®­ç»ƒæ¨¡å‹è¯æ±‡è¡¨çš„åˆ†è¯å™¨:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertTokenizer</span>
<span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="c1"># fast_tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert/distilbert-base-uncased&quot;)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> will try to load a <code class="docutils literal notranslate"><span class="pre">fast</span> <span class="pre">tokenizer</span></code>. You can disable this behavior by setting <code class="docutils literal notranslate"><span class="pre">use_fast=False</span></code> in from_pretrained.</p>
</div>
</section>
<section id="image-processor">
<h4>Image processor<a class="headerlink" href="#image-processor" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>å›¾åƒå¤„ç†å™¨å¤„ç†è§†è§‰è¾“å…¥ã€‚å®ƒç»§æ‰¿è‡ªImageProcessingMixinåŸºç±»ã€‚</p></li>
</ul>
</section>
<section id="backbone">
<h4>Backbone<a class="headerlink" href="#backbone" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
</ul>
<figure class="align-default" id="id23">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/bgPiyo.png" src="https://img.zhaoweiguo.com/uPic/2024/10/bgPiyo.png" />
<figcaption>
<p><span class="caption-text">Computer vision models consist of a <code class="docutils literal notranslate"><span class="pre">backbone</span></code>, <code class="docutils literal notranslate"><span class="pre">neck</span></code>, and <code class="docutils literal notranslate"><span class="pre">head</span></code>.</span><a class="headerlink" href="#id23" title="æ­¤å›¾åƒçš„æ°¸ä¹…é“¾æ¥">Â¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>backbone</strong> ä»è¾“å…¥å›¾åƒä¸­æå–ç‰¹å¾ï¼Œ <strong>neck</strong> ç»„åˆå¹¶å¢å¼ºæå–çš„ç‰¹å¾ï¼Œ <strong>head</strong> ç”¨äºä¸»è¦ä»»åŠ¡ï¼ˆä¾‹å¦‚ï¼Œå¯¹è±¡æ£€æµ‹ï¼‰ã€‚</p></li>
<li><p>é¦–å…ˆåœ¨æ¨¡å‹é…ç½®ä¸­åˆå§‹åŒ–ä¸»å¹²ï¼Œå¹¶æŒ‡å®šæ˜¯å¦è¦åŠ è½½é¢„è®­ç»ƒçš„æƒé‡æˆ–åŠ è½½éšæœºåˆå§‹åŒ–çš„æƒé‡ã€‚ç„¶åæ‚¨å¯ä»¥å°†æ¨¡å‹é…ç½®ä¼ é€’ç»™æ¨¡å‹å¤´ã€‚</p></li>
<li><p>The backbone extracts features from an input image, the neck combines and enhances the extracted features, and the head is used for the main task (e.g., object detection).</p></li>
<li><p>Start by initializing a backbone in the model config and specify whether you want to load pretrained weights or load randomly initialized weights. Then you can pass the model config to the model head.</p></li>
</ul>
</section>
<section id="feature-extractor">
<h4>Feature extractor<a class="headerlink" href="#feature-extractor" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>ç»§æ‰¿è‡ªFeatureExtractionMixinåŸºç±»ï¼Œä¹Ÿå¯ä»¥ç»§æ‰¿SequenceFeatureExtractorç±»æ¥å¤„ç†éŸ³é¢‘è¾“å…¥ã€‚</p></li>
</ul>
</section>
<section id="processor">
<h4>Processor<a class="headerlink" href="#processor" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>å¯¹äºæ”¯æŒå¤šæ¨¡å¼ä»»åŠ¡çš„æ¨¡å‹ï¼ŒğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªå¤„ç†å™¨ç±»ï¼Œå¯ä»¥æ–¹ä¾¿åœ°å°†ç‰¹å¾æå–å™¨å’Œåˆ†è¯å™¨ç­‰å¤„ç†ç±»åŒ…è£…åˆ°å•ä¸ªå¯¹è±¡ä¸­ã€‚</p></li>
</ul>
</section>
<section id="building-custom-models">
<h4>Building custom models<a class="headerlink" href="#building-custom-models" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>è®²äº†å¦‚ä½•è‡ªå·±å†™ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹</p></li>
<li><p>è®²äº†AutoXXXå¦‚ä½•å®ç°åŠ è½½æ¨¡å‹çš„</p></li>
</ul>
</section>
</section>
<section id="chat-templates">
<h3>Chat Templates<a class="headerlink" href="#chat-templates" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="introduce">
<h4>Introduce<a class="headerlink" href="#introduce" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>An increasingly common use case for LLMs is chat.</p></li>
<li><p>åœ¨èŠå¤©ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯¥æ¨¡å‹ä¸æ˜¯ç»§ç»­å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼ˆå¦‚æ ‡å‡†è¯­è¨€æ¨¡å‹çš„æƒ…å†µï¼‰ï¼Œè€Œæ˜¯ç»§ç»­ç”±ä¸€æ¡æˆ–å¤šæ¡æ¶ˆæ¯ç»„æˆçš„å¯¹è¯ï¼Œæ¯æ¡æ¶ˆæ¯éƒ½åŒ…å«ä¸€ä¸ªè§’è‰²ï¼Œä¾‹å¦‚â€œuserâ€æˆ–â€œassistsâ€ï¼Œä»¥åŠæ¶ˆæ¯æ–‡æœ¬ã€‚</p></li>
<li><p>ä¸æ ‡è®°åŒ–(tokenization)éå¸¸ç›¸ä¼¼ï¼Œä¸åŒçš„æ¨¡å‹æœŸæœ›èŠå¤©çš„è¾“å…¥æ ¼å¼æˆªç„¶ä¸åŒã€‚è¿™å°±æ˜¯æˆ‘ä»¬æ·»åŠ èŠå¤©æ¨¡æ¿ä½œä¸ºä¸€é¡¹åŠŸèƒ½çš„åŸå› ã€‚</p></li>
<li><p>èŠå¤©æ¨¡æ¿æ˜¯æ ‡è®°å™¨(tokenizer)çš„ä¸€éƒ¨åˆ†ã€‚å®ƒä»¬æŒ‡å®šå¦‚ä½•å°†è¡¨ç¤ºä¸ºæ¶ˆæ¯åˆ—è¡¨çš„å¯¹è¯è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›æ ¼å¼çš„å•ä¸ªå¯æ ‡è®°å­—ç¬¦ä¸²ã€‚</p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">},</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;I&#39;m doing great. How can I help you today?&quot;</span><span class="p">},</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;I&#39;d like to show off how chat templating works!&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># æ³¨: æ ‡è®° [INST] å’Œ [/INST] æ¥æŒ‡ç¤ºç”¨æˆ·æ¶ˆæ¯çš„å¼€å§‹å’Œç»“æŸ</span>
<span class="c1"># å…¶ä»–æ¨¡å‹å¯èƒ½ä½¿ç”¨åˆ«çš„æ ‡è®°æ¥æŒ‡ç¤º</span>
</pre></div>
</div>
</section>
<section id="how-do-i-use-chat-templates">
<h4>How do I use chat templates<a class="headerlink" href="#how-do-i-use-chat-templates" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;HuggingFaceH4/zephyr-7b-beta&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>  <span class="c1"># You may want to use bfloat16 and/or move to GPU here</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a friendly chatbot who always responds in the style of a pirate&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;How many helicopters can a human eat in one sitting?&quot;</span><span class="p">},</span>
 <span class="p">]</span>
<span class="n">tokenized_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_chat</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># è¾“å‡º</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&lt;|system|&gt;</span>
<span class="sd">You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;</span>
<span class="sd">&lt;|user|&gt;</span>
<span class="sd">How many helicopters can a human eat in one sitting?&lt;/s&gt;</span>
<span class="sd">&lt;|assistant|&gt;</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>æ¨¡å‹è¾“å‡º:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>outputs = model.generate(tokenized_chat, max_new_tokens=128)
print(tokenizer.decode(outputs[0]))
# è¾“å‡º
&lt;|system|&gt;
You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;
&lt;|user|&gt;
How many helicopters can a human eat in one sitting?&lt;/s&gt;
&lt;|assistant|&gt;
Matey, I&#39;m afraid I must .......
</pre></div>
</div>
</section>
<section id="id4">
<h4>æ ¸å¿ƒå‚æ•°<a class="headerlink" href="#id4" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="add-generation-prompt">
<h5>add_generation_prompt å‚æ•°<a class="headerlink" href="#add-generation-prompt" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç±»å‹ï¼šbool</p></li>
<li><p>åŠŸèƒ½ï¼šæ˜¯å¦åœ¨èŠå¤©æ¨¡æ¿çš„æœ«å°¾æ·»åŠ ä¸€ä¸ªæç¤ºï¼Œç”¨äºæŒ‡ç¤ºæ¨¡å‹ç”Ÿæˆä¸‹ä¸€æ¡æ¶ˆæ¯ã€‚è¿™å¯¹äºä¸€äº›èŠå¤©æ¨¡å‹è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦ä¸€ä¸ªç‰¹å®šçš„è§¦å‘æ ‡è®°æ¥å¼€å§‹ç”Ÿæˆã€‚</p></li>
</ul>
<p>å®ä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hi there!&quot;</span><span class="p">},</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Nice to meet you!&quot;</span><span class="p">},</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can I ask a question?&quot;</span><span class="p">}</span>
   <span class="p">]</span>

<span class="n">without</span> <span class="n">a</span> <span class="n">generation</span> <span class="n">prompt</span><span class="p">::</span>

   <span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;&lt;|im_start|&gt;user</span>
<span class="sd">   Hi there!&lt;|im_end|&gt;</span>
<span class="sd">   &lt;|im_start|&gt;assistant</span>
<span class="sd">   Nice to meet you!&lt;|im_end|&gt;</span>
<span class="sd">   &lt;|im_start|&gt;user</span>
<span class="sd">   Can I ask a question?&lt;|im_end|&gt;</span>
<span class="sd">   &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>with a generation prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&quot;&quot;&quot;&lt;|im_start|&gt;user</span>
<span class="go">Hi there!&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;assistant</span>
<span class="go">Nice to meet you!&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;user</span>
<span class="go">Can I ask a question?&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;assistant               # æ·»åŠ ç”Ÿæˆæç¤º</span>
<span class="go">&quot;&quot;&quot;</span>
</pre></div>
</div>
</section>
<section id="continue-final-message">
<h5>continue_final_message å‚æ•°<a class="headerlink" href="#continue-final-message" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç±»å‹ï¼šbool</p></li>
<li><p>åŠŸèƒ½ï¼šæ˜¯å¦åœ¨æœ€åä¸€æ¡æ¶ˆæ¯çš„ content æœ«å°¾ç»§ç»­ç”Ÿæˆã€‚è¿™åœ¨éœ€è¦æ¨¡å‹æ¥ç€æœªå®Œæˆçš„å¥å­ç”Ÿæˆæ—¶å¾ˆæœ‰ç”¨ã€‚</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can you format the answer in JSON?&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s1">&#39;{&quot;name&quot;: &quot;&#39;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">formatted_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">continue_final_message</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">formatted_chat</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>add_generation_promptæ·»åŠ å¼€å§‹æ–°æ¶ˆæ¯çš„æ ‡è®°ï¼Œè€Œcontinue_final_messageä»æœ€ç»ˆæ¶ˆæ¯ä¸­åˆ é™¤ä»»ä½•æ¶ˆæ¯ç»“æŸæ ‡è®°ï¼Œå› æ­¤å°†å®ƒä»¬ä¸€èµ·ä½¿ç”¨æ²¡æœ‰æ„ä¹‰ã€‚</p>
</div>
</section>
<section id="tokenize">
<h5>tokenize å‚æ•°<a class="headerlink" href="#tokenize" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>é»˜è®¤æƒ…å†µä¸‹ï¼ŒæŸäº›æ ‡è®°ç”Ÿæˆå™¨ä¼šå°†ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚&lt;bos&gt;å’Œ&lt;eos&gt;æ·»åŠ åˆ°å®ƒä»¬æ ‡è®°çš„æ–‡æœ¬ä¸­ã€‚èŠå¤©æ¨¡æ¿åº”è¯¥å·²ç»åŒ…å«å®ƒä»¬éœ€è¦çš„æ‰€æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œå› æ­¤é¢å¤–çš„ç‰¹æ®Šæ ‡è®°é€šå¸¸ä¼šä¸æ­£ç¡®æˆ–é‡å¤ï¼Œè¿™ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ã€‚</p>
</div>
<ul class="simple">
<li><p>å¦‚æœæ‚¨ä½¿ç”¨ä»¥ä¸‹æ ¼å¼è®¾ç½®æ–‡æœ¬æ ¼å¼ apply_chat_template(tokenize=False) ï¼Œå½“æ‚¨ç¨åæ ‡è®°è¯¥æ–‡æœ¬æ—¶ï¼Œæ‚¨åº”è¯¥è®¾ç½®å‚æ•°add_special_tokens=False ã€‚å¦‚æœä½ ä½¿ç”¨ apply_chat_template(tokenize=True) ï¼Œä½ ä¸éœ€è¦æ‹…å¿ƒè¿™ä¸ªï¼</p></li>
</ul>
</section>
</section>
<section id="advanced-how-do-chat-templates-work">
<h4>Advanced: How do chat templates work?<a class="headerlink" href="#advanced-how-do-chat-templates-work" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æ¨¡å‹çš„èŠå¤©æ¨¡æ¿å­˜å‚¨åœ¨tokenizer.chat_templateå±æ€§ä¸­</p></li>
</ul>
<p>ç¤ºä¾‹(Jinja æ¨¡æ¿):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="o">|&gt;</span>\<span class="n">n</span><span class="s1">&#39; }}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">eos_token</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
<span class="p">{</span><span class="o">%-</span> <span class="k">if</span> <span class="n">add_generation_prompt</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
<p>ç¤ºä¾‹2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;user&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="n">bos_token</span> <span class="o">+</span> <span class="s1">&#39;[INST] &#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; [/INST]&#39;</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">elif</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;system&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;&lt;SYS&gt;&gt;</span><span class="se">\\</span><span class="s1">n&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">n&lt;&lt;/SYS&gt;&gt;</span><span class="se">\\</span><span class="s1">n</span><span class="se">\\</span><span class="s1">n&#39;</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">elif</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;assistant&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39; &#39;</span>  <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">eos_token</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="advanced-adding-and-editing-chat-templates">
<h4>Advanced: Adding and editing chat templates<a class="headerlink" href="#advanced-adding-and-editing-chat-templates" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="how-do-i-create-a-chat-template">
<h5>How do I create a chat template?<a class="headerlink" href="#how-do-i-create-a-chat-template" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>åŸºäºåˆ«çš„tokenè¿›è¡Œä¿®æ”¹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">chat_template</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;SYS&quot;</span><span class="p">,</span> <span class="s2">&quot;SYSTEM&quot;</span><span class="p">)</span>  <span class="c1"># Change the system token</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">chat_template</span> <span class="o">=</span> <span class="n">template</span>  <span class="c1"># Set the new template</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;model_name&quot;</span><span class="p">)</span>  <span class="c1"># Upload your new template to the Hub!</span>
</pre></div>
</div>
<p>ä¸€ç§æµè¡Œçš„é€‰æ‹©æ˜¯ChatMLæ ¼å¼:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|im_start|&gt;&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;&lt;|im_end|&gt;&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-template-writing-tips">
<h4>Advanced: Template writing tips<a class="headerlink" href="#advanced-template-writing-tips" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="trimming-whitespace">
<h5>Trimming whitespace<a class="headerlink" href="#trimming-whitespace" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>å¼ºçƒˆå»ºè®®ä½¿ç”¨æ ¼å¼:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
<p>ä¸è¦ä½¿ç”¨æ ¼å¼:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="callable-functions">
<h5>Callable functions<a class="headerlink" href="#callable-functions" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">raise_exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="mf">2.</span> <span class="n">strftime_now</span><span class="p">(</span><span class="n">format_str</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="compatibility-with-non-python-jinja">
<h5>Compatibility with non-Python Jinja<a class="headerlink" href="#compatibility-with-non-python-jinja" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>é Python å®ç°åœ¨éƒ¨ç½²ç¯å¢ƒä¸­å°¤å…¶å¸¸è§ï¼Œå…¶ä¸­ JS å’Œ Rust éå¸¸æµè¡Œã€‚</p>
</div>
<ul>
<li><p>1.Replace Python methods with Jinja filters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>      <span class="o">=&gt;</span> <span class="n">string</span><span class="o">|</span><span class="n">lower</span>
<span class="nb">dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>        <span class="o">=&gt;</span> <span class="nb">dict</span><span class="o">|</span><span class="n">items</span>
<span class="n">string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>      <span class="o">=&gt;</span> <span class="n">string</span><span class="o">|</span><span class="n">trim</span>
</pre></div>
</div>
</li>
<li><p>2.Replace True, False and None, which are Python-specific, with true, false and none.</p></li>
<li><p>3.æ·»åŠ tojsonè¿‡æ»¤å™¨, é¿å…ç›´æ¥æ¸²æŸ“å­—å…¸æˆ–åˆ—è¡¨å¯èƒ½ä¼šåœ¨å…¶ä»–å®ç°ä¸­ç»™å‡ºä¸åŒçš„ç»“æœ</p></li>
<li><p>Jinjaå†…ç½®Filter: <a class="reference external" href="https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters">https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters</a></p></li>
</ul>
</section>
</section>
</section>
<section id="trainer">
<h3>Trainer<a class="headerlink" href="#trainer" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<p>Basic usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. perform a training step to calculate the loss(æ‰§è¡Œè®­ç»ƒæ­¥éª¤æ¥è®¡ç®—æŸå¤±)
2. calculate the gradients with the backward method(ä½¿ç”¨åå‘æ–¹æ³•è®¡ç®—æ¢¯åº¦)
3. update the weights based on the gradients(æ ¹æ®æ¢¯åº¦æ›´æ–°æƒé‡)
4. repeat this process until youâ€™ve reached a predetermined number of epochs(é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°é¢„å®šepochs)
</pre></div>
</div>
<p>class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Trainer</span>
<span class="n">Seq2SeqTrainer</span>
<span class="n">trl</span><span class="o">.</span><span class="n">SFTTrainer</span>
</pre></div>
</div>
<p>TrainingArguments class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;your-model&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="export-to-onnx">
<h3>Export to ONNX<a class="headerlink" href="#export-to-onnx" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>ğŸ¤— Optimum æ˜¯ Transformers çš„æ‰©å±•ï¼Œå¯ä»¥é€šè¿‡å…¶exportersæ¨¡å—å°†æ¨¡å‹ä» PyTorch å¯¼å‡ºä¸ºåºåˆ—åŒ–æ ¼å¼(serialized format)ï¼Œä¾‹å¦‚ ONNX å’Œ TFLiteã€‚</p></li>
<li><p>ONNXï¼ˆOpen Neural Network eXchangeï¼‰æ˜¯ä¸€ç§å¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨è¿ç®—ç¬¦å’Œé€šç”¨æ–‡ä»¶æ ¼å¼ï¼Œä»¥è¡¨ç¤ºå„ç§æ¡†æ¶ï¼ˆåŒ…æ‹¬ PyTorch å’Œ TensorFlowï¼‰ä¸­çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å½“æ¨¡å‹å¯¼å‡ºä¸º ONNX æ ¼å¼æ—¶ï¼Œè¿™äº›è¿ç®—ç¬¦ç”¨äºæ„å»ºè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸ºä¸­é—´è¡¨ç¤ºï¼‰ï¼Œè¯¥è®¡ç®—å›¾è¡¨ç¤ºé€šè¿‡ç¥ç»ç½‘ç»œçš„æ•°æ®æµã€‚</p></li>
<li><p>é€šè¿‡ä½¿ç”¨æ ‡å‡†åŒ–è¿ç®—ç¬¦å’Œæ•°æ®ç±»å‹å…¬å¼€å›¾è¡¨ï¼ŒONNX å¯ä»¥è½»æ¾åœ°åœ¨æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚ä¾‹å¦‚ï¼Œåœ¨ PyTorch ä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸º ONNX æ ¼å¼ï¼Œç„¶åå¯¼å…¥åˆ° TensorFlow ä¸­ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚</p></li>
</ul>
<section id="exporting-a-transformers-model-to-onnx-with-cli">
<h4>Exporting a ğŸ¤— Transformers model to ONNX with CLI<a class="headerlink" href="#exporting-a-transformers-model-to-onnx-with-cli" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºåˆ° ONNXï¼Œè¯·é¦–å…ˆå®‰è£…é¢å¤–çš„ä¾èµ–é¡¹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install optimum[exporters]
</pre></div>
</div>
<p>ç¤ºä¾‹-å¯¼å‡º:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># ä» ğŸ¤— Hub å¯¼å‡ºæ¨¡å‹çš„æ£€æŸ¥ç‚¹
$ optimum-cli export onnx --model distilbert/distilbert distilbert/

# å¯¼å‡ºæœ¬åœ°æ¨¡å‹
$ optimum-cli export onnx --model local_path --task question-answering distilbert/
</pre></div>
</div>
<p>ä½¿ç”¨ONNX RuntimeåŠ è½½å¹¶è¿è¡Œæ¨¡å‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForQuestionAnswering</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ORTModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;What am I using?&quot;</span><span class="p">,</span> <span class="s2">&quot;Using DistilBERT with ONNX Runtime!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime">
<h4>Exporting a ğŸ¤— Transformers model to ONNX with optimum.onnxruntime<a class="headerlink" href="#exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>ç¤ºä¾‹-å¯¼å‡º:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert_base_uncased_squad&quot;</span>
<span class="n">save_directory</span> <span class="o">=</span> <span class="s2">&quot;onnx/&quot;</span>

<span class="c1"># Load a model from transformers and export it to ONNX</span>
<span class="n">ort_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">export</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>

<span class="c1"># Save the onnx model and tokenizer</span>
<span class="n">ort_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exporting-a-model-with-transformers-onnx">
<h4>Exporting a model with transformers.onnx<a class="headerlink" href="#exporting-a-model-with-transformers-onnx" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>transformers.onnxä¸å†ç»´æŠ¤ï¼Œè¯·ä½¿ç”¨ä¸Šé¢2èŠ‚çš„ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹ã€‚æ­¤éƒ¨åˆ†å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚</p>
</div>
<p>ç¤ºä¾‹-å¯¼å‡º:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span><span class="p">[</span><span class="n">onnx</span><span class="p">]</span>

<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">transformers</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">model</span><span class="o">=</span><span class="n">distilbert</span><span class="o">/</span><span class="n">distilbert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="n">onnx</span><span class="o">/</span>
</pre></div>
</div>
<p>ç¤ºä¾‹-è¿è¡Œ:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;onnx/model.onnx&quot;</span><span class="p">)</span>
<span class="c1"># ONNX Runtime expects NumPy arrays as input</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Using DistilBERT with ONNX Runtime!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">],</span> <span class="n">input_feed</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>FP16 stands for mixed-precision meaning that computations within the model are done using a mixture of 16-bit and 32-bit floating-point operations</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half">https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half</a></p></li>
</ul>
</section>
</section>
<section id="interoperability-with-gguf-files">
<h3>Interoperability with GGUF files<a class="headerlink" href="#interoperability-with-gguf-files" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>The GGUF file format is used to store models for inference with <a class="reference external" href="https://github.com/ggerganov/ggml">GGML</a> and other libraries that depend on itï¼ˆå¦‚: llama.cpp or whisper.cppï¼‰</p></li>
<li><p>å®ƒæ˜¯Hugging Face Hub æ”¯æŒçš„ä¸€ç§æ–‡ä»¶æ ¼å¼ï¼Œå…·æœ‰å…è®¸å¿«é€Ÿæ£€æŸ¥æ–‡ä»¶ä¸­çš„å¼ é‡(tensors)å’Œå…ƒæ•°æ®(metadata)çš„åŠŸèƒ½ã€‚</p></li>
<li><p>è¿™ç§æ–‡ä»¶æ ¼å¼è¢«è®¾è®¡ä¸ºâ€œå•æ–‡ä»¶æ ¼å¼(single-file-format)â€ï¼Œå…¶ä¸­å•ä¸ªæ–‡ä»¶é€šå¸¸åŒ…å«é…ç½®å±æ€§(configuration attributes)ã€åˆ†è¯å™¨è¯æ±‡(tokenizer vocabulary)å’Œå…¶ä»–å±æ€§ï¼Œä»¥åŠè¦åœ¨æ¨¡å‹ä¸­åŠ è½½çš„æ‰€æœ‰å¼ é‡ã€‚</p></li>
</ul>
<p>Supported quantization types:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F32</span>
<span class="n">F16</span>
<span class="n">BF16</span>
<span class="n">Q4_0</span>
<span class="n">Q4_1</span>
<span class="n">Q5_0</span>
<span class="n">Q5_1</span>
<span class="n">Q8_0</span>
<span class="n">Q2_K</span>
<span class="n">Q3_K</span>
<span class="n">Q4_K</span>
<span class="n">Q5_K</span>
<span class="n">Q6_K</span>
<span class="n">IQ1_S</span>
<span class="n">IQ1_M</span>
<span class="n">IQ2_XXS</span>
<span class="n">IQ2_XS</span>
<span class="n">IQ2_S</span>
<span class="n">IQ3_XXS</span>
<span class="n">IQ3_S</span>
<span class="n">IQ4_XS</span>
<span class="n">IQ4_NL</span>
</pre></div>
</div>
<p>Supported model architectures:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LLaMa</span>
<span class="n">Mistral</span>
<span class="n">Qwen2</span>
<span class="n">Qwen2Moe</span>
<span class="n">Phi3</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># åŠ è½½ GGUF æ–‡ä»¶æ ¼å¼
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = &quot;TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF&quot;
filename = &quot;tinyllama-1.1b-chat-v1.0.Q6_K.gguf&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)


# ä¿å­˜æ¨¡å‹å¹¶å°†å…¶å¯¼å‡ºå›gguf
tokenizer.save_pretrained(&#39;directory&#39;)
model.save_pretrained(&#39;directory&#39;)
!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
</pre></div>
</div>
</section>
</section>
<section id="quantization-methods">
<h2>Quantization Methods<a class="headerlink" href="#quantization-methods" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>æœ¬èŠ‚ç®€å•æ•´ç†ï¼Œéœ€è¦æ—¶ç»†çœ‹</p>
</div>
<section id="quantization">
<h3>Quantization<a class="headerlink" href="#quantization" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<p>Quantization method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bitsandbytes</span>
<span class="n">GPTQ</span>
<span class="n">AWQ</span>
<span class="n">AQLM</span>
<span class="n">Quanto</span>
<span class="n">EETQ</span>
<span class="n">HQQ</span>
<span class="n">FBGEMM_FP8</span>
<span class="n">Optimum</span>
<span class="n">TorchAO</span>
<span class="n">compressed</span><span class="o">-</span><span class="n">tensors</span>
<span class="n">Contribute</span> <span class="n">new</span> <span class="n">quantization</span> <span class="n">method</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/7nWOfI.png" src="https://img.zhaoweiguo.com/uPic/2024/10/7nWOfI.png" />
</figure>
</section>
<section id="bitsandbytes">
<h3>bitsandbytes<a class="headerlink" href="#bitsandbytes" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></p></li>
<li><p>bitsandbytes is the easiest option for quantizing a model to 8 and 4-bit.</p></li>
<li><p>å®šä¹‰ï¼šå¼‚å¸¸å€¼(Outliers)æ˜¯æŒ‡åœ¨æ•°æ®é›†ä¸­æ˜æ˜¾åç¦»å…¶ä»–æ•°æ®ç‚¹çš„æ•°å€¼ã€‚å®ƒä»¬ä¸æ•°æ®é›†çš„å¹³å‡è¶‹åŠ¿æˆ–èŒƒå›´ç›¸æ¯”ï¼Œè¡¨ç°å¾—éå¸¸å¼‚å¸¸ï¼Œå¯èƒ½ç”±äºæµ‹é‡é”™è¯¯ã€æç«¯æƒ…å†µæˆ–æ•°æ®åˆ†å¸ƒä¸­çš„ç¨€æœ‰äº‹ä»¶å¼•èµ·ã€‚</p></li>
<li><p>å®šä¹‰ï¼šéå¼‚å¸¸å€¼(Non-Outliers)æ˜¯æŒ‡åœ¨æ•°æ®é›†ä¸­ç¬¦åˆæ€»ä½“è¶‹åŠ¿ã€èŒƒå›´æˆ–åˆ†å¸ƒçš„æ•°å€¼ã€‚å®ƒä»¬ä¸ä¼šæ˜æ˜¾åç¦»æ•°æ®çš„ä¸»æµç‰¹å¾ï¼Œé€šå¸¸ä½äºæ•°æ®çš„å¹³å‡å€¼é™„è¿‘ã€‚</p></li>
<li><p>åœ¨æœºå™¨å­¦ä¹ ä¸­çš„è¡¨ç°ï¼šåœ¨ç¥ç»ç½‘ç»œä¸­ï¼ŒæŸäº›æƒé‡æˆ–æ¿€æ´»å€¼å¯èƒ½éå¸¸å¤§æˆ–éå¸¸å°ï¼ˆç›¸å¯¹äºå…¶ä»–å€¼ï¼‰ï¼Œè¿™äº›å€¼ä¼šè¢«ç§°ä¸ºå¼‚å¸¸å€¼(Outliers)ã€‚å¦‚æœç›´æ¥ä½¿ç”¨ä½ç²¾åº¦ï¼ˆå¦‚8-bitï¼‰çš„é‡åŒ–ï¼Œå¼‚å¸¸å€¼å¯èƒ½å¯¼è‡´è¾ƒå¤§çš„ç²¾åº¦æŸå¤±ã€‚</p></li>
<li><p>å¤„ç†æ–¹å¼ï¼šåœ¨8-bité‡åŒ–è¿‡ç¨‹ä¸­ï¼Œå¼‚å¸¸å€¼å¾€å¾€ä¸ä¼šç›´æ¥é‡åŒ–ä¸º8ä½æ•´æ•°ï¼Œå› ä¸ºè¿™æ ·ä¼šå¯¼è‡´ç²¾åº¦æŸå¤±ã€‚é€šå¸¸ï¼Œè¿™äº›å¼‚å¸¸å€¼ä¼šä¿ç•™åœ¨æ›´é«˜ç²¾åº¦çš„æ ¼å¼ï¼ˆå¦‚FP16ï¼‰ä¸­å•ç‹¬å¤„ç†ã€‚</p></li>
<li><p>ã€é‡åŒ–è¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‘åœ¨é‡åŒ–ç¥ç»ç½‘ç»œæ—¶ï¼Œoutliers å’Œ non-outliers è¢«åˆ†å¼€å¤„ç†ã€‚éå¼‚å¸¸å€¼é€‚åˆç›´æ¥ç”¨8-bitè¡¨ç¤ºï¼Œèƒ½æå¤§åœ°å‡å°‘è®¡ç®—å’Œå­˜å‚¨çš„èµ„æºéœ€æ±‚ã€‚è€Œå¼‚å¸¸å€¼å› ä¸ºå¯èƒ½å¯¼è‡´ç²¾åº¦æŸå¤±ï¼Œé€šå¸¸ç”¨æ›´é«˜ç²¾åº¦çš„FP16è¡¨ç¤ºã€‚éšåï¼Œå°†è¿™ä¸¤éƒ¨åˆ†ï¼ˆFP16çš„å¼‚å¸¸å€¼å’ŒINT8çš„éå¼‚å¸¸å€¼ï¼‰ç›¸ä¹˜ã€åŠ æ€»ï¼Œä»¥ä¿æŒè®¡ç®—ç»“æœçš„ç²¾ç¡®æ€§ã€‚</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæ—¢èƒ½åˆ©ç”¨ä½ç²¾åº¦é‡åŒ–çš„ä¼˜åŠ¿ï¼ˆå‡å°‘æ¨¡å‹å¤§å°å’ŒåŠ é€Ÿæ¨ç†ï¼‰ï¼Œåˆèƒ½åœ¨å¤„ç†å¼‚å¸¸å€¼æ—¶ä¿æŒä¸€å®šçš„ç²¾åº¦ã€‚</p>
</div>
<ul class="simple">
<li><p>8 ä½é‡åŒ–å°† fp16 ä¸­çš„å¼‚å¸¸å€¼(outliers)ä¸ int8 ä¸­çš„éå¼‚å¸¸å€¼(non-outliers)ç›¸ä¹˜ï¼Œå°†éå¼‚å¸¸å€¼è½¬æ¢å› fp16ï¼Œç„¶åå°†å®ƒä»¬ç›¸åŠ ä»¥è¿”å› fp16 ä¸­çš„æƒé‡ã€‚è¿™å‡å°‘äº†å¼‚å¸¸å€¼å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</p></li>
<li><p>8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16.</p></li>
<li><p>4 ä½é‡åŒ–å¯ä»¥è¿›ä¸€æ­¥å‹ç¼©æ¨¡å‹ï¼Œé€šå¸¸ä¸QLoRAä¸€èµ·ä½¿ç”¨æ¥å¾®è°ƒé‡åŒ–çš„LLMs ã€‚</p></li>
</ul>
<p>8bit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bigscience/bloom-1b7&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span>
<span class="p">)</span>
</pre></div>
</div>
<p>4bit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model_4bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bigscience/bloom-1b7&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gptq">
<h3>GPTQ<a class="headerlink" href="#gptq" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a></p></li>
<li><p>AutoGPTQåº“å®ç°äº† GPTQ ç®—æ³•ï¼Œè¿™æ˜¯ä¸€ç§è®­ç»ƒåé‡åŒ–æŠ€æœ¯ï¼Œå…¶ä¸­æƒé‡çŸ©é˜µçš„æ¯ä¸€è¡Œéƒ½è¢«ç‹¬ç«‹é‡åŒ–ï¼Œä»¥æ‰¾åˆ°æœ€å°åŒ–è¯¯å·®çš„æƒé‡ç‰ˆæœ¬ã€‚</p></li>
<li><p>è¿™äº›æƒé‡è¢«é‡åŒ–ä¸º int4ï¼Œä½†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä¼šå³æ—¶æ¢å¤ä¸º fp16ã€‚</p></li>
<li><p>These weights are quantized to int4, but theyâ€™re restored to fp16 on the fly during inference.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;facebook/opt-125m&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
<span class="c1"># è®¾ç½®device_map=&quot;auto&quot;å¯è‡ªåŠ¨å°†æ¨¡å‹å¸è½½åˆ° CPUï¼Œä»¥å¸®åŠ©å°†æ¨¡å‹æ”¾å…¥å†…å­˜ä¸­ï¼Œå¹¶å…è®¸æ¨¡å‹æ¨¡å—åœ¨ CPU å’Œ GPU ä¹‹é—´ç§»åŠ¨ä»¥è¿›è¡Œé‡åŒ–ã€‚</span>
</pre></div>
</div>
<section id="exllama">
<h4>ExLlama<a class="headerlink" href="#exllama" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/turboderp/exllama">https://github.com/turboderp/exllama</a></p></li>
<li><p>ExLlamaæ˜¯Llamaæ¨¡å‹çš„ Python/C++/CUDA å®ç°ï¼Œæ—¨åœ¨ä½¿ç”¨ 4 ä½ GPTQ æƒé‡è¿›è¡Œæ›´å¿«çš„æ¨ç†</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">exllama_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
<p>ä»…å½“æ•´ä¸ªæ¨¡å‹ä½äº GPU ä¸Šæ—¶æ‰æ”¯æŒ ExLlama å†…æ ¸ã€‚å¦‚æœæ‚¨ä½¿ç”¨ AutoGPTQï¼ˆç‰ˆæœ¬ &gt; 0.4.2ï¼‰åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œåˆ™éœ€è¦ç¦ç”¨ ExLlama å†…æ ¸:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GPTQConfig</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">use_exllama</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="awq">
<h3>AWQ<a class="headerlink" href="#awq" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>Activation-aware Weight Quantization(AWQ): <a class="reference external" href="https://hf.co/papers/2306.00978">https://hf.co/papers/2306.00978</a></p></li>
<li><p>ä¸ä¼šé‡åŒ–æ¨¡å‹ä¸­çš„æ‰€æœ‰æƒé‡ï¼Œè€Œæ˜¯ä¿ç•™å¯¹LLMæ€§èƒ½å¾ˆé‡è¦çš„ä¸€å°éƒ¨åˆ†æƒé‡ã€‚è¿™æ˜¾ç€å‡å°‘äº†é‡åŒ–æŸå¤±ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥ä»¥ 4 ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼Œè€Œä¸ä¼šå‡ºç°ä»»ä½•æ€§èƒ½ä¸‹é™ã€‚</p></li>
<li><p>é€šè¿‡å¯¹æ¨¡å‹çš„æƒé‡è¿›è¡ŒåŠ æƒå¹³å‡å¤„ç†ï¼Œèƒ½å¤Ÿæ›´ç²¾ç¡®åœ°æ•æ‰æƒé‡åˆ†å¸ƒçš„ç‰¹ç‚¹ã€‚AWQåœ¨ä¿ç•™æ¨¡å‹æ€§èƒ½çš„åŒæ—¶ï¼Œèƒ½å¤Ÿæ˜¾è‘—å‡å°‘æ¨ç†æ—¶çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—å¤æ‚åº¦ã€‚ä¸€ç§æ”¹è¿›çš„é‡åŒ–æ–¹æ³•ï¼Œå®ƒé’ˆå¯¹ç¥ç»ç½‘ç»œçš„æƒé‡åˆ†å¸ƒç‰¹ç‚¹ï¼Œé€šè¿‡åŠ æƒå¹³å‡çš„æ–¹å¼é‡åŒ–å‚æ•°ï¼Œä»è€Œæ›´å¥½åœ°ä¿ç•™äº†æ¨¡å‹çš„ç²¾åº¦ã€‚åœ¨æ¨ç†æ—¶ï¼ŒAWQ å¯ä»¥ä½¿ç”¨ä½ç²¾åº¦çš„æƒé‡è¡¨ç¤ºï¼Œå‡å°‘å­˜å‚¨å’Œè®¡ç®—çš„æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹æ€§èƒ½çš„ç¨³å®šã€‚ä¸ä¼ ç»Ÿçš„é‡åŒ–æŠ€æœ¯ï¼ˆå¦‚ç›´æ¥çš„é€å±‚æˆ–é€é€šé“é‡åŒ–ï¼‰ç›¸æ¯”ï¼ŒAWQ å¯¹æƒé‡åˆ†å¸ƒçš„å¤„ç†æ›´åŠ ç²¾ç»†ï¼Œå› æ­¤åœ¨åŒç­‰é‡åŒ–ç²¾åº¦ä¸‹èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ¨ç†ç»“æœã€‚(ğŸˆ³from LLM)</p></li>
<li><p>æœ‰å‡ ä¸ªç”¨äºä½¿ç”¨ AWQ ç®—æ³•é‡åŒ–æ¨¡å‹çš„åº“ï¼Œä¾‹å¦‚</p></li>
<li><p>llm-awq: <a class="reference external" href="https://github.com/mit-han-lab/llm-awq">https://github.com/mit-han-lab/llm-awq</a></p></li>
<li><p>autoawq: <a class="reference external" href="https://github.com/casper-hansen/AutoAWQ">https://github.com/casper-hansen/AutoAWQ</a>&gt;</p></li>
<li><p>optimization-intel:</p></li>
</ul>
<p>Fused modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AwqConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/Mistral-7B-OpenOrca-AWQ&quot;</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">AwqConfig</span><span class="p">(</span>
    <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">fuse_max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">do_fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="aqlm">
<h3>AQLM<a class="headerlink" href="#aqlm" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p>Additive Quantization of Language Models (AQLM): ä¸€ç§å¤§å‹è¯­è¨€æ¨¡å‹å‹ç¼©æ–¹æ³•ã€‚å®ƒå°†å¤šä¸ªæƒé‡ä¸€èµ·é‡åŒ–å¹¶åˆ©ç”¨å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä¾èµ–æ€§ã€‚ AQLM å°† 8-16 ä¸ªæƒé‡ç»„è¡¨ç¤ºä¸ºå¤šä¸ªçŸ¢é‡ä»£ç çš„æ€»å’Œã€‚</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quanto">
<h3>Quanto<a class="headerlink" href="#quanto" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/quanto">https://github.com/huggingface/quanto</a></p></li>
<li><p>Quantoåº“æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ pytorch é‡åŒ–å·¥å…·åŒ…ã€‚ä½¿ç”¨çš„é‡åŒ–æ–¹æ³•æ˜¯çº¿æ€§é‡åŒ–</p></li>
</ul>
</section>
<section id="eetq">
<h3>EETQ<a class="headerlink" href="#eetq" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NetEase-FuXi/EETQ">https://github.com/NetEase-FuXi/EETQ</a></p></li>
<li><p>EETQåº“æ”¯æŒ NVIDIA GPUS çš„ int8 æ¯é€šé“ä»…æƒé‡é‡åŒ–ã€‚</p></li>
<li><p>é«˜æ€§èƒ½GEMMå’ŒGEMVå†…æ ¸æ¥è‡ªFasterTransformerå’ŒTensorRT- LLM ã€‚</p></li>
<li><p>å®ƒä¸éœ€è¦æ ¡å‡†æ•°æ®é›†ï¼Œä¹Ÿä¸éœ€è¦é¢„å…ˆé‡åŒ–æ‚¨çš„æ¨¡å‹ã€‚æ­¤å¤–ï¼Œç”±äºæ¯é€šé“é‡åŒ–ï¼Œç²¾åº¦ä¸‹é™å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚</p></li>
</ul>
</section>
<section id="hqq">
<h3>HQQ<a class="headerlink" href="#hqq" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mobiusml/hqq/">https://github.com/mobiusml/hqq/</a></p></li>
<li><p>Half-Quadratic Quantization (HQQ): é€šè¿‡å¿«é€Ÿé²æ£’ä¼˜åŒ–(fast robust optimization)å®ç°åŠ¨æ€é‡åŒ–(on-the-fly quantization)ã€‚</p></li>
<li><p>å®ƒä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼Œå¯ç”¨äºé‡åŒ–ä»»ä½•æ¨¡å‹ã€‚</p></li>
</ul>
</section>
<section id="fbgemm-fp8">
<h3>FBGEMM FP8<a class="headerlink" href="#fbgemm-fp8" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/FBGEMM">https://github.com/pytorch/FBGEMM</a></p></li>
<li><p>the weights will be quantized in 8bit (FP8) per channel</p></li>
<li><p>the activation will be quantized in 8bit (FP8) per token</p></li>
</ul>
</section>
<section id="optimum">
<h3>Optimum<a class="headerlink" href="#optimum" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/optimum/index">https://huggingface.co/docs/optimum/index</a></p></li>
<li><p>Optimumåº“æ”¯æŒ Intelã€Furiosaã€ONNX Runtimeã€GPTQ å’Œè¾ƒä½çº§åˆ« PyTorch é‡åŒ–å‡½æ•°çš„é‡åŒ–ã€‚</p></li>
<li><p>å¦‚æœæ‚¨ä½¿ç”¨ç‰¹å®šçš„ä¼˜åŒ–ç¡¬ä»¶ï¼ˆä¾‹å¦‚ Intel CPUã€Furiosa NPU æˆ– ONNX Runtime ç­‰æ¨¡å‹åŠ é€Ÿå™¨ï¼‰ï¼Œè¯·è€ƒè™‘ä½¿ç”¨ Optimum è¿›è¡Œé‡åŒ–ã€‚</p></li>
</ul>
</section>
<section id="torchao">
<h3>TorchAO<a class="headerlink" href="#torchao" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao">https://github.com/pytorch/ao</a></p></li>
<li><p>TorchAOæ˜¯ PyTorch çš„æ¶æ„ä¼˜åŒ–åº“ï¼Œå®ƒæä¾›äº†ç”¨äºæ¨ç†å’Œè®­ç»ƒçš„é«˜æ€§èƒ½æ•°æ®ç±»å‹ã€ä¼˜åŒ–æŠ€æœ¯å’Œå†…æ ¸ï¼Œå…·æœ‰ä¸torch.compile ã€ FSDP ç­‰åŸç”Ÿ PyTorch åŠŸèƒ½çš„å¯ç»„åˆæ€§ã€‚</p></li>
</ul>
</section>
<section id="compressed-tensors">
<h3>Compressed Tensors<a class="headerlink" href="#compressed-tensors" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/neuralmagic/compressed-tensors">https://github.com/neuralmagic/compressed-tensors</a></p></li>
<li><p>æä¾›äº†ä¸€ç§é€šç”¨ä¸”æœ‰æ•ˆçš„æ–¹æ³•æ¥å­˜å‚¨å’Œç®¡ç†å‹ç¼©æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚</p></li>
<li><p>è¯¥åº“æ”¯æŒå„ç§é‡åŒ–å’Œç¨€ç–æ–¹æ¡ˆï¼Œä½¿å…¶æˆä¸ºå¤„ç†ä¸åŒæ¨¡å‹ä¼˜åŒ–çš„ç»Ÿä¸€æ ¼å¼ï¼Œä¾‹å¦‚ GPTQã€AWQã€SmoothQuantã€INT8ã€FP8ã€SparseGPT ç­‰ã€‚</p></li>
</ul>
</section>
</section>
<section id="performance-and-scalability">
<h2>Performance and scalability<a class="headerlink" href="#performance-and-scalability" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="llm-inference-optimization">
<h3>LLM inference optimization<a class="headerlink" href="#llm-inference-optimization" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="static-kv-cache-and-torch-compile">
<h4>Static kv-cache and torch.compile<a class="headerlink" href="#static-kv-cache-and-torch-compile" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>ä½¿ç”¨ kv-cache æ¥å­˜å‚¨è¿‡å»çš„é”®å’Œå€¼ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—å®ƒä»¬ã€‚</p></li>
<li><p>ç„¶è€Œï¼Œç”±äº kv-cache åœ¨æ¯ä¸ªç”Ÿæˆæ­¥éª¤éƒ½æ˜¯åŠ¨æ€ä¸”å˜åŒ–çš„ï¼Œå› æ­¤å®ƒä¼šé˜»æ­¢æ‚¨åˆ©ç”¨ torch.compile ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ä¼˜åŒ–å·¥å…·ï¼Œå¯å°† PyTorch ä»£ç èåˆåˆ°å¿«é€Ÿä¸”ä¼˜åŒ–çš„å†…æ ¸ä¸­ã€‚</p></li>
<li><p>kv-cache çš„å®Œæ•´æŒ‡å—: å‚è§ä¸Šé¢çš„ <code class="docutils literal notranslate"><span class="pre">Best</span> <span class="pre">Practices</span> <span class="pre">for</span> <span class="pre">Generation</span> <span class="pre">with</span> <span class="pre">Cache</span></code></p></li>
<li><p>static kv-cache é€šè¿‡å°† kv-cache å¤§å°é¢„å…ˆåˆ†é…ä¸ºæœ€å¤§å€¼æ¥è§£å†³æ­¤é—®é¢˜ï¼Œè¿™å…è®¸æ‚¨å°†å…¶ä¸torch.compileç»“åˆä½¿ç”¨ï¼Œæœ€é«˜å¯æé«˜ 4 å€çš„é€Ÿåº¦ã€‚ç›®å‰ï¼Œåªæœ‰Llamaå’Œå…¶ä»–ä¸€äº›æ¨¡å‹æ”¯æŒ static kv-cache å’Œtorch.compile ã€‚</p></li>
</ul>
<p>é™æ€ kv ç¼“å­˜çš„ä½¿ç”¨åˆ†ä¸ºä¸‰ç§ç±»å‹ï¼Œå…·ä½“å–å†³äºä»»åŠ¡çš„å¤æ‚æ€§:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Basic</span> <span class="n">usage</span><span class="p">:</span> <span class="n">simply</span> <span class="nb">set</span> <span class="n">a</span> <span class="n">flag</span> <span class="ow">in</span> <span class="n">generation_config</span> <span class="p">(</span><span class="n">recommended</span><span class="p">);</span>
<span class="mf">2.</span> <span class="n">Advanced</span> <span class="n">usage</span><span class="p">:</span> <span class="n">handle</span> <span class="n">a</span> <span class="n">cache</span> <span class="nb">object</span> <span class="k">for</span> <span class="n">multi</span><span class="o">-</span><span class="n">turn</span> <span class="n">generation</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">custom</span> <span class="n">generation</span> <span class="n">loop</span><span class="p">;</span>
<span class="mf">3.</span> <span class="n">Advanced</span> <span class="n">usage</span><span class="p">:</span> <span class="nb">compile</span> <span class="n">the</span> <span class="n">entire</span> <span class="n">generate</span> <span class="n">function</span> <span class="n">into</span> <span class="n">a</span> <span class="n">single</span> <span class="n">graph</span><span class="p">,</span> <span class="k">if</span> <span class="n">having</span> <span class="n">a</span> <span class="n">single</span> <span class="n">graph</span> <span class="ow">is</span> <span class="n">relevant</span> <span class="k">for</span> <span class="n">you</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Speculative decoding<a class="headerlink" href="#id5" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æ¨æµ‹è§£ç </p></li>
<li><p>æ·±å…¥å‚è§åšå®¢æ–‡ç« : Assisted Generation: a new direction toward low-latency text generation: <a class="reference external" href="https://hf.co/blog/assisted-generation">https://hf.co/blog/assisted-generation</a></p></li>
<li><p>è‡ªå›å½’çš„å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¯¹äºæ¯ä¸ªè¾“å…¥æ ‡è®°ï¼Œæ‚¨éœ€è¦åœ¨å‰å‘ä¼ é€’è¿‡ç¨‹ä¸­æ¯æ¬¡åŠ è½½æ¨¡å‹æƒé‡ã€‚</p></li>
<li><p>å¯¹äºæ‹¥æœ‰æ•°åäº¿å‚æ•°çš„LLMsæ¥è¯´ï¼Œè¿™æ—¢ç¼“æ…¢åˆéº»çƒ¦ã€‚æ¨æµ‹æ€§è§£ç é€šè¿‡ä½¿ç”¨ç¬¬äºŒä¸ªæ›´å°ã€æ›´å¿«çš„è¾…åŠ©æ¨¡å‹æ¥ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œå¹¶åœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­ç”±æ›´å¤§çš„LLMè¿›è¡ŒéªŒè¯ï¼Œä»è€Œç¼“è§£äº†è¿™ç§é€Ÿåº¦ä¸‹é™çš„æƒ…å†µã€‚</p></li>
<li><p>å¦‚æœéªŒè¯çš„ä»¤ç‰Œæ˜¯æ­£ç¡®çš„ï¼Œ LLMåŸºæœ¬ä¸Šå¯ä»¥â€œå…è´¹â€è·å¾—å®ƒä»¬ï¼Œè€Œä¸å¿…è‡ªå·±ç”Ÿæˆå®ƒä»¬ã€‚</p></li>
</ul>
<section id="prompt-lookup-decoding">
<h5>Prompt lookup decoding<a class="headerlink" href="#prompt-lookup-decoding" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>Prompt lookup decoding æ˜¯æ¨æµ‹è§£ç çš„ä¸€ç§å˜ä½“</p></li>
<li><p>æç¤ºæŸ¥æ‰¾å¯¹äºåŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼ˆä¾‹å¦‚æ‘˜è¦ï¼‰ç‰¹åˆ«æœ‰æ•ˆï¼Œå…¶ä¸­æç¤ºå’Œè¾“å‡ºä¹‹é—´ç»å¸¸å­˜åœ¨é‡å çš„å•è¯ã€‚è¿™äº›é‡å çš„ n å…ƒè¯­æ³•è¢«ç”¨ä½œLLMå€™é€‰tokenã€‚</p></li>
</ul>
</section>
</section>
<section id="attention-optimizations">
<h4>Attention optimizations<a class="headerlink" href="#attention-optimizations" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Transformer æ¨¡å‹çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶åœ¨è®¡ç®—å’Œå†…å­˜ä¸­éšç€è¾“å…¥æ ‡è®°çš„æ•°é‡å‘ˆäºŒæ¬¡æ–¹å¢é•¿ã€‚</p></li>
<li><p>è¿™ç§é™åˆ¶ä¼šåœ¨å¤„ç†æ›´é•¿åºåˆ—çš„LLMsä¸­è¢«æ”¾å¤§ã€‚</p></li>
<li><p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¯·å°è¯• FlashAttention2 æˆ– PyTorch çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (scaled dot product attention, SDPA)ï¼Œå®ƒä»¬æ˜¯å†…å­˜æ•ˆç‡æ›´é«˜çš„æ³¨æ„åŠ›å®ç°ï¼Œå¯ä»¥åŠ é€Ÿæ¨ç†ã€‚</p></li>
</ul>
<section id="flashattention-2">
<h5>FlashAttention-2<a class="headerlink" href="#flashattention-2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>FlashAttention å’ŒFlashAttention-2å°†æ³¨æ„åŠ›è®¡ç®—åˆ†è§£ä¸ºæ›´å°çš„å—ï¼Œå¹¶å‡å°‘å¯¹ GPU å†…å­˜çš„ä¸­é—´è¯»/å†™æ“ä½œçš„æ•°é‡ï¼Œä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚</p></li>
<li><p>FlashAttention-2 é€šè¿‡åœ¨åºåˆ—é•¿åº¦ç»´åº¦ä¸Šå¹¶è¡ŒåŒ–ä»¥åŠæ›´å¥½çš„ç¡¬ä»¶åˆ†åŒºå·¥ä½œæ¥æ”¹è¿›åŸå§‹ FlashAttention ç®—æ³•ï¼Œä»¥å‡å°‘åŒæ­¥å’Œé€šä¿¡å¼€é”€ã€‚</p></li>
</ul>
</section>
<section id="pytorch-scaled-dot-product-attention">
<h5>PyTorch scaled dot product attention<a class="headerlink" href="#pytorch-scaled-dot-product-attention" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>PyTorch 2.0 ä¸­è‡ªåŠ¨å¯ç”¨äº†ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (SDPA)ï¼Œå®ƒæ”¯æŒ FlashAttentionã€xFormers å’Œ PyTorch çš„ C++ å®ç°ã€‚</p></li>
<li><p>å¦‚æœæ‚¨ä½¿ç”¨ CUDA åç«¯ï¼ŒSDPA ä¼šé€‰æ‹©æ€§èƒ½æœ€ä½³çš„æ³¨æ„åŠ›ç®—æ³•ã€‚å¯¹äºå…¶ä»–åç«¯ï¼ŒSDPA é»˜è®¤ä½¿ç”¨ PyTorch C++ å®ç°ã€‚</p></li>
<li><p>åªè¦æ‚¨å®‰è£…äº†æœ€æ–°çš„ PyTorch ç‰ˆæœ¬ï¼ŒSDPA å°±æ”¯æŒ FlashAttention-2ã€‚</p></li>
</ul>
</section>
</section>
<section id="id6">
<h4>Quantization<a class="headerlink" href="#id6" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>å¦‚æœæ‚¨ä¸å— GPU çš„é™åˆ¶ï¼Œåˆ™ä¸ä¸€å®šéœ€è¦é‡åŒ–æ¨¡å‹ï¼Œå› ä¸ºé‡åŒ–å’Œåé‡åŒ–æƒé‡æ‰€éœ€çš„é¢å¤–æ­¥éª¤å¯èƒ½ä¼šäº§ç”Ÿè¾ƒå°çš„å»¶è¿Ÿæˆæœ¬ï¼ˆAWQ å’Œèåˆ AWQ æ¨¡å—é™¤å¤–ï¼‰ã€‚</p></li>
</ul>
</section>
</section>
<section id="efficient-training-techniques">
<h3>Efficient training techniques<a class="headerlink" href="#efficient-training-techniques" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="methods-and-tools-for-efficient-training-on-a-single-gpu">
<h4>Methods and tools for efficient training on a single GPU<a class="headerlink" href="#methods-and-tools-for-efficient-training-on-a-single-gpu" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>åœ¨è®­ç»ƒå¤§å‹æ¨¡å‹æ—¶ï¼Œéœ€è¦åŒæ—¶è€ƒè™‘ä¸¤ä¸ªæ–¹é¢:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Data</span> <span class="n">throughput</span><span class="o">/</span><span class="n">training</span> <span class="n">time</span> <span class="n">æ•°æ®ååé‡</span><span class="o">/</span><span class="n">è®­ç»ƒæ—¶é—´</span>
<span class="mf">2.</span> <span class="n">Model</span> <span class="n">performance</span> <span class="n">æ¨¡å‹æ€§èƒ½</span>
</pre></div>
</div>
<p>Method/toolä¸å¯¹åº”çš„æ•ˆæœ:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Method</span><span class="o">/</span><span class="n">tool</span>                            <span class="o">|</span> <span class="n">Improves</span> <span class="n">training</span> <span class="n">speed</span> <span class="o">|</span> <span class="n">Optimizes</span> <span class="n">memory</span> <span class="n">utilization</span> <span class="o">|</span>
<span class="o">+========================================+=========================+==============================+</span>
<span class="o">|</span> <span class="n">Batch</span> <span class="n">size</span> <span class="n">choice</span>                      <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Gradient</span> <span class="n">accumulation</span>                  <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Gradient</span> <span class="n">checkpointing</span>                 <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Mixed</span> <span class="n">precision</span> <span class="n">training</span>               <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Maybe</span><span class="o">*</span>                       <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">torch_empty_cache_steps</span>                <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Optimizer</span> <span class="n">choice</span>                       <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Data</span> <span class="n">preloading</span>                        <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">No</span>                           <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">DeepSpeed</span> <span class="n">Zero</span>                         <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span>                          <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">No</span>                           <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Parameter</span><span class="o">-</span><span class="n">Efficient</span> <span class="n">Fine</span> <span class="n">Tuning</span> <span class="p">(</span><span class="n">PEFT</span><span class="p">)</span> <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
</pre></div>
</div>
<section id="batch-size-choice">
<h5>Batch size choice<a class="headerlink" href="#batch-size-choice" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ä¸ºäº†å®ç°æœ€ä½³æ€§èƒ½ï¼Œé¦–å…ˆè¦ç¡®å®šé€‚å½“çš„æ‰¹é‡å¤§å°ã€‚å»ºè®®ä½¿ç”¨å¤§å°ä¸º 2^N çš„æ‰¹é‡å¤§å°å’Œè¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°ã€‚é€šå¸¸å®ƒæ˜¯ 8 çš„å€æ•°ï¼Œä½†ä¹Ÿå¯èƒ½æ›´é«˜ï¼Œå…·ä½“å–å†³äºæ‰€ä½¿ç”¨çš„ç¡¬ä»¶å’Œæ¨¡å‹çš„æ•°æ®ç±»å‹ã€‚</p></li>
<li><p>ä½œä¸ºå‚è€ƒï¼Œè¯·æŸ¥çœ‹ NVIDIA å¯¹äºå…¨è¿æ¥å±‚ï¼ˆæ¶‰åŠ GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ï¼‰çš„ <a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features">è¾“å…¥/è¾“å‡ºç¥ç»å…ƒè®¡æ•°</a> å’Œ <a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size">æ‰¹é‡å¤§å°</a> çš„å»ºè®®ã€‚</p></li>
<li><p>For reference, check out NVIDIAâ€™s recommendation for input/output neuron counts and batch size for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).</p></li>
<li><p>å¼ é‡æ ¸å¿ƒè¦æ±‚æ ¹æ®æ•°æ®ç±»å‹å’Œç¡¬ä»¶å®šä¹‰ä¹˜æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äº fp16 æ•°æ®ç±»å‹ï¼Œå»ºè®®ä½¿ç”¨ 8 çš„å€æ•°ï¼Œé™¤éæ˜¯ A100 GPUï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ 64 çš„å€æ•°ã€‚</p></li>
</ul>
</section>
<section id="gradient-accumulation">
<h5>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>æ¢¯åº¦ç´¯ç§¯æ–¹æ³•æ—¨åœ¨ä»¥è¾ƒå°çš„å¢é‡è®¡ç®—æ¢¯åº¦ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡è®¡ç®—æ•´ä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦ã€‚</p></li>
<li><p>è¿™ç§æ–¹æ³•æ¶‰åŠé€šè¿‡å‘å‰å’Œå‘åéå†æ¨¡å‹å¹¶åœ¨æ­¤è¿‡ç¨‹ä¸­ç´¯ç§¯æ¢¯åº¦æ¥è¿­ä»£è®¡ç®—è¾ƒå°æ‰¹æ¬¡çš„æ¢¯åº¦ã€‚ä¸€æ—¦ç§¯ç´¯äº†è¶³å¤Ÿæ•°é‡çš„æ¢¯åº¦ï¼Œå°±ä¼šæ‰§è¡Œæ¨¡å‹çš„ä¼˜åŒ–æ­¥éª¤ã€‚</p></li>
<li><p>é€šè¿‡é‡‡ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œå¯ä»¥å°†æœ‰æ•ˆæ‰¹é‡å¤§å°(effective batch size)å¢åŠ åˆ°è¶…å‡º GPU å†…å­˜å®¹é‡çš„é™åˆ¶ã€‚</p></li>
<li><p>ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ¢¯åº¦ç´¯ç§¯å¼•å…¥çš„é¢å¤–å‰å‘å’Œåå‘ä¼ é€’å¯èƒ½ä¼šå‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚</p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
<span class="c1"># è¯´æ˜</span>
    <span class="mf">1.</span> <span class="n">é€šè¿‡å°†gradient_accumulation_stepså‚æ•°æ·»åŠ åˆ°TrainingArgumentsæ¥å¯ç”¨æ¢¯åº¦ç´¯ç§¯</span>
    <span class="mf">2.</span> <span class="n">æœ‰æ•ˆæ‰¹é‡å¤§å°å˜ä¸º</span> <span class="mi">4</span>
</pre></div>
</div>
</section>
<section id="gradient-checkpointing">
<h5>Gradient Checkpointing<a class="headerlink" href="#gradient-checkpointing" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å³ä½¿æ‰¹é‡å¤§å°è®¾ç½®ä¸º 1 å¹¶ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼Œä¸€äº›å¤§å‹æ¨¡å‹ä»ç„¶å¯èƒ½é¢ä¸´å†…å­˜é—®é¢˜ã€‚è¿™æ˜¯å› ä¸ºè¿˜æœ‰å…¶ä»–ç»„ä»¶ä¹Ÿéœ€è¦å†…å­˜å­˜å‚¨ã€‚</p></li>
<li><p>ä¿å­˜å‰å‘ä¼ é€’ä¸­çš„æ‰€æœ‰æ¿€æ´»ä»¥ä¾¿åœ¨åå‘ä¼ é€’æœŸé—´è®¡ç®—æ¢¯åº¦å¯èƒ½ä¼šå¯¼è‡´æ˜¾ç€çš„å†…å­˜å¼€é”€ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯åœ¨å‘åä¼ é€’è¿‡ç¨‹ä¸­ä¸¢å¼ƒæ¿€æ´»å¹¶åœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—å®ƒä»¬ï¼Œè¿™ä¼šå¸¦æ¥ç›¸å½“å¤§çš„è®¡ç®—å¼€é”€å¹¶å‡æ…¢è®­ç»ƒè¿‡ç¨‹ã€‚</p></li>
<li><p><strong>æ¢¯åº¦æ£€æŸ¥ç‚¹</strong> æä¾›äº†è¿™ä¸¤ç§æ–¹æ³•ä¹‹é—´çš„æŠ˜è¡·æ–¹æ¡ˆï¼Œå¹¶åœ¨æ•´ä¸ªè®¡ç®—å›¾ä¸­ä¿å­˜äº†æˆ˜ç•¥é€‰æ‹©çš„æ¿€æ´»ï¼Œå› æ­¤åªéœ€ä¸ºæ¢¯åº¦é‡æ–°è®¡ç®—ä¸€å°éƒ¨åˆ†æ¿€æ´»ã€‚æœ‰å…³æ¢¯åº¦æ£€æŸ¥ç‚¹çš„æ·±å…¥è§£é‡Šï¼Œè¯·å‚é˜…è¿™ç¯‡ <a class="reference external" href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">ç²¾å½©æ–‡ç« </a></p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mixed-precision-training">
<h5>Mixed precision training<a class="headerlink" href="#mixed-precision-training" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p><strong>æ··åˆç²¾åº¦è®­ç»ƒ</strong> æ˜¯ä¸€ç§æ—¨åœ¨é€šè¿‡å¯¹æŸäº›å˜é‡ä½¿ç”¨è¾ƒä½ç²¾åº¦çš„æ•°å€¼æ ¼å¼æ¥ä¼˜åŒ–è®­ç»ƒæ¨¡å‹çš„è®¡ç®—æ•ˆç‡çš„æŠ€æœ¯ã€‚ä¼ ç»Ÿä¸Šï¼Œå¤§å¤šæ•°æ¨¡å‹ä½¿ç”¨ 32 ä½æµ®ç‚¹ç²¾åº¦ï¼ˆfp32 æˆ– float32ï¼‰æ¥è¡¨ç¤ºå’Œå¤„ç†å˜é‡ã€‚ç„¶è€Œï¼Œå¹¶éæ‰€æœ‰å˜é‡éƒ½éœ€è¦å¦‚æ­¤é«˜ç²¾åº¦æ‰èƒ½è·å¾—å‡†ç¡®çš„ç»“æœã€‚é€šè¿‡å°†æŸäº›å˜é‡çš„ç²¾åº¦é™ä½ä¸ºè¾ƒä½çš„æ•°å€¼æ ¼å¼ï¼Œä¾‹å¦‚ 16 ä½æµ®ç‚¹ï¼ˆfp16 æˆ– float16ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ã€‚ç”±äºåœ¨è¿™ç§æ–¹æ³•ä¸­ï¼Œæœ‰äº›è®¡ç®—æ˜¯ä»¥åŠç²¾åº¦æ‰§è¡Œçš„ï¼Œè€Œæœ‰äº›è®¡ç®—ä»ç„¶æ˜¯å…¨ç²¾åº¦çš„ï¼Œå› æ­¤è¯¥æ–¹æ³•ç§°ä¸ºæ··åˆç²¾åº¦è®­ç»ƒã€‚</p></li>
<li><p>æœ€å¸¸è§çš„æ··åˆç²¾åº¦è®­ç»ƒæ˜¯é€šè¿‡ä½¿ç”¨ fp16 (float16) æ•°æ®ç±»å‹æ¥å®ç°çš„ï¼Œä½†æ˜¯ï¼Œä¸€äº› GPU æ¶æ„ï¼ˆä¾‹å¦‚ Ampere æ¶æ„ï¼‰æä¾› bf16 å’Œ tf32ï¼ˆCUDA å†…éƒ¨æ•°æ®ç±»å‹ï¼‰æ•°æ®ç±»å‹ã€‚æŸ¥çœ‹ <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/">NVIDIAåšå®¢</a></p></li>
</ul>
<section id="fp16">
<h6>fp16<a class="headerlink" href="#fp16" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸»è¦ä¼˜ç‚¹æ¥è‡ªäºä»¥åŠç²¾åº¦ï¼ˆfp16ï¼‰ä¿å­˜æ¿€æ´»ã€‚å°½ç®¡æ¢¯åº¦ä¹Ÿæ˜¯ä»¥åŠç²¾åº¦è®¡ç®—çš„ï¼Œä½†å®ƒä»¬åœ¨ä¼˜åŒ–æ­¥éª¤ä¸­ä¼šè½¬æ¢å›å…¨ç²¾åº¦ï¼Œå› æ­¤æ­¤å¤„ä¸ä¼šèŠ‚çœå†…å­˜ã€‚</p></li>
<li><p>è™½ç„¶æ··åˆç²¾åº¦è®­ç»ƒå¯ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦ï¼Œä½†å®ƒä¹Ÿä¼šå¯¼è‡´ä½¿ç”¨æ›´å¤š GPU å†…å­˜ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå°æ‰¹é‡å¤§å°ã€‚è¿™æ˜¯å› ä¸ºè¯¥æ¨¡å‹ç°åœ¨ä»¥ 16 ä½å’Œ 32 ä½ç²¾åº¦ï¼ˆGPU ä¸ŠåŸå§‹æ¨¡å‹çš„ 1.5 å€ï¼‰å‘ˆç°åœ¨ GPU ä¸Šã€‚</p></li>
</ul>
<p>è¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œè¯·å°†fp16æ ‡å¿—è®¾ç½®ä¸ºTrue:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bf16">
<h6>BF16<a class="headerlink" href="#bf16" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>å¦‚æœæ‚¨å¯ä»¥ä½¿ç”¨ Ampere æˆ–æ›´æ–°çš„ç¡¬ä»¶ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ bf16 è¿›è¡Œæ··åˆç²¾åº¦è®­ç»ƒå’Œè¯„ä¼°ã€‚</p></li>
<li><p>è™½ç„¶ bf16 çš„ç²¾åº¦æ¯” fp16 å·®ï¼Œä½†å®ƒçš„åŠ¨æ€èŒƒå›´è¦å¤§å¾—å¤šã€‚åœ¨ fp16 ä¸­ï¼Œæ‚¨å¯ä»¥æ‹¥æœ‰çš„æœ€å¤§æ•°å­—æ˜¯65504 ï¼Œä»»ä½•é«˜äºè¯¥æ•°å­—çš„æ•°å­—éƒ½ä¼šå¯¼è‡´æº¢å‡ºã€‚ bf16 æ•°å­—å¯ä»¥å¤§åˆ°3.39e+38 (!)ï¼Œè¿™ä¸ fp32 å¤§è‡´ç›¸åŒ - å› ä¸ºä¸¤è€…éƒ½æœ‰ 8 ä½ç”¨äºæ•°å­—èŒƒå›´ã€‚</p></li>
</ul>
<p>åœ¨ ğŸ¤— Trainer ä¸­å¯ç”¨ BF16:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tf32">
<h6>TF32<a class="headerlink" href="#tf32" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>Ampere ç¡¬ä»¶ä½¿ç”¨ä¸€ç§åä¸º tf32 çš„ç¥å¥‡æ•°æ®ç±»å‹ã€‚</p></li>
<li><p>å®ƒå…·æœ‰ä¸ fp32ï¼ˆ8 ä½ï¼‰ç›¸åŒçš„æ•°å€¼èŒƒå›´ï¼Œä½†ä¸æ˜¯ 23 ä½ç²¾åº¦ï¼Œè€Œæ˜¯åªæœ‰ 10 ä½ï¼ˆä¸ fp16 ç›¸åŒï¼‰ï¼Œå¹¶ä¸”æ€»å…±åªä½¿ç”¨ 19 ä½ã€‚</p></li>
<li><p>å®ƒçš„â€œç¥å¥‡â€ä¹‹å¤„åœ¨äºï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ™®é€šçš„ fp32 è®­ç»ƒå’Œ/æˆ–æ¨ç†ä»£ç ï¼Œå¹¶ä¸”é€šè¿‡å¯ç”¨ tf32 æ”¯æŒï¼Œæ‚¨å¯ä»¥è·å¾—é«˜è¾¾ 3 å€çš„ååé‡æå‡ã€‚</p></li>
</ul>
<p>æ‚¨éœ€è¦åšçš„å°±æ˜¯å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ä»£ç ä¸­:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TrainingArguments</span><span class="p">(</span><span class="n">tf32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>tf32 æ— æ³•ç›´æ¥é€šè¿‡tensor.to(dtype=torch.tf32)è®¿é—®ï¼Œå› ä¸ºå®ƒæ˜¯å†…éƒ¨ CUDA æ•°æ®ç±»å‹ã€‚æ‚¨éœ€è¦torch&gt;=1.7æ‰èƒ½ä½¿ç”¨ tf32 æ•°æ®ç±»å‹ã€‚</p>
</div>
</section>
</section>
<section id="flash-attention-2">
<h5>Flash Attention 2<a class="headerlink" href="#flash-attention-2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å¯ä»¥é€šè¿‡åœ¨ Transformer ä¸­ä½¿ç”¨ Flash Attention 2 é›†æˆæ¥åŠ é€Ÿè®­ç»ƒååé‡</p></li>
<li><p>å…·ä½“å‚è§åé¢çš„æ¨ç†ä¼˜åŒ–(Optimizing inference)</p></li>
</ul>
</section>
<section id="optimizer-choice">
<h5>Optimizer choice<a class="headerlink" href="#optimizer-choice" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ç”¨äºè®­ç»ƒ Transformer æ¨¡å‹çš„æœ€å¸¸è§ä¼˜åŒ–å™¨æ˜¯ Adam æˆ– AdamWï¼ˆå¸¦æœ‰æƒé‡è¡°å‡çš„ Adamï¼‰ã€‚</p></li>
<li><p>Adamé€šè¿‡å­˜å‚¨ä¹‹å‰æ¢¯åº¦çš„æ»šåŠ¨å¹³å‡å€¼å®ç°äº†è‰¯å¥½çš„æ”¶æ•›ï¼›ç„¶è€Œï¼Œå®ƒå¢åŠ äº†æ¨¡å‹å‚æ•°æ•°é‡æ•°é‡çº§çš„é¢å¤–å†…å­˜å ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ›¿ä»£ä¼˜åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä¸º NVIDIA GPU å®‰è£…äº†NVIDIA/apex ï¼Œæˆ–ä¸º AMD GPU å®‰è£…äº†ROCmSoftwarePlatform/apex ï¼Œ adamw_apex_fusedå°†ä¸ºæ‚¨æä¾›æ‰€æœ‰å—æ”¯æŒçš„ AdamW ä¼˜åŒ–å™¨ä¸­æœ€å¿«çš„è®­ç»ƒä½“éªŒã€‚</p></li>
<li><p>Traineré›†æˆäº†å„ç§å¯ç«‹å³ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼š adamw_hf ã€ adamw_torch ã€ adamw_torch_fused ã€ adamw_apex_fused ã€ adamw_anyprecision ã€ adafactoræˆ–adamw_bnb_8bit ã€‚å¯ä»¥é€šè¿‡ç¬¬ä¸‰æ–¹å®ç°æ’å…¥æ›´å¤šä¼˜åŒ–å™¨ã€‚</p></li>
</ul>
</section>
<section id="data-preloading">
<h5>Data preloading<a class="headerlink" href="#data-preloading" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸€åˆ‡éƒ½å‘ç”Ÿåœ¨ä¸»è¿›ç¨‹ä¸­ï¼Œå®ƒå¯èƒ½æ— æ³•è¶³å¤Ÿå¿«åœ°ä»ç£ç›˜è¯»å–æ•°æ®ï¼Œä»è€Œäº§ç”Ÿç“¶é¢ˆï¼Œå¯¼è‡´ GPU åˆ©ç”¨ç‡ä¸è¶³</p></li>
</ul>
</section>
<section id="deepspeed-zero">
<h5>DeepSpeed ZeRO<a class="headerlink" href="#deepspeed-zero" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>DeepSpeed æ˜¯ä¸€ä¸ªå¼€æºæ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œä¸ ğŸ¤— Transformers å’Œ ğŸ¤— Accelerate é›†æˆã€‚å®ƒæä¾›äº†å¹¿æ³›çš„åŠŸèƒ½å’Œä¼˜åŒ–ï¼Œæ—¨åœ¨æé«˜å¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒçš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚</p></li>
<li><p>å¦‚æœæ‚¨çš„æ¨¡å‹é€‚åˆå•ä¸ª GPU å¹¶ä¸”æ‚¨æœ‰è¶³å¤Ÿçš„ç©ºé—´æ¥å®¹çº³å°æ‰¹é‡å¤§å°ï¼Œåˆ™ä¸éœ€è¦ä½¿ç”¨ DeepSpeedï¼Œå› ä¸ºå®ƒåªä¼šå‡æ…¢é€Ÿåº¦ã€‚</p></li>
<li><p>ä½†æ˜¯ï¼Œå¦‚æœæ¨¡å‹ä¸é€‚åˆå•ä¸ª GPUï¼Œæˆ–è€…æ‚¨æ— æ³•é€‚åº”å°æ‰¹é‡ï¼Œåˆ™å¯ä»¥åˆ©ç”¨ DeepSpeed ZeRO + CPU Offload æˆ– NVMe Offload æ¥å¤„ç†æ›´å¤§çš„æ¨¡å‹ã€‚</p></li>
</ul>
</section>
<section id="using-torch-compile">
<h5>Using torch.compile<a class="headerlink" href="#using-torch-compile" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">torch_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>torch.compile uses Pythonâ€™s frame evaluation API to automatically <code class="docutils literal notranslate"><span class="pre">create</span> <span class="pre">a</span> <span class="pre">graph</span></code> from existing PyTorch programs. After capturing the graph,å¯ä»¥éƒ¨ç½²ä¸åŒçš„åç«¯æ¥å°†å›¾è¡¨é™ä½åˆ°ä¼˜åŒ–çš„å¼•æ“ã€‚</p></li>
<li><p>create a graph: é€šè¿‡ torch.compile è‡ªåŠ¨å°†ç°æœ‰çš„ PyTorch ç¨‹åºè½¬æ¢æˆè®¡ç®—å›¾ï¼ˆcomputation graphï¼‰</p>
<blockquote>
<div><ul class="simple">
<li><p>å…·ä½“æ¥è¯´ï¼ŒPyTorch é€šå¸¸æ˜¯åŠ¨æ€è®¡ç®—çš„ï¼ˆå³åŠ¨æ€å›¾ï¼Œä¹Ÿå« eager executionï¼‰ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªæ“ä½œï¼ˆå¦‚å¼ é‡åŠ æ³•ã€çŸ©é˜µä¹˜æ³•ç­‰ï¼‰éƒ½ä¼šç«‹å³æ‰§è¡Œã€‚</p></li>
<li><p>è€Œ torch.compile ä½¿ç”¨ Python çš„ â€œframe evaluation APIâ€ï¼Œå°†è¿™äº›åŠ¨æ€çš„æ“ä½œæ•è·ä¸‹æ¥ï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªä¼˜åŒ–åçš„é™æ€è®¡ç®—å›¾ï¼ˆstatic computation graphï¼‰ã€‚</p></li>
<li><p>è¿™ä¸ªè®¡ç®—å›¾åŒ…å«äº†æ•´ä¸ªæ¨¡å‹çš„æ“ä½œé¡ºåºå’Œä¾èµ–å…³ç³»ï¼Œç›¸å½“äºä¸€ç§é«˜æ•ˆçš„è¡¨è¾¾æ–¹å¼ã€‚é€šè¿‡å°†æ¨¡å‹çš„æ“ä½œå˜æˆå›¾ç»“æ„ï¼Œåç«¯å¯ä»¥å¯¹å…¶è¿›è¡Œä¼˜åŒ–å’ŒåŠ é€Ÿï¼Œåˆ©ç”¨ç¡¬ä»¶æ›´å¥½åœ°æ‰§è¡Œè¿™äº›æ“ä½œï¼Œæ¯”å¦‚é€šè¿‡ç¼–è¯‘æˆæ›´é«˜æ•ˆçš„ä»£ç æˆ–è€…åœ¨ä¸åŒçš„ç¡¬ä»¶æ¶æ„ä¸Šæ‰§è¡Œã€‚</p></li>
<li><p>å› æ­¤ï¼Œâ€create a graphâ€ çš„æ„æ€æ˜¯ï¼štorch.compile å°†åŸæœ¬æŒ‰æ­¥éª¤æ‰§è¡Œçš„æ¨¡å‹ä»£ç è½¬æ¢ä¸ºä¸€ä¸ªå¯ä¼˜åŒ–çš„å›¾ç»“æ„ï¼Œä¾¿äºè¿›ä¸€æ­¥çš„æ€§èƒ½ä¼˜åŒ–ã€‚</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>æœ€å¸¸ç”¨çš„åç«¯:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Debugging</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;eager&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_eager&quot;</span><span class="p">)</span>

<span class="mf">2.</span> <span class="n">Training</span> <span class="o">&amp;</span> <span class="n">inference</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;inductor&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;nvfuser&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_nvfuser&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_cudagraphs&quot;</span><span class="p">)</span>

<span class="mf">3.</span> <span class="n">Inference</span><span class="o">-</span><span class="n">only</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;ofi&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;fx2trt&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;onnxrt&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;ipex&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>æœ‰å…³å°†torch.compileä¸ ğŸ¤— Transformer ç»“åˆä½¿ç”¨çš„ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹è¿™ç¯‡å…³äºä½¿ç”¨æœ€æ–° PyTorch 2.0 åŠŸèƒ½å¾®è°ƒç”¨äºæ–‡æœ¬åˆ†ç±»çš„ BERT æ¨¡å‹çš„åšå®¢æ–‡ç« : <a class="reference external" href="https://www.philschmid.de/getting-started-pytorch-2-0-transformers">https://www.philschmid.de/getting-started-pytorch-2-0-transformers</a></p></li>
</ul>
</section>
<section id="using-peft">
<h5>Using ğŸ¤— PEFT<a class="headerlink" href="#using-peft" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•åœ¨å¾®è°ƒæœŸé—´å†»ç»“é¢„è®­ç»ƒçš„æ¨¡å‹å‚æ•°ï¼Œå¹¶åœ¨å…¶ä¸Šæ·»åŠ å°‘é‡å¯è®­ç»ƒå‚æ•°ï¼ˆé€‚é…å™¨ï¼‰ã€‚</p></li>
<li><p>As a result the memory associated to the <code class="docutils literal notranslate"><span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">and</span> <span class="pre">gradients</span></code> are greatly reduced.</p></li>
</ul>
<p>å¯¹äºæ™®é€š AdamWï¼Œä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜è¦æ±‚ä¸º:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">fp32</span> <span class="n">copy</span> <span class="n">of</span> <span class="n">parameters</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
<span class="mf">2.</span> <span class="n">Momentum</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
<span class="mf">3.</span> <span class="n">Variance</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
</pre></div>
</div>
<p>ä¸€ä¸ª7Bçš„æ¨¡å‹ and 200 million parameters injected with <code class="docutils literal notranslate"><span class="pre">Low</span> <span class="pre">Rank</span> <span class="pre">Adapters</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>æ™®é€šæ¨¡å‹ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚:
    12 * 7 = 84 GB
æ·»åŠ  Lora ä¼šç¨å¾®å¢åŠ ä¸æ¨¡å‹æƒé‡ç›¸å…³çš„å†…å­˜ï¼Œä½†ä¼šå°†ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜éœ€æ±‚å¤§å¹…é™ä½è‡³
    12 * 0.2 = 2.4GB
</pre></div>
</div>
</section>
<section id="using-accelerate">
<h5>Using ğŸ¤— Accelerate<a class="headerlink" href="#using-accelerate" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">default_args</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># å®Œæ•´ç¤ºä¾‹è®­ç»ƒå¾ªç¯</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">accelerate</span><span class="w"> </span><span class="kn">import</span> <span class="n">Accelerator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.dataloader</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">training_args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fp16</span><span class="o">=</span><span class="n">training_args</span><span class="o">.</span><span class="n">fp16</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adam_bnb_optim</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
    <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="multiple-gpus-and-parallelism">
<h4>Multiple GPUs and parallelism<a class="headerlink" href="#multiple-gpus-and-parallelism" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é‡‡ç”¨å¤šç§æŠ€æœ¯æ¥å®ç°å¹¶è¡Œæ€§ï¼Œä¾‹å¦‚æ•°æ®å¹¶è¡Œæ€§ã€å¼ é‡å¹¶è¡Œæ€§å’Œç®¡é“å¹¶è¡Œæ€§ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ²¡æœ‰ä¸€ç§ä¸‡èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œæœ€ä½³è®¾ç½®å–å†³äºæ‚¨æ‰€ä½¿ç”¨çš„ç‰¹å®šç¡¬ä»¶é…ç½®ã€‚</p></li>
</ul>
<p>Scalability strategy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Parallelization strategy for a single Node / multi-GPU setup
    Case 1: Your model fits onto a single GPU
        DDP - Distributed DataParallel
        Zero Redundancy Optimizer (ZeRO): https://arxiv.org/abs/1910.02054
    Case 2: Your model doesnâ€™t fit onto a single GPU:
        PipelineParallel (PP)
        ZeRO
        TensorParallel (TP)
    Case 3: Largest layer of your model does not fit onto a single GPU
        If you are not using ZeRO, you have to use TensorParallel (TP), because PipelineParallel (PP) alone wonâ€™t be sufficient to accommodate the large layer.
        If you are using ZeRO, additionally adopt techniques from the Methods and tools for efficient training on a single GPU.

2. Parallelization strategy for a multi Node / multi-GPU setup
    When you have fast inter-node connectivity (e.g., NVLINK or NVSwitch) consider using one of these options:
        ZeRO - as it requires close to no modifications to the model
        A combination of PipelineParallel(PP) with TensorParallel(TP) and DataParallel(DP)
    When you have slow inter-node connectivity and still low on GPU memory:
        Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP), and ZeRO.
</pre></div>
</div>
</section>
<section id="fully-sharded-data-parallel">
<h4>Fully Sharded Data Parallel<a class="headerlink" href="#fully-sharded-data-parallel" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">Fully Sharded Data Parallel (FSDP)</a> : å®Œå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œ (FSDP)æ˜¯ä¸€ç§æ•°æ®å¹¶è¡Œæ–¹æ³•ï¼Œå¯å°†æ¨¡å‹çš„å‚æ•°ã€æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡åˆ°å¯ç”¨ GPUï¼ˆä¹Ÿç§°ä¸ºå·¥ä½œçº¿ç¨‹æˆ–ç­‰çº§ï¼‰çš„æ•°é‡ä¸Šã€‚</p></li>
<li><p>ä¸DistributedDataParallelï¼ˆDDPï¼‰ä¸åŒï¼ŒFSDPé€šè¿‡åœ¨æ¯ä¸ªGPUä¸Šè¿›è¡Œæ¨¡å‹åˆ†ç‰‡è€Œéå®Œæ•´å¤åˆ¶ï¼Œä»è€Œé™ä½äº†å†…å­˜ä½¿ç”¨ã€‚è¿™æé«˜äº† GPU å†…å­˜æ•ˆç‡ï¼Œå¹¶å…è®¸æ‚¨åœ¨æ›´å°‘çš„ GPU ä¸Šè®­ç»ƒæ›´å¤§çš„æ¨¡å‹ã€‚</p></li>
<li><p>Unlike DistributedDataParallel (DDP), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs.</p></li>
<li><p>FSDP is integrated with the Accelerate</p></li>
</ul>
</section>
<section id="deepspeed">
<h4>DeepSpeed<a class="headerlink" href="#deepspeed" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>DeepSpeedæ˜¯ä¸€ä¸ª PyTorch ä¼˜åŒ–åº“ï¼Œå¯æé«˜åˆ†å¸ƒå¼è®­ç»ƒçš„å†…å­˜æ•ˆç‡å’Œé€Ÿåº¦ã€‚å…¶æ ¸å¿ƒæ˜¯é›¶å†—ä½™ä¼˜åŒ–å™¨ï¼ˆZeROï¼‰ï¼Œå®ƒå¯ä»¥å¤§è§„æ¨¡è®­ç»ƒå¤§å‹æ¨¡å‹ã€‚</p></li>
</ul>
</section>
<section id="efficient-training-on-cpu">
<h4>Efficient Training on CPU<a class="headerlink" href="#efficient-training-on-cpu" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_path</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">use_ipex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">use_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="distributed-cpu-training">
<h4>Distributed CPU training<a class="headerlink" href="#distributed-cpu-training" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>åŸºäº PyTorch çš„ DDPæ”¯æŒè¿›è¡Œåˆ†å¸ƒå¼ CPU è®­ç»ƒ</p></li>
</ul>
</section>
</section>
<section id="optimizing-inference">
<h3>Optimizing inference<a class="headerlink" href="#optimizing-inference" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="cpu-inference">
<h4>CPU inference<a class="headerlink" href="#cpu-inference" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é€šè¿‡ä¸€äº›ä¼˜åŒ–ï¼Œå¯ä»¥åœ¨ CPU ä¸Šé«˜æ•ˆè¿è¡Œå¤§å‹æ¨¡å‹æ¨ç†ã€‚</p></li>
<li><p>å…¶ä¸­ä¸€ç§ä¼˜åŒ–æŠ€æœ¯æ¶‰åŠå°† PyTorch ä»£ç ç¼–è¯‘ä¸ºé€‚ç”¨äº C++ ç­‰é«˜æ€§èƒ½ç¯å¢ƒçš„ä¸­é—´æ ¼å¼ã€‚</p></li>
<li><p>å¦ä¸€ç§æŠ€æœ¯å°†å¤šä¸ªæ“ä½œèåˆåˆ°ä¸€ä¸ªå†…æ ¸ä¸­ï¼Œä»¥å‡å°‘å•ç‹¬è¿è¡Œæ¯ä¸ªæ“ä½œçš„å¼€é”€ã€‚</p></li>
</ul>
<section id="id10">
<h5>ğŸ¤— Optimum<a class="headerlink" href="#id10" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ONNX Runtime (ORT) æ˜¯ä¸€ä¸ªæ¨¡å‹åŠ é€Ÿå™¨ï¼Œé»˜è®¤åœ¨ CPU ä¸Šè¿è¡Œæ¨ç†ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForQuestionAnswering</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ORTModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;optimum/roberta-base-squad2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;deepset/roberta-base-squad2&quot;</span><span class="p">)</span>

<span class="n">onnx_qa</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What&#39;s my name?&quot;</span>
<span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;My name is Philipp and I live in Nuremberg.&quot;</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">onnx_qa</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="gpu-inference">
<h4>GPU inference<a class="headerlink" href="#gpu-inference" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="id11">
<h5>FlashAttention-2<a class="headerlink" href="#id11" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="admonition warning">
<p class="admonition-title">è­¦å‘Š</p>
<p>FlashAttention-2 æ˜¯å®éªŒæ€§çš„ï¼Œåœ¨æœªæ¥çš„ç‰ˆæœ¬ä¸­å¯èƒ½ä¼šå‘ç”Ÿå¾ˆå¤§çš„å˜åŒ–ã€‚</p>
</div>
<ul>
<li><p>FlashAttention-2æ˜¯æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„æ›´å¿«ã€æ›´é«˜æ•ˆçš„å®ç°ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾ç€åŠ é€Ÿæ¨ç†:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">additionally</span> <span class="n">parallelizing</span> <span class="n">the</span> <span class="n">attention</span> <span class="n">computation</span> <span class="n">over</span> <span class="n">sequence</span> <span class="n">length</span>
<span class="o">-</span> <span class="n">partitioning</span> <span class="n">the</span> <span class="n">work</span> <span class="n">between</span> <span class="n">GPU</span> <span class="n">threads</span> <span class="n">to</span> <span class="n">reduce</span> <span class="n">communication</span> <span class="ow">and</span> <span class="n">shared</span> <span class="n">memory</span> <span class="n">reads</span><span class="o">/</span><span class="n">writes</span> <span class="n">between</span> <span class="n">them</span>
</pre></div>
</div>
</li>
<li><p>è¦å¯ç”¨ FlashAttention-2ï¼Œè¯·ä¼ é€’å‚æ•° attn_implementation=â€flash_attention_2â€ åˆ°from_pretrained() ï¼š</p></li>
</ul>
<p>å®‰è£…:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">flash</span><span class="o">-</span><span class="n">attn</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">build</span><span class="o">-</span><span class="n">isolation</span>
</pre></div>
</div>
</section>
<section id="bettertransformer">
<h5>BetterTransformer<a class="headerlink" href="#bettertransformer" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<p>BetterTransformer é€šè¿‡å…¶ fastpath æ‰§è¡ŒåŠ é€Ÿæ¨ç†ã€‚fastpath æ‰§è¡Œä¸­çš„ä¸¤ä¸ªä¼˜åŒ–æ˜¯:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>fusion, which combines multiple sequential operations into a single â€œkernelâ€ to reduce the number of computation steps
skipping the inherent sparsity of padding tokens to avoid unnecessary computation with nested tensors
</pre></div>
</div>
</section>
<section id="id12">
<h5>bitsandbytes<a class="headerlink" href="#id12" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>bitsandbytes æ˜¯ä¸€ä¸ªé‡åŒ–åº“ï¼Œæ”¯æŒ 4 ä½å’Œ 8 ä½é‡åŒ–ã€‚ä¸åŸç”Ÿå…¨ç²¾åº¦ç‰ˆæœ¬ç›¸æ¯”ï¼Œé‡åŒ–å¯å‡å°æ¨¡å‹å¤§å°ï¼Œä»è€Œæ›´è½»æ¾åœ°å°†å¤§å‹æ¨¡å‹å®‰è£…åˆ°å†…å­˜æœ‰é™çš„ GPU ä¸Šã€‚</p></li>
<li><p>åšå®¢- ä½¿ç”¨ Hugging Face Transformersã€Accelerate å’Œ BitsandBytes è¿›è¡Œå¤§è§„æ¨¡å˜å‹å™¨çš„ 8 ä½çŸ©é˜µä¹˜æ³•ç®€ä»‹: <a class="reference external" href="https://huggingface.co/blog/hf-bitsandbytes-integration">https://huggingface.co/blog/hf-bitsandbytes-integration</a></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">bitsandbytes</span><span class="o">&gt;=</span><span class="mf">0.39.0</span> <span class="n">accelerate</span><span class="o">&gt;=</span><span class="mf">0.20.0</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="others">
<h3>Others<a class="headerlink" href="#others" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="optimize-inference-using-torch-compile">
<h4>Optimize inference using torch.compile()<a class="headerlink" href="#optimize-inference-using-torch-compile" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æœ¬èŠ‚æ—¨åœ¨ä¸ºğŸ¤— Transformers ä¸­çš„è®¡ç®—æœºè§†è§‰æ¨¡å‹çš„torch.compile()å¼•å…¥çš„æ¨ç†åŠ é€Ÿæä¾›åŸºå‡†ã€‚</p></li>
<li><p>æ ¹æ®æ¨¡å‹å’Œ GPUï¼Œ torch.compile()åœ¨æ¨ç†è¿‡ç¨‹ä¸­å¯å®ç°é«˜è¾¾ 30% çš„åŠ é€Ÿ</p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForImageClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_ID</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="conceptual-guides">
<h2>Conceptual guides<a class="headerlink" href="#conceptual-guides" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="glossary">
<h3>Glossary<a class="headerlink" href="#glossary" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="dataparallel-dp">
<h4>DataParallel (DP)<a class="headerlink" href="#dataparallel-dp" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul>
<li><p>ã€åŸå§‹ã€‘Parallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance receiving a distinct data slice. The processing is done in parallel and all setups are synchronized at the end of each training step.</p></li>
<li><p>ã€å®šä¹‰ã€‘Data Parallelismï¼ˆæ•°æ®å¹¶è¡Œï¼‰æ˜¯ä¸€ç§å¸¸ç”¨çš„å¹¶è¡Œè®­ç»ƒæŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ç”¨äºå¤š GPU çš„**è®­ç»ƒ**åœºæ™¯ã€‚è¯¥æŠ€æœ¯é€šè¿‡åœ¨å¤šä¸ª GPU ä¸Šå¤åˆ¶ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ï¼Œå¹¶å°†è¾“å…¥æ•°æ®åˆ’åˆ†æˆå¤šä¸ªç‹¬ç«‹çš„éƒ¨åˆ†ï¼Œä½¿å¾—æ¯ä¸ª GPU å¤„ç†ä¸åŒçš„æ•°æ®å­é›†ï¼Œä»è€Œå¹¶è¡Œæ‰§è¡Œè®¡ç®—ä»»åŠ¡ã€‚æ¯ä¸ª GPU å¤„ç†è‡ªå·±çš„æ•°æ®å—ï¼Œè®¡ç®—æ¢¯åº¦ï¼Œæœ€ç»ˆæ‰€æœ‰ GPU çš„æ¢¯åº¦ä¼šè¿›è¡Œæ±‡æ€»ï¼ˆåŒæ­¥ï¼‰ï¼Œå¹¶æ›´æ–°æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚</p></li>
<li><p>ã€æ ¸å¿ƒæ€æƒ³ã€‘å°†æ•°æ®åˆ†ç‰‡ï¼Œæ¨¡å‹å¤åˆ¶ï¼Œåœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œè®¡ç®—ã€‚</p>
<blockquote>
<div><ul class="simple">
<li><p>æ¨¡å‹å¤åˆ¶ï¼šæ¯ä¸ª GPU éƒ½ä¼šå¾—åˆ°ä¸€ä¸ªå®Œå…¨ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ã€‚æ‰€æœ‰çš„ GPU éƒ½ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹ç»“æ„å’Œæƒé‡è¿›è¡Œè®¡ç®—ã€‚</p></li>
<li><p>æ•°æ®åˆ’åˆ†ï¼šè®­ç»ƒæ•°æ®è¢«åˆ’åˆ†ä¸ºå¤šä¸ªå­é›†ï¼Œæ¯ä¸ª GPU å¤„ç†ä¸åŒçš„å­é›†ï¼Œå®Œæˆå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚</p></li>
<li><p>æ¢¯åº¦åŒæ­¥ï¼šåœ¨æ¯ä¸ª GPU ä¸Šç‹¬ç«‹è®¡ç®—å®Œæ¢¯åº¦åï¼Œæ‰€æœ‰ GPU ä¹‹é—´ä¼šé€šè¿‡é€šä¿¡æœºåˆ¶å°†æ¢¯åº¦è¿›è¡Œæ±‡æ€»æˆ–å¹³å‡ï¼Œç„¶åæ›´æ–°æ¯ä¸ªæ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚è¿™ä¿è¯äº†æ‰€æœ‰ GPU çš„æ¨¡å‹åœ¨æ¯æ¬¡è®­ç»ƒæ­¥éª¤ç»“æŸåä¿æŒåŒæ­¥ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€å·¥ä½œåŸç†ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>æ•°æ®åˆ’åˆ†ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å« 1024 æ¡æ ·æœ¬çš„æ‰¹æ¬¡ï¼ˆbatchï¼‰ï¼Œå¹¶ä¸”ä½ æœ‰ 4 ä¸ª GPUã€‚Data Parallelism ä¼šå°†è¿™ 1024 æ¡æ ·æœ¬åˆ’åˆ†æˆ 4 ä¸ªå­é›†ï¼ˆæ¯ä¸ªå­é›† 256 æ¡æ ·æœ¬ï¼‰ï¼Œå¹¶åˆ†é…åˆ°ä¸åŒçš„ GPU ä¸Šã€‚</p></li>
<li><p>æ¨¡å‹å¤åˆ¶ï¼šæ¯ä¸ª GPU ä¸Šéƒ½ä¼šæœ‰ç›¸åŒçš„æ¨¡å‹å‰¯æœ¬ã€‚è¿™äº›å‰¯æœ¬ä¼šåˆå§‹åŒ–ä¸ºç›¸åŒçš„æƒé‡ï¼Œå¹¶ä¸”åœ¨æ¯ä¸€æ­¥è®­ç»ƒä¸­ï¼Œå®ƒä»¬çš„è®¡ç®—éƒ½æ˜¯åŒæ­¥çš„ã€‚</p></li>
<li><p>å¹¶è¡Œè®¡ç®—ï¼šæ¯ä¸ª GPU ç‹¬ç«‹åœ°å¤„ç†è‡ªå·±åˆ†é…åˆ°çš„æ•°æ®å­é›†ï¼Œæ‰§è¡Œå‰å‘ä¼ æ’­ï¼ˆforward passï¼‰å’Œåå‘ä¼ æ’­ï¼ˆbackward passï¼‰ã€‚è¿™ä¸€éƒ¨åˆ†è®¡ç®—æ˜¯å¹¶è¡Œè¿›è¡Œçš„ï¼Œæ¯ä¸ª GPU çš„è®¡ç®—äº’ä¸å¹²æ‰°ã€‚</p></li>
<li><p>æ¢¯åº¦åŒæ­¥ï¼šå½“æ¯ä¸ª GPU è®¡ç®—å®Œåå‘ä¼ æ’­å¹¶å¾—åˆ°æ¢¯åº¦åï¼Œæ‰€æœ‰ GPU ä¼šè¿›è¡Œæ¢¯åº¦åŒæ­¥ã€‚è¿™æ„å‘³ç€å„ GPU ä¹‹é—´ä¼šé€šè¿‡ç½‘ç»œé€šä¿¡ï¼Œå°†å®ƒä»¬å„è‡ªè®¡ç®—çš„æ¢¯åº¦æ±‡æ€»ï¼ˆé€šå¸¸æ˜¯å–å¹³å‡ï¼‰ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ä¸€è‡´æ›´æ–°ã€‚</p></li>
<li><p>å‚æ•°æ›´æ–°ï¼šæ¢¯åº¦åŒæ­¥å®Œæˆåï¼Œæ¯ä¸ª GPU ä¼šæ›´æ–°è‡ªå·±æ¨¡å‹çš„å‚æ•°ã€‚è¿™äº›å‚æ•°ä¼šé€šè¿‡æ±‡æ€»åçš„æ¢¯åº¦è¿›è¡Œæ›´æ–°ï¼Œä»è€Œä½¿å¾—æ‰€æœ‰ GPU ä¸Šçš„æ¨¡å‹åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸåä¿æŒç›¸åŒçš„æƒé‡ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€ä¼˜åŠ¿ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>è®¡ç®—åŠ é€Ÿï¼šé€šè¿‡å°†å¤§æ‰¹é‡æ•°æ®åˆ†æˆå¤šä¸ªå°å—ï¼Œå¹¶è¡Œå¤„ç†ä¸åŒçš„éƒ¨åˆ†ï¼Œå¯ä»¥æ˜¾è‘—åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚ç†è®ºä¸Šï¼Œä½¿ç”¨ N ä¸ª GPU è¿›è¡Œ Data Parallelismï¼Œå¯ä»¥å®ç°è¿‘ä¼¼ N å€çš„åŠ é€Ÿæ•ˆæœï¼ˆå—é™äºé€šä¿¡å¼€é”€å’Œè´Ÿè½½å‡è¡¡ï¼‰ã€‚</p></li>
<li><p>æ˜“äºå®ç°ï¼šç›¸æ¯”å…¶ä»–å¹¶è¡ŒæŠ€æœ¯ï¼ˆå¦‚ Tensor Parallelism æˆ– Pipeline Parallelismï¼‰ï¼ŒData Parallelism çš„å®ç°ç›¸å¯¹ç®€å•ï¼Œå› ä¸ºåªéœ€è¦å¤åˆ¶æ¨¡å‹ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œåˆ’åˆ†å’Œæ¢¯åº¦åŒæ­¥ã€‚</p></li>
<li><p>æ‰©å±•æ€§ï¼šData Parallelism å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°å¤šä¸ª GPUï¼Œç”šè‡³å¤šä¸ªæœºå™¨ï¼ˆé€šè¿‡åˆ†å¸ƒå¼è®­ç»ƒï¼‰ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†çš„å¤„ç†ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€åŠ£åŠ¿ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>æ˜¾å­˜å‹åŠ›ï¼šæ¯ä¸ª GPU ä¸Šéƒ½éœ€è¦å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å‚æ•°ä¼šè¢«å¤šæ¬¡å¤åˆ¶ã€‚å¦‚æœæ¨¡å‹éå¸¸å¤§ï¼ˆä¾‹å¦‚ GPT-3 è¿™æ ·çš„æ¨¡å‹ï¼‰ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ˜¾å­˜ä¸è¶³çš„é—®é¢˜ã€‚</p></li>
<li><p>é€šä¿¡å¼€é”€ï¼šåœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ç»“æŸæ—¶ï¼Œæ‰€æœ‰ GPU éœ€è¦åŒæ­¥æ¢¯åº¦ã€‚éšç€ GPU æ•°é‡çš„å¢åŠ ï¼Œé€šä¿¡å¼€é”€ä¼šé€æ¸å¢åŠ ï¼Œå°¤å…¶æ˜¯åœ¨å¤šä¸ªæœºå™¨ä¹‹é—´è¿›è¡ŒåŒæ­¥æ—¶ï¼Œç½‘ç»œé€šä¿¡å¯èƒ½ä¼šæˆä¸ºç“¶é¢ˆã€‚</p></li>
<li><p>è´Ÿè½½å‡è¡¡ï¼šå¦‚æœæ•°æ®åˆ’åˆ†ä¸å‡åŒ€ï¼ŒæŸäº› GPU å¯èƒ½éœ€è¦å¤„ç†è¾ƒé‡çš„å·¥ä½œï¼Œè€Œå…¶ä»– GPU åˆ™å¯èƒ½å¤„äºé—²ç½®çŠ¶æ€ï¼Œè¿™ä¼šå½±å“å¹¶è¡Œæ•ˆç‡ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€æ€»ç»“ã€‘Data Parallelism æ˜¯ä¸€ç§å°†æ¨¡å‹å‰¯æœ¬åˆ†é…åˆ°å¤šä¸ª GPU å¹¶è¡Œå¤„ç†ä¸åŒæ•°æ®å­é›†çš„è®­ç»ƒæŠ€æœ¯ã€‚é€šè¿‡åœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œå¤„ç†ï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯é€‚ç”¨äºå¤§å‹æ•°æ®é›†çš„å¤„ç†åœºæ™¯ã€‚è™½ç„¶å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†æ˜¾å­˜æ¶ˆè€—å’Œé€šä¿¡å¼€é”€æ˜¯ Data Parallelism é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜ã€‚</p></li>
</ul>
</section>
<section id="pipelineparallel-pp">
<h4>PipelineParallel (PP)<a class="headerlink" href="#pipelineparallel-pp" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul>
<li><p>ã€åŸå§‹æ–‡æ¡£ã€‘Parallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and working on a small chunk of the batch. Learn more about how PipelineParallel works here.</p></li>
<li><p>ã€å®šä¹‰ã€‘Pipeline Parallelismï¼ˆæµæ°´çº¿å¹¶è¡Œï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„å¹¶è¡ŒæŠ€æœ¯ï¼Œç‰¹åˆ«é€‚ç”¨äºè®­ç»ƒå¤§å‹ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†æ¨¡å‹æŒ‰å±‚çº§è¿›è¡Œåˆ’åˆ†ï¼Œå¹¶å°†è¿™äº›åˆ’åˆ†åçš„éƒ¨åˆ†åˆ†é…åˆ°ä¸åŒçš„ GPU ä¸Šï¼Œä»è€Œåœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œå¤„ç†æ¨¡å‹çš„è®¡ç®—ä»»åŠ¡ã€‚æ¯ä¸ª GPU åªè´Ÿè´£æ‰§è¡Œæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆå³æŸäº›ç‰¹å®šçš„å±‚ï¼‰ã€‚è¿™ç§åˆ’åˆ†æ–¹å¼è¢«ç§°ä¸ºçºµå‘åˆ‡åˆ†ï¼ˆvertical splitï¼‰ï¼Œç›¸å¯¹ä¼ ç»Ÿçš„æ•°æ®å¹¶è¡Œï¼ˆdata parallelismï¼‰ï¼Œå®ƒä¸æ˜¯åœ¨ä¸åŒè®¾å¤‡ä¸Šå¤„ç†ç›¸åŒçš„æ¨¡å‹ï¼Œè€Œæ˜¯å°†æ¨¡å‹æœ¬èº«æ‹†åˆ†å¼€æ¥ã€‚</p></li>
<li><p>ã€å·¥ä½œåŸç†ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>1.æ¨¡å‹åˆ’åˆ†ï¼ˆåˆ†å±‚ï¼‰ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ª 12 å±‚çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚ä½ å¯ä»¥å°†å‰ 4 å±‚æ”¾åœ¨ GPU 1 ä¸Šï¼Œæ¥ä¸‹æ¥çš„ 4 å±‚æ”¾åœ¨ GPU 2 ä¸Šï¼Œæœ€å 4 å±‚æ”¾åœ¨ GPU 3 ä¸Šã€‚æ¯ä¸ª GPU åªå­˜å‚¨å’Œè®¡ç®—æ¨¡å‹çš„ä¸€éƒ¨åˆ†ã€‚</p></li>
<li><p>2.æ‰¹é‡å¤„ç†ï¼šPipeline Parallelism é€šå¸¸ä¸æ‰¹å¤„ç†ï¼ˆbatch processingï¼‰ç»“åˆä½¿ç”¨ã€‚å‡è®¾è¾“å…¥çš„ batch æ˜¯ 128 æ¡æ ·æœ¬ï¼šGPU 1 å¤„ç†å‰ 4 å±‚æ—¶ï¼Œå®ƒä¼šå¤„ç†ç¬¬ä¸€å°å—æ•°æ®ï¼ˆæ¯”å¦‚ 64 æ¡æ ·æœ¬ï¼‰ï¼Œç„¶åå°†è¿™äº›æ ·æœ¬çš„è¾“å‡ºä¼ é€’ç»™ GPU 2ã€‚å½“ GPU 2 å¼€å§‹å¤„ç†è¿™äº›æ ·æœ¬æ—¶ï¼ŒGPU 1 å¯ä»¥å¼€å§‹å¤„ç† batch ä¸­çš„ä¸‹ä¸€å°å—æ•°æ®ã€‚è¿™æ ·ï¼Œå¤šä¸ª GPU èƒ½å¤Ÿå¹¶è¡Œå·¥ä½œï¼Œåƒæµæ°´çº¿ä¸€æ ·å¤„ç†æ•°æ®ï¼Œè¿™å°±æ˜¯æµæ°´çº¿å¹¶è¡Œçš„åç§°æ¥æºã€‚</p></li>
<li><ol class="arabic simple" start="3">
<li><p>æµæ°´çº¿æœºåˆ¶ï¼šå„ä¸ª GPU å¹¶ä¸æ˜¯å®Œå…¨ç‹¬ç«‹å·¥ä½œçš„ï¼Œè€Œæ˜¯æŒ‰é¡ºåºå¤„ç†æ•°æ®ã€‚æ¨¡å‹çš„æ¯ä¸€éƒ¨åˆ†ï¼ˆå±‚ï¼‰ä¾èµ–äºå‰ä¸€éƒ¨åˆ†çš„è¾“å‡ºã€‚è™½ç„¶å„ä¸ª GPU æ˜¯å¹¶è¡Œçš„ï¼Œä½†å®ƒä»¬å·¥ä½œåœ¨åŒä¸€æ¡æµæ°´çº¿ä¸Šï¼šå½“ GPU 1 å¤„ç†ç¬¬ä¸€ä¸ªæ•°æ®å—æ—¶ï¼ŒGPU 2 å¤„äºç©ºé—²çŠ¶æ€ï¼›å½“ GPU 1 å¤„ç†å®Œç¬¬ä¸€ä¸ªæ•°æ®å—å¹¶ä¼ é€’ç»™ GPU 2 æ—¶ï¼ŒGPU 2 å¼€å§‹å¤„ç†ç¬¬ä¸€å—æ•°æ®ï¼ŒåŒæ—¶ GPU 1 å¯ä»¥å¤„ç†ç¬¬äºŒå—æ•°æ®ï¼›å¦‚æ­¤å¾ªç¯ï¼Œç›´åˆ°æ•´ä¸ª batch è¢«å¤„ç†å®Œæ¯•ã€‚</p></li>
</ol>
</li>
</ul>
</div></blockquote>
</li>
<li><p>ã€å¥½å¤„ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>èŠ‚çœæ˜¾å­˜ï¼šå¯¹äºéå¸¸å¤§çš„æ¨¡å‹ï¼Œå•ä¸ª GPU å¯èƒ½æ— æ³•ä¸€æ¬¡æ€§å®¹çº³æ•´ä¸ªæ¨¡å‹çš„æ‰€æœ‰å±‚ã€‚é€šè¿‡å°†æ¨¡å‹åˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œæ¯ä¸ª GPU åªå­˜å‚¨ä¸€éƒ¨åˆ†æ¨¡å‹å‚æ•°ï¼Œæ˜¾è‘—å‡å°‘äº†å•ä¸ª GPU çš„æ˜¾å­˜å‹åŠ›ã€‚</p></li>
<li><p>å¹¶è¡Œæ•ˆç‡ï¼šPipeline Parallelism é€šè¿‡è®©ä¸åŒçš„ GPU åŒæ—¶å¤„ç†ä¸åŒçš„æ•°æ®å—ï¼Œå¢åŠ äº†è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡éœ€è¦ä¸€å®šçš„é€šä¿¡å’ŒåŒæ­¥ï¼Œä½†ç›¸æ¯”äºåœ¨å•ä¸ª GPU ä¸Šè¿è¡Œå®Œæ•´æ¨¡å‹ï¼Œæµæ°´çº¿å¹¶è¡Œå¯ä»¥åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€æŒ‘æˆ˜ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>é€šä¿¡å¼€é”€ï¼šç”±äºä¸åŒ GPU ä¹‹é—´éœ€è¦ç›¸äº’ä¼ é€’æ•°æ®ï¼ˆå³å‰ä¸€å±‚çš„è¾“å‡ºéœ€è¦ä¼ é€’åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼‰ï¼ŒGPU ä¹‹é—´çš„é€šä¿¡å¸¦æ¥äº†ä¸€å®šçš„å¼€é”€ï¼Œç‰¹åˆ«æ˜¯å½“ GPU æ•°é‡è¾ƒå¤šæ—¶ï¼Œè¿™ç§å¼€é”€å¯èƒ½ä¼šå˜å¾—æ˜¾è‘—ã€‚</p></li>
<li><p>å»¶è¿Ÿï¼šæµæ°´çº¿å¹¶è¡Œä¼šæœ‰ä¸€å®šçš„å¯åŠ¨å»¶è¿Ÿï¼ˆå³å‰ä¸€ä¸ªè®¾å¤‡å¿…é¡»å…ˆå¤„ç†å®Œéƒ¨åˆ†æ•°æ®åï¼Œæ‰èƒ½å°†æ•°æ®ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªè®¾å¤‡ï¼‰ã€‚å¯¹äºå° batch sizeï¼Œè¿™ç§å»¶è¿Ÿä¼šæ›´åŠ æ˜æ˜¾ã€‚</p></li>
<li><p>è´Ÿè½½å‡è¡¡ï¼šæ¨¡å‹å„å±‚çš„è®¡ç®—å¤æ‚åº¦ä¸åŒï¼ŒæŸäº›å±‚å¯èƒ½éœ€è¦æ›´å¤šçš„è®¡ç®—èµ„æºã€‚å¦‚æœæ¯ä¸ª GPU å¤„ç†çš„å±‚æ•°ç›¸åŒï¼Œä½†è®¡ç®—é‡ä¸åŒï¼Œå°±ä¼šå¯¼è‡´æŸäº› GPU å·¥ä½œè´Ÿè½½é‡ï¼Œå¦ä¸€äº› GPU é—²ç½®ï¼Œè¿™ç§è´Ÿè½½ä¸å‡è¡¡ä¹Ÿä¼šå½±å“å¹¶è¡Œæ•ˆç‡ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€æ€»ç»“ã€‘Pipeline Parallelism æ˜¯ä¸€ç§é€šè¿‡å°†æ¨¡å‹çºµå‘æ‹†åˆ†ï¼ˆæŒ‰å±‚åˆ’åˆ†ï¼‰å¹¶åˆ†å¸ƒåˆ°å¤šä¸ª GPU ä¸Šå¤„ç†çš„å¹¶è¡ŒæŠ€æœ¯ã€‚æ¯ä¸ª GPU è´Ÿè´£è®¡ç®—æ¨¡å‹çš„ä¸€éƒ¨åˆ†å±‚ï¼Œå¹¶ä¸”å„ GPU åƒæµæ°´çº¿ä¸€æ ·å¤„ç†æ‰¹é‡æ•°æ®ï¼Œè¿™æ—¢èƒ½å‡å°‘å•ä¸ª GPU çš„æ˜¾å­˜æ¶ˆè€—ï¼Œåˆèƒ½é€šè¿‡å¹¶è¡Œå¤„ç†åŠ é€Ÿè®¡ç®—ã€‚ä½†åŒæ—¶ä¹Ÿå¸¦æ¥äº†é€šä¿¡å¼€é”€å’Œè´Ÿè½½å‡è¡¡ç­‰æŒ‘æˆ˜ã€‚</p></li>
</ul>
</section>
<section id="tensor-parallelism-tp">
<h4>Tensor Parallelism (TP)<a class="headerlink" href="#tensor-parallelism-tp" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul>
<li><p>ã€åŸå§‹ã€‘Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets processed separately and in parallel on different GPUs and the results are synced at the end of the processing step. This is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level. Learn more about Tensor Parallelism here.</p></li>
<li><p>ã€å®šä¹‰ã€‘Tensor Parallelismï¼ˆå¼ é‡å¹¶è¡Œï¼‰æ˜¯ä¸€ç§åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼Œä¸»è¦ç”¨äºå°†æ¨¡å‹çš„å¼ é‡åˆ‡åˆ†ä¸ºå¤šä¸ªéƒ¨åˆ†ï¼Œå¹¶å°†è¿™äº›éƒ¨åˆ†åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¹¶è¡Œå¤„ç†ã€‚ä¸ Pipeline Parallelism ä¸åŒï¼ŒTensor Parallelism ä¸å°†æ¨¡å‹æŒ‰å±‚çº§åˆ’åˆ†ï¼Œè€Œæ˜¯å°†æ¯ä¸ªå¼ é‡ï¼ˆå¦‚æƒé‡çŸ©é˜µæˆ–è¾“å…¥æ•°æ®ï¼‰æ°´å¹³åˆ‡åˆ†ï¼ˆhorizontally splitï¼‰ï¼Œå› æ­¤ä¹Ÿè¢«ç§°ä¸ºæ°´å¹³å¹¶è¡Œï¼ˆhorizontal parallelismï¼‰ã€‚</p></li>
<li><p>ã€æ ¸å¿ƒæ€æƒ³ã€‘å°†æ¨¡å‹çš„å¼ é‡ï¼ˆåŒ…æ‹¬å‚æ•°ã€æ¿€æ´»å€¼ç­‰ï¼‰åˆ‡åˆ†ä¸ºå¤šä¸ªå—ï¼ˆshardsï¼‰ï¼Œå¹¶å°†è¿™äº›å—åˆ†å¸ƒåˆ°ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚è¿™æ ·å¯ä»¥å‡å°‘æ¯ä¸ª GPU çš„è®¡ç®—è´Ÿè½½å’Œæ˜¾å­˜å‹åŠ›ï¼ŒåŒæ—¶åŠ é€Ÿè®­ç»ƒã€‚</p></li>
<li><p>ã€å·¥ä½œåŸç†ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>å¼ é‡åˆ‡åˆ†ï¼šå‡è®¾ä½ æœ‰ä¸€ä¸ªå¼ é‡ï¼ˆä¾‹å¦‚æƒé‡çŸ©é˜µï¼‰å¤§å°ä¸º (1024, 1024)ï¼Œè€Œä½ æœ‰ 4 ä¸ª GPUã€‚ä½ å¯ä»¥å°†è¿™ä¸ªå¼ é‡æ°´å¹³åˆ‡åˆ†æˆ 4 ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†çš„å¤§å°ä¸º (256, 1024)ï¼Œåˆ†åˆ«æ”¾ç½®åœ¨ 4 ä¸ªä¸åŒçš„ GPU ä¸Šã€‚è¿™æ ·ï¼Œå¼ é‡çš„ä¸åŒéƒ¨åˆ†ä¼šåˆ†åˆ«åœ¨ä¸åŒçš„ GPU ä¸Šè¿›è¡Œå¤„ç†ã€‚</p></li>
<li><p>å¹¶è¡Œè®¡ç®—ï¼šæ¯ä¸ª GPU å¤„ç†å¼ é‡çš„ä¸åŒéƒ¨åˆ†ï¼Œå¹¶è¿›è¡Œç‹¬ç«‹çš„è®¡ç®—ã€‚ä¾‹å¦‚ï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œæ¯ä¸ª GPU ä¼šå¤„ç†å…¶åˆ†é…åˆ°çš„å¼ é‡éƒ¨åˆ†ã€‚åœ¨åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¹Ÿåœ¨å„è‡ªçš„ GPU ä¸Šè®¡ç®—ã€‚</p></li>
<li><p>ç»“æœåŒæ­¥ï¼šåœ¨æ¯ä¸ªè®¡ç®—æ­¥éª¤ï¼ˆå¦‚å‰å‘ä¼ æ’­æˆ–åå‘ä¼ æ’­ï¼‰ç»“æŸæ—¶ï¼Œå„ä¸ª GPU ä¼šå°†å®ƒä»¬çš„éƒ¨åˆ†ç»“æœè¿›è¡ŒåŒæ­¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ›´æ–°æ—¶æ‰€æœ‰å¼ é‡éƒ¨åˆ†çš„è®¡ç®—ç»“æœèƒ½å¤Ÿæ±‡æ€»ã€‚åŒæ­¥çš„è¿‡ç¨‹æ˜¯é€šè¿‡é€šä¿¡æœºåˆ¶å®Œæˆçš„ï¼Œé€šå¸¸ä½¿ç”¨åˆ†å¸ƒå¼æ¡†æ¶ï¼ˆå¦‚ NVIDIA çš„ NCCL åº“ï¼‰æ¥é«˜æ•ˆä¼ é€’æ•°æ®ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€ä¼˜åŠ¿ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>å‡å°‘æ˜¾å­˜å ç”¨ï¼šé€šè¿‡å°†å¤§çš„å¼ é‡åˆ†æˆå¤šä¸ªå°å—ï¼Œæ¯ä¸ª GPU åªéœ€è¦å­˜å‚¨å’Œè®¡ç®—è‡ªå·±è´Ÿè´£çš„å¼ é‡éƒ¨åˆ†ï¼Œæ˜¾è‘—å‡å°‘äº†å•ä¸ª GPU çš„æ˜¾å­˜æ¶ˆè€—ã€‚è¿™åœ¨å¤„ç†å¤§å‹æ¨¡å‹ï¼ˆå¦‚ GPT-3ï¼‰æ—¶éå¸¸é‡è¦ï¼Œå› ä¸ºå•ä¸ª GPU æ— æ³•å®¹çº³æ•´ä¸ªæ¨¡å‹çš„æƒé‡ã€‚</p></li>
<li><p>åŠ é€Ÿè®¡ç®—ï¼šç”±äºæ¯ä¸ª GPU åªè´Ÿè´£ä¸€éƒ¨åˆ†å¼ é‡ï¼Œè®¡ç®—å¯ä»¥åœ¨å¤šä¸ª GPU ä¸Šå¹¶è¡Œè¿›è¡Œï¼Œç†è®ºä¸Šå¯ä»¥çº¿æ€§åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€æŒ‘æˆ˜ã€‘</p>
<blockquote>
<div><ul class="simple">
<li><p>é€šä¿¡å¼€é”€ï¼šæ¯ä¸ªè®¡ç®—æ­¥éª¤ç»“æŸæ—¶ï¼Œå„ä¸ª GPU éœ€è¦åŒæ­¥ç»“æœã€‚å¯¹äºè¾ƒå¤§çš„æ¨¡å‹å’Œè¾ƒé¢‘ç¹çš„åŒæ­¥æ“ä½œï¼Œè¿™ä¼šå¯¼è‡´æ˜¾è‘—çš„é€šä¿¡å¼€é”€ï¼Œå½±å“æ•´ä½“æ€§èƒ½ã€‚</p></li>
<li><p>è´Ÿè½½å‡è¡¡ï¼šåœ¨å®é™…åº”ç”¨ä¸­ï¼ŒæŸäº›å¼ é‡å¯èƒ½è¾ƒå°ï¼Œåˆ‡åˆ†åä¸åŒçš„ GPU ä¸Šè®¡ç®—é‡å¯èƒ½ä¸å‡è¡¡ï¼Œå¯¼è‡´æŸäº› GPU è®¡ç®—è¾ƒæ…¢ï¼Œè¿›è€Œæ‹–æ…¢æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ã€‚</p></li>
<li><p>å®ç°å¤æ‚åº¦ï¼šç›¸æ¯” Data Parallelismï¼ŒTensor Parallelism çš„å®ç°æ›´ä¸ºå¤æ‚ï¼Œå› ä¸ºæ¶‰åŠåˆ°å¼ é‡çš„åˆ‡åˆ†ã€åˆ†é…ã€å¹¶è¡Œè®¡ç®—å’ŒåŒæ­¥ç­‰å¤šä¸ªæ­¥éª¤ã€‚</p></li>
</ul>
</div></blockquote>
</li>
<li><p>ã€æ€»ç»“ã€‘Tensor Parallelism æ˜¯ä¸€ç§é€šè¿‡æ°´å¹³åˆ‡åˆ†æ¨¡å‹å¼ é‡æ¥åˆ†é…åˆ°å¤šä¸ª GPU è¿›è¡Œå¹¶è¡Œè®¡ç®—çš„æŠ€æœ¯ã€‚è¿™ç§æŠ€æœ¯å¯ä»¥æ˜¾è‘—å‡å°‘æ˜¾å­˜æ¶ˆè€—ï¼Œå°¤å…¶é€‚åˆå¤„ç†éå¸¸å¤§çš„æ¨¡å‹ã€‚å®ƒé€šè¿‡åœ¨ä¸åŒ GPU ä¸Šå¹¶è¡Œå¤„ç†å¼ é‡çš„ä¸åŒéƒ¨åˆ†æ¥åŠ é€Ÿè®¡ç®—ï¼Œå¹¶åœ¨æ¯ä¸ªè®¡ç®—æ­¥éª¤åé€šè¿‡é€šä¿¡æœºåˆ¶åŒæ­¥ç»“æœã€‚ä¸å…¶ä»–å¹¶è¡ŒæŠ€æœ¯ç›¸æ¯”ï¼ŒTensor Parallelism åœ¨å¤„ç†è¶…å¤§æ¨¡å‹æ—¶éå¸¸æœ‰æ•ˆï¼Œä½†ä¹Ÿé¢ä¸´é€šä¿¡å¼€é”€å’Œå®ç°å¤æ‚åº¦çš„æŒ‘æˆ˜ã€‚</p></li>
</ul>
</section>
<section id="tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism">
<h4>Tensor Parallelism vs. Data Parallelism vs. Pipeline Parallelism<a class="headerlink" href="#tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Tensor Parallelismï¼šå¼ é‡ï¼ˆå¦‚æƒé‡çŸ©é˜µï¼‰è¢«åˆ‡åˆ†æˆå¤šä¸ªå°å—ï¼Œåˆ†å¸ƒåœ¨ä¸åŒçš„ GPU ä¸Šå¹¶è¡Œè®¡ç®—ã€‚é€‚ç”¨äºéå¸¸å¤§çš„å¼ é‡ï¼Œèƒ½æ˜¾è‘—å‡å°‘æ˜¾å­˜å‹åŠ›ã€‚</p></li>
<li><p>Data Parallelismï¼šç›¸åŒçš„æ¨¡å‹å‰¯æœ¬è¿è¡Œåœ¨æ¯ä¸ª GPU ä¸Šï¼Œä½†æ¯ä¸ª GPU å¤„ç†ä¸åŒçš„è¾“å…¥æ•°æ®ã€‚åœ¨æ¯ä¸ª GPU è®¡ç®—å®Œæ¢¯åº¦åï¼Œæ¢¯åº¦ä¼šè¿›è¡Œå¹³å‡å¹¶æ›´æ–°æ‰€æœ‰æ¨¡å‹å‰¯æœ¬çš„å‚æ•°ã€‚ä¼˜ç‚¹æ˜¯å®ç°ç›¸å¯¹ç®€å•ï¼Œä½†æ˜¾å­˜å‹åŠ›ä¾ç„¶å¾ˆå¤§ï¼Œå› ä¸ºæ¯ä¸ª GPU éƒ½éœ€è¦å­˜å‚¨å®Œæ•´çš„æ¨¡å‹å‚æ•°ã€‚</p></li>
<li><p>Pipeline Parallelismï¼šæ¨¡å‹æŒ‰å±‚çº§åˆ‡åˆ†ï¼Œä¸åŒçš„å±‚åˆ†é…åˆ°ä¸åŒçš„ GPUï¼Œå¤šä¸ª GPU ä»¥æµæ°´çº¿çš„æ–¹å¼å¤„ç†æ‰¹æ¬¡æ•°æ®ã€‚é€‚åˆéå¸¸æ·±çš„ç½‘ç»œï¼Œä½†éœ€è¦è§£å†³æµæ°´çº¿å¯åŠ¨å’ŒåŒæ­¥çš„é—®é¢˜ã€‚</p></li>
</ul>
</section>
<section id="zero-redundancy-optimizer-zero">
<h4>Zero Redundancy Optimizer (ZeRO)<a class="headerlink" href="#zero-redundancy-optimizer-zero" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>å‚è§ç›¸å…³å®šä¹‰</p></li>
</ul>
</section>
<section id="id13">
<h4>Fully Sharded Data Parallel<a class="headerlink" href="#id13" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>å‚è§ä¸Šé¢ç›¸å…³ç« èŠ‚</p>
</div>
</section>
</section>
<section id="optimizing-llms-for-speed-and-memory">
<h3>Optimizing LLMs for Speed and Memory<a class="headerlink" href="#optimizing-llms-for-speed-and-memory" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<p>effective techniques for efficient LLM deployment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Lower Precision
    é™ä½ç²¾åº¦ï¼ˆå³8 ä½å’Œ 4 ä½ï¼‰è¿è¡Œå¯ä»¥å®ç°è®¡ç®—ä¼˜åŠ¿ï¼Œè€Œä¸ä¼šæ˜¾ç€é™ä½æ¨¡å‹æ€§èƒ½
2. Flash Attention
    æ³¨æ„åŠ›ç®—æ³•çš„ä¸€ç§å˜ä½“ï¼Œå®ƒä¸ä»…æä¾›äº†ä¸€ç§æ›´èŠ‚çœå†…å­˜çš„æ–¹æ³•ï¼Œè€Œä¸”ç”±äºä¼˜åŒ–äº† GPU å†…å­˜åˆ©ç”¨ç‡è€Œå®ç°äº†æ•ˆç‡çš„æé«˜
3. Architectural Innovations
    æ¨¡å‹æ¶æ„ä¸­æœ€é‡è¦çš„è¿›æ­¥æœ‰:
        Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).
</pre></div>
</div>
<section id="lower-precision">
<h4>1. Lower Precision<a class="headerlink" href="#lower-precision" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>æ‰€æœ‰é‡åŒ–æŠ€æœ¯çš„å·¥ä½œåŸç†:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Quantize all weights to the target precision
    å°†æ‰€æœ‰æƒé‡é‡åŒ–åˆ°ç›®æ ‡ç²¾åº¦
2. Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision
    åŠ è½½é‡åŒ–æƒé‡ï¼Œå¹¶ä»¥ bfloat16 ç²¾åº¦ä¼ é€’å‘é‡çš„è¾“å…¥åºåˆ—
3. Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision
    å°†æƒé‡åŠ¨æ€åé‡åŒ–ä¸º bfloat16ï¼Œä»¥ä½¿ç”¨ bfloat16 ç²¾åº¦çš„è¾“å…¥å‘é‡æ‰§è¡Œè®¡ç®—
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">å¤‡æ³¨</p>
<p>ä½¿ç”¨é‡åŒ–æƒé‡æ—¶ï¼Œå…ˆæŠŠæƒé‡åé‡åŒ–ä¸º bfloat16ï¼Œè¾“å…¥åºåˆ—è¿˜æ˜¯ bfloat16ï¼Œè®¡ç®—ä¸¤è€…ä¹˜ç§¯ã€‚æ‰€ä»¥ï¼Œæ¨ç†æ—¶é—´é€šå¸¸ä¸ä¼šå‡å°‘ï¼Œåè€Œä¼šå¢åŠ ã€‚æ€»ä¹‹ï¼Œé‡è¦çš„æ˜¯è¦è®°ä½ï¼Œæ¨¡å‹é‡åŒ–ä»¥å‡†ç¡®æ€§å’Œåœ¨æŸäº›æƒ…å†µä¸‹ç‰ºç‰²æ¨ç†æ—¶é—´ä¸ºä»£ä»·æé«˜å†…å­˜æ•ˆç‡ã€‚</p>
</div>
</section>
<section id="flash-attention">
<h4>2. Flash Attention<a class="headerlink" href="#flash-attention" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>å¯¹äºå¤§å‹è¾“å…¥ä¸Šä¸‹æ–‡ï¼Œé»˜è®¤çš„è‡ªæ³¨æ„åŠ›ç®—æ³•å¾ˆå¿«å°±ä¼šå˜å¾—éå¸¸æ˜‚è´µçš„å†…å­˜æ¶ˆè€—ã€‚</p></li>
<li><p>é€šè¿‡è·Ÿè¸ª softmax å½’ä¸€åŒ–ç»Ÿè®¡æ•°æ®å¹¶ä½¿ç”¨ä¸€äº›æ™ºèƒ½æ•°å­¦ï¼Œä¸é»˜è®¤çš„è‡ªæ³¨æ„åŠ›å±‚ç›¸æ¯”ï¼ŒFlash Attention æä¾›äº†ç›¸åŒçš„æ•°å€¼è¾“å‡ºï¼Œè€Œå†…å­˜æˆæœ¬ä»…éšæ—¶é—´çº¿æ€§å¢åŠ  N</p></li>
<li><p>è€Œä¸”ä¸é»˜è®¤æ³¨æ„åŠ›ç›¸æ¯”ï¼Œé—ªå­˜æ³¨æ„åŠ›çš„æ¨ç†é€Ÿåº¦è¦å¿«å¾—å¤šï¼Œè¿™æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæ˜¾ç€å‡å°‘å¯¹ GPU (VRAM) è¾ƒæ…¢ã€é«˜å¸¦å®½å†…å­˜çš„éœ€æ±‚ï¼Œè€Œæ˜¯ä¸“æ³¨äºæ›´å¿«çš„ç‰‡ä¸Šå†…å­˜ (SRAM) ã€‚</p></li>
<li><p>å®é™…ä¸Šï¼Œç›®å‰ç»å¯¹æ²¡æœ‰ç†ç”±ä¸ä½¿ç”¨ Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰ã€‚è¯¥ç®—æ³•åœ¨æ•°å­¦ä¸Šç»™å‡ºç›¸åŒçš„è¾“å‡ºï¼Œå¹¶ä¸”é€Ÿåº¦æ›´å¿«ä¸”å†…å­˜æ•ˆç‡æ›´é«˜ã€‚</p></li>
</ul>
</section>
<section id="architectural-innovations">
<h4>3. Architectural Innovations<a class="headerlink" href="#architectural-innovations" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="improving-positional-embeddings-of-llms">
<h5>3.1 Improving positional embeddings of LLMs<a class="headerlink" href="#improving-positional-embeddings-of-llms" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ä½ç½®åµŒå…¥ï¼ˆpositional embeddingsï¼‰ æ˜¯è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ï¼Œè´Ÿè´£å¸®åŠ©æ¨¡å‹ç†è§£æ–‡æœ¬åºåˆ—ä¸­ä¸åŒ token ä¹‹é—´çš„é¡ºåºå…³ç³»</p></li>
<li><p>ã€èƒŒæ™¯ï¼šä½ç½®åµŒå…¥çš„å¿…è¦æ€§ã€‘**è‡ªæ³¨æ„åŠ›æœºåˆ¶**ä¸­çš„ Softmax(QK^T) æ“ä½œå°†æ¯ä¸ª token ä¸åºåˆ—ä¸­çš„å…¶ä»– token è¿›è¡Œå…³è”ï¼Œä½†é»˜è®¤æƒ…å†µä¸‹å®ƒæ— æ³•ç†è§£ token ä¹‹é—´çš„ç›¸å¯¹é¡ºåºã€‚æ²¡æœ‰ä½ç½®åµŒå…¥çš„æ¨¡å‹éš¾ä»¥åŒºåˆ†ä¸åŒçš„è¾“å…¥é¡ºåºã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ† â€œHello I love youâ€ å’Œ â€œYou love I helloâ€ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ²¡æœ‰ä½ç½®ä¿¡æ¯çš„æƒ…å†µä¸‹çœ‹èµ·æ¥æ˜¯ç­‰ä»·çš„ã€‚</p></li>
<li><p>å› æ­¤ï¼Œä½ç½®åµŒå…¥ï¼ˆpositional embeddingsï¼‰è¢«å¼•å…¥ï¼Œç”¨æ¥ç¼–ç æ¯ä¸ª token åœ¨å¥å­ä¸­çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹èƒ½å¤ŸåŒºåˆ†è¾“å…¥æ–‡æœ¬çš„é¡ºåºã€‚</p></li>
<li><p>ä¼ ç»Ÿæ–¹æ³•ä¸»è¦æœ‰ä¸¤ç§ï¼šå›ºå®šä½ç½®åµŒå…¥ï¼ˆsinusoidal embeddingsï¼‰ å’Œ å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼ˆlearned positional embeddingsï¼‰ã€‚</p></li>
<li><p>ã€é—®é¢˜ï¼šä¼ ç»Ÿä½ç½®åµŒå…¥çš„å±€é™æ€§ã€‘1.å¯¹é•¿æ–‡æœ¬è¡¨ç°è¾ƒå·®ï¼šç»å¯¹ä½ç½®åµŒå…¥ä¸ºæ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ç¼–ç ï¼ˆä¾‹å¦‚ 0 åˆ° N çš„ä½ç½®ç¼–å·ï¼‰ï¼Œä½†å¯¹äºé•¿æ–‡æœ¬ï¼Œæ¨¡å‹éš¾ä»¥æœ‰æ•ˆå»ºæ¨¡ token ä¹‹é—´çš„è¿œè·ç¦»å…³ç³»ã€‚2.å›ºå®šè¾“å…¥é•¿åº¦é—®é¢˜ï¼šå¦‚æœä½¿ç”¨å­¦ä¹ çš„ç»å¯¹ä½ç½®åµŒå…¥ï¼Œæ¨¡å‹åªèƒ½å¤„ç†ä¸è®­ç»ƒæ—¶é•¿åº¦ç›¸åŒçš„è¾“å…¥ã€‚å¦‚æœè¾“å…¥é•¿åº¦è¶…å‡ºè®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦ï¼Œæ¨¡å‹æ— æ³•å¾ˆå¥½åœ°è¿›è¡Œæ¨æ–­ã€‚</p></li>
<li><p>ã€è§£å†³ï¼šç›¸å¯¹ä½ç½®åµŒå…¥ï¼šRoPE å’Œ ALiBiã€‘ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œç ”ç©¶è€…æå‡ºäº†ç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆrelative positional embeddingsï¼‰ï¼Œè¿™ç§æ–¹æ³•ä¸å†ä¸ºæ¯ä¸ªä½ç½®åˆ†é…ç»å¯¹å€¼ï¼Œè€Œæ˜¯å…³æ³¨ token ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ã€‚ä¸¤ç§æµè¡Œçš„ç›¸å¯¹ä½ç½®åµŒå…¥æ–¹æ³•æ˜¯ <strong>RoPEï¼ˆRotary Position Embeddingï¼‰</strong> å’Œ <strong>ALiBiï¼ˆAttention with Linear Biasesï¼‰</strong>ã€‚å®ƒä»¬é€šè¿‡ä¿®æ”¹è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ QK^T çŸ©é˜µæ¥å¼•å…¥ä½ç½®ä¿¡æ¯ã€‚</p></li>
</ul>
</section>
<section id="the-key-value-cache">
<h5>3.2 The key-value cache<a class="headerlink" href="#the-key-value-cache" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>è¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰æ•ˆçš„ä¼˜åŒ–ç­–ç•¥ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†é•¿åºåˆ—æˆ–ç”Ÿæˆå¤§é‡æ–‡æœ¬çš„åœºæ™¯ã€‚</p></li>
<li><p>ã€èƒŒæ™¯ï¼šè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆã€‘åœ¨ LLM ä¸­ï¼Œè‡ªå›å½’æ–‡æœ¬ç”Ÿæˆé€šè¿‡é€æ­¥ç”Ÿæˆä¸‹ä¸€ä¸ª token æ¥å®Œæˆã€‚æ¯æ¬¡è¾“å…¥å…ˆå‰ç”Ÿæˆçš„ token åºåˆ—ï¼Œç„¶åæ¨¡å‹é¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œå¹¶å°†å…¶åŠ å…¥è¾“å…¥åºåˆ—ä¸­ï¼Œå¦‚æ­¤å¾ªç¯ç›´åˆ°ç”Ÿæˆç»“æŸã€‚åœ¨è¿™ç§é€æ­¥ç”Ÿæˆçš„è¿‡ç¨‹ä¸­ï¼Œéšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œæ¯æ¬¡éƒ½éœ€è¦å¯¹æ•´ä¸ªåºåˆ—é‡æ–°è®¡ç®— <code class="docutils literal notranslate"><span class="pre">QK^T</span></code> çŸ©é˜µï¼Œè¿›è€Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚è¿™æ„å‘³ç€æ¯ä¸€æ­¥çš„è®¡ç®—å¤æ‚åº¦ä¼šéšç€åºåˆ—é•¿åº¦çš„å¢åŠ è€Œå¢å¤§ã€‚</p></li>
<li><p>ã€é—®é¢˜ï¼šé‡å¤è®¡ç®—çš„æ•ˆç‡ä½ä¸‹ã€‘å¯¹äºè‡ªå›å½’ç”Ÿæˆæ¥è¯´ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶ï¼Œæ¯ä¸€æ­¥éƒ½éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰ä¹‹å‰çš„ token çš„ key å’Œ valueï¼Œå°½ç®¡è¿™äº›å€¼åœ¨ä¹‹å‰çš„æ­¥éª¤ä¸­å·²ç»è®¡ç®—è¿‡äº†ã€‚é‡å¤è®¡ç®—è¿™äº›ä¸å¿…è¦çš„ key-value å¯¹ä¼šå¯¼è‡´è®¡ç®—èµ„æºçš„æµªè´¹ï¼Œå¹¶ä¸”åœ¨ç”Ÿæˆé•¿åºåˆ—æ—¶æ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦å’Œæ˜¾å­˜å ç”¨ã€‚</p></li>
<li><p>ã€è§£å†³æ–¹æ¡ˆï¼šKey-Value Cacheã€‘é€šè¿‡ç¼“å­˜æ¯ä¸€å±‚çš„ key å’Œ value å‘é‡ï¼ˆå³ K å’Œ Vï¼‰ï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚</p></li>
<li><p>ã€åŸç†ã€‘**QK^T çŸ©é˜µä¼˜åŒ–**ï¼šåœ¨æ ‡å‡† Transformer æ¨¡å‹ä¸­ï¼ŒQK^T çŸ©é˜µæ˜¯é€šè¿‡å°†æ¯ä¸ª token çš„ query å‘é‡ï¼ˆQï¼‰ä¸æ‰€æœ‰ key å‘é‡ï¼ˆKï¼‰è¿›è¡Œç‚¹ç§¯è®¡ç®—å¾—å‡ºçš„ã€‚ç„¶è€Œï¼Œå¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œæˆ‘ä»¬æ¯æ¬¡åªéœ€è¦ä¸ºæ–°å¢çš„ token è®¡ç®— query ï¼ˆq_cï¼‰ä¸ä¹‹å‰ç¼“å­˜çš„ keyï¼ˆKï¼‰è¿›è¡Œç›¸ä¹˜ï¼Œè€Œä¸éœ€è¦é‡æ–°è®¡ç®—æ‰€æœ‰çš„ key å’Œ valueã€‚<strong>ç¼“å­˜æœºåˆ¶</strong>ï¼šåœ¨æ¯ä¸€æ­¥ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå°†ä¹‹å‰çš„ key-value å¯¹ä¿å­˜åœ¨ç¼“å­˜ä¸­ï¼Œä¸‹ä¸€æ­¥ç”Ÿæˆæ—¶åªéœ€è®¡ç®—å½“å‰æ–° token çš„ queryï¼Œç„¶åä¸ç¼“å­˜ä¸­çš„ key è¿›è¡Œè®¡ç®—ã€‚è¿™æ ·é¿å…äº†é‡å¤è®¡ç®—æ•´ä¸ªåºåˆ—çš„ key-value å¯¹ã€‚</p></li>
</ul>
<section id="multi-round-conversation">
<h6>3.2.1 Multi-round conversation<a class="headerlink" href="#multi-round-conversation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>é”®å€¼ç¼“å­˜å¯¹äºèŠå¤©ç­‰éœ€è¦å¤šæ¬¡è‡ªåŠ¨å›å½’è§£ç çš„åº”ç”¨ç¨‹åºç‰¹åˆ«æœ‰ç”¨</p></li>
</ul>
<p>ç¤ºä¾‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: How many people live in France?
Assistant: Roughly 75 million people live in France
User: And how many are in Germany?
Assistant: Germany has ca. 81 million inhabitants
</pre></div>
</div>
<ul class="simple">
<li><p>åœ¨æ­¤èŠå¤©ä¸­ï¼Œ LLMè¿è¡Œè‡ªå›å½’è§£ç ä¸¤æ¬¡ï¼š</p></li>
<li><p>ç¬¬ä¸€æ¬¡ï¼Œé”®å€¼ç¼“å­˜ä¸ºç©ºï¼Œè¾“å…¥æç¤ºä¸º â€œUser: How many people live in France?â€ æ¨¡å‹è‡ªåŠ¨å›å½’ç”Ÿæˆæ–‡æœ¬ â€œRoughly 75 million people live in Franceâ€ åŒæ—¶åœ¨æ¯ä¸ªè§£ç æ­¥éª¤å¢åŠ é”®å€¼ç¼“å­˜ã€‚</p></li>
<li><p>ç¬¬äºŒæ¬¡è¾“å…¥æç¤ºæ˜¯ â€œUser: How many people live in France? n Assistant: Roughly 75 million people live in France n User: And how many in Germany?â€ ã€‚ç”±äºç¼“å­˜ï¼Œå‰ä¸¤ä¸ªå¥å­çš„æ‰€æœ‰é”®å€¼å‘é‡éƒ½å·²ç»è®¡ç®—å‡ºæ¥ã€‚å› æ­¤è¾“å…¥æç¤ºä»…åŒ…å« â€œUser: And how many in Germany?â€ ã€‚åœ¨å¤„ç†ç¼©çŸ­çš„è¾“å…¥æç¤ºæ—¶ï¼Œå…¶è®¡ç®—å‡ºçš„é”®å€¼å‘é‡è¢«è¿æ¥åˆ°ç¬¬ä¸€æ¬¡è§£ç çš„é”®å€¼ç¼“å­˜ã€‚ç¬¬äºŒæ¬¡å›ç­” â€œGermany has ca. 81 million inhabitantsâ€ ç„¶åä½¿ç”¨ç”±ç¼–ç çš„é”®å€¼å‘é‡ç»„æˆçš„é”®å€¼ç¼“å­˜è‡ªåŠ¨ç”Ÿæˆ â€œUser: How many people live in France? n Assistant: Roughly 75 million people live in France n User: And how many are in Germany?â€ ã€‚</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generation as usual</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="o">+</span> <span class="s2">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Piping the returned `past_key_values` to speed up the next conversation round</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">decoded_output</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Question: How can I modify the function above to return Mega bytes instead?</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
  <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
  <span class="n">past_key_values</span><span class="o">=</span><span class="n">generation_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>    <span class="c1"># â‡ï¸</span>
  <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
  <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</pre></div>
</div>
</section>
<section id="multi-query-attention-mqa">
<h6>3.2.2 Multi-Query-Attention (MQA)<a class="headerlink" href="#multi-query-attention-mqa" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ã€å®šä¹‰ã€‘ä¸€ç§ä¼˜åŒ–è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æŠ€æœ¯ï¼Œæ—¨åœ¨å‡å°‘key-value ç¼“å­˜çš„å†…å­˜å ç”¨å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚è¯¥æ–¹æ³•ç”± Noam Shazeer åœ¨è®ºæ–‡ â€œFast Transformer Decoding: One Write-Head is All You Needâ€ ä¸­æå‡ºï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯å…±äº« key-value æŠ•å½±ï¼Œä»è€Œå¤§å¹…é™ä½å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰çš„å†…å­˜å¼€é”€ã€‚</p></li>
<li><p>ã€èƒŒæ™¯ï¼šä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„å†…å­˜ç“¶é¢ˆã€‘åœ¨ä¼ ç»Ÿçš„**å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Attentionï¼‰**ä¸­ï¼Œæ¨¡å‹ä¼šä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆattention headï¼‰è®¡ç®—ç‹¬ç«‹çš„ key-value å¯¹ã€‚å…·ä½“æ¥è¯´ï¼Œn ä¸ªæ³¨æ„åŠ›å¤´æ„å‘³ç€éœ€è¦è®¡ç®—å¹¶å­˜å‚¨ n ç»„ key-value å‘é‡ï¼Œé€šå¸¸ä¼šæ˜¾è‘—å¢åŠ å†…å­˜å’Œè®¡ç®—å¼€é”€ã€‚å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­ï¼Œé€šå¸¸æœ‰ 20 åˆ° 100 ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå› æ­¤å½“ç¼“å­˜è¿™äº› key-value å¯¹æ—¶ï¼Œå†…å­˜æ¶ˆè€—éå¸¸é«˜ï¼Œå°¤å…¶æ˜¯åœ¨é•¿æ–‡æœ¬ç”Ÿæˆæˆ–å¤šè½®å¯¹è¯ä¸­ã€‚</p></li>
<li><p>ã€æ ¸å¿ƒæ€æƒ³ã€‘å…³é”®åœ¨äºï¼šæ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ª key-value æŠ•å½±ï¼Œå³æ¯ä¸ªå¤´ä»ç„¶æ‹¥æœ‰ç‹¬ç«‹çš„ queryï¼ˆæŸ¥è¯¢å‘é‡ï¼‰ï¼Œä½† key å’Œ value å‘é‡åœ¨æ‰€æœ‰æ³¨æ„åŠ›å¤´ä¸­æ˜¯ç›¸åŒçš„ã€‚è¿™æ ·å¯ä»¥æ˜¾è‘—å‡å°‘å­˜å‚¨å’Œè®¡ç®—çš„å¼€é”€ï¼Œè€Œä¸ä¼šæ˜¾è‘—å½±å“æ¨¡å‹çš„æ€§èƒ½ã€‚</p></li>
<li><p>ã€åº”ç”¨ã€‘MQA æŠ€æœ¯å·²ç»è¢«è®¸å¤šä¸»æµçš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ‰€é‡‡ç”¨ï¼ŒåŒ…æ‹¬ï¼šFalconï¼ŒPaLMï¼ŒMPTï¼ŒBLOOM</p></li>
</ul>
</section>
<section id="grouped-query-attention-gqa">
<h6>3.2.3 Grouped-Query-Attention (GQA)<a class="headerlink" href="#grouped-query-attention-gqa" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h6>
<ul class="simple">
<li><p>ã€å®šä¹‰ã€‘ç”± Google ç ”ç©¶å‘˜ Ainslie ç­‰äººåœ¨è®ºæ–‡ä¸­æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒæ—¨åœ¨è§£å†³ Multi-Query Attention (MQA) å¸¦æ¥çš„æ€§èƒ½ä¸‹é™é—®é¢˜ï¼ŒåŒæ—¶ä¿ç•™å¤§éƒ¨åˆ†å†…å­˜å’Œè®¡ç®—æ•ˆç‡çš„æå‡ã€‚ç›¸æ¯” MQAï¼ŒGQA æä¾›äº†ä¸€ä¸ªæ›´åŠ æŠ˜ä¸­çš„è§£å†³æ–¹æ¡ˆï¼Œåœ¨æå‡è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå‡å°‘å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚</p></li>
<li><p>ã€èƒŒæ™¯ï¼šMQA çš„å±€é™æ€§ã€‘Multi-Query Attention (MQA) é€šè¿‡ä¸ºæ‰€æœ‰æ³¨æ„åŠ›å¤´å…±äº«ä¸€ä¸ª key-value æŠ•å½±ï¼Œæ˜¾è‘—å‡å°‘äº†å†…å­˜å’Œè®¡ç®—å¼€é”€ï¼Œå°¤å…¶åœ¨è‡ªå›å½’ç”Ÿæˆæ—¶æé«˜äº†æ¨ç†é€Ÿåº¦å¹¶é™ä½äº†æ˜¾å­˜å ç”¨ã€‚ç„¶è€Œï¼Œç ”ç©¶å‘ç° MQA çš„è¿™ç§æç«¯å…±äº«æœºåˆ¶ä¼šå¸¦æ¥ä¸€å®šçš„æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå› ä¸ºä¸åŒçš„ query å¤´æ— æ³•å†ç‹¬ç«‹å­¦ä¹ ä¸ key-value çš„å¯¹åº”å…³ç³»ï¼Œå¯¼è‡´æ³¨æ„åŠ›æœºåˆ¶çš„çµæ´»æ€§å—é™ã€‚</p></li>
<li><p>ã€æ ¸å¿ƒæ€æƒ³ã€‘Grouped-Query Attention (GQA) æå‡ºäº†ä¸€ä¸ªæŠ˜è¡·æ–¹æ¡ˆï¼šå‡å°‘æ³¨æ„åŠ›å¤´çš„ query æŠ•å½±æ•°é‡ï¼Œä½†ä¸å°†å…¶å®Œå…¨åˆå¹¶ä¸ºä¸€ä¸ªã€‚å…·ä½“æ¥è¯´ï¼ŒGQA æå‡ºå°†æ³¨æ„åŠ›å¤´åˆ†ç»„ï¼Œå¤šä¸ªå¤´å…±äº«ä¸€ç»„ key-value æŠ•å½±ï¼Œè€Œä¸æ˜¯æ¯ä¸ªå¤´éƒ½ä½¿ç”¨å®Œå…¨ç‹¬ç«‹çš„æŠ•å½±ï¼Œä¹Ÿä¸æ˜¯åƒ MQA é‚£æ ·æ‰€æœ‰å¤´å…±äº«åŒä¸€ç»„æŠ•å½±ã€‚</p></li>
</ul>
</section>
</section>
</section>
</section>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<section id="main-classes">
<h3>Main Classes<a class="headerlink" href="#main-classes" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="auto-classes">
<h4>Auto Classes<a class="headerlink" href="#auto-classes" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoConfig</span>
<span class="n">AutoTokenizer</span>
<span class="n">AutoFeatureExtractor</span>
<span class="n">AutoImageProcessor</span>
<span class="n">AutoProcessor</span>
</pre></div>
</div>
<p>Generic model classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModel</span>
</pre></div>
</div>
<p>Generic pretraining classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForPreTraining</span>
</pre></div>
</div>
<p>Natural Language Processing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForCausalLM</span>
<span class="n">AutoModelForMaskedLM</span>
<span class="n">AutoModelForMaskGeneration</span>
<span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">AutoModelForSequenceClassification</span>
<span class="n">AutoModelForMultipleChoice</span>
<span class="n">AutoModelForNextSentencePrediction</span>
<span class="n">AutoModelForTokenClassification</span>
<span class="n">AutoModelForQuestionAnswering</span>
<span class="n">AutoModelForTextEncoding</span>
</pre></div>
</div>
<p>Computer vision:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForDepthEstimation</span>
<span class="n">AutoModelForImageClassification</span>
<span class="n">AutoModelForVideoClassification</span>
<span class="n">AutoModelForKeypointDetection</span>
<span class="n">AutoModelForMaskedImageModeling</span>
<span class="n">AutoModelForObjectDetection</span>
<span class="n">AutoModelForImageSegmentation</span>
<span class="n">AutoModelForImageToImage</span>
<span class="n">AutoModelForSemanticSegmentation</span>
<span class="n">AutoModelForInstanceSegmentation</span>
<span class="n">AutoModelForUniversalSegmentation</span>
<span class="n">AutoModelForZeroShotImageClassification</span>
<span class="n">AutoModelForZeroShotObjectDetection</span>
</pre></div>
</div>
<p>Audio:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForAudioClassification</span>
<span class="n">AutoModelForAudioFrameClassification</span>
<span class="n">AutoModelForCTC</span>
<span class="n">AutoModelForSpeechSeq2Seq</span>
<span class="n">AutoModelForAudioXVector</span>
<span class="n">AutoModelForTextToSpectrogram</span>
<span class="n">AutoModelForTextToWaveform</span>
</pre></div>
</div>
<p>Multimodal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForTableQuestionAnswering</span>
<span class="n">AutoModelForDocumentQuestionAnswering</span>
<span class="n">AutoModelForVisualQuestionAnswering</span>
<span class="n">AutoModelForVision2Seq</span>
</pre></div>
</div>
</section>
<section id="id14">
<h4>Backbone<a class="headerlink" href="#id14" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul>
<li><p>backbone(ä¸»å¹²)æ˜¯ä¸€ç§ç”¨äºä¸ºæ›´é«˜çº§åˆ«çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆä¾‹å¦‚å¯¹è±¡æ£€æµ‹å’Œå›¾åƒåˆ†ç±»ï¼‰è¿›è¡Œç‰¹å¾æå–çš„æ¨¡å‹ã€‚</p></li>
<li><p>Transformers æä¾›äº†ä¸€ä¸ª <code class="docutils literal notranslate"><span class="pre">AutoBackbone</span></code> ç±»ï¼Œç”¨äºæ ¹æ®é¢„è®­ç»ƒçš„æ¨¡å‹æƒé‡åˆå§‹åŒ– Transformers ä¸»å¹²</p></li>
<li><p>ä¸¤ä¸ªå®ç”¨ç¨‹åºç±»:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoBackbone</span>

<span class="n">BackboneMixin</span>
<span class="n">BackboneConfigMixin</span>

<span class="n">TimmBackbone</span>
<span class="n">TimmBackboneConfig</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="callbacks">
<h4>Callbacks<a class="headerlink" href="#callbacks" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Callbacks æ˜¯å¯ä»¥åœ¨ PyTorch Trainerä¸­è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è¡Œä¸ºçš„å¯¹è±¡ï¼ˆè¯¥åŠŸèƒ½å°šæœªåœ¨ TensorFlow ä¸­å®ç°ï¼‰ï¼Œå¯ä»¥æ£€æŸ¥è®­ç»ƒå¾ªç¯çŠ¶æ€ï¼ˆç”¨äºè¿›åº¦æŠ¥å‘Šã€ç™»å½• TensorBoard æˆ–å…¶ä»– ML å¹³å°â€¦â€¦ï¼‰å¹¶åšå‡ºå†³ç­–ï¼ˆå¦‚æå‰åœæ­¢ï¼‰ã€‚</p></li>
</ul>
</section>
<section id="id15">
<h4>Configuration<a class="headerlink" href="#id15" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>base class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PretrainedConfig</span>
</pre></div>
</div>
<p>é€šç”¨å±æ€§æœ‰:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>hidden_size
num_attention_heads
num_hidden_layers

æ–‡æœ¬æ¨¡å‹è¿›ä¸€æ­¥å®ç°ï¼š vocab_size
</pre></div>
</div>
</section>
<section id="data-collator">
<h4>Data Collator<a class="headerlink" href="#data-collator" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>Data collators æ˜¯é€šè¿‡ä½¿ç”¨æ•°æ®é›†å…ƒç´ åˆ—è¡¨ä½œä¸ºè¾“å…¥æ¥å½¢æˆæ‰¹æ¬¡çš„å¯¹è±¡ã€‚è¿™äº›å…ƒç´ ä¸train_datasetæˆ–eval_datasetçš„å…ƒç´ å…·æœ‰ç›¸åŒçš„ç±»å‹ã€‚</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DefaultDataCollator</span>
<span class="n">DataCollatorWithPadding</span>
<span class="n">DataCollatorForTokenClassification</span>
<span class="n">DataCollatorForSeq2Seq</span>
<span class="n">DataCollatorForLanguageModeling</span>
<span class="n">DataCollatorForWholeWordMask</span>
<span class="n">DataCollatorForPermutationLanguageModeling</span>
<span class="n">DataCollatorWithFlattening</span>
</pre></div>
</div>
</section>
<section id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é»˜è®¤æ˜¯WARNING</p></li>
</ul>
<p>ä¿®æ”¹ä»£ç :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="n">transformers</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_info</span><span class="p">()</span>

<span class="n">ç¯å¢ƒå˜é‡</span> <span class="n">TRANSFORMERS_VERBOSITY</span><span class="p">:</span>
<span class="n">TRANSFORMERS_VERBOSITY</span><span class="o">=</span><span class="n">error</span> <span class="o">./</span><span class="n">myprogram</span><span class="o">.</span><span class="n">py</span>

<span class="n">ç¯å¢ƒå˜é‡æ¥ç¦ç”¨ä¸€äº›warnings</span> <span class="n">TRANSFORMERS_NO_ADVISORY_WARNINGS</span>
<span class="n">TRANSFORMERS_NO_ADVISORY_WARNINGS</span><span class="o">=</span><span class="mi">1</span> <span class="o">./</span><span class="n">myprogram</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="models">
<h4>Models<a class="headerlink" href="#models" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>åŸºç±»:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PreTrainedModel ã€ TFPreTrainedModelå’ŒFlaxPreTrainedModel

ModuleUtilsMixin
PushToHubMixin
</pre></div>
</div>
</section>
<section id="text-generation">
<h4>Text Generation<a class="headerlink" href="#text-generation" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æ¯ä¸ªæ¡†æ¶éƒ½æœ‰ä¸€ä¸ªç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨å„è‡ªçš„ <code class="docutils literal notranslate"><span class="pre">GenerationMixin</span></code> ç±»ä¸­å®ç°ï¼š</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GenerationConfig</span>
    <span class="n">WatermarkingConfig</span>

<span class="n">GenerationMixin</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GenerationConfig</span>

<span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>

<span class="c1"># E.g. config was saved using *save_pretrained(&#39;./test/saved_model/&#39;)*</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>

<span class="c1"># You can also specify configuration names to your generation configuration file</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">,</span> <span class="n">config_file_name</span><span class="o">=</span><span class="s2">&quot;my_configuration.json&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">,</span> <span class="s2">&quot;my_configuration.json&quot;</span><span class="p">)</span>

<span class="c1"># If you&#39;d like to try a minor variation to an existing configuration, you can also pass generation</span>
<span class="c1"># arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored</span>
<span class="n">generation_config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span>  <span class="c1"># 1</span>

<span class="n">unused_kwargs</span>   <span class="c1"># {&#39;foo&#39;: False}</span>
</pre></div>
</div>
</section>
<section id="onnx">
<h4>ONNX<a class="headerlink" href="#onnx" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>ä¸‰ä¸ªæŠ½è±¡ç±»:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">We</span> <span class="n">provide</span> <span class="n">three</span> <span class="n">abstract</span> <span class="n">classes</span> <span class="n">that</span> <span class="n">you</span> <span class="n">should</span> <span class="n">inherit</span> <span class="n">from</span><span class="p">,</span> <span class="n">depending</span> <span class="n">on</span> <span class="n">the</span> <span class="nb">type</span> <span class="n">of</span> <span class="n">model</span> <span class="n">architecture</span> <span class="n">you</span> <span class="n">wish</span> <span class="n">to</span> <span class="n">export</span><span class="p">:</span>

<span class="n">OnnxConfig</span>
<span class="n">OnnxConfigWithPast</span>
<span class="n">OnnxSeq2SeqConfigWithPast</span>
</pre></div>
</div>
<p>ONNX Features:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Each</span> <span class="n">ONNX</span> <span class="n">configuration</span> <span class="ow">is</span> <span class="n">associated</span> <span class="k">with</span> <span class="n">a</span> <span class="nb">set</span> <span class="n">of</span> <span class="n">features</span> <span class="n">that</span> <span class="n">enable</span> <span class="n">you</span> <span class="n">to</span> <span class="n">export</span> <span class="n">models</span> <span class="k">for</span> <span class="n">different</span> <span class="n">types</span> <span class="n">of</span> <span class="n">topologies</span> <span class="ow">or</span> <span class="n">tasks</span><span class="o">.</span>

<span class="n">FeaturesManager</span>
</pre></div>
</div>
</section>
<section id="optimization">
<h4>Optimization<a class="headerlink" href="#optimization" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<p>The .optimization module provides:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>an optimizer with weight decay fixed that can be used to fine-tuned models, and
    ä¸€ä¸ªå›ºå®šæƒé‡è¡°å‡çš„ä¼˜åŒ–å™¨ï¼Œå¯ç”¨äºå¾®è°ƒæ¨¡å‹
several schedules in the form of schedule objects that inherit from `_LRSchedule`:
a gradient accumulation class to accumulate the gradients of multiple batches
    ä¸€ä¸ªæ¢¯åº¦ç´¯ç§¯ç±»ï¼Œç”¨äºç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡çš„æ¢¯åº¦
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AdamW</span>
    <span class="n">Implements</span> <span class="n">Adam</span> <span class="n">algorithm</span> <span class="k">with</span> <span class="n">weight</span> <span class="n">decay</span> <span class="n">fix</span>
<span class="n">AdaFactor</span>
    <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="k">as</span> <span class="n">a</span> <span class="n">drop</span> <span class="ow">in</span> <span class="n">replacement</span> <span class="k">for</span> <span class="n">Adam</span> <span class="n">original</span> <span class="n">fairseq</span> <span class="n">code</span>

<span class="n">Schedules</span>
    <span class="n">SchedulerType</span>
</pre></div>
</div>
</section>
<section id="model-outputs">
<h4>Model outputs<a class="headerlink" href="#model-outputs" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>æ‰€æœ‰æ¨¡å‹çš„è¾“å‡ºéƒ½æ˜¯ModelOutputå­ç±»çš„å®ä¾‹ã€‚</p></li>
<li><p>All models have outputs that are instances of subclasses of ModelOutput. Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="c1"># outputså¯¹è±¡æ˜¯ä¸€ä¸ªSequenceClassifierOutput</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ModelOutput</span>
<span class="n">BaseModelOutput</span>
<span class="n">BaseModelOutputWithPooling</span>
<span class="n">BaseModelOutputWithCrossAttentions</span>
<span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span>
<span class="n">BaseModelOutputWithPast</span>
<span class="n">BaseModelOutputWithPastAndCrossAttentions</span>

<span class="n">Seq2SeqModelOutput</span>

<span class="n">CausalLMOutput</span>
<span class="n">CausalLMOutputWithCrossAttentions</span>
<span class="n">CausalLMOutputWithPast</span>

<span class="n">MaskedLMOutput</span>
<span class="n">Seq2SeqLMOutput</span>
<span class="n">NextSentencePredictorOutput</span>

<span class="n">SequenceClassifierOutput</span>
<span class="n">Seq2SeqSequenceClassifierOutput</span>

<span class="n">MultipleChoiceModelOutput</span>

<span class="n">TokenClassifierOutput</span>

<span class="n">QuestionAnsweringModelOutput</span>
<span class="n">Seq2SeqQuestionAnsweringModelOutput</span>
<span class="n">Seq2SeqSpectrogramOutput</span>
<span class="n">SemanticSegmenterOutput</span>

<span class="n">ImageClassifierOutput</span>
<span class="n">ImageClassifierOutputWithNoAttention</span>

<span class="n">DepthEstimatorOutput</span>
<span class="n">Wav2Vec2BaseModelOutput</span>
<span class="n">XVectorOutput</span>

<span class="n">Seq2SeqTSModelOutput</span>
<span class="n">Seq2SeqTSPredictionOutput</span>
<span class="n">SampleTSPredictionOutput</span>
</pre></div>
</div>
</section>
<section id="pipelines">
<h4>Pipelines<a class="headerlink" href="#pipelines" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;This restaurant is awesome&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="processors">
<h4>Processors<a class="headerlink" href="#processors" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="multi-modal-processors">
<h5>Multi-modal processors<a class="headerlink" href="#multi-modal-processors" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>ä»»ä½•å¤šæ¨¡æ€æ¨¡å‹éƒ½éœ€è¦ä¸€ä¸ªå¯¹è±¡æ¥ç¼–ç æˆ–è§£ç å°†å¤šç§æ¨¡æ€(modalities)ï¼ˆæ–‡æœ¬ã€è§†è§‰å’ŒéŸ³é¢‘ï¼‰åˆ†ç»„çš„æ•°æ®ã€‚</p></li>
<li><p>è¿™æ˜¯ç”±ç§°ä¸º <code class="docutils literal notranslate"><span class="pre">processors</span></code> çš„å¯¹è±¡æ¥å¤„ç†çš„ï¼Œå®ƒå°†ä¸¤ä¸ªæˆ–å¤šä¸ªå¤„ç†å¯¹è±¡(processing objects)ç»„åˆåœ¨ä¸€èµ·ï¼Œä¾‹å¦‚åˆ†è¯å™¨ï¼ˆç”¨äºæ–‡æœ¬æ¨¡æ€ï¼‰ã€å›¾åƒå¤„ç†å™¨ï¼ˆç”¨äºè§†è§‰ï¼‰å’Œç‰¹å¾æå–å™¨ï¼ˆç”¨äºéŸ³é¢‘ï¼‰ã€‚</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ProcessorMixin</span>
</pre></div>
</div>
</section>
</section>
<section id="id16">
<h4>Quantization<a class="headerlink" href="#id16" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<ul class="simple">
<li><p>é‡åŒ–æŠ€æœ¯é€šè¿‡ä½¿ç”¨ 8 ä½æ•´æ•° (int8) ç­‰è¾ƒä½ç²¾åº¦çš„æ•°æ®ç±»å‹è¡¨ç¤ºæƒé‡å’Œæ¿€æ´»æ¥å‡å°‘å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚è¿™å¯ä»¥åŠ è½½é€šå¸¸æ— æ³•è£…å…¥å†…å­˜çš„æ›´å¤§æ¨¡å‹ï¼Œå¹¶åŠ å¿«æ¨ç†é€Ÿåº¦ã€‚ Transformers æ”¯æŒ AWQ å’Œ GPTQ é‡åŒ–ç®—æ³•ï¼Œå¹¶ä¸”æ”¯æŒ 8 ä½å’Œ 4 ä½é‡åŒ–ï¼ˆbitsandbytesï¼‰ã€‚</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">QuantoConfig</span>
<span class="n">AqlmConfig</span>
<span class="n">AwqConfig</span>
<span class="n">EetqConfig</span>
<span class="n">GPTQConfig</span>
<span class="n">BitsAndBytesConfig</span>
<span class="n">HfQuantizer</span>
<span class="n">HqqConfig</span>
<span class="n">FbgemmFp8Config</span>
<span class="n">CompressedTensorsConfig</span>
<span class="n">TorchAoConfig</span>
</pre></div>
</div>
</section>
<section id="id17">
<h4>Tokenizer<a class="headerlink" href="#id17" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PreTrainedTokenizer</span>
<span class="n">PreTrainedTokenizerFast</span>

<span class="n">BatchEncoding</span>
</pre></div>
</div>
</section>
<section id="id18">
<h4>Trainer<a class="headerlink" href="#id18" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Trainer</span>
<span class="n">Seq2SeqTrainer</span>

<span class="n">TrainingArguments</span>
<span class="n">Seq2SeqTrainingArguments</span>
</pre></div>
</div>
</section>
</section>
<section id="id19">
<h3>Models<a class="headerlink" href="#id19" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h3>
<section id="text-models">
<h4>Text models<a class="headerlink" href="#text-models" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h4>
<section id="qwen2">
<h5>Qwen2<a class="headerlink" href="#qwen2" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>Qwen2 is the new model series of large language models from the Qwen team.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Qwen2Config</span>
<span class="n">Qwen2Tokenizer</span>
<span class="n">Qwen2TokenizerFast</span>
<span class="n">Qwen2Model</span>
<span class="n">Qwen2ForCausalLM</span>
<span class="n">Qwen2ForSequenceClassification</span>
<span class="n">Qwen2ForTokenClassification</span>
</pre></div>
</div>
</section>
<section id="qwen2-vl">
<h5>Qwen2_VL<a class="headerlink" href="#qwen2-vl" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Qwen2VLConfig</span>
<span class="n">Qwen2VLImageProcessor</span>
<span class="n">Qwen2VLProcessor</span>
<span class="n">Qwen2VLModel</span>
<span class="n">Qwen2VLForConditionalGeneration</span>
</pre></div>
</div>
</section>
<section id="cpm">
<h5>CPM<a class="headerlink" href="#cpm" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>The CPM model was proposed in CPM: <a class="reference external" href="https://arxiv.org/abs/2012.00413">A Large-scale Generative Chinese Pre-trained Language Model</a></p></li>
</ul>
</section>
<section id="dpr">
<h5>DPR<a class="headerlink" href="#dpr" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h5>
<ul class="simple">
<li><p>Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&amp;A research.</p></li>
<li><p>It was introduced in Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.</p></li>
<li><p>ç›¸å…³è®ºæ–‡: Dense Passage Retrieval for Open-Domain Question Answering(<a class="reference external" href="https://arxiv.org/abs/2004.04906">https://arxiv.org/abs/2004.04906</a>)</p></li>
<li><p>å¼€æ”¾åŸŸé—®ç­”ä¾èµ–äºé«˜æ•ˆçš„æ®µè½æ£€ç´¢æ¥é€‰æ‹©å€™é€‰ä¸Šä¸‹æ–‡ï¼Œå…¶ä¸­ä¼ ç»Ÿçš„ç¨€ç–å‘é‡ç©ºé—´æ¨¡å‹ï¼ˆå¦‚ TF-IDF æˆ– BM25ï¼‰æ˜¯äº‹å®ä¸Šçš„æ–¹æ³•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜æ£€ç´¢å¯ä»¥å•ç‹¬ä½¿ç”¨å¯†é›†è¡¨ç¤ºå®é™…å®ç°ï¼Œå…¶ä¸­åµŒå…¥æ˜¯é€šè¿‡ä¸€ä¸ªç®€å•çš„åŒç¼–ç å™¨æ¡†æ¶ä»å°‘é‡é—®é¢˜å’Œæ®µè½ä¸­å­¦ä¹ çš„ã€‚åœ¨å¹¿æ³›çš„å¼€æ”¾åŸŸ QA æ•°æ®é›†ä¸Šè¿›è¡Œè¯„ä¼°æ—¶ï¼Œæˆ‘ä»¬çš„å¯†é›†æ£€ç´¢å™¨åœ¨å‰ 20 åä¼ ä»£æ£€ç´¢å‡†ç¡®ç‡æ–¹é¢æ¯”å¼ºå¤§çš„ Lucene-BM25 ç³»ç»Ÿé«˜å‡º 9%-19%ï¼Œå¹¶å¸®åŠ©æˆ‘ä»¬çš„ç«¯åˆ°ç«¯ QA ç³»ç»Ÿåœ¨å¤šä¸ªå¼€æ”¾åŸŸ QA åŸºå‡†ä¸Šå»ºç«‹æ–°çš„æœ€å…ˆè¿›çš„æŠ€æœ¯ã€‚</p></li>
<li><p>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</p></li>
</ul>
<p>DPR consists in three models:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Question</span> <span class="n">encoder</span><span class="p">:</span> <span class="n">encode</span> <span class="n">questions</span> <span class="k">as</span> <span class="n">vectors</span>
<span class="mf">2.</span> <span class="n">Context</span> <span class="n">encoder</span><span class="p">:</span> <span class="n">encode</span> <span class="n">contexts</span> <span class="k">as</span> <span class="n">vectors</span>
<span class="mf">3.</span> <span class="n">Reader</span><span class="p">:</span> <span class="n">extract</span> <span class="n">the</span> <span class="n">answer</span> <span class="n">of</span> <span class="n">the</span> <span class="n">questions</span> <span class="n">inside</span> <span class="n">retrieved</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">along</span> <span class="k">with</span> <span class="n">a</span> <span class="n">relevance</span> <span class="n">score</span>
    <span class="p">(</span><span class="n">high</span> <span class="k">if</span> <span class="n">the</span> <span class="n">inferred</span> <span class="n">span</span> <span class="n">actually</span> <span class="n">answers</span> <span class="n">the</span> <span class="n">question</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<ul>
<li><p>Dense Passage Retrieval (DPR) æ˜¯ä¸€ç§ç”¨äºä¿¡æ¯æ£€ç´¢çš„æ·±åº¦å­¦ä¹ æ–¹æ³•ï¼Œå°¤å…¶é€‚åˆå›ç­”å¼€æ”¾é¢†åŸŸçš„é—®é¢˜ã€‚DPRé€šè¿‡åŒå¡”ç»“æ„ï¼ˆdual encoderï¼‰çš„ç¥ç»ç½‘ç»œæ¨¡å‹æ¥å®ç°æ–‡æœ¬çš„å‘é‡åŒ–è¡¨ç¤ºï¼Œä¸»è¦ç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šä¸€ä¸ªæŸ¥è¯¢ç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æ¡£ç¼–ç å™¨ã€‚è¿™ä¸¤ä¸ªç¼–ç å™¨é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„BERTæ¨¡å‹æˆ–å…¶ä»–Transformeræ¨¡å‹ï¼Œåˆ†åˆ«å¯¹ç”¨æˆ·æŸ¥è¯¢å’Œå€™é€‰æ–‡æ¡£è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå›ºå®šç»´åº¦çš„å¯†é›†å‘é‡ï¼ˆdense vectorï¼‰ã€‚</p></li>
<li><p>é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆä¾‹å¦‚ï¼ŒWikipedia passagesï¼‰ä¸Šè¿›è¡Œç›‘ç£å­¦ä¹ è®­ç»ƒï¼ŒDPRå¯ä»¥åœ¨å„ç§ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æ¡£æ£€ç´¢ï¼‰ä¸­å®ç°é«˜æ•ˆå’Œå‡†ç¡®çš„æ–‡æœ¬æ£€ç´¢ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç¨€ç–å‘é‡æ£€ç´¢ï¼ˆå¦‚TF-IDFæˆ–BM25ï¼‰ï¼ŒDPRçš„å¯†é›†è¡¨ç¤ºå¯ä»¥æ›´å¥½åœ°æ•æ‰è¯æ±‡çš„è¯­ä¹‰ç›¸ä¼¼æ€§ï¼Œå› æ­¤åœ¨éœ€è¦æ›´ç²¾å‡†è¯­ä¹‰åŒ¹é…çš„ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³ã€‚</p></li>
<li><p>DPRçš„å·¥ä½œåŸç†å¦‚ä¸‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. æŸ¥è¯¢ç¼–ç ï¼šå°†ç”¨æˆ·è¾“å…¥çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚é—®é¢˜æˆ–å…³é”®è¯ï¼‰è½¬åŒ–ä¸ºå¯†é›†å‘é‡ã€‚
2. æ–‡æ¡£ç¼–ç ï¼šå°†å€™é€‰æ–‡æ¡£åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£è½¬åŒ–ä¸ºå¯†é›†å‘é‡ã€‚
3. ç›¸ä¼¼åº¦è®¡ç®—ï¼šåˆ©ç”¨å‘é‡ç›¸ä¼¼åº¦åº¦é‡ï¼ˆå¦‚å†…ç§¯æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œä»è€Œé€‰å‡ºæœ€ç›¸å…³çš„æ–‡æ¡£ã€‚
</pre></div>
</div>
</li>
</ul>
<p>Related Models:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dpr-reader-single-nq-base
dpr-ctx_encoder-single-nq-base
dpr-question_encoder-single-nq-base

dpr-ctx_encoder-multiset-base
dpr-question_encoder-multiset-base
dpr-reader-multiset-base

è¯´æ˜:
    single-nq-base: é€‚åˆä¸“æ³¨äºNatural Questionsï¼ˆNQï¼‰æ•°æ®é›†çš„é—®é¢˜å›ç­”ï¼Œé€šå¸¸åœ¨ç‰¹å®šé¢†åŸŸçš„é—®ç­”ä»»åŠ¡ä¸­è¡¨ç°æ›´ä½³
    multiset-base: åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒï¼Œå…·å¤‡è·¨é¢†åŸŸé—®ç­”çš„èƒ½åŠ›ï¼Œæ›´åŠ é€šç”¨
</pre></div>
</div>
<ul class="simple">
<li><p>dpr-reader: ç”¨äºç­”æ¡ˆæŠ½å–çš„é˜…è¯»å™¨æ¨¡å‹ã€‚å®ƒåœ¨DPRçš„æ£€ç´¢åå¤„ç†é˜¶æ®µä½¿ç”¨ï¼Œä»¥ä»é€‰å®šçš„å€™é€‰æ®µè½ä¸­æ‰¾åˆ°å…·ä½“çš„ç­”æ¡ˆã€‚è¿™ç§æ¨¡å‹åŸºäºBERTæˆ–ç±»ä¼¼çš„Transformeræ¶æ„ï¼Œç»è¿‡è®­ç»ƒå¯ä»¥ä»å€™é€‰æ–‡æœ¬ä¸­æå–å‡ºæœ€æœ‰å¯èƒ½çš„ç­”æ¡ˆç‰‡æ®µã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå®ƒä¾èµ–äºæŸ¥è¯¢ç¼–ç å™¨å’Œæ–‡æ¡£ç¼–ç å™¨ç­›é€‰å‡ºçš„å€™é€‰æ®µè½ä½œä¸ºè¾“å…¥ã€‚</p></li>
<li><p>dpr-ctx_encoder: ç”¨äºå°†æ–‡æ¡£æˆ–æ®µè½ç¼–ç ä¸ºå¯†é›†å‘é‡çš„æ–‡æ¡£ï¼ˆæˆ–ä¸Šä¸‹æ–‡ï¼‰ç¼–ç å™¨ã€‚å…¶ä½œç”¨æ˜¯å°†å¤§é‡å€™é€‰æ–‡æ¡£æˆ–æ®µè½è½¬æ¢ä¸ºå›ºå®šç»´åº¦çš„å¯†é›†å‘é‡è¡¨ç¤ºã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œç³»ç»Ÿå¯ä»¥åˆ©ç”¨å‘é‡æ£€ç´¢æ–¹æ³•ï¼ˆå¦‚å†…ç§¯æˆ–ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å¤§è§„æ¨¡æ–‡æ¡£åº“ä¸­å¿«é€Ÿæ£€ç´¢å‡ºä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ–‡æ¡£ã€‚</p></li>
<li><p>dpr-question_encoder: ç”¨äºå°†ç”¨æˆ·é—®é¢˜ç¼–ç ä¸ºå¯†é›†å‘é‡çš„æŸ¥è¯¢ç¼–ç å™¨ã€‚è¿™ä¸ªæ¨¡å‹å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºå¯†é›†å‘é‡ï¼Œä»¥ä¾¿ä¸æ–‡æ¡£ç¼–ç å™¨ç”Ÿæˆçš„æ–‡æ¡£å‘é‡è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„å€™é€‰æ–‡æ¡£ã€‚</p></li>
</ul>
<p>DPR æ¨¡å‹çš„æµç¨‹:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Step 1: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜é€šè¿‡ dpr-question_encoder ç¼–ç ä¸ºå¯†é›†å‘é‡ã€‚
Step 2: å€™é€‰æ–‡æ¡£é€šè¿‡ dpr-ctx_encoder ç¼–ç ä¸ºå¯†é›†å‘é‡ã€‚
Step 3: é€šè¿‡è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ–‡æ¡£å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆé€šå¸¸æ˜¯å†…ç§¯ï¼‰ï¼Œæ£€ç´¢åˆ°ç›¸å…³åº¦æœ€é«˜çš„å€™é€‰æ–‡æ¡£ã€‚
Step 4: å°†è¿™äº›å€™é€‰æ–‡æ¡£äº¤ç»™ dpr-readerï¼Œç”±å…¶ä»ä¸­æŠ½å–å…·ä½“ç­”æ¡ˆç‰‡æ®µã€‚
</pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="fromgpt">
<h2>å…¶ä»–-fromGPT<a class="headerlink" href="#fromgpt" title="æ­¤æ ‡é¢˜çš„æ°¸ä¹…é“¾æ¥">Â¶</a></h2>
<p>Transformer çš„ä¸»è¦ç»„æˆéƒ¨åˆ†åŒ…æ‹¬:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Attention Blockã€
    æ³¨æ„åŠ›æ¨¡å—ï¼Œæœ€æ ¸å¿ƒçš„æ¨¡å—
2. Feed-Forward Network (FFN)
    åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼Œæ³¨æ„åŠ›æ¨¡å—åé¢æ˜¯ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªä¸¤å±‚çš„å…¨è¿æ¥ç½‘ç»œ
    FFN å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹æ“ä½œï¼Œä¸»è¦è´Ÿè´£éçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
    FFN çš„å‚æ•°è§„æ¨¡é€šå¸¸å¾ˆå¤§ï¼Œå› æ­¤ä¹Ÿæ˜¯è®¡ç®—å¼€é”€çš„ä¸€ä¸ªä¸»è¦æ¥æºã€‚
3. Embedding Layer
    Embedding å±‚ç”¨äºå°†è¾“å…¥çš„ç¦»æ•£æ ‡è®°ï¼ˆtokensï¼‰è½¬åŒ–ä¸ºè¿ç»­çš„å‘é‡è¡¨ç¤ºã€‚
    åœ¨ NLP æ¨¡å‹ä¸­ï¼Œè¾“å…¥å’Œè¾“å‡ºé€šå¸¸éƒ½æœ‰ embedding å±‚ï¼Œåˆ†åˆ«å°†è¾“å…¥æ ‡è®°è½¬ä¸ºå‘é‡å’Œå°†è¾“å‡ºå‘é‡è½¬å›æ ‡è®°ã€‚
    å¯¹äº NLP æ¨¡å‹ï¼Œè¯æ±‡è¡¨çš„å¤§å°ä¼šå½±å“ embedding å±‚çš„å‚æ•°è§„æ¨¡ã€‚
4. Positional Encoding
    Transformer ä¸å…·å¤‡å†…åœ¨çš„åºåˆ—ä½ç½®æ„ŸçŸ¥ï¼Œå› æ­¤éœ€è¦åŠ å…¥ä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰æ¥è¡¨ç¤ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®çš„ä¿¡æ¯ã€‚
    ä½ç½®ç¼–ç å¯ä»¥æ˜¯å›ºå®šçš„ï¼ˆä¾‹å¦‚é€šè¿‡æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ç”Ÿæˆï¼‰æˆ–æ˜¯å¯è®­ç»ƒçš„å‘é‡ã€‚
    è™½ç„¶ä½ç½®ç¼–ç ä¸åŒ…å«å¾ˆå¤šå‚æ•°ï¼Œä½†å®ƒåœ¨ Transformer ä¸­è‡³å…³é‡è¦ï¼Œå¸®åŠ©æ¨¡å‹åŒºåˆ†åºåˆ—ä¸­çš„ä½ç½®ã€‚
5. Layer Normalization
    æ¯ä¸ª Transformer å±‚é€šå¸¸åŒ…å«ä¸€ä¸ªæˆ–å¤šä¸ªå±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰æ­¥éª¤ï¼Œç”¨äºåŠ é€Ÿè®­ç»ƒæ”¶æ•›ï¼Œç¨³å®šæ¨¡å‹æ€§èƒ½ã€‚
    è™½ç„¶ Layer Normalization çš„å‚æ•°å¾ˆå°‘ï¼Œä½†å¯¹æ¨¡å‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½æœ‰è¾ƒå¤§å½±å“ã€‚
6. Residual Connections
    åœ¨æ¯ä¸ªæ¨¡å—ï¼ˆå¦‚ Attention å’Œ FFNï¼‰ä¸­ï¼Œé€šå¸¸ä½¿ç”¨æ®‹å·®è¿æ¥ï¼ˆresidual connectionsï¼‰æ¥è¿æ¥è¾“å…¥å’Œè¾“å‡ºï¼Œä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚
    æ®‹å·®è¿æ¥æœ¬èº«æ²¡æœ‰å‚æ•°ï¼Œä½†åœ¨è®¡ç®—å›¾ä¸­å¢åŠ äº†å¯¹è¾“å…¥çš„ç›´æ¥å¼•ç”¨ã€‚
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">ä¸»é¡µ</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">æ¨¡å—ç´¢å¼•</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">æœç´¢é¡µé¢</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Tokenizers_V0.13.3.html" class="btn btn-neutral float-right" title="7.3.3. Tokenizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Transformers.html" class="btn btn-neutral" title="Transformers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, æ–°æºª-gordon.

    </p>
  </div>
  <div>å¤‡æ¡ˆå· <a href="http://www.beian.miit.gov.cn">äº¬ICPå¤‡16018553å·</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>