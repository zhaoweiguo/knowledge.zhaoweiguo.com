

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Transformers 4.45.2 &mdash; 新溪-gordon V2025.07 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="7.3.3. Tokenizers" href="../Tokenizers_V0.13.3.html" />
    <link rel="prev" title="Transformers" href="Transformers.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script src="../../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/ml.html">1.3. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/bi.html">1.4. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/deep_learning.html">1.5. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/normal.html">1.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/history.html">1.5.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/monitor.html">1.6. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/algorithm.html">1.7. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/tool.html">1.8. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/question.html">1.9. 常见问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/%E6%9C%BA%E5%99%A8%E4%BA%BA.html">1.10. 机器人领域</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/tmp.html">2.1. 临时</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/ReAct.html">2.1.1. ReAct框架</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Reflection.html">2.1.2. Reflection反思</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/math.html">2.1.3. 数学</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/bag-of-words.html">2.1.4. bag-of-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/word2vec.html">2.1.5. Word2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/doc2vec.html">2.1.6. Doc2Vec</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/FastText.html">2.1.7. FastText</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LDA.html">2.1.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/overfitting-underfitting.html">2.1.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RAG.html">2.1.10. RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Agent.html">2.1.11. Agent</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/LLM.html">2.1.12. LLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/RL.html">2.1.13. RL-强化学习</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/prompt_engineering.html">2.1.14. Prompt Engineering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/finetune.html">2.1.15. LLM调优(finetune)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../theories/tmps/Workflow.html">2.1.16. Workflow</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Qwen3.html">3.2.1. Qwen3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/DeepSeek.html">3.2.2. DeepSeek-R1-推理模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/LLaMA.html">3.2.3. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatGLM.html">3.2.4. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BERT.html">3.2.5. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenAI.html">3.2.6. OpenAI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BART.html">3.2.7. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/T5.html">3.2.8. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatRWKV.html">3.2.9. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Open-Assistant.html">3.2.10. Open-Assistant</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/OpenGVLab.html">3.2.11. OpenGVLab</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Quantization%E9%87%8F%E5%8C%96.html">3.4. 模型量化(Quantization)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/normal.html">3.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Quantizations/GGUF.html">3.4.2. GGUF 文件</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/fileformat.html">3.5. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/normal.html">3.5.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/GGML.html">3.5.2. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/ONNX.html">3.5.3. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/NCNN.html">3.5.4. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/openai.html">3.6. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/normal.html">3.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/openai.html">3.6.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/prompt.html">3.7. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_chinese.html">3.7.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_english.html">3.7.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/skill.html">3.7.3. 示例</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/Android.html">3.8. Android版LLM相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/normal.html">3.8.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/Android%E7%89%88%E9%83%A8%E7%BD%B2.html">3.8.2. Android版部署</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/Androids/GPU.html">3.8.3. GPU</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../RAG.html">4. RAG相关</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../NLP.html">5. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/preprocess.html">5.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/normal.html">5.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">5.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">5.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">5.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">5.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">5.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">5.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/NER.html">5.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/normal.html">5.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/seq-label.html">5.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/BiLSTM%2BCRF.html">5.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/history.html">5.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/summary.html">5.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/summarys/normal.html">5.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">6. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Image.html">6.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Video.html">6.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/IPython.html">6.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/normal.html">6.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/magic.html">6.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/display.html">6.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Jupyter.html">6.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/NumPy.html">6.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/normal.html">6.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/Ndarray.html">6.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/function.html">6.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Pandas.html">6.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/normal.html">6.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_subset.html">6.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_analysis.html">6.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_sql.html">6.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_default_value.html">6.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_multi_index.html">6.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/practice.html">6.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_input_output.html">6.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_General.html">6.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Series.html">6.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_DataFrame.html">6.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Index.html">6.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Matplotlib.html">6.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/normal.html">6.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/install.html">6.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pyplot.html">6.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/matplotlib.patches.html">6.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/example.html">6.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pylab.html">6.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/SciPy.html">6.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/SciPys/normal.html">6.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/sklearn.html">6.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/normal.html">6.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/supervised.html">6.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/unsupervised.html">6.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/statsmodels.html">6.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/OpenCV.html">6.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/normal.html">6.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/example.html">6.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/struct.html">6.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Seaborn.html">6.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Seaborns/normal.html">6.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/jieba.html">6.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/gensim.html">6.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/normal.html">6.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Core_Tutorials.html">6.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Tutorials.html">6.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/How-to_Guides.html">6.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/LAC.html">6.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../framework.html">7. 学习框架</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../normal.html">7.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pytorch.html">7.2. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/normal.html">7.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/nn.html">7.2.2. nn模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/PyTorch.html">7.2.3. PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/ExecuTorch.html">7.2.4. ExecuTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../pytorchs/torchrun.html">7.2.5. torchrun (Elastic Launch)</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../huggingface.html">7.3. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../normal.html">7.3.1. 常用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../normals/huggingface_hub.html">Hugging Face Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/lib_python.html">Hub Python Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Datasets.html">Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Text_Generation_Inference_main.html">TGI: Text Generation Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../normals/Evaluate.html">Evaluate</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="reference internal" href="../Transformers.html">7.3.2. Transformers</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="Transformers.html">Transformers</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Transformers 4.45.2</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Tokenizers_V0.13.3.html">7.3.3. Tokenizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../PEFT.html">7.3.4. PEFT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT.html">PEFT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../PEFT/PEFT_V0.13.0.html">PEFT 0.13.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Accelerate.html">7.3.5. Accelerate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../TRL.html">7.3.6. TRL - Transformer Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../collect.html">7.3.7. 收集</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../collects/model.html">model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../collects/blog_decoding-methods.html">博文: decoding methods of LLM with transformers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../vLLM.html">7.4. vLLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/normal.html">7.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../vLLMs/vLLM_doc.html">7.4.2. vLLM官方文档</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../llama.cpp.html">7.5. llama.cpp框架</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/normal.html">7.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../llama.cpps/llama-cpp-python.html">7.5.2. Python bindings for llama.cpp</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepSpeed.html">7.6. DeepSpeed</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/huggingface.html">7.6.1. huggingface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/ZeRO.html">7.6.2. Zero Redundancy Optimizer (ZeRO)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../DeepSpeeds/deepspeed_doc.html">7.6.3. DeepSpeed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../mxnet.html">7.7. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/ndarray.html">7.7.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/gluon.html">7.7.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../mxnets/autograd.html">7.7.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../tensorflow.html">7.8. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Keras.html">7.9. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/normal.html">7.9.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Keras/demo.html">7.9.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html">7.10. 其他</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../website.html">8. 关键网站</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Papers%20with%20Code.html">8.1. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/Kaggle.html">8.2. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/ArXiv.html">8.3. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/video.html">8.4. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../websites/normal.html">8.5. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-inference-accelerate.html">10.9. LLM推理加速</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Evaluate.html">10.10. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/platform.html">10.11. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese.html">11.2. 中文数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese_image.html">11.3. 中文图片相关数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/huggingface.html">11.4. dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/website.html">11.5. 数据集相关网站</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../model.html">12. 常见模型</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">13. 图形&amp;计算加速技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/normal.html">13.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../cudas/cuda.html">13.2. cuda</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Evaluate.html">14. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/normal.html">14.1. 通用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/TruLens.html">14.2. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/Ragas.html">14.3. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/DeepEval.html">14.4. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/UpTrain.html">14.5. UpTrain</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/huggingface.html">14.6. evaluate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../%E4%BC%A0%E7%BB%9FAI.html">15. 传统AI</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../framework.html"><span class="section-number">7. </span>学习框架</a> &raquo;</li>
        
          <li><a href="../../huggingface.html"><span class="section-number">7.3. </span>huggingface</a> &raquo;</li>
        
          <li><a href="../Transformers.html"><span class="section-number">7.3.2. </span>Transformers</a> &raquo;</li>
        
      <li>Transformers 4.45.2</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/frameworks/huggingfaces/Transformers/Transformers_V4.45.2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">Transformers 4.45.2</a><ul>
<li><a class="reference internal" href="#tutorials">Tutorials</a><ul>
<li><a class="reference internal" href="#load-pretrained-instances-with-an-autoclass">Load pretrained instances with an AutoClass</a><ul>
<li><a class="reference internal" href="#autobackbone">AutoBackbone</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generation-with-llms">Generation with LLMs</a><ul>
<li><a class="reference internal" href="#generate-text">Generate text</a></li>
<li><a class="reference internal" href="#common-pitfalls">Common pitfalls</a><ul>
<li><a class="reference internal" href="#generated-output-is-too-short-long">Generated output is too short/long</a></li>
<li><a class="reference internal" href="#incorrect-generation-mode">Incorrect generation mode</a></li>
<li><a class="reference internal" href="#wrong-padding-side">Wrong padding side</a></li>
<li><a class="reference internal" href="#wrong-prompt">Wrong prompt</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#chatting-with-transformers">Chatting with Transformers</a><ul>
<li><a class="reference internal" href="#choosing-a-chat-model">Choosing a chat model</a></li>
<li><a class="reference internal" href="#id2">相关代码</a></li>
<li><a class="reference internal" href="#performance-memory-and-hardware">Performance, memory and hardware</a><ul>
<li><a class="reference internal" href="#memory-considerations">Memory considerations</a></li>
<li><a class="reference internal" href="#performance-considerations">Performance considerations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#task-guides">TASK GUIDES</a><ul>
<li><a class="reference internal" href="#computer-vision">COMPUTER VISION</a><ul>
<li><a class="reference internal" href="#image-to-image">Image-to-Image</a></li>
<li><a class="reference internal" href="#image-feature-extraction">Image Feature Extraction</a></li>
<li><a class="reference internal" href="#mask-generation">Mask Generation</a></li>
<li><a class="reference internal" href="#keypoint-detection">Keypoint Detection</a></li>
<li><a class="reference internal" href="#knowledge-distillation-for-computer-vision">Knowledge Distillation for Computer Vision</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal">MULTIMODAL</a></li>
<li><a class="reference internal" href="#generation">Generation</a><ul>
<li><a class="reference internal" href="#text-generation-strategies">Text generation strategies</a><ul>
<li><a class="reference internal" href="#default-text-generation-configuration">Default text generation configuration</a></li>
<li><a class="reference internal" href="#customize-text-generation">Customize text generation</a></li>
<li><a class="reference internal" href="#save-a-custom-decoding-strategy-with-your-model">Save a custom decoding strategy with your model</a></li>
<li><a class="reference internal" href="#streaming">Streaming</a></li>
<li><a class="reference internal" href="#watermarking">Watermarking</a></li>
<li><a class="reference internal" href="#decoding-strategies">Decoding strategies</a><ul>
<li><a class="reference internal" href="#greedy-search">Greedy Search</a></li>
<li><a class="reference internal" href="#contrastive-search">Contrastive search</a></li>
<li><a class="reference internal" href="#multinomial-sampling">Multinomial sampling</a></li>
<li><a class="reference internal" href="#beam-search-decoding">Beam-search decoding</a></li>
<li><a class="reference internal" href="#beam-search-multinomial-sampling">Beam-search multinomial sampling</a></li>
<li><a class="reference internal" href="#diverse-beam-search-decoding">Diverse beam search decoding</a></li>
<li><a class="reference internal" href="#speculative-decoding">Speculative Decoding</a></li>
<li><a class="reference internal" href="#universal-assisted-decoding">Universal Assisted Decoding</a></li>
<li><a class="reference internal" href="#dola-decoding">DoLa Decoding</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#best-practices-for-generation-with-cache">Best Practices for Generation with Cache</a><ul>
<li><a class="reference internal" href="#what-is-cache-and-why-we-should-care">What is Cache and why we should care</a></li>
<li><a class="reference internal" href="#generate-with-cache">Generate with Cache</a><ul>
<li><a class="reference internal" href="#quantized-cache">Quantized Cache</a></li>
<li><a class="reference internal" href="#offloaded-cache">Offloaded Cache</a></li>
<li><a class="reference internal" href="#static-cache">Static Cache</a></li>
<li><a class="reference internal" href="#offloaded-static-cache">Offloaded Static Cache</a></li>
<li><a class="reference internal" href="#sliding-window-cache">Sliding Window Cache</a></li>
<li><a class="reference internal" href="#sink-cache">Sink Cache</a></li>
<li><a class="reference internal" href="#encoder-decoder-cache">Encoder-Decoder Cache</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-specific-cache-classes">Model-specific Cache Classes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#prompting">Prompting</a><ul>
<li><a class="reference internal" href="#image-tasks-with-idefics">Image tasks with IDEFICS</a></li>
<li><a class="reference internal" href="#llm-prompting-guide">LLM prompting guide</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#developer-guides">Developer guides</a><ul>
<li><a class="reference internal" href="#use-fast-tokenizers-from-tokenizers">Use fast tokenizers from 🤗 Tokenizers</a><ul>
<li><a class="reference internal" href="#loading-directly-from-the-tokenizer-object">Loading directly from the tokenizer object</a></li>
<li><a class="reference internal" href="#loading-from-a-json-file">Loading from a JSON file</a></li>
</ul>
</li>
<li><a class="reference internal" href="#use-model-specific-apis">Use model-specific APIs</a><ul>
<li><a class="reference internal" href="#configuration">Configuration</a></li>
<li><a class="reference internal" href="#model">Model</a><ul>
<li><a class="reference internal" href="#model-heads">Model heads</a></li>
</ul>
</li>
<li><a class="reference internal" href="#tokenizer">Tokenizer</a></li>
<li><a class="reference internal" href="#image-processor">Image processor</a></li>
<li><a class="reference internal" href="#backbone">Backbone</a></li>
<li><a class="reference internal" href="#feature-extractor">Feature extractor</a></li>
<li><a class="reference internal" href="#processor">Processor</a></li>
<li><a class="reference internal" href="#building-custom-models">Building custom models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#chat-templates">Chat Templates</a><ul>
<li><a class="reference internal" href="#introduce">Introduce</a></li>
<li><a class="reference internal" href="#how-do-i-use-chat-templates">How do I use chat templates</a></li>
<li><a class="reference internal" href="#id4">核心参数</a><ul>
<li><a class="reference internal" href="#add-generation-prompt">add_generation_prompt 参数</a></li>
<li><a class="reference internal" href="#continue-final-message">continue_final_message 参数</a></li>
<li><a class="reference internal" href="#tokenize">tokenize 参数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-how-do-chat-templates-work">Advanced: How do chat templates work?</a></li>
<li><a class="reference internal" href="#advanced-adding-and-editing-chat-templates">Advanced: Adding and editing chat templates</a><ul>
<li><a class="reference internal" href="#how-do-i-create-a-chat-template">How do I create a chat template?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-template-writing-tips">Advanced: Template writing tips</a><ul>
<li><a class="reference internal" href="#trimming-whitespace">Trimming whitespace</a></li>
<li><a class="reference internal" href="#callable-functions">Callable functions</a></li>
<li><a class="reference internal" href="#compatibility-with-non-python-jinja">Compatibility with non-Python Jinja</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#trainer">Trainer</a></li>
<li><a class="reference internal" href="#export-to-onnx">Export to ONNX</a><ul>
<li><a class="reference internal" href="#exporting-a-transformers-model-to-onnx-with-cli">Exporting a 🤗 Transformers model to ONNX with CLI</a></li>
<li><a class="reference internal" href="#exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime">Exporting a 🤗 Transformers model to ONNX with optimum.onnxruntime</a></li>
<li><a class="reference internal" href="#exporting-a-model-with-transformers-onnx">Exporting a model with transformers.onnx</a></li>
</ul>
</li>
<li><a class="reference internal" href="#interoperability-with-gguf-files">Interoperability with GGUF files</a></li>
</ul>
</li>
<li><a class="reference internal" href="#quantization-methods">Quantization Methods</a><ul>
<li><a class="reference internal" href="#quantization">Quantization</a></li>
<li><a class="reference internal" href="#bitsandbytes">bitsandbytes</a></li>
<li><a class="reference internal" href="#gptq">GPTQ</a><ul>
<li><a class="reference internal" href="#exllama">ExLlama</a></li>
</ul>
</li>
<li><a class="reference internal" href="#awq">AWQ</a></li>
<li><a class="reference internal" href="#aqlm">AQLM</a></li>
<li><a class="reference internal" href="#quanto">Quanto</a></li>
<li><a class="reference internal" href="#eetq">EETQ</a></li>
<li><a class="reference internal" href="#hqq">HQQ</a></li>
<li><a class="reference internal" href="#fbgemm-fp8">FBGEMM FP8</a></li>
<li><a class="reference internal" href="#optimum">Optimum</a></li>
<li><a class="reference internal" href="#torchao">TorchAO</a></li>
<li><a class="reference internal" href="#compressed-tensors">Compressed Tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-and-scalability">Performance and scalability</a><ul>
<li><a class="reference internal" href="#llm-inference-optimization">LLM inference optimization</a><ul>
<li><a class="reference internal" href="#static-kv-cache-and-torch-compile">Static kv-cache and torch.compile</a></li>
<li><a class="reference internal" href="#id5">Speculative decoding</a><ul>
<li><a class="reference internal" href="#prompt-lookup-decoding">Prompt lookup decoding</a></li>
</ul>
</li>
<li><a class="reference internal" href="#attention-optimizations">Attention optimizations</a><ul>
<li><a class="reference internal" href="#flashattention-2">FlashAttention-2</a></li>
<li><a class="reference internal" href="#pytorch-scaled-dot-product-attention">PyTorch scaled dot product attention</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">Quantization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#efficient-training-techniques">Efficient training techniques</a><ul>
<li><a class="reference internal" href="#methods-and-tools-for-efficient-training-on-a-single-gpu">Methods and tools for efficient training on a single GPU</a><ul>
<li><a class="reference internal" href="#batch-size-choice">Batch size choice</a></li>
<li><a class="reference internal" href="#gradient-accumulation">Gradient Accumulation</a></li>
<li><a class="reference internal" href="#gradient-checkpointing">Gradient Checkpointing</a></li>
<li><a class="reference internal" href="#mixed-precision-training">Mixed precision training</a><ul>
<li><a class="reference internal" href="#fp16">fp16</a></li>
<li><a class="reference internal" href="#bf16">BF16</a></li>
<li><a class="reference internal" href="#tf32">TF32</a></li>
</ul>
</li>
<li><a class="reference internal" href="#flash-attention-2">Flash Attention 2</a></li>
<li><a class="reference internal" href="#optimizer-choice">Optimizer choice</a></li>
<li><a class="reference internal" href="#data-preloading">Data preloading</a></li>
<li><a class="reference internal" href="#deepspeed-zero">DeepSpeed ZeRO</a></li>
<li><a class="reference internal" href="#using-torch-compile">Using torch.compile</a></li>
<li><a class="reference internal" href="#using-peft">Using 🤗 PEFT</a></li>
<li><a class="reference internal" href="#using-accelerate">Using 🤗 Accelerate</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multiple-gpus-and-parallelism">Multiple GPUs and parallelism</a></li>
<li><a class="reference internal" href="#fully-sharded-data-parallel">Fully Sharded Data Parallel</a></li>
<li><a class="reference internal" href="#deepspeed">DeepSpeed</a></li>
<li><a class="reference internal" href="#efficient-training-on-cpu">Efficient Training on CPU</a></li>
<li><a class="reference internal" href="#distributed-cpu-training">Distributed CPU training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizing-inference">Optimizing inference</a><ul>
<li><a class="reference internal" href="#cpu-inference">CPU inference</a><ul>
<li><a class="reference internal" href="#id10">🤗 Optimum</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu-inference">GPU inference</a><ul>
<li><a class="reference internal" href="#id11">FlashAttention-2</a></li>
<li><a class="reference internal" href="#bettertransformer">BetterTransformer</a></li>
<li><a class="reference internal" href="#id12">bitsandbytes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#others">Others</a><ul>
<li><a class="reference internal" href="#optimize-inference-using-torch-compile">Optimize inference using torch.compile()</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#conceptual-guides">Conceptual guides</a><ul>
<li><a class="reference internal" href="#glossary">Glossary</a><ul>
<li><a class="reference internal" href="#dataparallel-dp">DataParallel (DP)</a></li>
<li><a class="reference internal" href="#pipelineparallel-pp">PipelineParallel (PP)</a></li>
<li><a class="reference internal" href="#tensor-parallelism-tp">Tensor Parallelism (TP)</a></li>
<li><a class="reference internal" href="#tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism">Tensor Parallelism vs. Data Parallelism vs. Pipeline Parallelism</a></li>
<li><a class="reference internal" href="#zero-redundancy-optimizer-zero">Zero Redundancy Optimizer (ZeRO)</a></li>
<li><a class="reference internal" href="#id13">Fully Sharded Data Parallel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#optimizing-llms-for-speed-and-memory">Optimizing LLMs for Speed and Memory</a><ul>
<li><a class="reference internal" href="#lower-precision">1. Lower Precision</a></li>
<li><a class="reference internal" href="#flash-attention">2. Flash Attention</a></li>
<li><a class="reference internal" href="#architectural-innovations">3. Architectural Innovations</a><ul>
<li><a class="reference internal" href="#improving-positional-embeddings-of-llms">3.1 Improving positional embeddings of LLMs</a></li>
<li><a class="reference internal" href="#the-key-value-cache">3.2 The key-value cache</a><ul>
<li><a class="reference internal" href="#multi-round-conversation">3.2.1 Multi-round conversation</a></li>
<li><a class="reference internal" href="#multi-query-attention-mqa">3.2.2 Multi-Query-Attention (MQA)</a></li>
<li><a class="reference internal" href="#grouped-query-attention-gqa">3.2.3 Grouped-Query-Attention (GQA)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#api">API</a><ul>
<li><a class="reference internal" href="#main-classes">Main Classes</a><ul>
<li><a class="reference internal" href="#auto-classes">Auto Classes</a></li>
<li><a class="reference internal" href="#id14">Backbone</a></li>
<li><a class="reference internal" href="#callbacks">Callbacks</a></li>
<li><a class="reference internal" href="#id15">Configuration</a></li>
<li><a class="reference internal" href="#data-collator">Data Collator</a></li>
<li><a class="reference internal" href="#logging">Logging</a></li>
<li><a class="reference internal" href="#models">Models</a></li>
<li><a class="reference internal" href="#text-generation">Text Generation</a></li>
<li><a class="reference internal" href="#onnx">ONNX</a></li>
<li><a class="reference internal" href="#optimization">Optimization</a></li>
<li><a class="reference internal" href="#model-outputs">Model outputs</a></li>
<li><a class="reference internal" href="#pipelines">Pipelines</a></li>
<li><a class="reference internal" href="#processors">Processors</a><ul>
<li><a class="reference internal" href="#multi-modal-processors">Multi-modal processors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id16">Quantization</a></li>
<li><a class="reference internal" href="#id17">Tokenizer</a></li>
<li><a class="reference internal" href="#id18">Trainer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id19">Models</a><ul>
<li><a class="reference internal" href="#text-models">Text models</a><ul>
<li><a class="reference internal" href="#qwen2">Qwen2</a></li>
<li><a class="reference internal" href="#qwen2-vl">Qwen2_VL</a></li>
<li><a class="reference internal" href="#cpm">CPM</a></li>
<li><a class="reference internal" href="#dpr">DPR</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#fromgpt">其他-fromGPT</a></li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="transformers-4-45-2">
<h1>Transformers 4.45.2<a class="headerlink" href="#transformers-4-45-2" title="此标题的永久链接">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>本文档是自 <code class="docutils literal notranslate"><span class="pre">v4.23.1</span></code> 版本到本版本的变动部分</p>
</div>
<ul class="simple">
<li><p>from: <a class="reference external" href="https://huggingface.co/docs/transformers/v4.45.2/en/index">https://huggingface.co/docs/transformers/v4.45.2/en/index</a></p></li>
</ul>
<section id="tutorials">
<h2>Tutorials<a class="headerlink" href="#tutorials" title="此标题的永久链接">¶</a></h2>
<section id="load-pretrained-instances-with-an-autoclass">
<h3>Load pretrained instances with an AutoClass<a class="headerlink" href="#load-pretrained-instances-with-an-autoclass" title="此标题的永久链接">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>其他 AutoXXX 参见原始transformers文档</p>
</div>
<section id="autobackbone">
<h4>AutoBackbone<a class="headerlink" href="#autobackbone" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id21">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/KqpDhA.png" src="https://img.zhaoweiguo.com/uPic/2024/10/KqpDhA.png" />
<figcaption>
<p><span class="caption-text">A Swin backbone with multiple stages for outputting a feature map.</span><a class="headerlink" href="#id21" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>AutoBackbone 允许您使用预训练模型作为主干，从主干的不同阶段获取特征图。</p></li>
<li><dl class="simple">
<dt>from_pretrained() 函数有两个参数:</dt><dd><ul>
<li><p>out_indices 是要从中获取特征图的层的索引</p></li>
<li><p>out_features 是要从中获取特征图的图层的名称</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<figure class="align-default" id="id22">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/8tHkPG.png" src="https://img.zhaoweiguo.com/uPic/2024/10/8tHkPG.png" />
<figcaption>
<p><span class="caption-text">A feature map from the first stage of the backbone. The patch partition refers to the model stem.</span><a class="headerlink" href="#id22" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoBackbone</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">requests</span>
<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/swin-tiny-patch4-window7-224&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoBackbone</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/swin-tiny-patch4-window7-224&quot;</span><span class="p">,</span> <span class="n">out_indices</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,))</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">feature_maps</span>

<span class="o">&gt;&gt;</span> <span class="nb">list</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="generation-with-llms">
<h3>Generation with LLMs<a class="headerlink" href="#generation-with-llms" title="此标题的永久链接">¶</a></h3>
<p>安装所有必要的库:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="n">bitsandbytes</span><span class="o">&gt;=</span><span class="mf">0.39.0</span> <span class="o">-</span><span class="n">q</span>
</pre></div>
</div>
<section id="generate-text">
<h4>Generate text<a class="headerlink" href="#generate-text" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>针对 <code class="docutils literal notranslate"><span class="pre">causal</span> <span class="pre">language</span> <span class="pre">modeling</span></code> 进行训练的语言模型将一系列文本标记作为输入，并返回下一个标记的概率分布。</p></li>
</ul>
<figure>
    <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/genLLM1.mov" width="620"></video>
    <figcaption style="display: block; margin-top: 10px; text-align: center;">🔵"Forward pass of an LLM"</figcaption>
</figure><ul class="simple">
<li><p>以迭代方式重复，直到达到某个停止条件。理想情况下，停止条件由模型决定，模型应该学习何时输出序列结束 （EOS） 令牌。如果不是这种情况，则当达到某个预定义的最大长度时，生成将停止。</p></li>
</ul>
<figure>
    <video controls src="https://img.zhaoweiguo.com/uPic/2024/10/gen_LLM2.mov" width="620"></video>
    <figcaption style="display: block; margin-top: 10px; text-align: center;">🔵"Autoregressive generation iteratively selects the next token from a probability distribution to generate text"</figcaption>
</figure><p>加载模型:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    &quot;mistralai/Mistral-7B-v0.1&quot;, device_map=&quot;auto&quot;, load_in_4bit=True
)

说明:
device_map 确保将模型移动到您的 GPU
load_in_4bit 应用 4 位动态量化，大幅降低资源需求
</pre></div>
</div>
<p>preprocess your text input with a tokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A list of colors: red, blue&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>call the generate() method to returns the generated tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># &#39;A list of colors: red, blue, green, yellow, orange, purple, pink,&#39;</span>
</pre></div>
</div>
<p>批处理，这将以较小的延迟和内存成本大大提高吞吐量:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;A list of colors: red, blue&quot;</span><span class="p">,</span> <span class="s2">&quot;Portugal is&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># [&#39;A list of colors: red, blue, green, yellow, orange, purple, pink,&#39;,</span>
<span class="c1">#    &#39;Portugal is a country in southwestern Europe, on the Iber&#39;]</span>
</pre></div>
</div>
</section>
<section id="common-pitfalls">
<h4>Common pitfalls<a class="headerlink" href="#common-pitfalls" title="此标题的永久链接">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>生成策略有很多，有时默认值可能不适合您的使用案例</p>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="generated-output-is-too-short-long">
<h5>Generated output is too short/long<a class="headerlink" href="#generated-output-is-too-short-long" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>如果未在 GenerationConfig 文件中指定，则 generate 默认最多返回 20 个令牌</p></li>
<li><p>强烈建议在 generate 调用中手动设置 max_new_tokens 以控制它可以返回的最大新令牌数。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>LLMs（更准确地说，仅解码器模型）也会将输入提示作为输出的一部分返回。Keep in mind LLMs (more precisely, decoder-only models) also return the input prompt as part of the output.</p>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;A sequence of numbers: 1, 2&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># By default, the output will contain up to 20 tokens</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="s1">&#39;A sequence of numbers: 1, 2, 3, 4, 5&#39;</span>


<span class="c1"># Setting `max_new_tokens` allows you to control the maximum length</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="s1">&#39;A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,&#39;</span>
</pre></div>
</div>
</section>
<section id="incorrect-generation-mode">
<h5>Incorrect generation mode<a class="headerlink" href="#incorrect-generation-mode" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>默认情况下，除非在 GenerationConfig 文件中指定，否则 generate 在每次迭代（贪婪解码）时选择最可能的 token。</p></li>
<li><p>根据您的任务，选择不同的方法</p></li>
<li><p>像聊天机器人或写论文这样的创造性任务适合指定 <code class="docutils literal notranslate"><span class="pre">do_sample=True</span></code></p></li>
<li><p>而音频转录或翻译等基于输入的任务受益于贪婪解码。</p></li>
<li><p>相关博客文章: <a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seed for reproducibility -- you don&#39;t need this unless you want full reproducibility</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="s2">&quot;I am a cat.&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># LLM + greedy decoding = repetitive, boring output</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出:</span>
<span class="s1">&#39;I am a cat. I am a cat. I am a cat. I am a cat&#39;</span>


<span class="c1"># With sampling, the output becomes more creative!</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="s1">&#39;I am a cat.  Specifically, I am an indoor-only cat.  I&#39;</span>
</pre></div>
</div>
</section>
<section id="wrong-padding-side">
<h5>Wrong padding side<a class="headerlink" href="#wrong-padding-side" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>LLMs are decoder-only architectures, meaning they continue to iterate on your input prompt.</p></li>
<li><p>If your inputs do not have the same length, they need to be padded.(下面示例里面的123和ABCDE长度不同)</p></li>
<li><p>Since LLMs are not trained to continue from pad tokens, your input needs to be left-padded.</p></li>
<li><p>Make sure you also don’t forget to pass the attention mask to generate!</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># The tokenizer initialized above has right-padding active by default: the 1st sequence,</span>
<span class="c1"># which is shorter, has padding on the right side. Generation fails to capture the logic.</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;right&quot;</span><span class="p">)</span>  <span class="c1"># 默认是right</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;1, 2, 3&quot;</span><span class="p">,</span> <span class="s2">&quot;A, B, C, D, E&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="c1"># &#39;1, 2, 33333333333&#39;</span>



<span class="c1"># With left-padding, it works as expected!</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>  <span class="c1"># Most LLMs don&#39;t have a pad token by default</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;1, 2, 3&quot;</span><span class="p">,</span> <span class="s2">&quot;A, B, C, D, E&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="c1"># &#39;1, 2, 3, 4, 5, 6,&#39;</span>
</pre></div>
</div>
</section>
<section id="wrong-prompt">
<h5>Wrong prompt<a class="headerlink" href="#wrong-prompt" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>某些模型和任务需要某种输入提示格式才能正常工作。</p></li>
<li><p>有关提示的更多信息，包括哪些模型和任务需要小心，请参阅 <code class="docutils literal notranslate"><span class="pre">Task</span> <span class="pre">Guides</span> <span class="pre">-&gt;</span> <span class="pre">Prompting</span> <span class="pre">-&gt;</span> <span class="pre">LLM</span> <span class="pre">prompting</span> <span class="pre">guide</span></code> 。</p></li>
<li><p>下面看一个 <code class="docutils literal notranslate"><span class="pre">chat</span> <span class="pre">templating</span></code> 例子(使用 <code class="docutils literal notranslate"><span class="pre">tokenizer.apply_chat_template()</span></code> 函数)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;HuggingFaceH4/zephyr-7b-alpha&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;How many helicopters can a human eat in one sitting? Reply as a thug.&quot;&quot;&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">prompt</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">input_length</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">input_length</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># 输出</span>
<span class="c1"># &quot;I&#39;m not a thug, but i can tell you that a human cannot eat&quot;</span>
<span class="c1"># 说明</span>
<span class="c1"># Oh no, it did not follow our instruction to reply as a thug!</span>


<span class="c1"># write a better prompt and use the right template for this model</span>
<span class="c1"># (through `tokenizer.apply_chat_template`)</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a friendly chatbot who always responds in the style of a thug&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;How many helicopters can a human eat in one sitting?&quot;</span><span class="p">},</span>
<span class="p">]</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">input_length</span> <span class="o">=</span> <span class="n">model_inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">generated_ids</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">[:,</span> <span class="n">input_length</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># 输出</span>
<span class="c1"># &#39;None, you thug. How bout you try to focus on more useful questions?&#39;</span>
<span class="c1"># 说明</span>
<span class="c1"># As we can see, it followed a proper thug style 😎</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="chatting-with-transformers">
<h3>Chatting with Transformers<a class="headerlink" href="#chatting-with-transformers" title="此标题的永久链接">¶</a></h3>
<section id="choosing-a-chat-model">
<h4>Choosing a chat model<a class="headerlink" href="#choosing-a-chat-model" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>“8B”或“70B”。这是模型中的参数数。如果没有量化，每个参数大约需要 2 字节的内存。这意味着具有 80 亿个参数的“8B”模型将需要大约 16GB 的内存来适应参数，再加上一些额外的其他开销。它非常适合具有 24GB 显存</p></li>
<li><p>“Mixed of Experts” 模型。这些可能会以不同的方式列出它们的尺寸，例如“8x7B”或“141B-A35B”。这里的数字有点模糊，但一般来说，你可以把它理解为模型在第一种情况下大约有 56 （8x7） 亿个参数，在第二种情况下有 1410 亿个参数。</p></li>
</ul>
</section>
<section id="id2">
<h4>相关代码<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Prepare the input as before</span>
<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hey, can you tell me any fun things to do in New York?&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># 1: Load the model and tokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">)</span>

<span class="c1"># 2: Apply the chat template</span>
<span class="n">formatted_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Formatted chat:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">formatted_chat</span><span class="p">)</span>

<span class="c1"># 3: Tokenize the chat (This can be combined with the previous step using tokenize=True)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">formatted_chat</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Move the tokenized inputs to the same device the model is on (GPU/CPU)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">tensor</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized inputs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="c1"># 4: Generate text from the model</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated tokens:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="c1"># 5: Decode the output back to a string</span>
<span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">):],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded output:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">decoded_output</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-memory-and-hardware">
<h4>Performance, memory and hardware<a class="headerlink" href="#performance-memory-and-hardware" title="此标题的永久链接">¶</a></h4>
<section id="memory-considerations">
<h5>Memory considerations<a class="headerlink" href="#memory-considerations" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>大多数现代语言模型都以“bfloat16”精度进行训练，每个参数仅使用 2 个字节，而不使用占 4 个字节的float32</p></li>
<li><p>使用 “quantization” 可以降低到 16 位以下，这是一种有损压缩模型权重的方法。这允许将每个参数压缩到 8 位、4 位甚至更少。</p></li>
</ul>
<p>量化:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># You can also try load_in_4bit</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="performance-considerations">
<h5>Performance considerations<a class="headerlink" href="#performance-considerations" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>聊天模型生成文本相对不同，因为它的瓶颈是 <strong>内存带宽</strong> 而不是 <strong>计算能力</strong> ，因为它必须为模型生成的每个 token 从内存中读取每一个active parameter。这意味着您每秒可以从聊天模型生成的 token 数量通常与它这个表达式成正比：<code class="docutils literal notranslate"><span class="pre">内存总带宽除以模型的大小</span></code> 。一个8B的模型，以 bfloat16 精度加载时，模型大小为 ~16GB。这意味着必须为模型生成的每个令牌从内存中读取 16GB。</p></li>
<li><p>总内存带宽从消费类 CPU 的 <code class="docutils literal notranslate"><span class="pre">20-100GB/秒</span></code> 到消费类 GPU、Intel Xeon、AMD Threadripper/Epyc 或高端 Apple Silicon 等专用 CPU 的 <code class="docutils literal notranslate"><span class="pre">200-900GB/秒</span></code> 不等，最后高达 <code class="docutils literal notranslate"><span class="pre">2-3TB/秒</span></code> 的数据中心 GPU，如 Nvidia A100 或 H100。这应该可以让您很好地了解这些不同硬件类型的生成速度。</p></li>
<li><p>assisted generation的变体：也称为 “推测性采样(speculative sampling)”，通常使用较小的“草稿模型(draft model)”尝试一次猜测多个未来的 token，然后用聊天模型确认这些generations。如果通过聊天模型验证了猜测结果，则每次forward pass可以生成多个 Token，大大缓解了带宽瓶颈，提高了生成速度。</p></li>
<li><p>MoE 模型：几种流行的聊天模型，如 Mixtral、Qwen-MoE 和 DBRX，都是 MoE 模型。在这些模型中，并非每个参数对于生成的每个 Token 都处于活动状态。因此，MoE 模型通常具有低得多的内存带宽要求，即使它们的总大小可能相当大。因此，它们可以比相同大小的普通 “密集” 模型快几倍。然而，像辅助生成(assisted generation)这样的技术通常对这些模型无效，因为每个新的推测令牌都会有更多的参数变得活跃，这将抵消 MoE 架构提供的带宽和速度优势。</p></li>
</ul>
</section>
</section>
</section>
</section>
<section id="task-guides">
<h2>TASK GUIDES<a class="headerlink" href="#task-guides" title="此标题的永久链接">¶</a></h2>
<section id="computer-vision">
<h3>COMPUTER VISION<a class="headerlink" href="#computer-vision" title="此标题的永久链接">¶</a></h3>
<section id="image-to-image">
<h4>Image-to-Image<a class="headerlink" href="#image-to-image" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>image enhancement (super resolution, low light enhancement, deraining and so on)
    图像增强(超分辨率、弱光增强、去污等)
image inpainting
    图像修复
</pre></div>
</div>
</section>
<section id="image-feature-extraction">
<h4>Image Feature Extraction<a class="headerlink" href="#image-feature-extraction" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">image</span> <span class="n">similarity</span>
    <span class="n">图像相似度</span>
<span class="n">image</span> <span class="n">retrieval</span>
    <span class="n">图像检索</span>
</pre></div>
</div>
<ul class="simple">
<li><p>remove the task-specific head (image classification, object detection etc) and get the features</p></li>
<li><p>These features are very useful on a higher level: edge detection, corner detection and so on.</p></li>
<li><p>They may also contain information about the real world (e.g. what a cat looks like) depending on how deep the model is.</p></li>
<li><p>Therefore, these outputs can be used to train new classifiers on a specific dataset.</p></li>
</ul>
</section>
<section id="mask-generation">
<h4>Mask Generation<a class="headerlink" href="#mask-generation" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><strong>Mask Generation</strong> 为图像生成语义上有意义的掩模的任务。该任务与图像分割非常相似，但存在许多差异。图像分割模型在标记数据集上进行训练，并且仅限于它们在训练期间看到的类；给定图像，它们返回一组掩码和相应的类。</p></li>
<li><p>Mask generation is the task of generating semantically meaningful masks for an image. This task is very similar to image segmentation, but many differences exist.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Image</span> <span class="pre">segmentation</span></code> models are trained on labeled datasets and are limited to the classes they have seen during training; they return a set of masks and corresponding classes, given an image.</p></li>
</ul>
<p>Mask generation models are trained on large amounts of data and operate in two modes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Prompting mode:
    模型接收图像和提示，其中提示可以是对象内图像中的 2D 点位置（XY 坐标）或对象周围的边界框。
    在提示模式下，模型仅返回提示所指向的对象上的mask
2. Segment Everything mode:
    给定一张图像，模型会生成图像中的每个蒙版。
    为此，将生成一个点网格并将其叠加在图像上以进行推理。
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/hvrrET.png" src="https://img.zhaoweiguo.com/uPic/2024/10/hvrrET.png" />
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Point</span> <span class="n">Prompting</span>
<span class="n">Box</span> <span class="n">Prompting</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>具体细看文档吧(有空运行一下相关代码再细分析吧)</p>
</div>
</section>
<section id="keypoint-detection">
<h4>Keypoint Detection<a class="headerlink" href="#keypoint-detection" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>关键点检测识别并定位图像中的特定兴趣点。这些关键点也称为地标，代表对象的有意义的特征，例如面部特征或对象部分。</p></li>
<li><p>Keypoint detection identifies and locates specific points of interest within an image. These keypoints, also known as landmarks, represent meaningful features of objects, such as facial features(面部特征) or object parts(对象部位).</p></li>
</ul>
<p>These models take an image input and return the following outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Keypoints and Scores:
    兴趣点及其置信度分数
    Points of interest and their confidence scores.
2. Descriptors:
    每个关键点周围的图像区域的表示形式，捕获其纹理、渐变、方向和其他属性
    A representation of the image region surrounding each keypoint,
        capturing its texture, gradient, orientation and other properties.
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/KEzcue.png" src="https://img.zhaoweiguo.com/uPic/2024/11/KEzcue.png" />
</figure>
</section>
<section id="knowledge-distillation-for-computer-vision">
<h4>Knowledge Distillation for Computer Vision<a class="headerlink" href="#knowledge-distillation-for-computer-vision" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student).</p></li>
</ul>
</section>
</section>
<section id="multimodal">
<h3>MULTIMODAL<a class="headerlink" href="#multimodal" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Visual Question Answering: 基于图像回答开放式问题的任务。支持此任务的模型的输入通常是图像和问题的组合，输出是用自然语言表达的答案。</p></li>
<li><p>Image-text-to-tex: 也称为视觉语言模型 (VLM: vision language models)，是采用图像输入的语言模型。这些模型可以处理各种任务，从视觉问答(visual question answering)到图像分割(image segmentation)。此任务与图像到文本(image-to-text)有许多相似之处，并且在一些使用场景上有重叠，如：图像字幕(image captioning)。图像到文本(Image-to-text)模型仅接受图像输入并且通常完成特定任务，而 VLM 接受开放式文本和图像输入，并且是更通用的模型。</p></li>
<li><p>Video-text-to-text: 也称为视频语言模型(video language models)或具有视频输入的视觉语言模型(vision language models with video input)，是采用视频输入的语言模型。这些模型可以处理各种任务，从视频问答(video question answering)到视频字幕(video captioning)。</p></li>
</ul>
</section>
<section id="generation">
<h3>Generation<a class="headerlink" href="#generation" title="此标题的永久链接">¶</a></h3>
<section id="text-generation-strategies">
<h4>Text generation strategies<a class="headerlink" href="#text-generation-strategies" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>The process of selecting output tokens to generate text is known as decoding, and you can customize the decoding strategy that the generate() method will use.</p></li>
<li><p>选择输出token来生成文本的过程称为 decoding ，您可以自定义generate()方法将使用的解码策略(decoding strategy)</p></li>
<li><p>修改解码策略不会改变任何可训练参数的值。但是，它会对生成的输出的质量产生显着影响。</p></li>
</ul>
<section id="default-text-generation-configuration">
<h5>Default text generation configuration<a class="headerlink" href="#default-text-generation-configuration" title="此标题的永久链接">¶</a></h5>
<p>当您显式加载模型时，您可以通过 <code class="docutils literal notranslate"><span class="pre">model.generation_config</span></code> 检查模型附带的生成配置:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilgpt2&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">generation_config</span>
<span class="go">GenerationConfig {</span>
<span class="go">  &quot;bos_token_id&quot;: 50256,</span>
<span class="go">  &quot;eos_token_id&quot;: 50256</span>
<span class="go">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>打印model.generation_config仅显示与默认生成配置不同的值，并且不列出任何默认值。</p>
</div>
<p>default generation configuration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="n">prompt</span> <span class="n">to</span> <span class="n">a</span> <span class="n">maximum</span><span class="p">:</span> <span class="mi">20</span> <span class="n">token</span>
<span class="n">default</span> <span class="n">decoding</span> <span class="n">strategy</span> <span class="ow">is</span> <span class="n">greedy</span> <span class="n">search</span>
</pre></div>
</div>
</section>
<section id="customize-text-generation">
<h5>Customize text generation<a class="headerlink" href="#customize-text-generation" title="此标题的永久链接">¶</a></h5>
<p>通过将参数及其值直接传递给generate方法来覆盖任何generation_config:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>经常调整的参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. max_new_tokens:
    要生成的最大令牌数(the maximum number of tokens to generate)
    输出序列的大小，不包括输入prompt
2. num_beams:
    通过指定大于 1 的波束数量，您可以有效地从贪婪搜索切换到波束搜索
3. do_sample:
    如果设置为True ，此参数启用解码策略，例如多项式采样、波束搜索多项式采样、Top-K 采样和 Top-p 采样。
4. num_return_sequences:
    每个输入返回的序列候选数
    该选项仅适用于支持多个序列候选的解码策略，例如波束搜索(beam_search)和 采样(sampling)
    贪婪搜索(greedy_search)和对比搜索(contrastive_search)等解码策略返回单个输出序列
</pre></div>
</div>
</section>
<section id="save-a-custom-decoding-strategy-with-your-model">
<h5>Save a custom decoding strategy with your model<a class="headerlink" href="#save-a-custom-decoding-strategy-with-your-model" title="此标题的永久链接">¶</a></h5>
<p>specific generation configuration:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GenerationConfig</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;my_account/my_model&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span>
<span class="p">)</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;my_account/my_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>如果您想为单个模型存储多个生成配置（例如，一种用于通过采样生成创意文本，一种用于通过集束搜索进行摘要）时会很有用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 使用GenerationConfig.save_pretrained()中的config_file_name参数将多个生成配置存储在单个目录中。</span>
<span class="c1"># 使用GenerationConfig.from_pretrained()实例化它们</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GenerationConfig</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-small&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-t5/t5-small&quot;</span><span class="p">)</span>

<span class="n">translation_generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="p">(</span>
    <span class="n">num_beams</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">early_stopping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">decoder_start_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">eos_token_id</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">,</span>
    <span class="n">pad_token</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># 说明：通过指令配置文件名把相关配置写入到指定文件和从指定文件加载</span>
<span class="n">translation_generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;/tmp&quot;</span><span class="p">,</span> <span class="s2">&quot;translation_generation_config.json&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;/tmp&quot;</span><span class="p">,</span> <span class="s2">&quot;translation_generation_config.json&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;translate English to French: Configuration files are easy to use!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">generation_config</span><span class="o">=</span><span class="n">generation_config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="streaming">
<h5>Streaming<a class="headerlink" href="#streaming" title="此标题的永久链接">¶</a></h5>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>流媒体类的 API 仍在开发中，将来可能会发生变化。</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">TextStreamer</span>

<span class="n">tok</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tok</span><span class="p">([</span><span class="s2">&quot;An increasing sequence: one,&quot;</span><span class="p">],</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">streamer</span> <span class="o">=</span> <span class="n">TextStreamer</span><span class="p">(</span><span class="n">tok</span><span class="p">)</span>

<span class="c1"># Despite returning the usual output, the streamer will also print the generated text to stdout.</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">streamer</span><span class="o">=</span><span class="n">streamer</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="c1"># 输出:</span>
<span class="n">An</span> <span class="n">increasing</span> <span class="n">sequence</span><span class="p">:</span> <span class="n">one</span><span class="p">,</span> <span class="n">two</span><span class="p">,</span> <span class="n">three</span><span class="p">,</span> <span class="n">four</span><span class="p">,</span> <span class="n">five</span><span class="p">,</span> <span class="n">six</span><span class="p">,</span> <span class="n">seven</span><span class="p">,</span> <span class="n">eight</span><span class="p">,</span> <span class="n">nine</span><span class="p">,</span> <span class="n">ten</span><span class="p">,</span> <span class="n">eleven</span><span class="p">,</span>
</pre></div>
</div>
</section>
<section id="watermarking">
<h5>Watermarking<a class="headerlink" href="#watermarking" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>论文: On the Reliability of Watermarks for Large Language Models: <a class="reference external" href="https://arxiv.org/abs/2306.04634">https://arxiv.org/abs/2306.04634</a></p></li>
</ul>
</section>
<section id="decoding-strategies">
<h5>Decoding strategies<a class="headerlink" href="#decoding-strategies" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>常见解码策略的工作原理: <a class="reference external" href="https://huggingface.co/blog/how-to-generate">https://huggingface.co/blog/how-to-generate</a></p></li>
<li><p>影响模型的generate()结果有2</p></li>
<li><ol class="arabic simple">
<li><p>解码策略(decoding strategies)主要基于 Logits(下一个标记的概率分布)，因此选择一个好的 Logits操作策略(logits manipulation strategy)可以大有帮助！</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>除了选择解码策略之外，操作逻辑(manipulating the logits)是您可以采取的另一个方法。流行的 logits 操作策略包括top_p 、 min_p和repetition_penalty</p></li>
</ol>
</li>
</ul>
<section id="greedy-search">
<h6>Greedy Search<a class="headerlink" href="#greedy-search" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>默认使用贪婪搜索解码，因此您不必传递任何参数来启用它。</p></li>
<li><p>这意味着参数num_beams设置为 1 且do_sample=False 。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;I look forward to&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert/distilgpt2&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="p">[</span><span class="s1">&#39;I look forward to seeing you all again!</span><span class="se">\n\n\n\n\n\n\n\n\n\n\n</span><span class="s1">&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="contrastive-search">
<h6>Contrastive search<a class="headerlink" href="#contrastive-search" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>论文A Contrastive Framework for Neural Text Generation: <a class="reference external" href="https://arxiv.org/abs/2202.06417">https://arxiv.org/abs/2202.06417</a></p></li>
<li><p>对比搜索的工作原理: <a class="reference external" href="https://huggingface.co/blog/introducing-csearch">https://huggingface.co/blog/introducing-csearch</a></p></li>
<li><p>启用和控制对比搜索行为的两个主要参数是penalty_alpha和top_k</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-large&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Hugging Face Company is&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">penalty_alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># [&#39;Hugging Face Company is a family owned and operated business. .... We look forward to hearing from you!&#39;]</span>
</pre></div>
</div>
</section>
<section id="multinomial-sampling">
<h6>Multinomial sampling<a class="headerlink" href="#multinomial-sampling" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>与总是选择概率最高的标记作为下一个标记的贪婪搜索相反，多项式采样（也称为祖先采样）根据模型给出的整个词汇表的概率分布随机选择下一个标记。</p></li>
<li><p>每个具有非零概率的令牌都有被选择的机会，从而降低了重复的风险。</p></li>
<li><p>设置do_sample=True和num_beams=1</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-large&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Today was an amazing day because&quot;</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="beam-search-decoding">
<h6>Beam-search decoding<a class="headerlink" href="#beam-search-decoding" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>与贪婪搜索不同，波束搜索解码在每个时间步保留多个假设，并最终选择整个序列总体概率最高的假设。</p></li>
<li><p>这样做的优点是可以识别以较低概率初始标记开始的高概率序列，并且会被贪婪搜索忽略。</p></li>
</ul>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/XNPDom.png" src="https://img.zhaoweiguo.com/uPic/2024/10/XNPDom.png" />
</figure>
<ul class="simple">
<li><p>交互式演示: <a class="reference external" href="https://huggingface.co/spaces/m-ric/beam_search_visualizer">https://huggingface.co/spaces/m-ric/beam_search_visualizer</a></p></li>
<li><p>要启用此解码策略，请指定大于 1 的num_beams （也称为要跟踪的假设数）</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;It is astonishing how one can&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai-community/gpt2-medium&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="beam-search-multinomial-sampling">
<h6>Beam-search multinomial sampling<a class="headerlink" href="#beam-search-multinomial-sampling" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>这种解码策略将波束搜索与多项式采样相结合。</p></li>
<li><p>指定num_beams大于 1，并设置do_sample=True才能使用此解码策略。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="p">,</span> <span class="n">set_seed</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># For reproducibility</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;translate English to German: The house is wonderful.&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google-t5/t5-small&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="diverse-beam-search-decoding">
<h6>Diverse beam search decoding<a class="headerlink" href="#diverse-beam-search-decoding" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>多样化波束搜索解码策略是波束搜索策略的扩展，允许生成更多样化的波束序列集以供选择。</p></li>
<li><p>工作原理，请参阅Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models: <a class="reference external" href="https://arxiv.org/pdf/1610.02424.pdf">https://arxiv.org/pdf/1610.02424.pdf</a></p></li>
<li><p>该方法具有三个主要参数： <code class="docutils literal notranslate"><span class="pre">num_beams</span></code> 、 <code class="docutils literal notranslate"><span class="pre">num_beam_groups</span></code> 和 <code class="docutils literal notranslate"><span class="pre">diversity_penalty</span></code></p></li>
<li><p>多样性惩罚确保输出在组之间是不同的，并且在每个组内使用波束搜索。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/pegasus-xsum&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">&quot;The Permaculture Design Principles are a set of universal design principles &quot;</span>
    <span class="o">...</span>
    <span class="s2">&quot;efficient way possible.&quot;</span>
<span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_beams</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">num_beam_groups</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">diversity_penalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="speculative-decoding">
<h6>Speculative Decoding<a class="headerlink" href="#speculative-decoding" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>推测解码（也称为辅助解码）是上述解码策略的一种修改，它使用辅助模型（最好是更小的模型）来生成一些候选标记。</p></li>
<li><p>然后，主模型在一次前向传递中验证候选标记，从而加快解码过程。</p></li>
<li><p>如果do_sample=True ，则使用推测解码论文中引入的带有重采样的令牌验证。</p></li>
<li><p>辅助解码假设主模型和辅助模型具有相同的分词器，否则，请参阅下面的通用辅助解码。</p></li>
<li><p>目前辅助解码仅支持贪婪搜索和采样，辅助解码不支持批量输入。</p></li>
<li><p>要了解有关辅助解码的更多信息，请查看此博客文章: <a class="reference external" href="https://huggingface.co/blog/assisted-generation">https://huggingface.co/blog/assisted-generation</a></p></li>
<li><p>要启用辅助解码，请使用模型设置assistant_model参数。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Alice and Bob&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/pythia-1.4b-deduped&quot;</span>
<span class="n">assistant_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;EleutherAI/pythia-160m-deduped&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="universal-assisted-decoding">
<h6>Universal Assisted Decoding<a class="headerlink" href="#universal-assisted-decoding" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>通用辅助解码 (UAD) 添加了对具有不同标记器的主模型和辅助模型的支持。</p></li>
<li><p>要使用它，只需使用tokenizer和assistant_tokenizer参数传递标记器（见下文）。</p></li>
<li><p>在内部，主模型输入标记被重新编码为辅助模型标记，然后在辅助编码中生成候选标记，这些候选标记又被重新编码为主模型候选标记。然后验证按照上面的解释进行。重新编码步骤涉及将令牌 ID 解码为文本，然后使用不同的令牌生成器对文本进行编码。由于重新编码令牌可能会导致令牌化差异，因此 UAD 会找到源编码和目标编码之间的最长公共子序列，以确保新令牌包含正确的提示后缀。</p></li>
<li><p>如果主模型和辅助模型具有不同的标记器，请使用通用辅助解码。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Alice and Bob&quot;</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/gemma-2-9b&quot;</span>
<span class="n">assistant_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;double7/vicuna-68m&quot;</span>

<span class="n">assistant_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">assistant_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">assistant_checkpoint</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">assistant_model</span><span class="o">=</span><span class="n">assistant_model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">assistant_tokenizer</span><span class="o">=</span><span class="n">assistant_tokenizer</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="dola-decoding">
<h6>DoLa Decoding<a class="headerlink" href="#dola-decoding" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>Decoding by Contrasting Layers (DoLa) 是一种对比解码策略，旨在提高事实性并减少LLMs的幻觉</p></li>
<li><p>如 ICLR 2024 DoLa 的论文所述：Decoding by Contrasting Layers Improves Factuality in Large Language Models: <a class="reference external" href="https://arxiv.org/abs/2309.03883">https://arxiv.org/abs/2309.03883</a></p></li>
<li><p>DoLa 是通过对比最终层与早期层获得的 logits 差异来实现的，从而放大了变压器层特定部分的事实知识。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>更详细的再细看吗</p>
</div>
</section>
</section>
</section>
<section id="best-practices-for-generation-with-cache">
<h4>Best Practices for Generation with Cache<a class="headerlink" href="#best-practices-for-generation-with-cache" title="此标题的永久链接">¶</a></h4>
<section id="what-is-cache-and-why-we-should-care">
<h5>What is Cache and why we should care<a class="headerlink" href="#what-is-cache-and-why-we-should-care" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>KV Cache</p></li>
<li><p>模型一次只能生成一个token，并且每个新预测都取决于先前的上下文</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># past_key_values is the key-value cache</span>
<span class="n">generated_tokens</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">next_logits</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to_tuple</span><span class="p">()</span>
  <span class="n">next_logits</span> <span class="o">=</span> <span class="n">next_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
  <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shape of input_ids&quot;</span><span class="p">,</span> <span class="n">next_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of key-value cache&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">past_key_values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]))</span>  <span class="c1"># past_key_values are of shape [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]</span>
  <span class="n">generated_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">)</span>
<span class="n">generated_text</span>

<span class="c1"># 输出</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">20</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">21</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">22</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">23</span>
<span class="n">shape</span> <span class="n">of</span> <span class="n">input_ids</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">length</span> <span class="n">of</span> <span class="n">key</span><span class="o">-</span><span class="n">value</span> <span class="n">cache</span> <span class="mi">24</span>
<span class="p">[</span><span class="s1">&#39; Here&#39;</span><span class="p">,</span> <span class="s1">&#39; is&#39;</span><span class="p">,</span> <span class="s1">&#39; a&#39;</span><span class="p">,</span> <span class="s1">&#39; Python&#39;</span><span class="p">,</span> <span class="s1">&#39; function&#39;</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>这意味着，要在 Generation 中预测编号为 1000 的token，您需要来自之前 999 个token的信息，这些信息以 token 形式的矩阵乘法计算。</p></li>
<li><p>但是要预测令牌编号 1001，您还需要前 999 个令牌中的相同信息，以及令牌编号 1000 中的附加信息。</p></li>
<li><p>这就是使用键值缓存(KV Cache)来优化顺序生成过程的地方，方法是存储先前的计算以便在后续中重用令牌，因此不需要再次计算它们。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>请注意，缓存只能在推理中使用，并且在训练时应禁用，否则可能会导致意外错误。</p>
</div>
</section>
<section id="generate-with-cache">
<h5>Generate with Cache<a class="headerlink" href="#generate-with-cache" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>默认情况下，所有模型都使用缓存生成，其中〜DynamicCache类是大多数模型的默认缓存。</p></li>
<li><p>如果由于某种原因您不想使用缓存，则可以将use_cache=False传递到generate()方法中。</p></li>
<li><p>缓存类可以在生成时使用cache_implementation参数进行设置。</p></li>
</ul>
<section id="quantized-cache">
<h6>Quantized Cache<a class="headerlink" href="#quantized-cache" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>键和值缓存会占用很大一部分内存，成为长上下文生成的瓶颈，特别是对于大型语言模型。</p></li>
<li><p>使用generate()时使用量化缓存可以显着减少内存需求，但代价是速度。</p></li>
<li><p>transformers中的 <code class="docutils literal notranslate"><span class="pre">KV</span> <span class="pre">Cache量化</span></code> 很大程度上受到此论文启发: KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache: <a class="reference external" href="https://arxiv.org/abs/2402.02750">https://arxiv.org/abs/2402.02750</a></p></li>
<li><p>如果您使用quanto后端，建议将缓存配置中的axis-key/axis-value参数设置为0；如果您使用HQQ后端，建议将其设置为1 。对于其他配置值，请使用默认值</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;I like rock music because&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;quantized&quot;</span><span class="p">,</span> <span class="n">cache_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nbits&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;backend&quot;</span><span class="p">:</span> <span class="s2">&quot;quanto&quot;</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="offloaded-cache">
<h6>Offloaded Cache<a class="headerlink" href="#offloaded-cache" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>与 KV 缓存量化类似， ~OffloadedCache策略旨在减少 GPU VRAM 使用。</p></li>
<li><p>它通过将大多数层的 KV 缓存移至 CPU 来实现这一点。</p></li>
<li><p>当模型的forward()方法迭代各层时，该策略会在GPU上维护当前层缓存。同时，它异步预取下一层缓存，并将上一层缓存发送回 CPU。</p></li>
<li><p>与 KV 缓存量化不同，此策略始终产生与默认 KV 缓存实现相同的结果。因此，它可以作为它的直接替代品或后备方案。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p><code class="docutils literal notranslate"><span class="pre">Cache</span> <span class="pre">offloading</span></code> 需要 GPU，并且可能比 <code class="docutils literal notranslate"><span class="pre">dynamic</span> <span class="pre">KV</span> <span class="pre">cache</span></code> 慢。如果您遇到 CUDA 内存不足错误，请使用它。Cache offloading requires a GPU and can be slower than dynamic KV cache. Use it if you are getting CUDA out of memory errors.</p>
</div>
<ul class="simple">
<li><p>示例-如何使用 KV 缓存卸载作为后备策略</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>
<span class="k">def</span><span class="w"> </span><span class="nf">resilient_generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">oom</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">except</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">OutOfMemoryError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;retrying with cache_implementation=&#39;offloaded&#39;&quot;</span><span class="p">)</span>
        <span class="n">oom</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="k">if</span> <span class="n">oom</span><span class="p">:</span>  <span class="c1"># 如果OOM,则启动后备策略</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;cache_implementation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;offloaded&quot;</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">ckpt</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckpt</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">ckpt</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">prompt</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;okay &quot;</span><span class="o">*</span><span class="mi">1000</span> <span class="o">+</span> <span class="s2">&quot;Fun fact: The most&quot;</span><span class="p">]</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">beams</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;num_beams&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;num_beam_groups&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;num_return_sequences&quot;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span> <span class="s2">&quot;diversity_penalty&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;max_new_tokens&quot;</span><span class="p">:</span> <span class="mi">23</span><span class="p">,</span> <span class="s2">&quot;early_stopping&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">out</span> <span class="o">=</span> <span class="n">resilient_generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="o">**</span><span class="n">beams</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">responses</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span><span class="o">-</span><span class="mi">28</span><span class="p">:],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="static-cache">
<h6>Static Cache<a class="headerlink" href="#static-cache" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>由于“DynamicCache”随着每个生成步骤动态增长，因此它会阻止您利用 JIT 优化。</p></li>
<li><p>~StaticCache为键和值预先分配特定的最大大小，允许您生成最大长度而无需修改缓存大小。</p></li>
<li><p>有关静态缓存和 JIT 编译的更多示例，请查看StaticCache &amp; torchcompile: <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/llm_optims#static-kv-cache-and-torchcompile">https://huggingface.co/docs/transformers/main/en/llm_optims#static-kv-cache-and-torchcompile</a></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># simply pass the cache implementation=&quot;static&quot;</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;static&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="offloaded-static-cache">
<h6>Offloaded Static Cache<a class="headerlink" href="#offloaded-static-cache" title="此标题的永久链接">¶</a></h6>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my name is&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># simply pass the cache implementation=&quot;static&quot;</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;offloaded_static&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="sliding-window-cache">
<h6>Sliding Window Cache<a class="headerlink" href="#sliding-window-cache" title="此标题的永久链接">¶</a></h6>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>注意，您只能将此缓存用于支持滑动窗口的模型，例如 Mistral 模型。</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">SinkCache</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Yesterday I was on a rock concert and.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># can be used by passing in cache implementation</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">cache_implementation</span><span class="o">=</span><span class="s2">&quot;sliding_window&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="sink-cache">
<h6>Sink Cache<a class="headerlink" href="#sink-cache" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>论文: Efficient Streaming Language Models with Attention Sinks: <a class="reference external" href="https://arxiv.org/abs/2309.17453">https://arxiv.org/abs/2309.17453</a></p></li>
<li><p>允许您生成长文本序列（根据论文“无限长度”），无需任何微调。这是通过智能处理以前的键和值来实现的，特别是它保留了序列中的一些初始标记，称为“接收器标记”。这是基于这样的观察：这些初始令牌在生成过程中吸引了很大一部分注意力分数。 “接收器令牌”之后的令牌将在滑动窗口的基础上被丢弃，仅保留最新的window_size令牌。通过将这些初始标记保留为“注意力池”，即使在处理很长的文本时，模型也能保持稳定的性能，从而丢弃大部分先前的知识。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">SinkCache</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;This is a long story about unicorns, fairies and magic.&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># get our cache, specify number of sink tokens and window size</span>
<span class="c1"># Note that window size already includes sink tokens, so has to be larger</span>
<span class="n">past_key_values</span> <span class="o">=</span> <span class="n">SinkCache</span><span class="p">(</span><span class="n">window_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">num_sink_tokens</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>与其他缓存类不同，这个缓存类不能通过指示cache_implementation来直接使用。您必须在调用generate()之前初始化缓存</p>
</div>
</section>
<section id="encoder-decoder-cache">
<h6>Encoder-Decoder Cache<a class="headerlink" href="#encoder-decoder-cache" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>~EncoderDecoderCache是一个包装器，旨在处理编码器-解码器模型的缓存需求。这种缓存类型是专门为管理自注意力和交叉注意力缓存而构建的，确保存储和检索这些复杂模型所需的过去的键/值。</p></li>
</ul>
</section>
</section>
<section id="model-specific-cache-classes">
<h5>Model-specific Cache Classes<a class="headerlink" href="#model-specific-cache-classes" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>有些模型需要以特定的方式存储以前的键、值或状态，并且不能使用上述缓存类。对于这种情况，我们有几个专为特定模型设计的专用缓存类。</p></li>
<li><p>示例包括用于Gemma2系列模型的~HybridCache或用于Mamba架构模型的~MambaCache 。</p></li>
</ul>
</section>
</section>
</section>
<section id="prompting">
<h3>Prompting<a class="headerlink" href="#prompting" title="此标题的永久链接">¶</a></h3>
<section id="image-tasks-with-idefics">
<h4>Image tasks with IDEFICS<a class="headerlink" href="#image-tasks-with-idefics" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>对于把图像先转为文本再进行分析的LLM，这种图像类的task也可以像普通的语言LLM一样使用prompt</p></li>
</ul>
</section>
<section id="llm-prompting-guide">
<h4>LLM prompting guide<a class="headerlink" href="#llm-prompting-guide" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>编码器-解码器式模型通常用于输出严重依赖输入的生成任务，例如翻译和摘要。仅解码器模型用于所有其他类型的生成任务。</p></li>
<li><p>具体看prompt相关文档</p></li>
</ul>
</section>
</section>
</section>
<section id="developer-guides">
<h2>Developer guides<a class="headerlink" href="#developer-guides" title="此标题的永久链接">¶</a></h2>
<section id="use-fast-tokenizers-from-tokenizers">
<h3>Use fast tokenizers from 🤗 Tokenizers<a class="headerlink" href="#use-fast-tokenizers-from-tokenizers" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>PreTrainedTokenizer：这是一个纯 Python 实现的分词器基类，所有的分词和编码操作都是通过 Python 代码执行的。</p></li>
<li><p>PreTrainedTokenizerFast：基于 Rust 编写的 🤗 Tokenizers 库，实现了更高效的分词算法。PreTrainedTokenizerFast 通过绑定 Rust 实现，提供了更快的分词速度。</p></li>
<li><p>需要注意的是，并非所有模型的分词器都有对应的 “Fast” 实现，特别是基于 SentencePiece 的分词器（如 T5、ALBERT、CamemBERT、XLMRoBERTa 和 XLNet 等模型）目前尚无 “Fast” 版本可用</p></li>
<li><p>创建一个虚拟分词器(dummy tokenizer)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.models</span><span class="w"> </span><span class="kn">import</span> <span class="n">BPE</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.trainers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BpeTrainer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tokenizers.pre_tokenizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Whitespace</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">(</span><span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">))</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">special_tokens</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[UNK]&quot;</span><span class="p">,</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="s2">&quot;[SEP]&quot;</span><span class="p">,</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="s2">&quot;[MASK]&quot;</span><span class="p">])</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">Whitespace</span><span class="p">()</span>
<span class="n">files</span> <span class="o">=</span> <span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">files</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span>

<span class="c1"># 保存</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&quot;tokenizer.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="loading-directly-from-the-tokenizer-object">
<h4>Loading directly from the tokenizer object<a class="headerlink" href="#loading-directly-from-the-tokenizer-object" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizerFast</span>
<span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">(</span><span class="n">tokenizer_object</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loading-from-a-json-file">
<h4>Loading from a JSON file<a class="headerlink" href="#loading-from-a-json-file" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">PreTrainedTokenizerFast</span>
<span class="n">fast_tokenizer</span> <span class="o">=</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">(</span><span class="n">tokenizer_file</span><span class="o">=</span><span class="s2">&quot;tokenizer.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="use-model-specific-apis">
<h3>Use model-specific APIs<a class="headerlink" href="#use-model-specific-apis" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Create a custom architecture</p></li>
<li><p>AutoClass自动推断模型架构并下载预训练的配置和权重。一般来说，我们 <strong>建议</strong> 使用AutoClass来生成与检查点无关的代码。</p></li>
<li><p>本节主要了解如何创建不使用AutoClass自定义模型</p></li>
</ul>
<section id="configuration">
<h4>Configuration<a class="headerlink" href="#configuration" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Configuration 指模型的特定属性。每个模型配置都有不同的属性</p></li>
<li><p>示例 <code class="docutils literal notranslate"><span class="pre">DistilBertConfig</span></code> displays all the default attributes used to build a base <code class="docutils literal notranslate"><span class="pre">DistilBertModel</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertConfig</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="go">DistilBertConfig {</span>
<span class="go">  &quot;activation&quot;: &quot;gelu&quot;,</span>
<span class="go">  &quot;attention_dropout&quot;: 0.1,</span>
<span class="go">  &quot;dim&quot;: 768,</span>
<span class="go">  &quot;dropout&quot;: 0.1,</span>
<span class="go">  &quot;hidden_dim&quot;: 3072,</span>
<span class="go">  &quot;initializer_range&quot;: 0.02,</span>
<span class="go">  &quot;max_position_embeddings&quot;: 512,</span>
<span class="go">  &quot;model_type&quot;: &quot;distilbert&quot;,</span>
<span class="go">  &quot;n_heads&quot;: 12,</span>
<span class="go">  &quot;n_layers&quot;: 6,</span>
<span class="go">  &quot;pad_token_id&quot;: 0,</span>
<span class="go">  &quot;qa_dropout&quot;: 0.1,</span>
<span class="go">  &quot;seq_classif_dropout&quot;: 0.2,</span>
<span class="go">  &quot;sinusoidal_pos_embds&quot;: false,</span>
<span class="go">  &quot;transformers_version&quot;: &quot;4.16.2&quot;,</span>
<span class="go">  &quot;vocab_size&quot;: 30522</span>
<span class="go">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>所有属性均可定制，如下示例</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>
<span class="go">DistilBertConfig {</span>
<span class="go">  &quot;activation&quot;: &quot;relu&quot;,             # gelu-&gt;relu</span>
<span class="go">  &quot;attention_dropout&quot;: 0.4,         # 0.1-&gt;0.4</span>
<span class="go">  &quot;dim&quot;: 768,</span>
<span class="go">  &quot;dropout&quot;: 0.1,</span>
<span class="go">  &quot;hidden_dim&quot;: 3072,</span>
<span class="go">  &quot;initializer_range&quot;: 0.02,</span>
<span class="go">  &quot;max_position_embeddings&quot;: 512,</span>
<span class="go">  &quot;model_type&quot;: &quot;distilbert&quot;,</span>
<span class="go">  &quot;n_heads&quot;: 12,</span>
<span class="go">  &quot;n_layers&quot;: 6,</span>
<span class="go">  &quot;pad_token_id&quot;: 0,</span>
<span class="go">  &quot;qa_dropout&quot;: 0.1,</span>
<span class="go">  &quot;seq_classif_dropout&quot;: 0.2,</span>
<span class="go">  &quot;sinusoidal_pos_embds&quot;: false,</span>
<span class="go">  &quot;transformers_version&quot;: &quot;4.16.2&quot;,</span>
<span class="go">  &quot;vocab_size&quot;: 30522</span>
<span class="go">}</span>
</pre></div>
</div>
<p>保存&amp;加载:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">attention_dropout</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="c1"># 保存</span>
<span class="n">my_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="o">=</span><span class="s2">&quot;./your_model_save_path&quot;</span><span class="p">)</span>
<span class="c1"># 加载</span>
<span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your_model_save_path/config.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model">
<h4>Model<a class="headerlink" href="#model" title="此标题的永久链接">¶</a></h4>
<p>加载:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 将自定义配置属性加载到模型中</span>
<span class="c1"># 这将创建一个具有随机值而不是预训练权重的模型</span>
<span class="c1"># 注意：在训练该模型之前，您还无法将该模型用于任何有用的事情</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertModel</span>
<span class="n">my_config</span> <span class="o">=</span> <span class="n">DistilBertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your_model_save_path/config.json&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>

<span class="c1"># 自动加载默认模型配置的预训练模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>

<span class="c1"># 使用自己的模型配置属性</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">my_config</span><span class="p">)</span>
</pre></div>
</div>
<section id="model-heads">
<h5>Model heads<a class="headerlink" href="#model-heads" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>At this point, you have a base <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code> model which outputs the <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span></code> are passed as inputs to a <code class="docutils literal notranslate"><span class="pre">model</span> <span class="pre">head</span></code> to produce the final output.</p></li>
<li><p>🤗 Transformers provides a different model head for each task as long as a model supports the task</p></li>
<li><p>(i.e., you can’t use DistilBERT for a sequence-to-sequence task like translation).</p></li>
<li><p>示例</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code> is a base <code class="docutils literal notranslate"><span class="pre">DistilBERT</span></code> model with a <code class="docutils literal notranslate"><span class="pre">sequence</span> <span class="pre">classification</span></code> head.</p></li>
<li><p>The sequence classification head is a linear layer on top of the <code class="docutils literal notranslate"><span class="pre">pooled</span> <span class="pre">outputs</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>通过切换到不同的 model head，可以轻松地将此checkpoint重复用于其他任务。</p></li>
<li><p>对于问答任务，您将使用 <code class="docutils literal notranslate"><span class="pre">DistilBertForQuestionAnswering</span></code> 模型头(model head)。</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">question</span> <span class="pre">answering</span> <span class="pre">head</span></code> is similar to the <code class="docutils literal notranslate"><span class="pre">sequence</span> <span class="pre">classification</span> <span class="pre">head</span></code> except it is a linear layer on top of the <code class="docutils literal notranslate"><span class="pre">hidden</span> <span class="pre">states</span> <span class="pre">output</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertForQuestionAnswering</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DistilBertForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="tokenizer">
<h4>Tokenizer<a class="headerlink" href="#tokenizer" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>PreTrainedTokenizer ：分词器的 Python 实现。</p></li>
<li><p>PreTrainedTokenizerFast ：来自我们基于 Rust 的🤗 Tokenizer库的 tokenizer。</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>并非每个模型都支持快速分词器。查看此 <a class="reference external" href="https://huggingface.co/docs/transformers/main/en/index#supported-frameworks">表</a> 以检查模型是否具有快速分词器支持。</p>
</div>
<p>如果您想训练自己的分词器，则可以从词汇表文件创建一个分词器:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertTokenizer</span>
<span class="n">my_tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="p">(</span><span class="n">vocab_file</span><span class="o">=</span><span class="s2">&quot;my_vocab_file.txt&quot;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_side</span><span class="o">=</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>创建具有预训练模型词汇表的分词器:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistilBertTokenizer</span>
<span class="n">slow_tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="c1"># fast_tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert/distilbert-base-uncased&quot;)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> will try to load a <code class="docutils literal notranslate"><span class="pre">fast</span> <span class="pre">tokenizer</span></code>. You can disable this behavior by setting <code class="docutils literal notranslate"><span class="pre">use_fast=False</span></code> in from_pretrained.</p>
</div>
</section>
<section id="image-processor">
<h4>Image processor<a class="headerlink" href="#image-processor" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>图像处理器处理视觉输入。它继承自ImageProcessingMixin基类。</p></li>
</ul>
</section>
<section id="backbone">
<h4>Backbone<a class="headerlink" href="#backbone" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
</ul>
<figure class="align-default" id="id23">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/bgPiyo.png" src="https://img.zhaoweiguo.com/uPic/2024/10/bgPiyo.png" />
<figcaption>
<p><span class="caption-text">Computer vision models consist of a <code class="docutils literal notranslate"><span class="pre">backbone</span></code>, <code class="docutils literal notranslate"><span class="pre">neck</span></code>, and <code class="docutils literal notranslate"><span class="pre">head</span></code>.</span><a class="headerlink" href="#id23" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>backbone</strong> 从输入图像中提取特征， <strong>neck</strong> 组合并增强提取的特征， <strong>head</strong> 用于主要任务（例如，对象检测）。</p></li>
<li><p>首先在模型配置中初始化主干，并指定是否要加载预训练的权重或加载随机初始化的权重。然后您可以将模型配置传递给模型头。</p></li>
<li><p>The backbone extracts features from an input image, the neck combines and enhances the extracted features, and the head is used for the main task (e.g., object detection).</p></li>
<li><p>Start by initializing a backbone in the model config and specify whether you want to load pretrained weights or load randomly initialized weights. Then you can pass the model config to the model head.</p></li>
</ul>
</section>
<section id="feature-extractor">
<h4>Feature extractor<a class="headerlink" href="#feature-extractor" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>继承自FeatureExtractionMixin基类，也可以继承SequenceFeatureExtractor类来处理音频输入。</p></li>
</ul>
</section>
<section id="processor">
<h4>Processor<a class="headerlink" href="#processor" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>todo</p></li>
<li><p>对于支持多模式任务的模型，🤗 Transformers 提供了一个处理器类，可以方便地将特征提取器和分词器等处理类包装到单个对象中。</p></li>
</ul>
</section>
<section id="building-custom-models">
<h4>Building custom models<a class="headerlink" href="#building-custom-models" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>讲了如何自己写一个自定义模型</p></li>
<li><p>讲了AutoXXX如何实现加载模型的</p></li>
</ul>
</section>
</section>
<section id="chat-templates">
<h3>Chat Templates<a class="headerlink" href="#chat-templates" title="此标题的永久链接">¶</a></h3>
<section id="introduce">
<h4>Introduce<a class="headerlink" href="#introduce" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>An increasingly common use case for LLMs is chat.</p></li>
<li><p>在聊天上下文中，该模型不是继续单个文本字符串（如标准语言模型的情况），而是继续由一条或多条消息组成的对话，每条消息都包含一个角色，例如“user”或“assists”，以及消息文本。</p></li>
<li><p>与标记化(tokenization)非常相似，不同的模型期望聊天的输入格式截然不同。这就是我们添加聊天模板作为一项功能的原因。</p></li>
<li><p>聊天模板是标记器(tokenizer)的一部分。它们指定如何将表示为消息列表的对话转换为模型期望格式的单个可标记字符串。</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mistral-7B-Instruct-v0.1&quot;</span><span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">},</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;I&#39;m doing great. How can I help you today?&quot;</span><span class="p">},</span>
  <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;I&#39;d like to show off how chat templating works!&quot;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># 注: 标记 [INST] 和 [/INST] 来指示用户消息的开始和结束</span>
<span class="c1"># 其他模型可能使用别的标记来指示</span>
</pre></div>
</div>
</section>
<section id="how-do-i-use-chat-templates">
<h4>How do I use chat templates<a class="headerlink" href="#how-do-i-use-chat-templates" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;HuggingFaceH4/zephyr-7b-beta&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>  <span class="c1"># You may want to use bfloat16 and/or move to GPU here</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a friendly chatbot who always responds in the style of a pirate&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;How many helicopters can a human eat in one sitting?&quot;</span><span class="p">},</span>
 <span class="p">]</span>
<span class="n">tokenized_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenized_chat</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># 输出</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&lt;|system|&gt;</span>
<span class="sd">You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;</span>
<span class="sd">&lt;|user|&gt;</span>
<span class="sd">How many helicopters can a human eat in one sitting?&lt;/s&gt;</span>
<span class="sd">&lt;|assistant|&gt;</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>模型输出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>outputs = model.generate(tokenized_chat, max_new_tokens=128)
print(tokenizer.decode(outputs[0]))
# 输出
&lt;|system|&gt;
You are a friendly chatbot who always responds in the style of a pirate&lt;/s&gt;
&lt;|user|&gt;
How many helicopters can a human eat in one sitting?&lt;/s&gt;
&lt;|assistant|&gt;
Matey, I&#39;m afraid I must .......
</pre></div>
</div>
</section>
<section id="id4">
<h4>核心参数<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h4>
<section id="add-generation-prompt">
<h5>add_generation_prompt 参数<a class="headerlink" href="#add-generation-prompt" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>类型：bool</p></li>
<li><p>功能：是否在聊天模板的末尾添加一个提示，用于指示模型生成下一条消息。这对于一些聊天模型至关重要，因为它们需要一个特定的触发标记来开始生成。</p></li>
</ul>
<p>实例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Hi there!&quot;</span><span class="p">},</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Nice to meet you!&quot;</span><span class="p">},</span>
       <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can I ask a question?&quot;</span><span class="p">}</span>
   <span class="p">]</span>

<span class="n">without</span> <span class="n">a</span> <span class="n">generation</span> <span class="n">prompt</span><span class="p">::</span>

   <span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;&lt;|im_start|&gt;user</span>
<span class="sd">   Hi there!&lt;|im_end|&gt;</span>
<span class="sd">   &lt;|im_start|&gt;assistant</span>
<span class="sd">   Nice to meet you!&lt;|im_end|&gt;</span>
<span class="sd">   &lt;|im_start|&gt;user</span>
<span class="sd">   Can I ask a question?&lt;|im_end|&gt;</span>
<span class="sd">   &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>with a generation prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">&quot;&quot;&quot;&lt;|im_start|&gt;user</span>
<span class="go">Hi there!&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;assistant</span>
<span class="go">Nice to meet you!&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;user</span>
<span class="go">Can I ask a question?&lt;|im_end|&gt;</span>
<span class="go">&lt;|im_start|&gt;assistant               # 添加生成提示</span>
<span class="go">&quot;&quot;&quot;</span>
</pre></div>
</div>
</section>
<section id="continue-final-message">
<h5>continue_final_message 参数<a class="headerlink" href="#continue-final-message" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>类型：bool</p></li>
<li><p>功能：是否在最后一条消息的 content 末尾继续生成。这在需要模型接着未完成的句子生成时很有用。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">chat</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Can you format the answer in JSON?&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;assistant&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s1">&#39;{&quot;name&quot;: &quot;&#39;</span><span class="p">},</span>
<span class="p">]</span>

<span class="n">formatted_chat</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">chat</span><span class="p">,</span> <span class="n">tokenize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">continue_final_message</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">formatted_chat</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>add_generation_prompt添加开始新消息的标记，而continue_final_message从最终消息中删除任何消息结束标记，因此将它们一起使用没有意义。</p>
</div>
</section>
<section id="tokenize">
<h5>tokenize 参数<a class="headerlink" href="#tokenize" title="此标题的永久链接">¶</a></h5>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>默认情况下，某些标记生成器会将特殊标记（如&lt;bos&gt;和&lt;eos&gt;添加到它们标记的文本中。聊天模板应该已经包含它们需要的所有特殊标记，因此额外的特殊标记通常会不正确或重复，这会损害模型性能。</p>
</div>
<ul class="simple">
<li><p>如果您使用以下格式设置文本格式 apply_chat_template(tokenize=False) ，当您稍后标记该文本时，您应该设置参数add_special_tokens=False 。如果你使用 apply_chat_template(tokenize=True) ，你不需要担心这个！</p></li>
</ul>
</section>
</section>
<section id="advanced-how-do-chat-templates-work">
<h4>Advanced: How do chat templates work?<a class="headerlink" href="#advanced-how-do-chat-templates-work" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>模型的聊天模板存储在tokenizer.chat_template属性中</p></li>
</ul>
<p>示例(Jinja 模板):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="o">|&gt;</span>\<span class="n">n</span><span class="s1">&#39; }}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">eos_token</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
<span class="p">{</span><span class="o">%-</span> <span class="k">if</span> <span class="n">add_generation_prompt</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|assistant|&gt;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
<p>示例2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;user&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="n">bos_token</span> <span class="o">+</span> <span class="s1">&#39;[INST] &#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; [/INST]&#39;</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">elif</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;system&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;&lt;SYS&gt;&gt;</span><span class="se">\\</span><span class="s1">n&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">n&lt;&lt;/SYS&gt;&gt;</span><span class="se">\\</span><span class="s1">n</span><span class="se">\\</span><span class="s1">n&#39;</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="k">elif</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;assistant&#39;</span> <span class="o">%</span><span class="p">}</span>
        <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39; &#39;</span>  <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">eos_token</span> <span class="p">}}</span>
    <span class="p">{</span><span class="o">%-</span> <span class="n">endif</span> <span class="o">%</span><span class="p">}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="advanced-adding-and-editing-chat-templates">
<h4>Advanced: Adding and editing chat templates<a class="headerlink" href="#advanced-adding-and-editing-chat-templates" title="此标题的永久链接">¶</a></h4>
<section id="how-do-i-create-a-chat-template">
<h5>How do I create a chat template?<a class="headerlink" href="#how-do-i-create-a-chat-template" title="此标题的永久链接">¶</a></h5>
<p>基于别的token进行修改:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">template</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">chat_template</span>
<span class="n">template</span> <span class="o">=</span> <span class="n">template</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;SYS&quot;</span><span class="p">,</span> <span class="s2">&quot;SYSTEM&quot;</span><span class="p">)</span>  <span class="c1"># Change the system token</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">chat_template</span> <span class="o">=</span> <span class="n">template</span>  <span class="c1"># Set the new template</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;model_name&quot;</span><span class="p">)</span>  <span class="c1"># Upload your new template to the Hub!</span>
</pre></div>
</div>
<p>一种流行的选择是ChatML格式:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="s1">&#39;&lt;|im_start|&gt;&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="s1">&#39;&lt;|im_end|&gt;&#39;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
</section>
<section id="advanced-template-writing-tips">
<h4>Advanced: Template writing tips<a class="headerlink" href="#advanced-template-writing-tips" title="此标题的永久链接">¶</a></h4>
<section id="trimming-whitespace">
<h5>Trimming whitespace<a class="headerlink" href="#trimming-whitespace" title="此标题的永久链接">¶</a></h5>
<p>强烈建议使用格式:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%-</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span><span class="o">-</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%-</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
<p>不要使用格式:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="o">%</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">messages</span> <span class="o">%</span><span class="p">}</span>
    <span class="p">{{</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="p">}}</span>
<span class="p">{</span><span class="o">%</span> <span class="n">endfor</span> <span class="o">%</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="callable-functions">
<h5>Callable functions<a class="headerlink" href="#callable-functions" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">raise_exception</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
<span class="mf">2.</span> <span class="n">strftime_now</span><span class="p">(</span><span class="n">format_str</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="compatibility-with-non-python-jinja">
<h5>Compatibility with non-Python Jinja<a class="headerlink" href="#compatibility-with-non-python-jinja" title="此标题的永久链接">¶</a></h5>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>非 Python 实现在部署环境中尤其常见，其中 JS 和 Rust 非常流行。</p>
</div>
<ul>
<li><p>1.Replace Python methods with Jinja filters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">string</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>      <span class="o">=&gt;</span> <span class="n">string</span><span class="o">|</span><span class="n">lower</span>
<span class="nb">dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>        <span class="o">=&gt;</span> <span class="nb">dict</span><span class="o">|</span><span class="n">items</span>
<span class="n">string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>      <span class="o">=&gt;</span> <span class="n">string</span><span class="o">|</span><span class="n">trim</span>
</pre></div>
</div>
</li>
<li><p>2.Replace True, False and None, which are Python-specific, with true, false and none.</p></li>
<li><p>3.添加tojson过滤器, 避免直接渲染字典或列表可能会在其他实现中给出不同的结果</p></li>
<li><p>Jinja内置Filter: <a class="reference external" href="https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters">https://jinja.palletsprojects.com/en/3.1.x/templates/#builtin-filters</a></p></li>
</ul>
</section>
</section>
</section>
<section id="trainer">
<h3>Trainer<a class="headerlink" href="#trainer" title="此标题的永久链接">¶</a></h3>
<p>Basic usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. perform a training step to calculate the loss(执行训练步骤来计算损失)
2. calculate the gradients with the backward method(使用后向方法计算梯度)
3. update the weights based on the gradients(根据梯度更新权重)
4. repeat this process until you’ve reached a predetermined number of epochs(重复此过程，直到达到预定epochs)
</pre></div>
</div>
<p>class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Trainer</span>
<span class="n">Seq2SeqTrainer</span>
<span class="n">trl</span><span class="o">.</span><span class="n">SFTTrainer</span>
</pre></div>
</div>
<p>TrainingArguments class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;your-model&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">save_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="export-to-onnx">
<h3>Export to ONNX<a class="headerlink" href="#export-to-onnx" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>🤗 Optimum 是 Transformers 的扩展，可以通过其exporters模块将模型从 PyTorch 导出为序列化格式(serialized format)，例如 ONNX 和 TFLite。</p></li>
<li><p>ONNX（Open Neural Network eXchange）是一种开放标准，定义了一组通用运算符和通用文件格式，以表示各种框架（包括 PyTorch 和 TensorFlow）中的深度学习模型。当模型导出为 ONNX 格式时，这些运算符用于构建计算图（通常称为中间表示），该计算图表示通过神经网络的数据流。</p></li>
<li><p>通过使用标准化运算符和数据类型公开图表，ONNX 可以轻松地在框架之间切换。例如，在 PyTorch 中训练的模型可以导出为 ONNX 格式，然后导入到 TensorFlow 中（反之亦然）。</p></li>
</ul>
<section id="exporting-a-transformers-model-to-onnx-with-cli">
<h4>Exporting a 🤗 Transformers model to ONNX with CLI<a class="headerlink" href="#exporting-a-transformers-model-to-onnx-with-cli" title="此标题的永久链接">¶</a></h4>
<p>要将 🤗 Transformers 模型导出到 ONNX，请首先安装额外的依赖项:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ pip install optimum[exporters]
</pre></div>
</div>
<p>示例-导出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 从 🤗 Hub 导出模型的检查点
$ optimum-cli export onnx --model distilbert/distilbert distilbert/

# 导出本地模型
$ optimum-cli export onnx --model local_path --task question-answering distilbert/
</pre></div>
</div>
<p>使用ONNX Runtime加载并运行模型:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForQuestionAnswering</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ORTModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert&quot;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;What am I using?&quot;</span><span class="p">,</span> <span class="s2">&quot;Using DistilBERT with ONNX Runtime!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime">
<h4>Exporting a 🤗 Transformers model to ONNX with optimum.onnxruntime<a class="headerlink" href="#exporting-a-transformers-model-to-onnx-with-optimum-onnxruntime" title="此标题的永久链接">¶</a></h4>
<p>示例-导出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForSequenceClassification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;distilbert_base_uncased_squad&quot;</span>
<span class="n">save_directory</span> <span class="o">=</span> <span class="s2">&quot;onnx/&quot;</span>

<span class="c1"># Load a model from transformers and export it to ONNX</span>
<span class="n">ort_model</span> <span class="o">=</span> <span class="n">ORTModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">,</span> <span class="n">export</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_checkpoint</span><span class="p">)</span>

<span class="c1"># Save the onnx model and tokenizer</span>
<span class="n">ort_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">save_directory</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exporting-a-model-with-transformers-onnx">
<h4>Exporting a model with transformers.onnx<a class="headerlink" href="#exporting-a-model-with-transformers-onnx" title="此标题的永久链接">¶</a></h4>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>transformers.onnx不再维护，请使用上面2节的 🤗 Optimum 导出模型。此部分将在未来版本中删除。</p>
</div>
<p>示例-导出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span><span class="p">[</span><span class="n">onnx</span><span class="p">]</span>

<span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">transformers</span><span class="o">.</span><span class="n">onnx</span> <span class="o">--</span><span class="n">model</span><span class="o">=</span><span class="n">distilbert</span><span class="o">/</span><span class="n">distilbert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="n">onnx</span><span class="o">/</span>
</pre></div>
</div>
<p>示例-运行:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">InferenceSession</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert/distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="s2">&quot;onnx/model.onnx&quot;</span><span class="p">)</span>
<span class="c1"># ONNX Runtime expects NumPy arrays as input</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Using DistilBERT with ONNX Runtime!&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;np&quot;</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;last_hidden_state&quot;</span><span class="p">],</span> <span class="n">input_feed</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>FP16 stands for mixed-precision meaning that computations within the model are done using a mixture of 16-bit and 32-bit floating-point operations</p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half">https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half</a></p></li>
</ul>
</section>
</section>
<section id="interoperability-with-gguf-files">
<h3>Interoperability with GGUF files<a class="headerlink" href="#interoperability-with-gguf-files" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>The GGUF file format is used to store models for inference with <a class="reference external" href="https://github.com/ggerganov/ggml">GGML</a> and other libraries that depend on it（如: llama.cpp or whisper.cpp）</p></li>
<li><p>它是Hugging Face Hub 支持的一种文件格式，具有允许快速检查文件中的张量(tensors)和元数据(metadata)的功能。</p></li>
<li><p>这种文件格式被设计为“单文件格式(single-file-format)”，其中单个文件通常包含配置属性(configuration attributes)、分词器词汇(tokenizer vocabulary)和其他属性，以及要在模型中加载的所有张量。</p></li>
</ul>
<p>Supported quantization types:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F32</span>
<span class="n">F16</span>
<span class="n">BF16</span>
<span class="n">Q4_0</span>
<span class="n">Q4_1</span>
<span class="n">Q5_0</span>
<span class="n">Q5_1</span>
<span class="n">Q8_0</span>
<span class="n">Q2_K</span>
<span class="n">Q3_K</span>
<span class="n">Q4_K</span>
<span class="n">Q5_K</span>
<span class="n">Q6_K</span>
<span class="n">IQ1_S</span>
<span class="n">IQ1_M</span>
<span class="n">IQ2_XXS</span>
<span class="n">IQ2_XS</span>
<span class="n">IQ2_S</span>
<span class="n">IQ3_XXS</span>
<span class="n">IQ3_S</span>
<span class="n">IQ4_XS</span>
<span class="n">IQ4_NL</span>
</pre></div>
</div>
<p>Supported model architectures:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LLaMa</span>
<span class="n">Mistral</span>
<span class="n">Qwen2</span>
<span class="n">Qwen2Moe</span>
<span class="n">Phi3</span>
</pre></div>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># 加载 GGUF 文件格式
from transformers import AutoTokenizer, AutoModelForCausalLM
model_id = &quot;TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF&quot;
filename = &quot;tinyllama-1.1b-chat-v1.0.Q6_K.gguf&quot;
tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)
model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)


# 保存模型并将其导出回gguf
tokenizer.save_pretrained(&#39;directory&#39;)
model.save_pretrained(&#39;directory&#39;)
!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}
</pre></div>
</div>
</section>
</section>
<section id="quantization-methods">
<h2>Quantization Methods<a class="headerlink" href="#quantization-methods" title="此标题的永久链接">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>本节简单整理，需要时细看</p>
</div>
<section id="quantization">
<h3>Quantization<a class="headerlink" href="#quantization" title="此标题的永久链接">¶</a></h3>
<p>Quantization method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">bitsandbytes</span>
<span class="n">GPTQ</span>
<span class="n">AWQ</span>
<span class="n">AQLM</span>
<span class="n">Quanto</span>
<span class="n">EETQ</span>
<span class="n">HQQ</span>
<span class="n">FBGEMM_FP8</span>
<span class="n">Optimum</span>
<span class="n">TorchAO</span>
<span class="n">compressed</span><span class="o">-</span><span class="n">tensors</span>
<span class="n">Contribute</span> <span class="n">new</span> <span class="n">quantization</span> <span class="n">method</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/10/7nWOfI.png" src="https://img.zhaoweiguo.com/uPic/2024/10/7nWOfI.png" />
</figure>
</section>
<section id="bitsandbytes">
<h3>bitsandbytes<a class="headerlink" href="#bitsandbytes" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/TimDettmers/bitsandbytes">https://github.com/TimDettmers/bitsandbytes</a></p></li>
<li><p>bitsandbytes is the easiest option for quantizing a model to 8 and 4-bit.</p></li>
<li><p>定义：异常值(Outliers)是指在数据集中明显偏离其他数据点的数值。它们与数据集的平均趋势或范围相比，表现得非常异常，可能由于测量错误、极端情况或数据分布中的稀有事件引起。</p></li>
<li><p>定义：非异常值(Non-Outliers)是指在数据集中符合总体趋势、范围或分布的数值。它们不会明显偏离数据的主流特征，通常位于数据的平均值附近。</p></li>
<li><p>在机器学习中的表现：在神经网络中，某些权重或激活值可能非常大或非常小（相对于其他值），这些值会被称为异常值(Outliers)。如果直接使用低精度（如8-bit）的量化，异常值可能导致较大的精度损失。</p></li>
<li><p>处理方式：在8-bit量化过程中，异常值往往不会直接量化为8位整数，因为这样会导致精度损失。通常，这些异常值会保留在更高精度的格式（如FP16）中单独处理。</p></li>
<li><p>【量化过程中的作用】在量化神经网络时，outliers 和 non-outliers 被分开处理。非异常值适合直接用8-bit表示，能极大地减少计算和存储的资源需求。而异常值因为可能导致精度损失，通常用更高精度的FP16表示。随后，将这两部分（FP16的异常值和INT8的非异常值）相乘、加总，以保持计算结果的精确性。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>通过这种方法，既能利用低精度量化的优势（减少模型大小和加速推理），又能在处理异常值时保持一定的精度。</p>
</div>
<ul class="simple">
<li><p>8 位量化将 fp16 中的异常值(outliers)与 int8 中的非异常值(non-outliers)相乘，将非异常值转换回 fp16，然后将它们相加以返回 fp16 中的权重。这减少了异常值对模型性能的影响。</p></li>
<li><p>8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16.</p></li>
<li><p>4 位量化可以进一步压缩模型，通常与QLoRA一起使用来微调量化的LLMs 。</p></li>
</ul>
<p>8bit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bigscience/bloom-1b7&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span>
<span class="p">)</span>
</pre></div>
</div>
<p>4bit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model_4bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bigscience/bloom-1b7&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="gptq">
<h3>GPTQ<a class="headerlink" href="#gptq" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/PanQiWei/AutoGPTQ">https://github.com/PanQiWei/AutoGPTQ</a></p></li>
<li><p>AutoGPTQ库实现了 GPTQ 算法，这是一种训练后量化技术，其中权重矩阵的每一行都被独立量化，以找到最小化误差的权重版本。</p></li>
<li><p>这些权重被量化为 int4，但在推理过程中会即时恢复为 fp16。</p></li>
<li><p>These weights are quantized to int4, but they’re restored to fp16 on the fly during inference.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;facebook/opt-125m&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span><span class="o">=</span><span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
<span class="c1"># 设置device_map=&quot;auto&quot;可自动将模型卸载到 CPU，以帮助将模型放入内存中，并允许模型模块在 CPU 和 GPU 之间移动以进行量化。</span>
</pre></div>
</div>
<section id="exllama">
<h4>ExLlama<a class="headerlink" href="#exllama" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/turboderp/exllama">https://github.com/turboderp/exllama</a></p></li>
<li><p>ExLlama是Llama模型的 Python/C++/CUDA 实现，旨在使用 4 位 GPTQ 权重进行更快的推理</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GPTQConfig</span>

<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">exllama_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
<p>仅当整个模型位于 GPU 上时才支持 ExLlama 内核。如果您使用 AutoGPTQ（版本 &gt; 0.4.2）在 CPU 上进行推理，则需要禁用 ExLlama 内核:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">GPTQConfig</span>
<span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">use_exllama</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="awq">
<h3>AWQ<a class="headerlink" href="#awq" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Activation-aware Weight Quantization(AWQ): <a class="reference external" href="https://hf.co/papers/2306.00978">https://hf.co/papers/2306.00978</a></p></li>
<li><p>不会量化模型中的所有权重，而是保留对LLM性能很重要的一小部分权重。这显着减少了量化损失，以便您可以以 4 位精度运行模型，而不会出现任何性能下降。</p></li>
<li><p>通过对模型的权重进行加权平均处理，能够更精确地捕捉权重分布的特点。AWQ在保留模型性能的同时，能够显著减少推理时的内存使用和计算复杂度。一种改进的量化方法，它针对神经网络的权重分布特点，通过加权平均的方式量化参数，从而更好地保留了模型的精度。在推理时，AWQ 可以使用低精度的权重表示，减少存储和计算的成本，同时保持模型性能的稳定。与传统的量化技术（如直接的逐层或逐通道量化）相比，AWQ 对权重分布的处理更加精细，因此在同等量化精度下能够获得更好的推理结果。(🈳from LLM)</p></li>
<li><p>有几个用于使用 AWQ 算法量化模型的库，例如</p></li>
<li><p>llm-awq: <a class="reference external" href="https://github.com/mit-han-lab/llm-awq">https://github.com/mit-han-lab/llm-awq</a></p></li>
<li><p>autoawq: <a class="reference external" href="https://github.com/casper-hansen/AutoAWQ">https://github.com/casper-hansen/AutoAWQ</a>&gt;</p></li>
<li><p>optimization-intel:</p></li>
</ul>
<p>Fused modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AwqConfig</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/Mistral-7B-OpenOrca-AWQ&quot;</span>

<span class="n">quantization_config</span> <span class="o">=</span> <span class="n">AwqConfig</span><span class="p">(</span>
    <span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">fuse_max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">do_fuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="aqlm">
<h3>AQLM<a class="headerlink" href="#aqlm" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Additive Quantization of Language Models (AQLM): 一种大型语言模型压缩方法。它将多个权重一起量化并利用它们之间的相互依赖性。 AQLM 将 8-16 个权重组表示为多个矢量代码的总和。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&quot;</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quanto">
<h3>Quanto<a class="headerlink" href="#quanto" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/huggingface/quanto">https://github.com/huggingface/quanto</a></p></li>
<li><p>Quanto库是一个多功能的 pytorch 量化工具包。使用的量化方法是线性量化</p></li>
</ul>
</section>
<section id="eetq">
<h3>EETQ<a class="headerlink" href="#eetq" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/NetEase-FuXi/EETQ">https://github.com/NetEase-FuXi/EETQ</a></p></li>
<li><p>EETQ库支持 NVIDIA GPUS 的 int8 每通道仅权重量化。</p></li>
<li><p>高性能GEMM和GEMV内核来自FasterTransformer和TensorRT- LLM 。</p></li>
<li><p>它不需要校准数据集，也不需要预先量化您的模型。此外，由于每通道量化，精度下降可以忽略不计。</p></li>
</ul>
</section>
<section id="hqq">
<h3>HQQ<a class="headerlink" href="#hqq" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mobiusml/hqq/">https://github.com/mobiusml/hqq/</a></p></li>
<li><p>Half-Quadratic Quantization (HQQ): 通过快速鲁棒优化(fast robust optimization)实现动态量化(on-the-fly quantization)。</p></li>
<li><p>它不需要校准数据，可用于量化任何模型。</p></li>
</ul>
</section>
<section id="fbgemm-fp8">
<h3>FBGEMM FP8<a class="headerlink" href="#fbgemm-fp8" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/FBGEMM">https://github.com/pytorch/FBGEMM</a></p></li>
<li><p>the weights will be quantized in 8bit (FP8) per channel</p></li>
<li><p>the activation will be quantized in 8bit (FP8) per token</p></li>
</ul>
</section>
<section id="optimum">
<h3>Optimum<a class="headerlink" href="#optimum" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/optimum/index">https://huggingface.co/docs/optimum/index</a></p></li>
<li><p>Optimum库支持 Intel、Furiosa、ONNX Runtime、GPTQ 和较低级别 PyTorch 量化函数的量化。</p></li>
<li><p>如果您使用特定的优化硬件（例如 Intel CPU、Furiosa NPU 或 ONNX Runtime 等模型加速器），请考虑使用 Optimum 进行量化。</p></li>
</ul>
</section>
<section id="torchao">
<h3>TorchAO<a class="headerlink" href="#torchao" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/pytorch/ao">https://github.com/pytorch/ao</a></p></li>
<li><p>TorchAO是 PyTorch 的架构优化库，它提供了用于推理和训练的高性能数据类型、优化技术和内核，具有与torch.compile 、 FSDP 等原生 PyTorch 功能的可组合性。</p></li>
</ul>
</section>
<section id="compressed-tensors">
<h3>Compressed Tensors<a class="headerlink" href="#compressed-tensors" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/neuralmagic/compressed-tensors">https://github.com/neuralmagic/compressed-tensors</a></p></li>
<li><p>提供了一种通用且有效的方法来存储和管理压缩模型检查点。</p></li>
<li><p>该库支持各种量化和稀疏方案，使其成为处理不同模型优化的统一格式，例如 GPTQ、AWQ、SmoothQuant、INT8、FP8、SparseGPT 等。</p></li>
</ul>
</section>
</section>
<section id="performance-and-scalability">
<h2>Performance and scalability<a class="headerlink" href="#performance-and-scalability" title="此标题的永久链接">¶</a></h2>
<section id="llm-inference-optimization">
<h3>LLM inference optimization<a class="headerlink" href="#llm-inference-optimization" title="此标题的永久链接">¶</a></h3>
<section id="static-kv-cache-and-torch-compile">
<h4>Static kv-cache and torch.compile<a class="headerlink" href="#static-kv-cache-and-torch-compile" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>使用 kv-cache 来存储过去的键和值，而不是每次都重新计算它们。</p></li>
<li><p>然而，由于 kv-cache 在每个生成步骤都是动态且变化的，因此它会阻止您利用 torch.compile ，这是一个功能强大的优化工具，可将 PyTorch 代码融合到快速且优化的内核中。</p></li>
<li><p>kv-cache 的完整指南: 参见上面的 <code class="docutils literal notranslate"><span class="pre">Best</span> <span class="pre">Practices</span> <span class="pre">for</span> <span class="pre">Generation</span> <span class="pre">with</span> <span class="pre">Cache</span></code></p></li>
<li><p>static kv-cache 通过将 kv-cache 大小预先分配为最大值来解决此问题，这允许您将其与torch.compile结合使用，最高可提高 4 倍的速度。目前，只有Llama和其他一些模型支持 static kv-cache 和torch.compile 。</p></li>
</ul>
<p>静态 kv 缓存的使用分为三种类型，具体取决于任务的复杂性:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Basic</span> <span class="n">usage</span><span class="p">:</span> <span class="n">simply</span> <span class="nb">set</span> <span class="n">a</span> <span class="n">flag</span> <span class="ow">in</span> <span class="n">generation_config</span> <span class="p">(</span><span class="n">recommended</span><span class="p">);</span>
<span class="mf">2.</span> <span class="n">Advanced</span> <span class="n">usage</span><span class="p">:</span> <span class="n">handle</span> <span class="n">a</span> <span class="n">cache</span> <span class="nb">object</span> <span class="k">for</span> <span class="n">multi</span><span class="o">-</span><span class="n">turn</span> <span class="n">generation</span> <span class="ow">or</span> <span class="n">a</span> <span class="n">custom</span> <span class="n">generation</span> <span class="n">loop</span><span class="p">;</span>
<span class="mf">3.</span> <span class="n">Advanced</span> <span class="n">usage</span><span class="p">:</span> <span class="nb">compile</span> <span class="n">the</span> <span class="n">entire</span> <span class="n">generate</span> <span class="n">function</span> <span class="n">into</span> <span class="n">a</span> <span class="n">single</span> <span class="n">graph</span><span class="p">,</span> <span class="k">if</span> <span class="n">having</span> <span class="n">a</span> <span class="n">single</span> <span class="n">graph</span> <span class="ow">is</span> <span class="n">relevant</span> <span class="k">for</span> <span class="n">you</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Speculative decoding<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>推测解码</p></li>
<li><p>深入参见博客文章: Assisted Generation: a new direction toward low-latency text generation: <a class="reference external" href="https://hf.co/blog/assisted-generation">https://hf.co/blog/assisted-generation</a></p></li>
<li><p>自回归的另一个问题是，对于每个输入标记，您需要在前向传递过程中每次加载模型权重。</p></li>
<li><p>对于拥有数十亿参数的LLMs来说，这既缓慢又麻烦。推测性解码通过使用第二个更小、更快的辅助模型来生成候选标记，并在单次前向传递中由更大的LLM进行验证，从而缓解了这种速度下降的情况。</p></li>
<li><p>如果验证的令牌是正确的， LLM基本上可以“免费”获得它们，而不必自己生成它们。</p></li>
</ul>
<section id="prompt-lookup-decoding">
<h5>Prompt lookup decoding<a class="headerlink" href="#prompt-lookup-decoding" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Prompt lookup decoding 是推测解码的一种变体</p></li>
<li><p>提示查找对于基于输入的任务（例如摘要）特别有效，其中提示和输出之间经常存在重叠的单词。这些重叠的 n 元语法被用作LLM候选token。</p></li>
</ul>
</section>
</section>
<section id="attention-optimizations">
<h4>Attention optimizations<a class="headerlink" href="#attention-optimizations" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Transformer 模型的一个已知问题是，自注意力机制在计算和内存中随着输入标记的数量呈二次方增长。</p></li>
<li><p>这种限制会在处理更长序列的LLMs中被放大。</p></li>
<li><p>为了解决这个问题，请尝试 FlashAttention2 或 PyTorch 的缩放点积注意力 (scaled dot product attention, SDPA)，它们是内存效率更高的注意力实现，可以加速推理。</p></li>
</ul>
<section id="flashattention-2">
<h5>FlashAttention-2<a class="headerlink" href="#flashattention-2" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>FlashAttention 和FlashAttention-2将注意力计算分解为更小的块，并减少对 GPU 内存的中间读/写操作的数量，以加快推理速度。</p></li>
<li><p>FlashAttention-2 通过在序列长度维度上并行化以及更好的硬件分区工作来改进原始 FlashAttention 算法，以减少同步和通信开销。</p></li>
</ul>
</section>
<section id="pytorch-scaled-dot-product-attention">
<h5>PyTorch scaled dot product attention<a class="headerlink" href="#pytorch-scaled-dot-product-attention" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>PyTorch 2.0 中自动启用了缩放点积注意力 (SDPA)，它支持 FlashAttention、xFormers 和 PyTorch 的 C++ 实现。</p></li>
<li><p>如果您使用 CUDA 后端，SDPA 会选择性能最佳的注意力算法。对于其他后端，SDPA 默认使用 PyTorch C++ 实现。</p></li>
<li><p>只要您安装了最新的 PyTorch 版本，SDPA 就支持 FlashAttention-2。</p></li>
</ul>
</section>
</section>
<section id="id6">
<h4>Quantization<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>如果您不受 GPU 的限制，则不一定需要量化模型，因为量化和反量化权重所需的额外步骤可能会产生较小的延迟成本（AWQ 和融合 AWQ 模块除外）。</p></li>
</ul>
</section>
</section>
<section id="efficient-training-techniques">
<h3>Efficient training techniques<a class="headerlink" href="#efficient-training-techniques" title="此标题的永久链接">¶</a></h3>
<section id="methods-and-tools-for-efficient-training-on-a-single-gpu">
<h4>Methods and tools for efficient training on a single GPU<a class="headerlink" href="#methods-and-tools-for-efficient-training-on-a-single-gpu" title="此标题的永久链接">¶</a></h4>
<p>在训练大型模型时，需要同时考虑两个方面:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Data</span> <span class="n">throughput</span><span class="o">/</span><span class="n">training</span> <span class="n">time</span> <span class="n">数据吞吐量</span><span class="o">/</span><span class="n">训练时间</span>
<span class="mf">2.</span> <span class="n">Model</span> <span class="n">performance</span> <span class="n">模型性能</span>
</pre></div>
</div>
<p>Method/tool与对应的效果:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Method</span><span class="o">/</span><span class="n">tool</span>                            <span class="o">|</span> <span class="n">Improves</span> <span class="n">training</span> <span class="n">speed</span> <span class="o">|</span> <span class="n">Optimizes</span> <span class="n">memory</span> <span class="n">utilization</span> <span class="o">|</span>
<span class="o">+========================================+=========================+==============================+</span>
<span class="o">|</span> <span class="n">Batch</span> <span class="n">size</span> <span class="n">choice</span>                      <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Gradient</span> <span class="n">accumulation</span>                  <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Gradient</span> <span class="n">checkpointing</span>                 <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Mixed</span> <span class="n">precision</span> <span class="n">training</span>               <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Maybe</span><span class="o">*</span>                       <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">torch_empty_cache_steps</span>                <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Optimizer</span> <span class="n">choice</span>                       <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Data</span> <span class="n">preloading</span>                        <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">No</span>                           <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">DeepSpeed</span> <span class="n">Zero</span>                         <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span>                          <span class="o">|</span> <span class="n">Yes</span>                     <span class="o">|</span> <span class="n">No</span>                           <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
<span class="o">|</span> <span class="n">Parameter</span><span class="o">-</span><span class="n">Efficient</span> <span class="n">Fine</span> <span class="n">Tuning</span> <span class="p">(</span><span class="n">PEFT</span><span class="p">)</span> <span class="o">|</span> <span class="n">No</span>                      <span class="o">|</span> <span class="n">Yes</span>                          <span class="o">|</span>
<span class="o">+----------------------------------------+-------------------------+------------------------------+</span>
</pre></div>
</div>
<section id="batch-size-choice">
<h5>Batch size choice<a class="headerlink" href="#batch-size-choice" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>为了实现最佳性能，首先要确定适当的批量大小。建议使用大小为 2^N 的批量大小和输入/输出神经元计数。通常它是 8 的倍数，但也可能更高，具体取决于所使用的硬件和模型的数据类型。</p></li>
<li><p>作为参考，请查看 NVIDIA 对于全连接层（涉及 GEMM（通用矩阵乘法））的 <a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#input-features">输入/输出神经元计数</a> 和 <a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-fully-connected/index.html#batch-size">批量大小</a> 的建议。</p></li>
<li><p>For reference, check out NVIDIA’s recommendation for input/output neuron counts and batch size for fully connected layers (which are involved in GEMMs (General Matrix Multiplications)).</p></li>
<li><p>张量核心要求根据数据类型和硬件定义乘数。例如，对于 fp16 数据类型，建议使用 8 的倍数，除非是 A100 GPU，在这种情况下使用 64 的倍数。</p></li>
</ul>
</section>
<section id="gradient-accumulation">
<h5>Gradient Accumulation<a class="headerlink" href="#gradient-accumulation" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>梯度累积方法旨在以较小的增量计算梯度，而不是一次计算整个批次的梯度。</p></li>
<li><p>这种方法涉及通过向前和向后遍历模型并在此过程中累积梯度来迭代计算较小批次的梯度。一旦积累了足够数量的梯度，就会执行模型的优化步骤。</p></li>
<li><p>通过采用梯度累积，可以将有效批量大小(effective batch size)增加到超出 GPU 内存容量的限制。</p></li>
<li><p>然而，值得注意的是，梯度累积引入的额外前向和后向传递可能会减慢训练过程。</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
<span class="c1"># 说明</span>
    <span class="mf">1.</span> <span class="n">通过将gradient_accumulation_steps参数添加到TrainingArguments来启用梯度累积</span>
    <span class="mf">2.</span> <span class="n">有效批量大小变为</span> <span class="mi">4</span>
</pre></div>
</div>
</section>
<section id="gradient-checkpointing">
<h5>Gradient Checkpointing<a class="headerlink" href="#gradient-checkpointing" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>即使批量大小设置为 1 并使用梯度累积，一些大型模型仍然可能面临内存问题。这是因为还有其他组件也需要内存存储。</p></li>
<li><p>保存前向传递中的所有激活以便在后向传递期间计算梯度可能会导致显着的内存开销。另一种方法是在向后传递过程中丢弃激活并在需要时重新计算它们，这会带来相当大的计算开销并减慢训练过程。</p></li>
<li><p><strong>梯度检查点</strong> 提供了这两种方法之间的折衷方案，并在整个计算图中保存了战略选择的激活，因此只需为梯度重新计算一小部分激活。有关梯度检查点的深入解释，请参阅这篇 <a class="reference external" href="https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9">精彩文章</a></p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="mixed-precision-training">
<h5>Mixed precision training<a class="headerlink" href="#mixed-precision-training" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p><strong>混合精度训练</strong> 是一种旨在通过对某些变量使用较低精度的数值格式来优化训练模型的计算效率的技术。传统上，大多数模型使用 32 位浮点精度（fp32 或 float32）来表示和处理变量。然而，并非所有变量都需要如此高精度才能获得准确的结果。通过将某些变量的精度降低为较低的数值格式，例如 16 位浮点（fp16 或 float16），我们可以加快计算速度。由于在这种方法中，有些计算是以半精度执行的，而有些计算仍然是全精度的，因此该方法称为混合精度训练。</p></li>
<li><p>最常见的混合精度训练是通过使用 fp16 (float16) 数据类型来实现的，但是，一些 GPU 架构（例如 Ampere 架构）提供 bf16 和 tf32（CUDA 内部数据类型）数据类型。查看 <a class="reference external" href="https://developer.nvidia.com/blog/accelerating-ai-training-with-tf32-tensor-cores/">NVIDIA博客</a></p></li>
</ul>
<section id="fp16">
<h6>fp16<a class="headerlink" href="#fp16" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>混合精度训练的主要优点来自于以半精度（fp16）保存激活。尽管梯度也是以半精度计算的，但它们在优化步骤中会转换回全精度，因此此处不会节省内存。</p></li>
<li><p>虽然混合精度训练可以加快计算速度，但它也会导致使用更多 GPU 内存，特别是对于小批量大小。这是因为该模型现在以 16 位和 32 位精度（GPU 上原始模型的 1.5 倍）呈现在 GPU 上。</p></li>
</ul>
<p>要启用混合精度训练，请将fp16标志设置为True:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bf16">
<h6>BF16<a class="headerlink" href="#bf16" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>如果您可以使用 Ampere 或更新的硬件，则可以使用 bf16 进行混合精度训练和评估。</p></li>
<li><p>虽然 bf16 的精度比 fp16 差，但它的动态范围要大得多。在 fp16 中，您可以拥有的最大数字是65504 ，任何高于该数字的数字都会导致溢出。 bf16 数字可以大到3.39e+38 (!)，这与 fp32 大致相同 - 因为两者都有 8 位用于数字范围。</p></li>
</ul>
<p>在 🤗 Trainer 中启用 BF16:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tf32">
<h6>TF32<a class="headerlink" href="#tf32" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>Ampere 硬件使用一种名为 tf32 的神奇数据类型。</p></li>
<li><p>它具有与 fp32（8 位）相同的数值范围，但不是 23 位精度，而是只有 10 位（与 fp16 相同），并且总共只使用 19 位。</p></li>
<li><p>它的“神奇”之处在于，您可以使用普通的 fp32 训练和/或推理代码，并且通过启用 tf32 支持，您可以获得高达 3 倍的吞吐量提升。</p></li>
</ul>
<p>您需要做的就是将以下内容添加到您的代码中:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">matmul</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">allow_tf32</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TrainingArguments</span><span class="p">(</span><span class="n">tf32</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>tf32 无法直接通过tensor.to(dtype=torch.tf32)访问，因为它是内部 CUDA 数据类型。您需要torch&gt;=1.7才能使用 tf32 数据类型。</p>
</div>
</section>
</section>
<section id="flash-attention-2">
<h5>Flash Attention 2<a class="headerlink" href="#flash-attention-2" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>可以通过在 Transformer 中使用 Flash Attention 2 集成来加速训练吞吐量</p></li>
<li><p>具体参见后面的推理优化(Optimizing inference)</p></li>
</ul>
</section>
<section id="optimizer-choice">
<h5>Optimizer choice<a class="headerlink" href="#optimizer-choice" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>用于训练 Transformer 模型的最常见优化器是 Adam 或 AdamW（带有权重衰减的 Adam）。</p></li>
<li><p>Adam通过存储之前梯度的滚动平均值实现了良好的收敛；然而，它增加了模型参数数量数量级的额外内存占用。为了解决这个问题，您可以使用替代优化器。例如，如果您为 NVIDIA GPU 安装了NVIDIA/apex ，或为 AMD GPU 安装了ROCmSoftwarePlatform/apex ， adamw_apex_fused将为您提供所有受支持的 AdamW 优化器中最快的训练体验。</p></li>
<li><p>Trainer集成了各种可立即使用的优化器： adamw_hf 、 adamw_torch 、 adamw_torch_fused 、 adamw_apex_fused 、 adamw_anyprecision 、 adafactor或adamw_bnb_8bit 。可以通过第三方实现插入更多优化器。</p></li>
</ul>
</section>
<section id="data-preloading">
<h5>Data preloading<a class="headerlink" href="#data-preloading" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>默认情况下，一切都发生在主进程中，它可能无法足够快地从磁盘读取数据，从而产生瓶颈，导致 GPU 利用率不足</p></li>
</ul>
</section>
<section id="deepspeed-zero">
<h5>DeepSpeed ZeRO<a class="headerlink" href="#deepspeed-zero" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>DeepSpeed 是一个开源深度学习优化库，与 🤗 Transformers 和 🤗 Accelerate 集成。它提供了广泛的功能和优化，旨在提高大规模深度学习训练的效率和可扩展性。</p></li>
<li><p>如果您的模型适合单个 GPU 并且您有足够的空间来容纳小批量大小，则不需要使用 DeepSpeed，因为它只会减慢速度。</p></li>
<li><p>但是，如果模型不适合单个 GPU，或者您无法适应小批量，则可以利用 DeepSpeed ZeRO + CPU Offload 或 NVMe Offload 来处理更大的模型。</p></li>
</ul>
</section>
<section id="using-torch-compile">
<h5>Using torch.compile<a class="headerlink" href="#using-torch-compile" title="此标题的永久链接">¶</a></h5>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">torch_compile</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">default_args</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>torch.compile uses Python’s frame evaluation API to automatically <code class="docutils literal notranslate"><span class="pre">create</span> <span class="pre">a</span> <span class="pre">graph</span></code> from existing PyTorch programs. After capturing the graph,可以部署不同的后端来将图表降低到优化的引擎。</p></li>
<li><p>create a graph: 通过 torch.compile 自动将现有的 PyTorch 程序转换成计算图（computation graph）</p>
<blockquote>
<div><ul class="simple">
<li><p>具体来说，PyTorch 通常是动态计算的（即动态图，也叫 eager execution），这意味着每个操作（如张量加法、矩阵乘法等）都会立即执行。</p></li>
<li><p>而 torch.compile 使用 Python 的 “frame evaluation API”，将这些动态的操作捕获下来，并将它们组合成一个优化后的静态计算图（static computation graph）。</p></li>
<li><p>这个计算图包含了整个模型的操作顺序和依赖关系，相当于一种高效的表达方式。通过将模型的操作变成图结构，后端可以对其进行优化和加速，利用硬件更好地执行这些操作，比如通过编译成更高效的代码或者在不同的硬件架构上执行。</p></li>
<li><p>因此，”create a graph” 的意思是：torch.compile 将原本按步骤执行的模型代码转换为一个可优化的图结构，便于进一步的性能优化。</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>最常用的后端:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Debugging</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;eager&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_eager&quot;</span><span class="p">)</span>

<span class="mf">2.</span> <span class="n">Training</span> <span class="o">&amp;</span> <span class="n">inference</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;inductor&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;nvfuser&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_nvfuser&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;aot_cudagraphs&quot;</span><span class="p">)</span>

<span class="mf">3.</span> <span class="n">Inference</span><span class="o">-</span><span class="n">only</span> <span class="n">backends</span><span class="p">:</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;ofi&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;fx2trt&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;onnxrt&quot;</span><span class="p">)</span>
    <span class="n">dynamo</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="s2">&quot;ipex&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>有关将torch.compile与 🤗 Transformer 结合使用的示例，请查看这篇关于使用最新 PyTorch 2.0 功能微调用于文本分类的 BERT 模型的博客文章: <a class="reference external" href="https://www.philschmid.de/getting-started-pytorch-2-0-transformers">https://www.philschmid.de/getting-started-pytorch-2-0-transformers</a></p></li>
</ul>
</section>
<section id="using-peft">
<h5>Using 🤗 PEFT<a class="headerlink" href="#using-peft" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>参数高效微调（PEFT）方法在微调期间冻结预训练的模型参数，并在其上添加少量可训练参数（适配器）。</p></li>
<li><p>As a result the memory associated to the <code class="docutils literal notranslate"><span class="pre">optimizer</span> <span class="pre">states</span> <span class="pre">and</span> <span class="pre">gradients</span></code> are greatly reduced.</p></li>
</ul>
<p>对于普通 AdamW，优化器状态的内存要求为:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">fp32</span> <span class="n">copy</span> <span class="n">of</span> <span class="n">parameters</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
<span class="mf">2.</span> <span class="n">Momentum</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
<span class="mf">3.</span> <span class="n">Variance</span><span class="p">:</span> <span class="mi">4</span> <span class="nb">bytes</span><span class="o">/</span><span class="n">param</span>
</pre></div>
</div>
<p>一个7B的模型 and 200 million parameters injected with <code class="docutils literal notranslate"><span class="pre">Low</span> <span class="pre">Rank</span> <span class="pre">Adapters</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>普通模型优化器状态的内存需求:
    12 * 7 = 84 GB
添加 Lora 会稍微增加与模型权重相关的内存，但会将优化器状态的内存需求大幅降低至
    12 * 0.2 = 2.4GB
</pre></div>
</div>
</section>
<section id="using-accelerate">
<h5>Using 🤗 Accelerate<a class="headerlink" href="#using-accelerate" title="此标题的永久链接">¶</a></h5>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">default_args</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1"># 完整示例训练循环</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">accelerate</span><span class="w"> </span><span class="kn">import</span> <span class="n">Accelerator</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data.dataloader</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">training_args</span><span class="o">.</span><span class="n">per_device_train_batch_size</span><span class="p">)</span>

<span class="k">if</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_checkpointing</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">fp16</span><span class="o">=</span><span class="n">training_args</span><span class="o">.</span><span class="n">fp16</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">adam_bnb_optim</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span><span class="o">.</span><span class="n">loss</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span>
    <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="n">training_args</span><span class="o">.</span><span class="n">gradient_accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="multiple-gpus-and-parallelism">
<h4>Multiple GPUs and parallelism<a class="headerlink" href="#multiple-gpus-and-parallelism" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>采用多种技术来实现并行性，例如数据并行性、张量并行性和管道并行性。需要注意的是，没有一种万能的解决方案，最佳设置取决于您所使用的特定硬件配置。</p></li>
</ul>
<p>Scalability strategy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Parallelization strategy for a single Node / multi-GPU setup
    Case 1: Your model fits onto a single GPU
        DDP - Distributed DataParallel
        Zero Redundancy Optimizer (ZeRO): https://arxiv.org/abs/1910.02054
    Case 2: Your model doesn’t fit onto a single GPU:
        PipelineParallel (PP)
        ZeRO
        TensorParallel (TP)
    Case 3: Largest layer of your model does not fit onto a single GPU
        If you are not using ZeRO, you have to use TensorParallel (TP), because PipelineParallel (PP) alone won’t be sufficient to accommodate the large layer.
        If you are using ZeRO, additionally adopt techniques from the Methods and tools for efficient training on a single GPU.

2. Parallelization strategy for a multi Node / multi-GPU setup
    When you have fast inter-node connectivity (e.g., NVLINK or NVSwitch) consider using one of these options:
        ZeRO - as it requires close to no modifications to the model
        A combination of PipelineParallel(PP) with TensorParallel(TP) and DataParallel(DP)
    When you have slow inter-node connectivity and still low on GPU memory:
        Employ a combination of DataParallel(DP) with PipelineParallel(PP), TensorParallel(TP), and ZeRO.
</pre></div>
</div>
</section>
<section id="fully-sharded-data-parallel">
<h4>Fully Sharded Data Parallel<a class="headerlink" href="#fully-sharded-data-parallel" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/">Fully Sharded Data Parallel (FSDP)</a> : 完全分片数据并行 (FSDP)是一种数据并行方法，可将模型的参数、梯度和优化器状态分片到可用 GPU（也称为工作线程或等级）的数量上。</p></li>
<li><p>与DistributedDataParallel（DDP）不同，FSDP通过在每个GPU上进行模型分片而非完整复制，从而降低了内存使用。这提高了 GPU 内存效率，并允许您在更少的 GPU 上训练更大的模型。</p></li>
<li><p>Unlike DistributedDataParallel (DDP), FSDP reduces memory-usage because a model is replicated on each GPU. This improves GPU memory-efficiency and allows you to train much larger models on fewer GPUs.</p></li>
<li><p>FSDP is integrated with the Accelerate</p></li>
</ul>
</section>
<section id="deepspeed">
<h4>DeepSpeed<a class="headerlink" href="#deepspeed" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>DeepSpeed是一个 PyTorch 优化库，可提高分布式训练的内存效率和速度。其核心是零冗余优化器（ZeRO），它可以大规模训练大型模型。</p></li>
</ul>
</section>
<section id="efficient-training-on-cpu">
<h4>Efficient Training on CPU<a class="headerlink" href="#efficient-training-on-cpu" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_path</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">bf16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">use_ipex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="o">+</span>   <span class="n">use_cpu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="distributed-cpu-training">
<h4>Distributed CPU training<a class="headerlink" href="#distributed-cpu-training" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>基于 PyTorch 的 DDP支持进行分布式 CPU 训练</p></li>
</ul>
</section>
</section>
<section id="optimizing-inference">
<h3>Optimizing inference<a class="headerlink" href="#optimizing-inference" title="此标题的永久链接">¶</a></h3>
<section id="cpu-inference">
<h4>CPU inference<a class="headerlink" href="#cpu-inference" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>通过一些优化，可以在 CPU 上高效运行大型模型推理。</p></li>
<li><p>其中一种优化技术涉及将 PyTorch 代码编译为适用于 C++ 等高性能环境的中间格式。</p></li>
<li><p>另一种技术将多个操作融合到一个内核中，以减少单独运行每个操作的开销。</p></li>
</ul>
<section id="id10">
<h5>🤗 Optimum<a class="headerlink" href="#id10" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>ONNX Runtime (ORT) 是一个模型加速器，默认在 CPU 上运行推理。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">optimum.onnxruntime</span><span class="w"> </span><span class="kn">import</span> <span class="n">ORTModelForQuestionAnswering</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ORTModelForQuestionAnswering</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;optimum/roberta-base-squad2&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;deepset/roberta-base-squad2&quot;</span><span class="p">)</span>

<span class="n">onnx_qa</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;question-answering&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;What&#39;s my name?&quot;</span>
<span class="n">context</span> <span class="o">=</span> <span class="s2">&quot;My name is Philipp and I live in Nuremberg.&quot;</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">onnx_qa</span><span class="p">(</span><span class="n">question</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="gpu-inference">
<h4>GPU inference<a class="headerlink" href="#gpu-inference" title="此标题的永久链接">¶</a></h4>
<section id="id11">
<h5>FlashAttention-2<a class="headerlink" href="#id11" title="此标题的永久链接">¶</a></h5>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>FlashAttention-2 是实验性的，在未来的版本中可能会发生很大的变化。</p>
</div>
<ul>
<li><p>FlashAttention-2是标准注意力机制的更快、更高效的实现，可以通过以下方式显着加速推理:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">additionally</span> <span class="n">parallelizing</span> <span class="n">the</span> <span class="n">attention</span> <span class="n">computation</span> <span class="n">over</span> <span class="n">sequence</span> <span class="n">length</span>
<span class="o">-</span> <span class="n">partitioning</span> <span class="n">the</span> <span class="n">work</span> <span class="n">between</span> <span class="n">GPU</span> <span class="n">threads</span> <span class="n">to</span> <span class="n">reduce</span> <span class="n">communication</span> <span class="ow">and</span> <span class="n">shared</span> <span class="n">memory</span> <span class="n">reads</span><span class="o">/</span><span class="n">writes</span> <span class="n">between</span> <span class="n">them</span>
</pre></div>
</div>
</li>
<li><p>要启用 FlashAttention-2，请传递参数 attn_implementation=”flash_attention_2” 到from_pretrained() ：</p></li>
</ul>
<p>安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">flash</span><span class="o">-</span><span class="n">attn</span> <span class="o">--</span><span class="n">no</span><span class="o">-</span><span class="n">build</span><span class="o">-</span><span class="n">isolation</span>
</pre></div>
</div>
</section>
<section id="bettertransformer">
<h5>BetterTransformer<a class="headerlink" href="#bettertransformer" title="此标题的永久链接">¶</a></h5>
<p>BetterTransformer 通过其 fastpath 执行加速推理。fastpath 执行中的两个优化是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>fusion, which combines multiple sequential operations into a single “kernel” to reduce the number of computation steps
skipping the inherent sparsity of padding tokens to avoid unnecessary computation with nested tensors
</pre></div>
</div>
</section>
<section id="id12">
<h5>bitsandbytes<a class="headerlink" href="#id12" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>bitsandbytes 是一个量化库，支持 4 位和 8 位量化。与原生全精度版本相比，量化可减小模型大小，从而更轻松地将大型模型安装到内存有限的 GPU 上。</p></li>
<li><p>博客- 使用 Hugging Face Transformers、Accelerate 和 BitsandBytes 进行大规模变压器的 8 位矩阵乘法简介: <a class="reference external" href="https://huggingface.co/blog/hf-bitsandbytes-integration">https://huggingface.co/blog/hf-bitsandbytes-integration</a></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">bitsandbytes</span><span class="o">&gt;=</span><span class="mf">0.39.0</span> <span class="n">accelerate</span><span class="o">&gt;=</span><span class="mf">0.20.0</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="others">
<h3>Others<a class="headerlink" href="#others" title="此标题的永久链接">¶</a></h3>
<section id="optimize-inference-using-torch-compile">
<h4>Optimize inference using torch.compile()<a class="headerlink" href="#optimize-inference-using-torch-compile" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>本节旨在为🤗 Transformers 中的计算机视觉模型的torch.compile()引入的推理加速提供基准。</p></li>
<li><p>根据模型和 GPU， torch.compile()在推理过程中可实现高达 30% 的加速</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForImageClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_ID</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="conceptual-guides">
<h2>Conceptual guides<a class="headerlink" href="#conceptual-guides" title="此标题的永久链接">¶</a></h2>
<section id="glossary">
<h3>Glossary<a class="headerlink" href="#glossary" title="此标题的永久链接">¶</a></h3>
<section id="dataparallel-dp">
<h4>DataParallel (DP)<a class="headerlink" href="#dataparallel-dp" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>【原始】Parallelism technique for training on multiple GPUs where the same setup is replicated multiple times, with each instance receiving a distinct data slice. The processing is done in parallel and all setups are synchronized at the end of each training step.</p></li>
<li><p>【定义】Data Parallelism（数据并行）是一种常用的并行训练技术，特别是在深度学习中用于多 GPU 的**训练**场景。该技术通过在多个 GPU 上复制相同的模型副本，并将输入数据划分成多个独立的部分，使得每个 GPU 处理不同的数据子集，从而并行执行计算任务。每个 GPU 处理自己的数据块，计算梯度，最终所有 GPU 的梯度会进行汇总（同步），并更新所有模型副本的参数。</p></li>
<li><p>【核心思想】将数据分片，模型复制，在多个 GPU 上并行计算。</p>
<blockquote>
<div><ul class="simple">
<li><p>模型复制：每个 GPU 都会得到一个完全相同的模型副本。所有的 GPU 都使用相同的模型结构和权重进行计算。</p></li>
<li><p>数据划分：训练数据被划分为多个子集，每个 GPU 处理不同的子集，完成前向传播和反向传播。</p></li>
<li><p>梯度同步：在每个 GPU 上独立计算完梯度后，所有 GPU 之间会通过通信机制将梯度进行汇总或平均，然后更新每个模型副本的参数。这保证了所有 GPU 的模型在每次训练步骤结束后保持同步。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【工作原理】</p>
<blockquote>
<div><ul class="simple">
<li><p>数据划分：假设你有一个包含 1024 条样本的批次（batch），并且你有 4 个 GPU。Data Parallelism 会将这 1024 条样本划分成 4 个子集（每个子集 256 条样本），并分配到不同的 GPU 上。</p></li>
<li><p>模型复制：每个 GPU 上都会有相同的模型副本。这些副本会初始化为相同的权重，并且在每一步训练中，它们的计算都是同步的。</p></li>
<li><p>并行计算：每个 GPU 独立地处理自己分配到的数据子集，执行前向传播（forward pass）和反向传播（backward pass）。这一部分计算是并行进行的，每个 GPU 的计算互不干扰。</p></li>
<li><p>梯度同步：当每个 GPU 计算完反向传播并得到梯度后，所有 GPU 会进行梯度同步。这意味着各 GPU 之间会通过网络通信，将它们各自计算的梯度汇总（通常是取平均），以确保所有模型副本的参数一致更新。</p></li>
<li><p>参数更新：梯度同步完成后，每个 GPU 会更新自己模型的参数。这些参数会通过汇总后的梯度进行更新，从而使得所有 GPU 上的模型在每个训练步骤结束后保持相同的权重。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【优势】</p>
<blockquote>
<div><ul class="simple">
<li><p>计算加速：通过将大批量数据分成多个小块，并行处理不同的部分，可以显著加速训练过程。理论上，使用 N 个 GPU 进行 Data Parallelism，可以实现近似 N 倍的加速效果（受限于通信开销和负载均衡）。</p></li>
<li><p>易于实现：相比其他并行技术（如 Tensor Parallelism 或 Pipeline Parallelism），Data Parallelism 的实现相对简单，因为只需要复制模型，并对数据进行划分和梯度同步。</p></li>
<li><p>扩展性：Data Parallelism 可以很容易地扩展到多个 GPU，甚至多个机器（通过分布式训练），适合大规模数据集的处理。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【劣势】</p>
<blockquote>
<div><ul class="simple">
<li><p>显存压力：每个 GPU 上都需要存储完整的模型副本，这意味着模型参数会被多次复制。如果模型非常大（例如 GPT-3 这样的模型），可能会导致显存不足的问题。</p></li>
<li><p>通信开销：在每个训练步骤结束时，所有 GPU 需要同步梯度。随着 GPU 数量的增加，通信开销会逐渐增加，尤其是在多个机器之间进行同步时，网络通信可能会成为瓶颈。</p></li>
<li><p>负载均衡：如果数据划分不均匀，某些 GPU 可能需要处理较重的工作，而其他 GPU 则可能处于闲置状态，这会影响并行效率。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【总结】Data Parallelism 是一种将模型副本分配到多个 GPU 并行处理不同数据子集的训练技术。通过在多个 GPU 上并行处理，可以加速模型训练，特别是适用于大型数据集的处理场景。虽然实现相对简单，但显存消耗和通信开销是 Data Parallelism 面临的主要挑战。</p></li>
</ul>
</section>
<section id="pipelineparallel-pp">
<h4>PipelineParallel (PP)<a class="headerlink" href="#pipelineparallel-pp" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>【原始文档】Parallelism technique in which the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are placed on a single GPU. Each GPU processes in parallel different stages of the pipeline and working on a small chunk of the batch. Learn more about how PipelineParallel works here.</p></li>
<li><p>【定义】Pipeline Parallelism（流水线并行）是一种在深度学习中常用的并行技术，特别适用于训练大型神经网络模型，它通过将模型按层级进行划分，并将这些划分后的部分分配到不同的 GPU 上，从而在多个设备上并行处理模型的计算任务。每个 GPU 只负责执行模型的一部分（即某些特定的层）。这种划分方式被称为纵向切分（vertical split），相对传统的数据并行（data parallelism），它不是在不同设备上处理相同的模型，而是将模型本身拆分开来。</p></li>
<li><p>【工作原理】</p>
<blockquote>
<div><ul class="simple">
<li><p>1.模型划分（分层）：假设你有一个 12 层的深度神经网络模型。你可以将前 4 层放在 GPU 1 上，接下来的 4 层放在 GPU 2 上，最后 4 层放在 GPU 3 上。每个 GPU 只存储和计算模型的一部分。</p></li>
<li><p>2.批量处理：Pipeline Parallelism 通常与批处理（batch processing）结合使用。假设输入的 batch 是 128 条样本：GPU 1 处理前 4 层时，它会处理第一小块数据（比如 64 条样本），然后将这些样本的输出传递给 GPU 2。当 GPU 2 开始处理这些样本时，GPU 1 可以开始处理 batch 中的下一小块数据。这样，多个 GPU 能够并行工作，像流水线一样处理数据，这就是流水线并行的名称来源。</p></li>
<li><ol class="arabic simple" start="3">
<li><p>流水线机制：各个 GPU 并不是完全独立工作的，而是按顺序处理数据。模型的每一部分（层）依赖于前一部分的输出。虽然各个 GPU 是并行的，但它们工作在同一条流水线上：当 GPU 1 处理第一个数据块时，GPU 2 处于空闲状态；当 GPU 1 处理完第一个数据块并传递给 GPU 2 时，GPU 2 开始处理第一块数据，同时 GPU 1 可以处理第二块数据；如此循环，直到整个 batch 被处理完毕。</p></li>
</ol>
</li>
</ul>
</div></blockquote>
</li>
<li><p>【好处】</p>
<blockquote>
<div><ul class="simple">
<li><p>节省显存：对于非常大的模型，单个 GPU 可能无法一次性容纳整个模型的所有层。通过将模型切分到多个 GPU 上，每个 GPU 只存储一部分模型参数，显著减少了单个 GPU 的显存压力。</p></li>
<li><p>并行效率：Pipeline Parallelism 通过让不同的 GPU 同时处理不同的数据块，增加了计算效率。尽管需要一定的通信和同步，但相比于在单个 GPU 上运行完整模型，流水线并行可以加速训练过程。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【挑战】</p>
<blockquote>
<div><ul class="simple">
<li><p>通信开销：由于不同 GPU 之间需要相互传递数据（即前一层的输出需要传递到下一层的输入），GPU 之间的通信带来了一定的开销，特别是当 GPU 数量较多时，这种开销可能会变得显著。</p></li>
<li><p>延迟：流水线并行会有一定的启动延迟（即前一个设备必须先处理完部分数据后，才能将数据传递到下一个设备）。对于小 batch size，这种延迟会更加明显。</p></li>
<li><p>负载均衡：模型各层的计算复杂度不同，某些层可能需要更多的计算资源。如果每个 GPU 处理的层数相同，但计算量不同，就会导致某些 GPU 工作负载重，另一些 GPU 闲置，这种负载不均衡也会影响并行效率。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【总结】Pipeline Parallelism 是一种通过将模型纵向拆分（按层划分）并分布到多个 GPU 上处理的并行技术。每个 GPU 负责计算模型的一部分层，并且各 GPU 像流水线一样处理批量数据，这既能减少单个 GPU 的显存消耗，又能通过并行处理加速计算。但同时也带来了通信开销和负载均衡等挑战。</p></li>
</ul>
</section>
<section id="tensor-parallelism-tp">
<h4>Tensor Parallelism (TP)<a class="headerlink" href="#tensor-parallelism-tp" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>【原始】Parallelism technique for training on multiple GPUs in which each tensor is split up into multiple chunks, so instead of having the whole tensor reside on a single GPU, each shard of the tensor resides on its designated GPU. Shards gets processed separately and in parallel on different GPUs and the results are synced at the end of the processing step. This is what is sometimes called horizontal parallelism, as the splitting happens on horizontal level. Learn more about Tensor Parallelism here.</p></li>
<li><p>【定义】Tensor Parallelism（张量并行）是一种在深度学习中常用的并行计算技术，主要用于将模型的张量切分为多个部分，并将这些部分分布到不同的 GPU 上进行并行处理。与 Pipeline Parallelism 不同，Tensor Parallelism 不将模型按层级划分，而是将每个张量（如权重矩阵或输入数据）水平切分（horizontally split），因此也被称为水平并行（horizontal parallelism）。</p></li>
<li><p>【核心思想】将模型的张量（包括参数、激活值等）切分为多个块（shards），并将这些块分布到不同的 GPU 上进行并行计算。这样可以减少每个 GPU 的计算负载和显存压力，同时加速训练。</p></li>
<li><p>【工作原理】</p>
<blockquote>
<div><ul class="simple">
<li><p>张量切分：假设你有一个张量（例如权重矩阵）大小为 (1024, 1024)，而你有 4 个 GPU。你可以将这个张量水平切分成 4 个部分，每个部分的大小为 (256, 1024)，分别放置在 4 个不同的 GPU 上。这样，张量的不同部分会分别在不同的 GPU 上进行处理。</p></li>
<li><p>并行计算：每个 GPU 处理张量的不同部分，并进行独立的计算。例如，在前向传播时，每个 GPU 会处理其分配到的张量部分。在反向传播时，梯度也在各自的 GPU 上计算。</p></li>
<li><p>结果同步：在每个计算步骤（如前向传播或反向传播）结束时，各个 GPU 会将它们的部分结果进行同步，以确保模型更新时所有张量部分的计算结果能够汇总。同步的过程是通过通信机制完成的，通常使用分布式框架（如 NVIDIA 的 NCCL 库）来高效传递数据。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【优势】</p>
<blockquote>
<div><ul class="simple">
<li><p>减少显存占用：通过将大的张量分成多个小块，每个 GPU 只需要存储和计算自己负责的张量部分，显著减少了单个 GPU 的显存消耗。这在处理大型模型（如 GPT-3）时非常重要，因为单个 GPU 无法容纳整个模型的权重。</p></li>
<li><p>加速计算：由于每个 GPU 只负责一部分张量，计算可以在多个 GPU 上并行进行，理论上可以线性加速训练过程。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【挑战】</p>
<blockquote>
<div><ul class="simple">
<li><p>通信开销：每个计算步骤结束时，各个 GPU 需要同步结果。对于较大的模型和较频繁的同步操作，这会导致显著的通信开销，影响整体性能。</p></li>
<li><p>负载均衡：在实际应用中，某些张量可能较小，切分后不同的 GPU 上计算量可能不均衡，导致某些 GPU 计算较慢，进而拖慢整个训练过程。</p></li>
<li><p>实现复杂度：相比 Data Parallelism，Tensor Parallelism 的实现更为复杂，因为涉及到张量的切分、分配、并行计算和同步等多个步骤。</p></li>
</ul>
</div></blockquote>
</li>
<li><p>【总结】Tensor Parallelism 是一种通过水平切分模型张量来分配到多个 GPU 进行并行计算的技术。这种技术可以显著减少显存消耗，尤其适合处理非常大的模型。它通过在不同 GPU 上并行处理张量的不同部分来加速计算，并在每个计算步骤后通过通信机制同步结果。与其他并行技术相比，Tensor Parallelism 在处理超大模型时非常有效，但也面临通信开销和实现复杂度的挑战。</p></li>
</ul>
</section>
<section id="tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism">
<h4>Tensor Parallelism vs. Data Parallelism vs. Pipeline Parallelism<a class="headerlink" href="#tensor-parallelism-vs-data-parallelism-vs-pipeline-parallelism" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Tensor Parallelism：张量（如权重矩阵）被切分成多个小块，分布在不同的 GPU 上并行计算。适用于非常大的张量，能显著减少显存压力。</p></li>
<li><p>Data Parallelism：相同的模型副本运行在每个 GPU 上，但每个 GPU 处理不同的输入数据。在每个 GPU 计算完梯度后，梯度会进行平均并更新所有模型副本的参数。优点是实现相对简单，但显存压力依然很大，因为每个 GPU 都需要存储完整的模型参数。</p></li>
<li><p>Pipeline Parallelism：模型按层级切分，不同的层分配到不同的 GPU，多个 GPU 以流水线的方式处理批次数据。适合非常深的网络，但需要解决流水线启动和同步的问题。</p></li>
</ul>
</section>
<section id="zero-redundancy-optimizer-zero">
<h4>Zero Redundancy Optimizer (ZeRO)<a class="headerlink" href="#zero-redundancy-optimizer-zero" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>参见相关定义</p></li>
</ul>
</section>
<section id="id13">
<h4>Fully Sharded Data Parallel<a class="headerlink" href="#id13" title="此标题的永久链接">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>参见上面相关章节</p>
</div>
</section>
</section>
<section id="optimizing-llms-for-speed-and-memory">
<h3>Optimizing LLMs for Speed and Memory<a class="headerlink" href="#optimizing-llms-for-speed-and-memory" title="此标题的永久链接">¶</a></h3>
<p>effective techniques for efficient LLM deployment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Lower Precision
    降低精度（即8 位和 4 位）运行可以实现计算优势，而不会显着降低模型性能
2. Flash Attention
    注意力算法的一种变体，它不仅提供了一种更节省内存的方法，而且由于优化了 GPU 内存利用率而实现了效率的提高
3. Architectural Innovations
    模型架构中最重要的进步有:
        Alibi, Rotary embeddings, Multi-Query Attention (MQA) and Grouped-Query-Attention (GQA).
</pre></div>
</div>
<section id="lower-precision">
<h4>1. Lower Precision<a class="headerlink" href="#lower-precision" title="此标题的永久链接">¶</a></h4>
<p>所有量化技术的工作原理:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Quantize all weights to the target precision
    将所有权重量化到目标精度
2. Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision
    加载量化权重，并以 bfloat16 精度传递向量的输入序列
3. Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision
    将权重动态反量化为 bfloat16，以使用 bfloat16 精度的输入向量执行计算
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>使用量化权重时，先把权重反量化为 bfloat16，输入序列还是 bfloat16，计算两者乘积。所以，推理时间通常不会减少，反而会增加。总之，重要的是要记住，模型量化以准确性和在某些情况下牺牲推理时间为代价提高内存效率。</p>
</div>
</section>
<section id="flash-attention">
<h4>2. Flash Attention<a class="headerlink" href="#flash-attention" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>对于大型输入上下文，默认的自注意力算法很快就会变得非常昂贵的内存消耗。</p></li>
<li><p>通过跟踪 softmax 归一化统计数据并使用一些智能数学，与默认的自注意力层相比，Flash Attention 提供了相同的数值输出，而内存成本仅随时间线性增加 N</p></li>
<li><p>而且与默认注意力相比，闪存注意力的推理速度要快得多，这是因为它能够显着减少对 GPU (VRAM) 较慢、高带宽内存的需求，而是专注于更快的片上内存 (SRAM) 。</p></li>
<li><p>实际上，目前绝对没有理由不使用 Flash Attention（如果可用）。该算法在数学上给出相同的输出，并且速度更快且内存效率更高。</p></li>
</ul>
</section>
<section id="architectural-innovations">
<h4>3. Architectural Innovations<a class="headerlink" href="#architectural-innovations" title="此标题的永久链接">¶</a></h4>
<section id="improving-positional-embeddings-of-llms">
<h5>3.1 Improving positional embeddings of LLMs<a class="headerlink" href="#improving-positional-embeddings-of-llms" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>位置嵌入（positional embeddings） 是自注意力机制的核心组成部分，负责帮助模型理解文本序列中不同 token 之间的顺序关系</p></li>
<li><p>【背景：位置嵌入的必要性】**自注意力机制**中的 Softmax(QK^T) 操作将每个 token 与序列中的其他 token 进行关联，但默认情况下它无法理解 token 之间的相对顺序。没有位置嵌入的模型难以区分不同的输入顺序。例如，模型无法区分 “Hello I love you” 和 “You love I hello”，因为它们在没有位置信息的情况下看起来是等价的。</p></li>
<li><p>因此，位置嵌入（positional embeddings）被引入，用来编码每个 token 在句子中的位置信息，使模型能够区分输入文本的顺序。</p></li>
<li><p>传统方法主要有两种：固定位置嵌入（sinusoidal embeddings） 和 学习的绝对位置嵌入（learned positional embeddings）。</p></li>
<li><p>【问题：传统位置嵌入的局限性】1.对长文本表现较差：绝对位置嵌入为每个位置生成一个唯一的编码（例如 0 到 N 的位置编号），但对于长文本，模型难以有效建模 token 之间的远距离关系。2.固定输入长度问题：如果使用学习的绝对位置嵌入，模型只能处理与训练时长度相同的输入。如果输入长度超出训练时的最大长度，模型无法很好地进行推断。</p></li>
<li><p>【解决：相对位置嵌入：RoPE 和 ALiBi】为了解决上述问题，研究者提出了相对位置嵌入（relative positional embeddings），这种方法不再为每个位置分配绝对值，而是关注 token 之间的相对距离。两种流行的相对位置嵌入方法是 <strong>RoPE（Rotary Position Embedding）</strong> 和 <strong>ALiBi（Attention with Linear Biases）</strong>。它们通过修改自注意力机制中的 QK^T 矩阵来引入位置信息。</p></li>
</ul>
</section>
<section id="the-key-value-cache">
<h5>3.2 The key-value cache<a class="headerlink" href="#the-key-value-cache" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>这是一个非常有效的优化策略，特别适用于处理长序列或生成大量文本的场景。</p></li>
<li><p>【背景：自回归文本生成】在 LLM 中，自回归文本生成通过逐步生成下一个 token 来完成。每次输入先前生成的 token 序列，然后模型预测下一个 token，并将其加入输入序列中，如此循环直到生成结束。在这种逐步生成的过程中，随着序列长度的增加，每次都需要对整个序列重新计算 <code class="docutils literal notranslate"><span class="pre">QK^T</span></code> 矩阵，进而得到注意力权重。这意味着每一步的计算复杂度会随着序列长度的增加而增大。</p></li>
<li><p>【问题：重复计算的效率低下】对于自回归生成来说，模型在生成下一个 token 时，每一步都需要重新计算所有之前的 token 的 key 和 value，尽管这些值在之前的步骤中已经计算过了。重复计算这些不必要的 key-value 对会导致计算资源的浪费，并且在生成长序列时显著增加计算复杂度和显存占用。</p></li>
<li><p>【解决方案：Key-Value Cache】通过缓存每一层的 key 和 value 向量（即 K 和 V），避免重复计算，从而提高效率。</p></li>
<li><p>【原理】**QK^T 矩阵优化**：在标准 Transformer 模型中，QK^T 矩阵是通过将每个 token 的 query 向量（Q）与所有 key 向量（K）进行点积计算得出的。然而，对于自回归生成，我们每次只需要为新增的 token 计算 query （q_c）与之前缓存的 key（K）进行相乘，而不需要重新计算所有的 key 和 value。<strong>缓存机制</strong>：在每一步生成过程中，将之前的 key-value 对保存在缓存中，下一步生成时只需计算当前新 token 的 query，然后与缓存中的 key 进行计算。这样避免了重复计算整个序列的 key-value 对。</p></li>
</ul>
<section id="multi-round-conversation">
<h6>3.2.1 Multi-round conversation<a class="headerlink" href="#multi-round-conversation" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>键值缓存对于聊天等需要多次自动回归解码的应用程序特别有用</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: How many people live in France?
Assistant: Roughly 75 million people live in France
User: And how many are in Germany?
Assistant: Germany has ca. 81 million inhabitants
</pre></div>
</div>
<ul class="simple">
<li><p>在此聊天中， LLM运行自回归解码两次：</p></li>
<li><p>第一次，键值缓存为空，输入提示为 “User: How many people live in France?” 模型自动回归生成文本 “Roughly 75 million people live in France” 同时在每个解码步骤增加键值缓存。</p></li>
<li><p>第二次输入提示是 “User: How many people live in France? n Assistant: Roughly 75 million people live in France n User: And how many in Germany?” 。由于缓存，前两个句子的所有键值向量都已经计算出来。因此输入提示仅包含 “User: And how many in Germany?” 。在处理缩短的输入提示时，其计算出的键值向量被连接到第一次解码的键值缓存。第二次回答 “Germany has ca. 81 million inhabitants” 然后使用由编码的键值向量组成的键值缓存自动生成 “User: How many people live in France? n Assistant: Roughly 75 million people live in France n User: And how many are in Germany?” 。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generation as usual</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="o">+</span> <span class="s2">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Piping the returned `past_key_values` to speed up the next conversation round</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">decoded_output</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Question: How can I modify the function above to return Mega bytes instead?</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
<span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
<span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
  <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
  <span class="n">past_key_values</span><span class="o">=</span><span class="n">generation_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>    <span class="c1"># ❇️</span>
  <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
  <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</pre></div>
</div>
</section>
<section id="multi-query-attention-mqa">
<h6>3.2.2 Multi-Query-Attention (MQA)<a class="headerlink" href="#multi-query-attention-mqa" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>【定义】一种优化自注意力机制的技术，旨在减少key-value 缓存的内存占用并提高计算效率。该方法由 Noam Shazeer 在论文 “Fast Transformer Decoding: One Write-Head is All You Need” 中提出，核心思想是共享 key-value 投影，从而大幅降低多头注意力（Multi-Head Attention）的内存开销。</p></li>
<li><p>【背景：传统多头注意力机制的内存瓶颈】在传统的**多头注意力机制（Multi-Head Attention）**中，模型会为每个注意力头（attention head）计算独立的 key-value 对。具体来说，n 个注意力头意味着需要计算并存储 n 组 key-value 向量，通常会显著增加内存和计算开销。大规模语言模型（LLMs）中，通常有 20 到 100 个注意力头，因此当缓存这些 key-value 对时，内存消耗非常高，尤其是在长文本生成或多轮对话中。</p></li>
<li><p>【核心思想】关键在于：所有注意力头共享一个 key-value 投影，即每个头仍然拥有独立的 query（查询向量），但 key 和 value 向量在所有注意力头中是相同的。这样可以显著减少存储和计算的开销，而不会显著影响模型的性能。</p></li>
<li><p>【应用】MQA 技术已经被许多主流的大规模语言模型（LLMs）所采用，包括：Falcon，PaLM，MPT，BLOOM</p></li>
</ul>
</section>
<section id="grouped-query-attention-gqa">
<h6>3.2.3 Grouped-Query-Attention (GQA)<a class="headerlink" href="#grouped-query-attention-gqa" title="此标题的永久链接">¶</a></h6>
<ul class="simple">
<li><p>【定义】由 Google 研究员 Ainslie 等人在论文中提出的注意力机制优化方法，它旨在解决 Multi-Query Attention (MQA) 带来的性能下降问题，同时保留大部分内存和计算效率的提升。相比 MQA，GQA 提供了一个更加折中的解决方案，在提升计算效率的同时，减少对模型性能的影响。</p></li>
<li><p>【背景：MQA 的局限性】Multi-Query Attention (MQA) 通过为所有注意力头共享一个 key-value 投影，显著减少了内存和计算开销，尤其在自回归生成时提高了推理速度并降低了显存占用。然而，研究发现 MQA 的这种极端共享机制会带来一定的模型性能下降，因为不同的 query 头无法再独立学习与 key-value 的对应关系，导致注意力机制的灵活性受限。</p></li>
<li><p>【核心思想】Grouped-Query Attention (GQA) 提出了一个折衷方案：减少注意力头的 query 投影数量，但不将其完全合并为一个。具体来说，GQA 提出将注意力头分组，多个头共享一组 key-value 投影，而不是每个头都使用完全独立的投影，也不是像 MQA 那样所有头共享同一组投影。</p></li>
</ul>
</section>
</section>
</section>
</section>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="此标题的永久链接">¶</a></h2>
<section id="main-classes">
<h3>Main Classes<a class="headerlink" href="#main-classes" title="此标题的永久链接">¶</a></h3>
<section id="auto-classes">
<h4>Auto Classes<a class="headerlink" href="#auto-classes" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoConfig</span>
<span class="n">AutoTokenizer</span>
<span class="n">AutoFeatureExtractor</span>
<span class="n">AutoImageProcessor</span>
<span class="n">AutoProcessor</span>
</pre></div>
</div>
<p>Generic model classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModel</span>
</pre></div>
</div>
<p>Generic pretraining classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForPreTraining</span>
</pre></div>
</div>
<p>Natural Language Processing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForCausalLM</span>
<span class="n">AutoModelForMaskedLM</span>
<span class="n">AutoModelForMaskGeneration</span>
<span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">AutoModelForSequenceClassification</span>
<span class="n">AutoModelForMultipleChoice</span>
<span class="n">AutoModelForNextSentencePrediction</span>
<span class="n">AutoModelForTokenClassification</span>
<span class="n">AutoModelForQuestionAnswering</span>
<span class="n">AutoModelForTextEncoding</span>
</pre></div>
</div>
<p>Computer vision:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForDepthEstimation</span>
<span class="n">AutoModelForImageClassification</span>
<span class="n">AutoModelForVideoClassification</span>
<span class="n">AutoModelForKeypointDetection</span>
<span class="n">AutoModelForMaskedImageModeling</span>
<span class="n">AutoModelForObjectDetection</span>
<span class="n">AutoModelForImageSegmentation</span>
<span class="n">AutoModelForImageToImage</span>
<span class="n">AutoModelForSemanticSegmentation</span>
<span class="n">AutoModelForInstanceSegmentation</span>
<span class="n">AutoModelForUniversalSegmentation</span>
<span class="n">AutoModelForZeroShotImageClassification</span>
<span class="n">AutoModelForZeroShotObjectDetection</span>
</pre></div>
</div>
<p>Audio:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForAudioClassification</span>
<span class="n">AutoModelForAudioFrameClassification</span>
<span class="n">AutoModelForCTC</span>
<span class="n">AutoModelForSpeechSeq2Seq</span>
<span class="n">AutoModelForAudioXVector</span>
<span class="n">AutoModelForTextToSpectrogram</span>
<span class="n">AutoModelForTextToWaveform</span>
</pre></div>
</div>
<p>Multimodal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForTableQuestionAnswering</span>
<span class="n">AutoModelForDocumentQuestionAnswering</span>
<span class="n">AutoModelForVisualQuestionAnswering</span>
<span class="n">AutoModelForVision2Seq</span>
</pre></div>
</div>
</section>
<section id="id14">
<h4>Backbone<a class="headerlink" href="#id14" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>backbone(主干)是一种用于为更高级别的计算机视觉任务（例如对象检测和图像分类）进行特征提取的模型。</p></li>
<li><p>Transformers 提供了一个 <code class="docutils literal notranslate"><span class="pre">AutoBackbone</span></code> 类，用于根据预训练的模型权重初始化 Transformers 主干</p></li>
<li><p>两个实用程序类:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoBackbone</span>

<span class="n">BackboneMixin</span>
<span class="n">BackboneConfigMixin</span>

<span class="n">TimmBackbone</span>
<span class="n">TimmBackboneConfig</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="callbacks">
<h4>Callbacks<a class="headerlink" href="#callbacks" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Callbacks 是可以在 PyTorch Trainer中自定义训练循环行为的对象（该功能尚未在 TensorFlow 中实现），可以检查训练循环状态（用于进度报告、登录 TensorBoard 或其他 ML 平台……）并做出决策（如提前停止）。</p></li>
</ul>
</section>
<section id="id15">
<h4>Configuration<a class="headerlink" href="#id15" title="此标题的永久链接">¶</a></h4>
<p>base class:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PretrainedConfig</span>
</pre></div>
</div>
<p>通用属性有:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>hidden_size
num_attention_heads
num_hidden_layers

文本模型进一步实现： vocab_size
</pre></div>
</div>
</section>
<section id="data-collator">
<h4>Data Collator<a class="headerlink" href="#data-collator" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Data collators 是通过使用数据集元素列表作为输入来形成批次的对象。这些元素与train_dataset或eval_dataset的元素具有相同的类型。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DefaultDataCollator</span>
<span class="n">DataCollatorWithPadding</span>
<span class="n">DataCollatorForTokenClassification</span>
<span class="n">DataCollatorForSeq2Seq</span>
<span class="n">DataCollatorForLanguageModeling</span>
<span class="n">DataCollatorForWholeWordMask</span>
<span class="n">DataCollatorForPermutationLanguageModeling</span>
<span class="n">DataCollatorWithFlattening</span>
</pre></div>
</div>
</section>
<section id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>默认是WARNING</p></li>
</ul>
<p>修改代码:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>
<span class="n">transformers</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_info</span><span class="p">()</span>

<span class="n">环境变量</span> <span class="n">TRANSFORMERS_VERBOSITY</span><span class="p">:</span>
<span class="n">TRANSFORMERS_VERBOSITY</span><span class="o">=</span><span class="n">error</span> <span class="o">./</span><span class="n">myprogram</span><span class="o">.</span><span class="n">py</span>

<span class="n">环境变量来禁用一些warnings</span> <span class="n">TRANSFORMERS_NO_ADVISORY_WARNINGS</span>
<span class="n">TRANSFORMERS_NO_ADVISORY_WARNINGS</span><span class="o">=</span><span class="mi">1</span> <span class="o">./</span><span class="n">myprogram</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
<section id="models">
<h4>Models<a class="headerlink" href="#models" title="此标题的永久链接">¶</a></h4>
<p>基类:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>PreTrainedModel 、 TFPreTrainedModel和FlaxPreTrainedModel

ModuleUtilsMixin
PushToHubMixin
</pre></div>
</div>
</section>
<section id="text-generation">
<h4>Text Generation<a class="headerlink" href="#text-generation" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>每个框架都有一个用于文本生成的生成方法，在各自的 <code class="docutils literal notranslate"><span class="pre">GenerationMixin</span></code> 类中实现：</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GenerationConfig</span>
    <span class="n">WatermarkingConfig</span>

<span class="n">GenerationMixin</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GenerationConfig</span>

<span class="c1"># Download configuration from huggingface.co and cache.</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">)</span>

<span class="c1"># E.g. config was saved using *save_pretrained(&#39;./test/saved_model/&#39;)*</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">)</span>

<span class="c1"># You can also specify configuration names to your generation configuration file</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">,</span> <span class="n">config_file_name</span><span class="o">=</span><span class="s2">&quot;my_configuration.json&quot;</span><span class="p">)</span>
<span class="n">generation_config</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./test/saved_model/&quot;</span><span class="p">,</span> <span class="s2">&quot;my_configuration.json&quot;</span><span class="p">)</span>

<span class="c1"># If you&#39;d like to try a minor variation to an existing configuration, you can also pass generation</span>
<span class="c1"># arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored</span>
<span class="n">generation_config</span><span class="p">,</span> <span class="n">unused_kwargs</span> <span class="o">=</span> <span class="n">GenerationConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;openai-community/gpt2&quot;</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">foo</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_unused_kwargs</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">generation_config</span><span class="o">.</span><span class="n">top_k</span>  <span class="c1"># 1</span>

<span class="n">unused_kwargs</span>   <span class="c1"># {&#39;foo&#39;: False}</span>
</pre></div>
</div>
</section>
<section id="onnx">
<h4>ONNX<a class="headerlink" href="#onnx" title="此标题的永久链接">¶</a></h4>
<p>三个抽象类:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">We</span> <span class="n">provide</span> <span class="n">three</span> <span class="n">abstract</span> <span class="n">classes</span> <span class="n">that</span> <span class="n">you</span> <span class="n">should</span> <span class="n">inherit</span> <span class="n">from</span><span class="p">,</span> <span class="n">depending</span> <span class="n">on</span> <span class="n">the</span> <span class="nb">type</span> <span class="n">of</span> <span class="n">model</span> <span class="n">architecture</span> <span class="n">you</span> <span class="n">wish</span> <span class="n">to</span> <span class="n">export</span><span class="p">:</span>

<span class="n">OnnxConfig</span>
<span class="n">OnnxConfigWithPast</span>
<span class="n">OnnxSeq2SeqConfigWithPast</span>
</pre></div>
</div>
<p>ONNX Features:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Each</span> <span class="n">ONNX</span> <span class="n">configuration</span> <span class="ow">is</span> <span class="n">associated</span> <span class="k">with</span> <span class="n">a</span> <span class="nb">set</span> <span class="n">of</span> <span class="n">features</span> <span class="n">that</span> <span class="n">enable</span> <span class="n">you</span> <span class="n">to</span> <span class="n">export</span> <span class="n">models</span> <span class="k">for</span> <span class="n">different</span> <span class="n">types</span> <span class="n">of</span> <span class="n">topologies</span> <span class="ow">or</span> <span class="n">tasks</span><span class="o">.</span>

<span class="n">FeaturesManager</span>
</pre></div>
</div>
</section>
<section id="optimization">
<h4>Optimization<a class="headerlink" href="#optimization" title="此标题的永久链接">¶</a></h4>
<p>The .optimization module provides:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>an optimizer with weight decay fixed that can be used to fine-tuned models, and
    一个固定权重衰减的优化器，可用于微调模型
several schedules in the form of schedule objects that inherit from `_LRSchedule`:
a gradient accumulation class to accumulate the gradients of multiple batches
    一个梯度累积类，用于累积多个批次的梯度
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AdamW</span>
    <span class="n">Implements</span> <span class="n">Adam</span> <span class="n">algorithm</span> <span class="k">with</span> <span class="n">weight</span> <span class="n">decay</span> <span class="n">fix</span>
<span class="n">AdaFactor</span>
    <span class="n">can</span> <span class="n">be</span> <span class="n">used</span> <span class="k">as</span> <span class="n">a</span> <span class="n">drop</span> <span class="ow">in</span> <span class="n">replacement</span> <span class="k">for</span> <span class="n">Adam</span> <span class="n">original</span> <span class="n">fairseq</span> <span class="n">code</span>

<span class="n">Schedules</span>
    <span class="n">SchedulerType</span>
</pre></div>
</div>
</section>
<section id="model-outputs">
<h4>Model outputs<a class="headerlink" href="#model-outputs" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>所有模型的输出都是ModelOutput子类的实例。</p></li>
<li><p>All models have outputs that are instances of subclasses of ModelOutput. Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-uncased&quot;</span><span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;Hello, my dog is cute&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Batch size 1</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="c1"># outputs对象是一个SequenceClassifierOutput</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ModelOutput</span>
<span class="n">BaseModelOutput</span>
<span class="n">BaseModelOutputWithPooling</span>
<span class="n">BaseModelOutputWithCrossAttentions</span>
<span class="n">BaseModelOutputWithPoolingAndCrossAttentions</span>
<span class="n">BaseModelOutputWithPast</span>
<span class="n">BaseModelOutputWithPastAndCrossAttentions</span>

<span class="n">Seq2SeqModelOutput</span>

<span class="n">CausalLMOutput</span>
<span class="n">CausalLMOutputWithCrossAttentions</span>
<span class="n">CausalLMOutputWithPast</span>

<span class="n">MaskedLMOutput</span>
<span class="n">Seq2SeqLMOutput</span>
<span class="n">NextSentencePredictorOutput</span>

<span class="n">SequenceClassifierOutput</span>
<span class="n">Seq2SeqSequenceClassifierOutput</span>

<span class="n">MultipleChoiceModelOutput</span>

<span class="n">TokenClassifierOutput</span>

<span class="n">QuestionAnsweringModelOutput</span>
<span class="n">Seq2SeqQuestionAnsweringModelOutput</span>
<span class="n">Seq2SeqSpectrogramOutput</span>
<span class="n">SemanticSegmenterOutput</span>

<span class="n">ImageClassifierOutput</span>
<span class="n">ImageClassifierOutputWithNoAttention</span>

<span class="n">DepthEstimatorOutput</span>
<span class="n">Wav2Vec2BaseModelOutput</span>
<span class="n">XVectorOutput</span>

<span class="n">Seq2SeqTSModelOutput</span>
<span class="n">Seq2SeqTSPredictionOutput</span>
<span class="n">SampleTSPredictionOutput</span>
</pre></div>
</div>
</section>
<section id="pipelines">
<h4>Pipelines<a class="headerlink" href="#pipelines" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-classification&quot;</span><span class="p">)</span>
<span class="n">pipe</span><span class="p">(</span><span class="s2">&quot;This restaurant is awesome&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="processors">
<h4>Processors<a class="headerlink" href="#processors" title="此标题的永久链接">¶</a></h4>
<section id="multi-modal-processors">
<h5>Multi-modal processors<a class="headerlink" href="#multi-modal-processors" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>任何多模态模型都需要一个对象来编码或解码将多种模态(modalities)（文本、视觉和音频）分组的数据。</p></li>
<li><p>这是由称为 <code class="docutils literal notranslate"><span class="pre">processors</span></code> 的对象来处理的，它将两个或多个处理对象(processing objects)组合在一起，例如分词器（用于文本模态）、图像处理器（用于视觉）和特征提取器（用于音频）。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ProcessorMixin</span>
</pre></div>
</div>
</section>
</section>
<section id="id16">
<h4>Quantization<a class="headerlink" href="#id16" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>量化技术通过使用 8 位整数 (int8) 等较低精度的数据类型表示权重和激活来减少内存和计算成本。这可以加载通常无法装入内存的更大模型，并加快推理速度。 Transformers 支持 AWQ 和 GPTQ 量化算法，并且支持 8 位和 4 位量化（bitsandbytes）。</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">QuantoConfig</span>
<span class="n">AqlmConfig</span>
<span class="n">AwqConfig</span>
<span class="n">EetqConfig</span>
<span class="n">GPTQConfig</span>
<span class="n">BitsAndBytesConfig</span>
<span class="n">HfQuantizer</span>
<span class="n">HqqConfig</span>
<span class="n">FbgemmFp8Config</span>
<span class="n">CompressedTensorsConfig</span>
<span class="n">TorchAoConfig</span>
</pre></div>
</div>
</section>
<section id="id17">
<h4>Tokenizer<a class="headerlink" href="#id17" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">PreTrainedTokenizer</span>
<span class="n">PreTrainedTokenizerFast</span>

<span class="n">BatchEncoding</span>
</pre></div>
</div>
</section>
<section id="id18">
<h4>Trainer<a class="headerlink" href="#id18" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Trainer</span>
<span class="n">Seq2SeqTrainer</span>

<span class="n">TrainingArguments</span>
<span class="n">Seq2SeqTrainingArguments</span>
</pre></div>
</div>
</section>
</section>
<section id="id19">
<h3>Models<a class="headerlink" href="#id19" title="此标题的永久链接">¶</a></h3>
<section id="text-models">
<h4>Text models<a class="headerlink" href="#text-models" title="此标题的永久链接">¶</a></h4>
<section id="qwen2">
<h5>Qwen2<a class="headerlink" href="#qwen2" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Qwen2 is the new model series of large language models from the Qwen team.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Qwen2Config</span>
<span class="n">Qwen2Tokenizer</span>
<span class="n">Qwen2TokenizerFast</span>
<span class="n">Qwen2Model</span>
<span class="n">Qwen2ForCausalLM</span>
<span class="n">Qwen2ForSequenceClassification</span>
<span class="n">Qwen2ForTokenClassification</span>
</pre></div>
</div>
</section>
<section id="qwen2-vl">
<h5>Qwen2_VL<a class="headerlink" href="#qwen2-vl" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Qwen2VLConfig</span>
<span class="n">Qwen2VLImageProcessor</span>
<span class="n">Qwen2VLProcessor</span>
<span class="n">Qwen2VLModel</span>
<span class="n">Qwen2VLForConditionalGeneration</span>
</pre></div>
</div>
</section>
<section id="cpm">
<h5>CPM<a class="headerlink" href="#cpm" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>The CPM model was proposed in CPM: <a class="reference external" href="https://arxiv.org/abs/2012.00413">A Large-scale Generative Chinese Pre-trained Language Model</a></p></li>
</ul>
</section>
<section id="dpr">
<h5>DPR<a class="headerlink" href="#dpr" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&amp;A research.</p></li>
<li><p>It was introduced in Dense Passage Retrieval for Open-Domain Question Answering by Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.</p></li>
<li><p>相关论文: Dense Passage Retrieval for Open-Domain Question Answering(<a class="reference external" href="https://arxiv.org/abs/2004.04906">https://arxiv.org/abs/2004.04906</a>)</p></li>
<li><p>开放域问答依赖于高效的段落检索来选择候选上下文，其中传统的稀疏向量空间模型（如 TF-IDF 或 BM25）是事实上的方法。在这项工作中，我们表明检索可以单独使用密集表示实际实现，其中嵌入是通过一个简单的双编码器框架从少量问题和段落中学习的。在广泛的开放域 QA 数据集上进行评估时，我们的密集检索器在前 20 名传代检索准确率方面比强大的 Lucene-BM25 系统高出 9%-19%，并帮助我们的端到端 QA 系统在多个开放域 QA 基准上建立新的最先进的技术。</p></li>
<li><p>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.</p></li>
</ul>
<p>DPR consists in three models:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Question</span> <span class="n">encoder</span><span class="p">:</span> <span class="n">encode</span> <span class="n">questions</span> <span class="k">as</span> <span class="n">vectors</span>
<span class="mf">2.</span> <span class="n">Context</span> <span class="n">encoder</span><span class="p">:</span> <span class="n">encode</span> <span class="n">contexts</span> <span class="k">as</span> <span class="n">vectors</span>
<span class="mf">3.</span> <span class="n">Reader</span><span class="p">:</span> <span class="n">extract</span> <span class="n">the</span> <span class="n">answer</span> <span class="n">of</span> <span class="n">the</span> <span class="n">questions</span> <span class="n">inside</span> <span class="n">retrieved</span> <span class="n">contexts</span><span class="p">,</span> <span class="n">along</span> <span class="k">with</span> <span class="n">a</span> <span class="n">relevance</span> <span class="n">score</span>
    <span class="p">(</span><span class="n">high</span> <span class="k">if</span> <span class="n">the</span> <span class="n">inferred</span> <span class="n">span</span> <span class="n">actually</span> <span class="n">answers</span> <span class="n">the</span> <span class="n">question</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<ul>
<li><p>Dense Passage Retrieval (DPR) 是一种用于信息检索的深度学习方法，尤其适合回答开放领域的问题。DPR通过双塔结构（dual encoder）的神经网络模型来实现文本的向量化表示，主要由两部分组成：一个查询编码器和一个文档编码器。这两个编码器通常使用预训练的BERT模型或其他Transformer模型，分别对用户查询和候选文档进行编码，生成固定维度的密集向量（dense vector）。</p></li>
<li><p>通过在大规模数据集（例如，Wikipedia passages）上进行监督学习训练，DPR可以在各种任务（如问答系统、文档检索）中实现高效和准确的文本检索。相比于传统的稀疏向量检索（如TF-IDF或BM25），DPR的密集表示可以更好地捕捉词汇的语义相似性，因此在需要更精准语义匹配的任务中表现更佳。</p></li>
<li><p>DPR的工作原理如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. 查询编码：将用户输入的自然语言查询（如问题或关键词）转化为密集向量。
2. 文档编码：将候选文档库中的每个文档转化为密集向量。
3. 相似度计算：利用向量相似度度量（如内积或余弦相似度）计算查询与文档之间的相似度分数，从而选出最相关的文档。
</pre></div>
</div>
</li>
</ul>
<p>Related Models:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>dpr-reader-single-nq-base
dpr-ctx_encoder-single-nq-base
dpr-question_encoder-single-nq-base

dpr-ctx_encoder-multiset-base
dpr-question_encoder-multiset-base
dpr-reader-multiset-base

说明:
    single-nq-base: 适合专注于Natural Questions（NQ）数据集的问题回答，通常在特定领域的问答任务中表现更佳
    multiset-base: 在多个数据集上训练，具备跨领域问答的能力，更加通用
</pre></div>
</div>
<ul class="simple">
<li><p>dpr-reader: 用于答案抽取的阅读器模型。它在DPR的检索后处理阶段使用，以从选定的候选段落中找到具体的答案。这种模型基于BERT或类似的Transformer架构，经过训练可以从候选文本中提取出最有可能的答案片段。一般来说，它依赖于查询编码器和文档编码器筛选出的候选段落作为输入。</p></li>
<li><p>dpr-ctx_encoder: 用于将文档或段落编码为密集向量的文档（或上下文）编码器。其作用是将大量候选文档或段落转换为固定维度的密集向量表示。通过这种方式，系统可以利用向量检索方法（如内积或余弦相似度）在大规模文档库中快速检索出与查询最相关的文档。</p></li>
<li><p>dpr-question_encoder: 用于将用户问题编码为密集向量的查询编码器。这个模型将自然语言查询转换为密集向量，以便与文档编码器生成的文档向量进行相似度匹配，找到最相关的候选文档。</p></li>
</ul>
<p>DPR 模型的流程:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Step 1: 用户输入的问题通过 dpr-question_encoder 编码为密集向量。
Step 2: 候选文档通过 dpr-ctx_encoder 编码为密集向量。
Step 3: 通过计算查询向量与文档向量之间的相似度（通常是内积），检索到相关度最高的候选文档。
Step 4: 将这些候选文档交给 dpr-reader，由其从中抽取具体答案片段。
</pre></div>
</div>
</section>
</section>
</section>
</section>
<section id="fromgpt">
<h2>其他-fromGPT<a class="headerlink" href="#fromgpt" title="此标题的永久链接">¶</a></h2>
<p>Transformer 的主要组成部分包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Attention Block、
    注意力模块，最核心的模块
2. Feed-Forward Network (FFN)
    在每个 Transformer 层中，注意力模块后面是一个前馈神经网络（Feed-Forward Network），通常是一个两层的全连接网络
    FFN 对每个位置独立操作，主要负责非线性变换，增强模型的表示能力。
    FFN 的参数规模通常很大，因此也是计算开销的一个主要来源。
3. Embedding Layer
    Embedding 层用于将输入的离散标记（tokens）转化为连续的向量表示。
    在 NLP 模型中，输入和输出通常都有 embedding 层，分别将输入标记转为向量和将输出向量转回标记。
    对于 NLP 模型，词汇表的大小会影响 embedding 层的参数规模。
4. Positional Encoding
    Transformer 不具备内在的序列位置感知，因此需要加入位置编码（positional encoding）来表示序列中每个位置的信息。
    位置编码可以是固定的（例如通过正弦和余弦函数生成）或是可训练的向量。
    虽然位置编码不包含很多参数，但它在 Transformer 中至关重要，帮助模型区分序列中的位置。
5. Layer Normalization
    每个 Transformer 层通常包含一个或多个层归一化（Layer Normalization）步骤，用于加速训练收敛，稳定模型性能。
    虽然 Layer Normalization 的参数很少，但对模型的稳定性和性能有较大影响。
6. Residual Connections
    在每个模块（如 Attention 和 FFN）中，通常使用残差连接（residual connections）来连接输入和输出，以避免梯度消失和梯度爆炸问题。
    残差连接本身没有参数，但在计算图中增加了对输入的直接引用。
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../Tokenizers_V0.13.3.html" class="btn btn-neutral float-right" title="7.3.3. Tokenizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Transformers.html" class="btn btn-neutral" title="Transformers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>