

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7.1.7. PEFT &mdash; 新溪-gordon V1.7.17 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="7.1.8. 收集" href="collect.html" />
    <link rel="prev" title="7.1.6. Evaluate" href="Evaluate.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V1.7.17
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/concept.html">1.3. 关键定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/ml.html">1.4. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/bi.html">1.5. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/deep_learning.html">1.6. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/normal.html">1.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/history.html">1.6.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/monitor.html">1.7. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/algorithm.html">1.8. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/tool.html">1.9. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/question.html">1.10. 常见问题</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../theories/ReAct.html">2.1. ReAct框架</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/Reflection.html">2.2. Reflection反思</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/math.html">2.3. 数学</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/bag-of-words.html">2.4. bag-of-words</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/word2vec.html">2.5. Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/doc2vec.html">2.6. Doc2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/FastText.html">2.7. FastText</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/LDA.html">2.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/overfitting-underfitting.html">2.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/evaluate.html">2.10. evaluate评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/RAG.html">2.11. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/Agent.html">2.12. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/LLM.html">2.13. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/prompt_engineering.html">2.14. Prompt Engineering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/LLaMA.html">3.2.1. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatGLM.html">3.2.2. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BERT.html">3.2.3. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/GPT.html">3.2.4. GPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BART.html">3.2.5. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/T5.html">3.2.6. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatRWKV.html">3.2.7. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Open-Assistant.html">3.2.8. Open-Assistant</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/fileformat.html">3.4. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/GGML.html">3.4.1. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/ONNX.html">3.4.2. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/NCNN.html">3.4.3. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/openai.html">3.5. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/normal.html">3.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/openai.html">3.5.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/prompt.html">3.6. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_chinese.html">3.6.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_english.html">3.6.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/skill.html">3.6.3. 示例</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../NLP.html">4. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/normal.html">4.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/preprocess.html">4.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/normal.html">4.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">4.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">4.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">4.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">4.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">4.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">4.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/NER.html">4.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/normal.html">4.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/seq-label.html">4.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/BiLSTM%2BCRF.html">4.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/history.html">4.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/summary.html">4.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/summarys/normal.html">4.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">5. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Image.html">5.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Video.html">5.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/IPython.html">5.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/normal.html">5.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/magic.html">5.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/display.html">5.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Jupyter.html">5.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/NumPy.html">5.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/normal.html">5.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/Ndarray.html">5.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/function.html">5.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Pandas.html">5.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/normal.html">5.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_subset.html">5.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_analysis.html">5.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_sql.html">5.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_default_value.html">5.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_multi_index.html">5.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/practice.html">5.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_input_output.html">5.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_General.html">5.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Series.html">5.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_DataFrame.html">5.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Index.html">5.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Matplotlib.html">5.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/normal.html">5.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/install.html">5.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pyplot.html">5.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/matplotlib.patches.html">5.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/example.html">5.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pylab.html">5.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/SciPy.html">5.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/SciPys/normal.html">5.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/sklearn.html">5.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/normal.html">5.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/supervised.html">5.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/unsupervised.html">5.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/statsmodels.html">5.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/OpenCV.html">5.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/normal.html">5.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/example.html">5.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/struct.html">5.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Seaborn.html">5.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Seaborns/normal.html">5.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/jieba.html">5.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/gensim.html">5.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/normal.html">5.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Core_Tutorials.html">5.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Tutorials.html">5.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/How-to_Guides.html">5.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/LAC.html">5.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../framework.html">6. 学习框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/mxnet.html">6.2. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/ndarray.html">6.2.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/gluon.html">6.2.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/autograd.html">6.2.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/pytorch.html">6.3. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/pytorchs/normal.html">6.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/pytorchs/nn.html">6.3.2. nn模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/tensorflow.html">6.4. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/Keras.html">6.5. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/Keras/normal.html">6.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/Keras/demo.html">6.5.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/other.html">6.6. 其他</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../website.html">7. 关键网站</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../huggingface.html">7.1. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="normal.html">7.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="huggingface_hub.html">7.1.2. Hugging Face Hub</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib_python.html">7.1.3. Hub Python Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="Datasets.html">7.1.4. Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="Transformers.html">7.1.5. Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="Evaluate.html">7.1.6. Evaluate</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">7.1.7. PEFT</a></li>
<li class="toctree-l3"><a class="reference internal" href="collect.html">7.1.8. 收集</a><ul>
<li class="toctree-l4"><a class="reference internal" href="collects/dataset.html">dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/evaluate.html">evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/model.html">model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Papers%20with%20Code.html">7.2. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Kaggle.html">7.3. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ArXiv.html">7.4. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../video.html">7.5. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normal.html">7.6. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../concept.html">8. 关键定义</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/LLM.html">8.1. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Sigmoid.html">8.2. Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/RELU.html">8.3. ReLU(激活函数)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Leaky-ReLU.html">8.4. Leaky ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Tanh.html">8.5. Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/GELU.html">8.6. GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">8.7. HMM-隐马尔可夫模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/WWM-%E5%85%A8%E8%AF%8DMask.html">8.8. WWM-Whole Word Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/CRF-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html">8.9. CRF-条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/MLE-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.html">8.10. MLE-最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/ANN.html">8.11. ANN(NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.12. 深度神经网络(Deep Neural Network, DNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.13. 卷积神经网络(Convolutional Neural Network, CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91.html">8.14. RNN: 循环神经网(Recurrent Neural Network, RNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/LSTM-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86.html">8.15. LSTM: 长短时记忆(Long Short Term Memory, LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/others/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8Bvs%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B.html">8.16. 判别式模型vs生成式模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/others/Embedding%E6%A8%A1%E5%9E%8B.html">8.17. Embedding 模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Evaluate.html">10.9. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/platform.html">10.10. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese.html">11.2. 中文数据集</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Evaluate.html">12. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/TruLens.html">12.1. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/Ragas.html">12.2. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/DeepEval.html">12.3. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/UpTrain.html">12.4. UpTrain</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../website.html"><span class="section-number">7. </span>关键网站</a> &raquo;</li>
        
          <li><a href="../huggingface.html"><span class="section-number">7.1. </span>huggingface</a> &raquo;</li>
        
      <li><span class="section-number">7.1.7. </span>PEFT</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/websites/huggingfaces/PEFT.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">7.1.7. PEFT</a><ul>
<li><a class="reference internal" href="#get-started">GET STARTED</a><ul>
<li><a class="reference internal" href="#id2">安装</a></li>
<li><a class="reference internal" href="#quicktour">Quicktour</a><ul>
<li><a class="reference internal" href="#peftconfig">PeftConfig</a></li>
<li><a class="reference internal" href="#peftmodel">PeftModel</a></li>
<li><a class="reference internal" href="#save-and-load-a-model">Save and load a model</a></li>
<li><a class="reference internal" href="#easy-loading-with-auto-classes">Easy loading with Auto classes</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#task-guides">TASK GUIDES</a><ul>
<li><a class="reference internal" href="#image-classification-using-lora">Image classification using LoRA</a><ul>
<li><a class="reference internal" href="#install-dependencies">Install dependencies</a></li>
<li><a class="reference internal" href="#a-helper-function">a helper function</a></li>
<li><a class="reference internal" href="#load-and-prepare-a-model">Load and prepare a model</a></li>
<li><a class="reference internal" href="#run-inference">run inference</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prefix-tuning-for-conditional-generation">Prefix tuning for conditional generation</a><ul>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#load-dataset">Load dataset</a></li>
<li><a class="reference internal" href="#preprocess-dataset">Preprocess dataset</a></li>
<li><a class="reference internal" href="#train-model">Train model</a></li>
<li><a class="reference internal" href="#inference">Inference</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prompt-tuning-for-causal-language-modeling">Prompt tuning for causal language modeling</a><ul>
<li><a class="reference internal" href="#id3">Setup</a></li>
<li><a class="reference internal" href="#id4">Load dataset</a></li>
<li><a class="reference internal" href="#train">Train</a></li>
<li><a class="reference internal" href="#id5">Inference</a></li>
</ul>
</li>
<li><a class="reference internal" href="#semantic-segmentation-using-lora">Semantic segmentation using LoRA</a><ul>
<li><a class="reference internal" href="#prepare-datasets-for-training-and-evaluation">Prepare datasets for training and evaluation</a></li>
<li><a class="reference internal" href="#load-a-base-model">Load a base model</a></li>
<li><a class="reference internal" href="#wrap-the-base-model-as-a-peftmodel-for-lora-training">Wrap the base model as a PeftModel for LoRA training</a></li>
</ul>
</li>
<li><a class="reference internal" href="#p-tuning-for-sequence-classification">P-tuning for sequence classification</a><ul>
<li><a class="reference internal" href="#id6">Setup</a></li>
<li><a class="reference internal" href="#load-dataset-and-metric">Load dataset and metric</a></li>
<li><a class="reference internal" href="#id7">Train</a></li>
<li><a class="reference internal" href="#id8">Inference</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#conceptual-guides">CONCEPTUAL GUIDES</a><ul>
<li><a class="reference internal" href="#lora">LoRA</a><ul>
<li><a class="reference internal" href="#merge-lora-weights-into-the-base-model">Merge LoRA weights into the base model</a></li>
<li><a class="reference internal" href="#utils-for-lora">Utils for LoRA</a></li>
<li><a class="reference internal" href="#common-lora-parameters-in-peft">Common LoRA parameters in PEFT</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prompting">Prompting</a><ul>
<li><a class="reference internal" href="#prompt-tuning">Prompt tuning</a></li>
<li><a class="reference internal" href="#prefix-tuning">Prefix tuning</a></li>
<li><a class="reference internal" href="#p-tuning">P-tuning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ia3">IA3</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9">参考</a></li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="peft">
<h1><span class="section-number">7.1.7. </span>PEFT<a class="headerlink" href="#peft" title="此标题的永久链接">¶</a></h1>
<section id="get-started">
<h2>GET STARTED<a class="headerlink" href="#get-started" title="此标题的永久链接">¶</a></h2>
<p>PEFT, or Parameter-Efficient Fine-Tuning (PEFT), is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model’s parameters.</p>
<p>Supported methods:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LoRA</span>
<span class="n">Prefix</span> <span class="n">Tuning</span>
<span class="n">P</span><span class="o">-</span><span class="n">Tuning</span>
<span class="n">Prompt</span> <span class="n">Tuning</span>
<span class="n">AdaLoRA</span>
<span class="n">LLaMA</span><span class="o">-</span><span class="n">Adapter</span>
<span class="n">IA3</span>
</pre></div>
</div>
<section id="id2">
<h3>安装<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">peft</span>

<span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">peft</span>
</pre></div>
</div>
</section>
<section id="quicktour">
<h3>Quicktour<a class="headerlink" href="#quicktour" title="此标题的永久链接">¶</a></h3>
<section id="peftconfig">
<h4>PeftConfig<a class="headerlink" href="#peftconfig" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">TaskType</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
        <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_2_SEQ_LM</span><span class="p">,</span>
        <span class="n">inference_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">r</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="peftmodel">
<h4>PeftModel<a class="headerlink" href="#peftmodel" title="此标题的永久链接">¶</a></h4>
<p>loading the base model you want to finetune:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/mt0-large&quot;</span>
<span class="n">tokenizer_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/mt0-large&quot;</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
</pre></div>
</div>
<p>Wrap your base model and peft_config with the get_peft_model function to create a PeftModel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">get_peft_model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;output: trainable params: 2359296 || all params: 1231940608 || trainable%: 0.19151053100118282&quot;</span>
</pre></div>
</div>
</section>
<section id="save-and-load-a-model">
<h4>Save and load a model<a class="headerlink" href="#save-and-load-a-model" title="此标题的永久链接">¶</a></h4>
<p>save your model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;output_dir&quot;</span><span class="p">)</span>

<span class="c1"># if pushing to Hub</span>
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">notebook_login</span>

<span class="n">notebook_login</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;my_awesome_peft_model&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>load your model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>  <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>
<span class="o">+</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">PeftConfig</span>

<span class="o">+</span> <span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;smangrul/twitter_complaints_bigscience_T0_3B_LORA_SEQ_2_SEQ_LM&quot;</span>
<span class="o">+</span> <span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
  <span class="n">msg</span> <span class="o">=</span> <span class="s2">&quot;Tweet text : @Honda Service has been horrible during the recall process... Label :&quot;</span>
  <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
  <span class="c1"># 输出</span>
  <span class="s1">&#39;complaint&#39;</span>
</pre></div>
</div>
</section>
<section id="easy-loading-with-auto-classes">
<h4>Easy loading with Auto classes<a class="headerlink" href="#easy-loading-with-auto-classes" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">PeftModel</span>
<span class="o">-</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
<span class="o">+</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">AutoPeftModelForCausalLM</span>

<span class="o">-</span> <span class="n">peft_config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ybelkada/opt-350m-lora&quot;</span><span class="p">)</span>
<span class="o">-</span> <span class="n">base_model_path</span> <span class="o">=</span> <span class="n">peft_config</span><span class="o">.</span><span class="n">base_model_name_or_path</span>
<span class="o">-</span> <span class="n">transformers_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model_path</span><span class="p">)</span>
<span class="o">-</span> <span class="n">peft_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="o">+</span> <span class="n">peft_model</span> <span class="o">=</span> <span class="n">AutoPeftModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;ybelkada/opt-350m-lora&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Currently, supported auto classes are:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoPeftModelForCausalLM</span>
<span class="n">AutoPeftModelForSequenceClassification</span>
<span class="n">AutoPeftModelForSeq2SeqLM</span>
<span class="n">AutoPeftModelForTokenClassification</span>
<span class="n">AutoPeftModelForQuestionAnswering</span> <span class="ow">and</span>
<span class="n">AutoPeftModelForFeatureExtraction</span>
</pre></div>
</div>
<p>For other tasks (e.g. Whisper, StableDiffusion), you can load the model with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">PeftConfig</span>
<span class="o">+</span> <span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">AutoPeftModel</span>
<span class="o">-</span> <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">WhisperForConditionalGeneration</span>

<span class="o">-</span> <span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;smangrul/openai-whisper-large-v2-LORA-colab&quot;</span>

<span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;smangrul/openai-whisper-large-v2-LORA-colab&quot;</span>
<span class="o">-</span> <span class="n">peft_config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
<span class="o">-</span> <span class="n">model</span> <span class="o">=</span> <span class="n">WhisperForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
<span class="o">-</span>     <span class="n">peft_config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">,</span> <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span>
<span class="o">-</span> <span class="p">)</span>
<span class="o">-</span> <span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
<span class="o">+</span> <span class="n">model</span> <span class="o">=</span> <span class="n">AutoPeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="task-guides">
<h2>TASK GUIDES<a class="headerlink" href="#task-guides" title="此标题的永久链接">¶</a></h2>
<section id="image-classification-using-lora">
<h3>Image classification using LoRA<a class="headerlink" href="#image-classification-using-lora" title="此标题的永久链接">¶</a></h3>
<section id="install-dependencies">
<h4>Install dependencies<a class="headerlink" href="#install-dependencies" title="此标题的永久链接">¶</a></h4>
<p>安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install transformers accelerate evaluate datasets peft -q
</pre></div>
</div>
<p>Check:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">import</span> <span class="nn">accelerate</span>
<span class="kn">import</span> <span class="nn">peft</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Transformers version: </span><span class="si">{</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accelerate version: </span><span class="si">{</span><span class="n">accelerate</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;PEFT version: </span><span class="si">{</span><span class="n">peft</span><span class="o">.</span><span class="n">__version__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;Transformers version: 4.27.4&quot;</span>
<span class="s2">&quot;Accelerate version: 0.18.0&quot;</span>
<span class="s2">&quot;PEFT version: 0.2.0&quot;</span>
</pre></div>
</div>
</section>
<section id="a-helper-function">
<h4>a helper function<a class="headerlink" href="#a-helper-function" title="此标题的永久链接">¶</a></h4>
<p>helper function to check the total number of parameters a model has:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">trainable_params</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">all_param</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="n">all_param</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="n">trainable_params</span> <span class="o">+=</span> <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;trainable params: </span><span class="si">{</span><span class="n">trainable_params</span><span class="si">}</span>
                <span class="o">||</span> <span class="nb">all</span> <span class="n">params</span><span class="p">:</span> <span class="p">{</span><span class="n">all_param</span><span class="p">}</span>
                <span class="o">||</span> <span class="n">trainable</span><span class="o">%</span><span class="p">:</span> <span class="p">{</span><span class="mi">100</span> <span class="o">*</span> <span class="n">trainable_params</span> <span class="o">/</span> <span class="n">all_param</span><span class="p">:</span><span class="mf">.2</span><span class="n">f</span><span class="p">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>生成model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/vit-base-patch16-224-in21k&quot;</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">model_checkpoint</span><span class="p">,</span>
    <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
    <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
    <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># provide this in case you&#39;re planning to fine-tune an already fine-tuned checkpoint</span>
<span class="p">)</span>
</pre></div>
</div>
<p>使用示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="go">&quot;trainable params: 85876325 || all params: 85876325 || trainable%: 100.00&quot;</span>
</pre></div>
</div>
</section>
<section id="load-and-prepare-a-model">
<h4>Load and prepare a model<a class="headerlink" href="#load-and-prepare-a-model" title="此标题的永久链接">¶</a></h4>
<p>use <code class="docutils literal notranslate"><span class="pre">get_peft_model</span></code> to wrap the base model so that “update” matrices are added to the respective places:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">modules_to_save</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;classifier&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
<p>参数说明:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>r:
        The dimension used by the LoRA update matrices.
alpha:
        Scaling factor.
bias:
        Specifies if the bias parameters should be trained.
        None denotes none of the bias parameters will be trained.
target_modules:
        We’re only interested in targeting the query and value matrices
                of the attention blocks of the base model.
modules_to_save:
        we want the `classifier` parameters to be trained too
                when fine-tuning the base model on our custom dataset.
        ensure that the `classifier` parameters are also trained
        ensures that these modules are serialized alongside the LoRA trainable parameters
                when using utilities like save_pretrained() and push_to_hub().
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>r and alpha together control the total number of final trainable parameters giving you the flexibility to balance a trade-off between end performance  and compute efficiency.</p>
</div>
<p>get a new model where only the LoRA parameters are trainable (so-called “update matrices”) while the pre-trained parameters are kept frozen:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">lora_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>打印新模型(可训练参数变少好多):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">lora_model</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;trainable params: 667493 || all params: 86466149 || trainable%: 0.77&quot;</span>
</pre></div>
</div>
</section>
<section id="run-inference">
<h4>run inference<a class="headerlink" href="#run-inference" title="此标题的永久链接">¶</a></h4>
<p>load the LoRA updated parameters along with our base model for inference:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">PeftModel</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">repo_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">,</span>
    <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
    <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
    <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># provide this in case you&#39;re planning to fine-tune an already fine-tuned checkpoint</span>
<span class="p">)</span>
<span class="c1"># Load the LoRA model</span>
<span class="n">inference_model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">repo_name</span><span class="p">)</span>
</pre></div>
</div>
<p>fetch an example image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg&quot;</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">raw</span><span class="p">)</span>
<span class="n">image</span>
</pre></div>
</div>
<p>instantiate an image_processor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">repo_name</span><span class="p">)</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="n">image_processor</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">),</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>run inference:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">inference_model</span><span class="p">(</span><span class="o">**</span><span class="n">encoding</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>

<span class="n">predicted_class_idx</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted class:&quot;</span><span class="p">,</span> <span class="n">inference_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span><span class="p">[</span><span class="n">predicted_class_idx</span><span class="p">])</span>
<span class="s2">&quot;Predicted class: beignets&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="prefix-tuning-for-conditional-generation">
<h3>Prefix tuning for conditional generation<a class="headerlink" href="#prefix-tuning-for-conditional-generation" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or <strong>prefix</strong>.</p></li>
<li><p>Only the prefix parameters are optimized and added to the hidden states in every layer of the model.</p></li>
<li><p>The tokens of the input sequence can still attend to the prefix as <strong>virtual tokens</strong>.</p></li>
</ul>
<section id="setup">
<h4>Setup<a class="headerlink" href="#setup" title="此标题的永久链接">¶</a></h4>
<p>defining the model and tokenizer, text and label columns, and some hyperparameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;TOKENIZERS_PARALLELISM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;false&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;3&quot;</span>

<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span>
<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;t5-large&quot;</span>
<span class="n">tokenizer_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;t5-large&quot;</span>

<span class="n">text_column</span> <span class="o">=</span> <span class="s2">&quot;sentence&quot;</span>
<span class="n">label_column</span> <span class="o">=</span> <span class="s2">&quot;text_label&quot;</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
</pre></div>
</div>
</section>
<section id="load-dataset">
<h4>Load dataset<a class="headerlink" href="#load-dataset" title="此标题的永久链接">¶</a></h4>
<p>数据集是情感分析数据集:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;financial_phrasebank&quot;</span><span class="p">,</span> <span class="s2">&quot;sentences_allagree&quot;</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="p">{</span>
        <span class="s2">&quot;sentence&quot;</span><span class="p">:</span> <span class="s2">&quot;Profit before taxes was EUR 4.0 mn , down from EUR 4.9 mn .&quot;</span><span class="p">,</span>
        <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;text_label&quot;</span><span class="p">:</span> <span class="s2">&quot;negative&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>label标签:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">]</span><span class="o">.</span>
<span class="c1"># 输出</span>
<span class="p">[</span><span class="s1">&#39;negative&#39;</span><span class="p">,</span> <span class="s1">&#39;neutral&#39;</span><span class="p">,</span> <span class="s1">&#39;positive&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="preprocess-dataset">
<h4>Preprocess dataset<a class="headerlink" href="#preprocess-dataset" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="n">text_column</span><span class="p">]</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">examples</span><span class="p">[</span><span class="n">label_column</span><span class="p">]</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
    <span class="n">labels</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
    <span class="n">model_inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">return</span> <span class="n">model_inputs</span>

<span class="n">processed_datasets</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">preprocess_function</span><span class="p">,</span>
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_proc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">remove_columns</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">column_names</span><span class="p">,</span>
    <span class="n">load_from_cache_file</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Running tokenizer on dataset&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Create a DataLoader from the train and eval datasets:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">processed_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">processed_datasets</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">]</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">default_data_collator</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">eval_dataset</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">default_data_collator</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="train-model">
<h4>Train model<a class="headerlink" href="#train-model" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">peft_config</span> <span class="o">=</span> <span class="n">PrefixTuningConfig</span><span class="p">(</span>
        <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">SEQ_2_SEQ_LM</span><span class="p">,</span>
        <span class="n">inference_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">num_virtual_tokens</span><span class="o">=</span><span class="mi">20</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;trainable params: 983040 || all params: 738651136 || trainable%: 0.13308583065659835&quot;</span>
</pre></div>
</div>
<p>Setup the optimizer and learning rate scheduler:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">num_training_steps</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>write a training loop to begin:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">eval_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">eval_preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">eval_preds</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">eval_epoch_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)</span>
    <span class="n">eval_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eval_epoch_loss</span><span class="p">)</span>
    <span class="n">train_epoch_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    <span class="n">train_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">train_epoch_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">epoch</span><span class="si">=}</span><span class="s2">: </span><span class="si">{</span><span class="n">train_ppl</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">train_epoch_loss</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">eval_ppl</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">eval_epoch_loss</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>model performs on the validation set:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">true</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">eval_preds</span><span class="p">,</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;validation&quot;</span><span class="p">][</span><span class="s2">&quot;text_label&quot;</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">pred</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="o">==</span> <span class="n">true</span><span class="o">.</span><span class="n">strip</span><span class="p">():</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span> <span class="o">*</span> <span class="mi">100</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">accuracy</span><span class="si">=}</span><span class="s2"> % on the evaluation dataset&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">eval_preds</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">][</span><span class="s1">&#39;text_label&#39;</span><span class="p">][:</span><span class="mi">10</span><span class="p">]</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="s2">&quot;accuracy=97.3568281938326 </span><span class="si">% o</span><span class="s2">n the evaluation dataset&quot;</span>
<span class="s2">&quot;eval_preds[:10]=[&#39;neutral&#39;, &#39;positive&#39;, ...]&quot;</span>
<span class="s2">&quot;dataset[&#39;validation&#39;][&#39;text_label&#39;][:10]=[&#39;neutral&#39;, &#39;positive&#39;, ...]&quot;</span>
</pre></div>
</div>
</section>
<section id="inference">
<h4>Inference<a class="headerlink" href="#inference" title="此标题的永久链接">¶</a></h4>
<p>Load the configuration and model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">PeftConfig</span>
<span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;stevhliu/t5-large_PREFIX_TUNING_SEQ2SEQ&quot;</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Get and tokenize some text about financial news:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="s2">&quot;The Lithuanian beer market made up 14.41 million liters in January ,</span>
        <span class="n">a</span> <span class="n">rise</span> <span class="n">of</span> <span class="mf">0.8</span> <span class="n">percent</span> <span class="kn">from</span> <span class="nn">the</span> <span class="n">year</span><span class="o">-</span><span class="n">earlier</span> <span class="n">figure</span> <span class="p">,</span>
        <span class="n">the</span> <span class="n">Lithuanian</span> <span class="n">Brewers</span> <span class="s1">&#39; Association reporting citing the results from its members .&quot;,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>generate the predicted text sentiment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">],</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="p">[</span><span class="s2">&quot;positive&quot;</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="prompt-tuning-for-causal-language-modeling">
<h3>Prompt tuning for causal language modeling<a class="headerlink" href="#prompt-tuning-for-causal-language-modeling" title="此标题的永久链接">¶</a></h3>
<section id="id3">
<h4>Setup<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloomz-560m&quot;</span>
<span class="n">tokenizer_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloomz-560m&quot;</span>
<span class="n">dataset_name</span> <span class="o">=</span> <span class="s2">&quot;twitter_complaints&quot;</span>
<span class="n">checkpoint_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">model_name_or_path</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">peft_config</span><span class="o">.</span><span class="n">peft_type</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">peft_config</span><span class="o">.</span><span class="n">task_type</span><span class="si">}</span><span class="s2">_v1.pt&quot;</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span>
    <span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span>
<span class="p">)</span>
<span class="n">text_column</span> <span class="o">=</span> <span class="s2">&quot;Tweet text&quot;</span>
<span class="n">label_column</span> <span class="o">=</span> <span class="s2">&quot;text_label&quot;</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">3e-2</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
</pre></div>
</div>
</section>
<section id="id4">
<h4>Load dataset<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;ought/raft&quot;</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="c1"># 输出</span>
<span class="p">{</span><span class="s2">&quot;Tweet text&quot;</span><span class="p">:</span> <span class="s2">&quot;@HMRCcustomers No this is my first job&quot;</span><span class="p">,</span> <span class="s2">&quot;ID&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Label&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="train">
<h4>Train<a class="headerlink" href="#train" title="此标题的永久链接">¶</a></h4>
<p>peft_config:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">peft_config</span> <span class="o">=</span> <span class="n">PromptTuningConfig</span><span class="p">(</span>
    <span class="n">task_type</span><span class="o">=</span><span class="n">TaskType</span><span class="o">.</span><span class="n">CAUSAL_LM</span><span class="p">,</span>
    <span class="n">prompt_tuning_init</span><span class="o">=</span><span class="n">PromptTuningInit</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span>
    <span class="n">num_virtual_tokens</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">prompt_tuning_init_text</span><span class="o">=</span><span class="s2">&quot;Classify if the tweet is a complaint or not:&quot;</span><span class="p">,</span>
    <span class="n">tokenizer_name_or_path</span><span class="o">=</span><span class="n">model_name_or_path</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Initialize a base model from AutoModelForCausalLM, and pass it and peft_config to the get_peft_model() function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">())</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;trainable params: 8192 || all params: 559222784 || trainable%: 0.0014648902430985358&quot;</span>
</pre></div>
</div>
<p>Setup an optimizer and learning rate scheduler:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">num_training_steps</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_epochs</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<p>start training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">eval_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">eval_preds</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tqdm</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">eval_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="n">eval_preds</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">eval_epoch_loss</span> <span class="o">=</span> <span class="n">eval_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_dataloader</span><span class="p">)</span>
    <span class="n">eval_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">eval_epoch_loss</span><span class="p">)</span>
    <span class="n">train_epoch_loss</span> <span class="o">=</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    <span class="n">train_ppl</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">train_epoch_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">epoch</span><span class="si">=}</span><span class="s2">: </span><span class="si">{</span><span class="n">train_ppl</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">train_epoch_loss</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">eval_ppl</span><span class="si">=}</span><span class="s2"> </span><span class="si">{</span><span class="n">eval_epoch_loss</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Inference<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h4>
<p>load the prompt tuned model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">PeftModel</span><span class="p">,</span> <span class="n">PeftConfig</span>

<span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;stevhliu/bloomz-560m_PROMPT_TUNING_CAUSAL_LM&quot;</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Grab a tweet and tokenize it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>inputs = tokenizer(
    f&#39;{text_column} : {&quot;@nationalgridus I have no water and the bill is current and paid.
         Can you do something about this?&quot;} Label : &#39;,
    return_tensors=&quot;pt&quot;,
)
</pre></div>
</div>
<p>run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>model.to(device)

with torch.no_grad():
    inputs = {k: v.to(device) for k, v in inputs.items()}
    outputs = model.generate(
        input_ids=inputs[&quot;input_ids&quot;],
        attention_mask=inputs[&quot;attention_mask&quot;],
        max_new_tokens=10,
        eos_token_id=3
    )
    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))
[
    &quot;Tweet text : @nationalgridus I have no water and the bill is current and paid.
         Can you do something about this? Label : complaint&quot;
]
</pre></div>
</div>
</section>
</section>
<section id="semantic-segmentation-using-lora">
<h3>Semantic segmentation using LoRA<a class="headerlink" href="#semantic-segmentation-using-lora" title="此标题的永久链接">¶</a></h3>
<section id="prepare-datasets-for-training-and-evaluation">
<h4>Prepare datasets for training and evaluation<a class="headerlink" href="#prepare-datasets-for-training-and-evaluation" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;nvidia/mit-b0&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">reduce_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="load-a-base-model">
<h4>Load a base model<a class="headerlink" href="#load-a-base-model" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSemanticSegmentation</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span> <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span> <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="wrap-the-base-model-as-a-peftmodel-for-lora-training">
<h4>Wrap the base model as a PeftModel for LoRA training<a class="headerlink" href="#wrap-the-base-model-as-a-peftmodel-for-lora-training" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">get_peft_model</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;query&quot;</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">],</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="s2">&quot;lora_only&quot;</span><span class="p">,</span>
    <span class="n">modules_to_save</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;decode_head&quot;</span><span class="p">],</span>
<span class="p">)</span>
<span class="n">lora_model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">print_trainable_parameters</span><span class="p">(</span><span class="n">lora_model</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="p-tuning-for-sequence-classification">
<h3>P-tuning for sequence classification<a class="headerlink" href="#p-tuning-for-sequence-classification" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>P-tuning is a method for automatically searching and optimizing for better prompts in a continuous space.</p></li>
</ul>
<section id="id6">
<h4>Setup<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;roberta-large&quot;</span>
<span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;mrpc&quot;</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
</pre></div>
</div>
</section>
<section id="load-dataset-and-metric">
<h4>Load dataset and metric<a class="headerlink" href="#load-dataset-and-metric" title="此标题的永久链接">¶</a></h4>
<p>Load dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;glue&quot;</span><span class="p">,</span> <span class="n">task</span><span class="p">)</span>
<span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="p">{</span>
    <span class="s2">&quot;sentence1&quot;</span><span class="p">:</span> <span class="s1">&#39;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;</span><span class="p">,</span>
    <span class="s2">&quot;sentence2&quot;</span><span class="p">:</span> <span class="s1">&#39;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#39;</span><span class="p">,</span>
    <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;idx&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>load a metric for evaluating the model’s performance:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;glue&quot;</span><span class="p">,</span> <span class="n">task</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id7">
<h4>Train<a class="headerlink" href="#id7" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">peft_config</span> <span class="o">=</span> <span class="n">PromptEncoderConfig</span><span class="p">(</span><span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;SEQ_CLS&quot;</span><span class="p">,</span> <span class="n">num_virtual_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">encoder_hidden_size</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
<p>wrap the base model and peft_config with get_peft_model() to create a PeftModel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
<span class="c1"># 输出</span>
<span class="s2">&quot;trainable params: 1351938 || all params: 355662082 || trainable%: 0.38011867680626127&quot;</span>
</pre></div>
</div>
</section>
<section id="id8">
<h4>Inference<a class="headerlink" href="#id8" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">peft_model_id</span> <span class="o">=</span> <span class="s2">&quot;smangrul/roberta-large-peft-p-tuning&quot;</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PeftConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">peft_model_id</span><span class="p">)</span>
<span class="n">inference_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">base_model_name_or_path</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">inference_model</span><span class="p">,</span> <span class="n">peft_model_id</span><span class="p">)</span>
</pre></div>
</div>
<p>Get some text and tokenize it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;not equivalent&quot;</span><span class="p">,</span> <span class="s2">&quot;equivalent&quot;</span><span class="p">]</span>

<span class="n">sentence1</span> <span class="o">=</span> <span class="s2">&quot;Coast redwood trees are the tallest trees on the planet and can grow over 300 feet tall.&quot;</span>
<span class="n">sentence2</span> <span class="o">=</span> <span class="s2">&quot;The coast redwood trees, which can attain a height of over 300 feet, are the tallest trees on earth.&quot;</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;longest&quot;</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Pass the inputs to the model to classify the sentences:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">logits</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

<span class="n">paraphrased_text</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">classes</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">classes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">paraphrased_text</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">))</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="s2">&quot;not equivalent: 4%&quot;</span>
<span class="s2">&quot;equivalent: 96%&quot;</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="conceptual-guides">
<h2>CONCEPTUAL GUIDES<a class="headerlink" href="#conceptual-guides" title="此标题的永久链接">¶</a></h2>
<section id="lora">
<h3>LoRA<a class="headerlink" href="#lora" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>To make fine-tuning more efficient, LoRA’s approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition.</p></li>
</ul>
<section id="merge-lora-weights-into-the-base-model">
<h4>Merge LoRA weights into the base model<a class="headerlink" href="#merge-lora-weights-into-the-base-model" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id11">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/Jrryup.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/Jrryup.jpg" />
<figcaption>
<p><span class="caption-text">While LoRA is significantly smaller and faster to train, you may encounter latency issues during inference due to separately loading the base model and the LoRA model. To eliminate latency, use the merge_and_unload() function to merge the adapter weights with the base model which allows you to effectively use the newly merged model as a standalone model.</span><a class="headerlink" href="#id11" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="utils-for-lora">
<h4>Utils for LoRA<a class="headerlink" href="#utils-for-lora" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">merge_adapter</span><span class="p">()</span>
<span class="n">unmerge_adapter</span><span class="p">()</span>
<span class="n">unload</span><span class="p">()</span>
<span class="n">delete_adapter</span><span class="p">()</span>
<span class="n">add_weighted_adapter</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="common-lora-parameters-in-peft">
<h4>Common LoRA parameters in PEFT<a class="headerlink" href="#common-lora-parameters-in-peft" title="此标题的永久链接">¶</a></h4>
<p>fine-tune a model using LoRA, you need to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Instantiate a base model.
2. Create a configuration (`LoraConfig`) where you define LoRA-specific parameters.
3. Wrap the base model with `get_peft_model()` to get a trainable `PeftModel`.
4. Train the `PeftModel` as you normally would train the base model.
</pre></div>
</div>
<p>LoraConfig allows you to control how LoRA is applied to the base model through the following parameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>r: the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.
target_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.
alpha: LoRA scaling factor.
bias: Specifies if the bias parameters should be trained. Can be &#39;none&#39;, &#39;all&#39; or &#39;lora_only&#39;.
modules_to_save: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model’s custom head that is randomly initialized for the fine-tuning task.
layers_to_transform: List of layers to be transformed by LoRA. If not specified, all layers in target_modules are transformed.
layers_pattern: Pattern to match layer names in target_modules, if layers_to_transform is specified. By default PeftModel will look at common layer pattern (layers, h, blocks, etc.), use it for exotic and custom models.
</pre></div>
</div>
</section>
</section>
<section id="prompting">
<h3>Prompting<a class="headerlink" href="#prompting" title="此标题的永久链接">¶</a></h3>
<section id="prompt-tuning">
<h4>Prompt tuning<a class="headerlink" href="#prompt-tuning" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id12">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/L7WpQy.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/L7WpQy.jpg" />
<figcaption>
<p><span class="caption-text">Only train and store a significantly smaller set of task-specific prompt parameters</span><a class="headerlink" href="#id12" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Prompt tuning was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task.</p></li>
<li><p>For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are generated. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.</p></li>
</ul>
</section>
<section id="prefix-tuning">
<h4>Prefix tuning<a class="headerlink" href="#prefix-tuning" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/uEGkK0.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/uEGkK0.jpg" />
</figure>
<ul class="simple">
<li><p>Prefix tuning was designed for natural language generation (NLG) tasks on GPT models.</p></li>
<li><p>It is very similar to prompt tuning; prefix tuning also prepends a sequence of task-specific vectors to the input that can be trained and updated while keeping the rest of the pretrained model’s parameters frozen.</p></li>
<li><p>The main difference is that the prefix parameters are inserted in all of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.</p></li>
</ul>
</section>
<section id="p-tuning">
<h4>P-tuning<a class="headerlink" href="#p-tuning" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/sJkM6X.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/sJkM6X.jpg" />
</figure>
<ul>
<li><p>P-tuning is designed for natural language understanding (NLU) tasks and all language models.</p></li>
<li><p>It is another variation of a soft prompt method; P-tuning also adds a trainable embedding tensor that can be optimized to find better prompts, and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters.</p></li>
<li><p>Unlike prefix tuning though:</p>
<blockquote>
<div><p>the prompt tokens can be inserted anywhere in the input sequence, and it isn’t restricted to only the beginning
the prompt tokens are only added to the input instead of adding them to every layer of the model
introducing anchor tokens can improve performance because they indicate characteristics of a component in the input sequence</p>
</div></blockquote>
</li>
</ul>
</section>
</section>
<section id="ia3">
<h3>IA3<a class="headerlink" href="#ia3" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)</p></li>
</ul>
</section>
</section>
<section id="id9">
<h2>参考<a class="headerlink" href="#id9" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>[论文]Prefix-Tuning: Optimizing Continuous Prompts for Generation: <a class="reference external" href="https://arxiv.org/abs/2101.00190">https://arxiv.org/abs/2101.00190</a></p></li>
<li><p>[论文]LoRA: Low-Rank Adaptation of Large Language Models: <a class="reference external" href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a></p></li>
<li><p>[论文]The Power of Scale for Parameter-Efficient Prompt Tuning: <a class="reference external" href="https://arxiv.org/abs/2104.08691">https://arxiv.org/abs/2104.08691</a></p></li>
<li><p>[论文]GPT Understands, Too(p-tuning): <a class="reference external" href="https://arxiv.org/abs/2103.10385">https://arxiv.org/abs/2103.10385</a></p></li>
<li><p>[论文]LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale: <a class="reference external" href="https://arxiv.org/abs/2208.07339">https://arxiv.org/abs/2208.07339</a></p></li>
<li><p>示例: <a class="reference external" href="https://github.com/huggingface/peft/tree/main/examples">https://github.com/huggingface/peft/tree/main/examples</a></p></li>
</ul>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="collect.html" class="btn btn-neutral float-right" title="7.1.8. 收集" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Evaluate.html" class="btn btn-neutral" title="7.1.6. Evaluate" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2024, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V1.7.17',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>