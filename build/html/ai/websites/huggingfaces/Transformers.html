

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>7.1.5. Transformers &mdash; 新溪-gordon V1.7.17 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="7.1.6. Evaluate" href="Evaluate.html" />
    <link rel="prev" title="7.1.4. Datasets" href="Datasets.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V1.7.17
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/concept.html">1.3. 关键定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/ml.html">1.4. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/bi.html">1.5. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/deep_learning.html">1.6. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/normal.html">1.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../normals/deep_learnings/history.html">1.6.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/monitor.html">1.7. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/algorithm.html">1.8. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/tool.html">1.9. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normals/question.html">1.10. 常见问题</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../theories/ReAct.html">2.1. ReAct框架</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/Reflection.html">2.2. Reflection反思</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/math.html">2.3. 数学</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/bag-of-words.html">2.4. bag-of-words</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/word2vec.html">2.5. Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/doc2vec.html">2.6. Doc2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/FastText.html">2.7. FastText</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/LDA.html">2.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/overfitting-underfitting.html">2.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/evaluate.html">2.10. evaluate评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/RAG.html">2.11. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/Agent.html">2.12. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/LLM.html">2.13. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../theories/prompt_engineering.html">2.14. Prompt Engineering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/LLaMA.html">3.2.1. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatGLM.html">3.2.2. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BERT.html">3.2.3. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/GPT.html">3.2.4. GPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/BART.html">3.2.5. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/T5.html">3.2.6. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/ChatRWKV.html">3.2.7. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/models/Open-Assistant.html">3.2.8. Open-Assistant</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/fileformat.html">3.4. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/GGML.html">3.4.1. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/ONNX.html">3.4.2. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/fileformats/NCNN.html">3.4.3. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/openai.html">3.5. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/normal.html">3.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/openais/openai.html">3.5.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLMs/prompt.html">3.6. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_chinese.html">3.6.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/demo_english.html">3.6.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/prompts/skill.html">3.6.3. 示例</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../NLP.html">4. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/normal.html">4.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/preprocess.html">4.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/normal.html">4.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">4.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">4.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">4.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">4.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">4.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">4.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/NER.html">4.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/normal.html">4.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/seq-label.html">4.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/BiLSTM%2BCRF.html">4.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/NERs/history.html">4.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../NLPs/summary.html">4.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../NLPs/summarys/normal.html">4.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../library.html">5. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Image.html">5.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Video.html">5.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/IPython.html">5.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/normal.html">5.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/magic.html">5.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/IPythons/display.html">5.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Jupyter.html">5.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/NumPy.html">5.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/normal.html">5.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/Ndarray.html">5.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/NumPys/function.html">5.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Pandas.html">5.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/normal.html">5.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_subset.html">5.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_analysis.html">5.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_sql.html">5.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_default_value.html">5.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/example_multi_index.html">5.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/practice.html">5.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_input_output.html">5.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_General.html">5.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Series.html">5.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_DataFrame.html">5.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Pandas/api_Index.html">5.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Matplotlib.html">5.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/normal.html">5.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/install.html">5.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pyplot.html">5.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/matplotlib.patches.html">5.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/example.html">5.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Matplotlibs/pylab.html">5.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/SciPy.html">5.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/SciPys/normal.html">5.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/sklearn.html">5.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/normal.html">5.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/supervised.html">5.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/sklearns/unsupervised.html">5.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/statsmodels.html">5.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/OpenCV.html">5.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/normal.html">5.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/example.html">5.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/OpenCVs/struct.html">5.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/Seaborn.html">5.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/Seaborns/normal.html">5.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/jieba.html">5.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/gensim.html">5.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/normal.html">5.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Core_Tutorials.html">5.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/Tutorials.html">5.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../libraries/gensims/How-to_Guides.html">5.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../libraries/LAC.html">5.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../framework.html">6. 学习框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/mxnet.html">6.2. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/ndarray.html">6.2.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/gluon.html">6.2.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/mxnets/autograd.html">6.2.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/pytorch.html">6.3. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/pytorchs/normal.html">6.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/pytorchs/nn.html">6.3.2. nn模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/tensorflow.html">6.4. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/Keras.html">6.5. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/Keras/normal.html">6.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../frameworks/Keras/demo.html">6.5.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../frameworks/Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../frameworks/other.html">6.6. 其他</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../website.html">7. 关键网站</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../huggingface.html">7.1. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="normal.html">7.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="huggingface_hub.html">7.1.2. Hugging Face Hub</a></li>
<li class="toctree-l3"><a class="reference internal" href="lib_python.html">7.1.3. Hub Python Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="Datasets.html">7.1.4. Datasets</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">7.1.5. Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="Evaluate.html">7.1.6. Evaluate</a></li>
<li class="toctree-l3"><a class="reference internal" href="PEFT.html">7.1.7. PEFT</a></li>
<li class="toctree-l3"><a class="reference internal" href="collect.html">7.1.8. 收集</a><ul>
<li class="toctree-l4"><a class="reference internal" href="collects/dataset.html">dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/evaluate.html">evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="collects/model.html">model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Papers%20with%20Code.html">7.2. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Kaggle.html">7.3. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ArXiv.html">7.4. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../video.html">7.5. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../normal.html">7.6. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../concept.html">8. 关键定义</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/LLM.html">8.1. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Sigmoid.html">8.2. Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/RELU.html">8.3. ReLU(激活函数)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Leaky-ReLU.html">8.4. Leaky ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Tanh.html">8.5. Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/GELU.html">8.6. GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">8.7. HMM-隐马尔可夫模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/WWM-%E5%85%A8%E8%AF%8DMask.html">8.8. WWM-Whole Word Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/CRF-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html">8.9. CRF-条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/ners/MLE-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.html">8.10. MLE-最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/ANN.html">8.11. ANN(NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.12. 深度神经网络(Deep Neural Network, DNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.13. 卷积神经网络(Convolutional Neural Network, CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91.html">8.14. RNN: 循环神经网(Recurrent Neural Network, RNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/dls/LSTM-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86.html">8.15. LSTM: 长短时记忆(Long Short Term Memory, LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/others/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8Bvs%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B.html">8.16. 判别式模型vs生成式模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concepts/others/Embedding%E6%A8%A1%E5%9E%8B.html">8.17. Embedding 模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/Evaluate.html">10.9. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../opensources/platform.html">10.10. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/chinese.html">11.2. 中文数据集</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Evaluate.html">12. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/TruLens.html">12.1. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/Ragas.html">12.2. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/DeepEval.html">12.3. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Evaluates/UpTrain.html">12.4. UpTrain</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../website.html"><span class="section-number">7. </span>关键网站</a> &raquo;</li>
        
          <li><a href="../huggingface.html"><span class="section-number">7.1. </span>huggingface</a> &raquo;</li>
        
      <li><span class="section-number">7.1.5. </span>Transformers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/websites/huggingfaces/Transformers.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">7.1.5. Transformers</a><ul>
<li><a class="reference internal" href="#id2">简介</a></li>
<li><a class="reference internal" href="#get-started">GET STARTED</a><ul>
<li><a class="reference internal" href="#quick-tour">Quick tour</a><ul>
<li><a class="reference internal" href="#pipeline">Pipeline</a></li>
<li><a class="reference internal" href="#autoclass">AutoClass</a></li>
<li><a class="reference internal" href="#custom-model-builds">Custom model builds</a></li>
<li><a class="reference internal" href="#trainer">Trainer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#id3">环境变量</a><ul>
<li><a class="reference internal" href="#fetch-models-and-tokenizers-to-use-offline">Fetch models and tokenizers to use offline</a><ul>
<li><a class="reference internal" href="#use-the-from-pretrained-and-save-pretrained-workflow">Use the from_pretrained() and save_pretrained() workflow</a></li>
<li><a class="reference internal" href="#programmatically-download-files-with-the-huggingface-hub-library">Programmatically download files with the huggingface_hub library</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tutorials">TUTORIALS</a><ul>
<li><a class="reference internal" href="#pipelines-for-inference">Pipelines for inference</a></li>
<li><a class="reference internal" href="#load-pretrained-instances-with-an-autoclass">Load pretrained instances with an AutoClass</a><ul>
<li><a class="reference internal" href="#autotokenizer">AutoTokenizer</a></li>
<li><a class="reference internal" href="#autoimageprocessor">AutoImageProcessor</a></li>
<li><a class="reference internal" href="#autofeatureextractor">AutoFeatureExtractor</a></li>
<li><a class="reference internal" href="#autoprocessor">AutoProcessor</a></li>
<li><a class="reference internal" href="#automodel">AutoModel</a></li>
</ul>
</li>
<li><a class="reference internal" href="#preprocess-data">Preprocess data</a><ul>
<li><a class="reference internal" href="#natural-language-processing">Natural Language Processing</a><ul>
<li><a class="reference internal" href="#pad">Pad</a></li>
<li><a class="reference internal" href="#truncation">Truncation</a></li>
<li><a class="reference internal" href="#build-tensors">Build tensors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#audio">Audio</a><ul>
<li><a class="reference internal" href="#pading">Pading</a></li>
</ul>
</li>
<li><a class="reference internal" href="#computer-vision">Computer vision</a><ul>
<li><a class="reference internal" href="#image-augmentation">image augmentation</a></li>
<li><a class="reference internal" href="#id4">Pading</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tune-a-pretrained-model">Fine-tune a pretrained model</a><ul>
<li><a class="reference internal" href="#train-with-pytorch-trainer">Train with PyTorch Trainer</a><ul>
<li><a class="reference internal" href="#training-hyperparameters">Training hyperparameters</a></li>
<li><a class="reference internal" href="#evaluate">Evaluate</a></li>
<li><a class="reference internal" href="#id6">Trainer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#train-in-native-pytorch">Train in native PyTorch</a><ul>
<li><a class="reference internal" href="#dataloader">DataLoader</a></li>
<li><a class="reference internal" href="#optimizer-and-learning-rate-scheduler">Optimizer and learning rate scheduler</a></li>
<li><a class="reference internal" href="#training-loop">Training loop</a></li>
<li><a class="reference internal" href="#id7">Evaluate</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#train-with-a-script">Train with a script</a></li>
<li><a class="reference internal" href="#distributed-training-with-accelerate">Distributed training with Accelerate</a><ul>
<li><a class="reference internal" href="#backward">Backward</a></li>
</ul>
</li>
<li><a class="reference internal" href="#transformers-agent">Transformers Agent</a><ul>
<li><a class="reference internal" href="#id8">示例</a></li>
<li><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li><a class="reference internal" href="#single-execution-run">Single execution (run)</a></li>
<li><a class="reference internal" href="#chat-based-execution-chat">Chat-based execution (chat)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9">原理</a><ul>
<li><a class="reference internal" href="#agents">Agents</a></li>
<li><a class="reference internal" href="#tools">Tools</a></li>
</ul>
</li>
<li><a class="reference internal" href="#resource">Resource</a><ul>
<li><a class="reference internal" href="#a-curated-set-of-tools">A curated set of tools</a></li>
<li><a class="reference internal" href="#custom-tools">Custom tools</a></li>
</ul>
</li>
<li><a class="reference internal" href="#code-generation">Code generation</a></li>
<li><a class="reference internal" href="#practice">Practice</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#task-guides">TASK GUIDES</a><ul>
<li><a class="reference internal" href="#id11">NATURAL LANGUAGE PROCESSING</a></li>
<li><a class="reference internal" href="#id12">AUDIO</a></li>
<li><a class="reference internal" href="#id13">COMPUTER VISION</a></li>
<li><a class="reference internal" href="#multimodal">MULTIMODAL</a></li>
</ul>
</li>
<li><a class="reference internal" href="#developer-guides">DEVELOPER GUIDES</a><ul>
<li><a class="reference internal" href="#transformers-notebooks-with-examples">Transformers Notebooks with examples</a></li>
<li><a class="reference internal" href="#community-resources">Community resources</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performance-and-scalability">PERFORMANCE AND SCALABILITY</a></li>
<li><a class="reference internal" href="#conceptual-guides">CONCEPTUAL GUIDES</a><ul>
<li><a class="reference internal" href="#philosophy">Philosophy</a><ul>
<li><a class="reference internal" href="#main-concepts">Main concepts</a></li>
</ul>
</li>
<li><a class="reference internal" href="#glossary">Glossary</a></li>
<li><a class="reference internal" href="#how-transformers-solve-tasks">How Transformers solve tasks</a></li>
<li><a class="reference internal" href="#the-transformer-model-family">The Transformer model family</a><ul>
<li><a class="reference internal" href="#id14">Computer vision</a></li>
<li><a class="reference internal" href="#id15">Natural language processing</a></li>
<li><a class="reference internal" href="#id16">Audio</a></li>
<li><a class="reference internal" href="#id17">Multimodal</a></li>
<li><a class="reference internal" href="#reinforcement-learning">Reinforcement learning</a></li>
</ul>
</li>
<li><a class="reference internal" href="#summary-of-the-tokenizers">Summary of the tokenizers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#api">API</a><ul>
<li><a class="reference internal" href="#main-classes">MAIN CLASSES</a><ul>
<li><a class="reference internal" href="#id18">Agents</a></li>
<li><a class="reference internal" href="#auto-classes">Auto Classes</a></li>
<li><a class="reference internal" href="#callbacks">Callbacks</a></li>
<li><a class="reference internal" href="#logging">Logging</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="transformers">
<h1><span class="section-number">7.1.5. </span>Transformers<a class="headerlink" href="#transformers" title="此标题的永久链接">¶</a></h1>
<section id="id2">
<h2>简介<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h2>
<ol class="arabic">
<li><p>GET STARTED:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">provides</span> <span class="n">a</span> <span class="n">quick</span> <span class="n">tour</span> <span class="n">of</span> <span class="n">the</span> <span class="n">library</span> <span class="ow">and</span> <span class="n">installation</span> <span class="n">instructions</span> <span class="n">to</span> <span class="n">get</span> <span class="n">up</span> <span class="ow">and</span> <span class="n">running</span><span class="o">.</span>
</pre></div>
</div>
</li>
<li><p>TUTORIALS:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>are a great place to start if you’re a beginner.
This section will help you gain the basic skills you need to start using the library.
</pre></div>
</div>
</li>
<li><p>HOW-TO GUIDES:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">show</span> <span class="n">you</span> <span class="n">how</span> <span class="n">to</span> <span class="n">achieve</span> <span class="n">a</span> <span class="n">specific</span> <span class="n">goal</span><span class="p">,</span>
<span class="n">like</span> <span class="n">finetuning</span> <span class="n">a</span> <span class="n">pretrained</span> <span class="n">model</span> <span class="k">for</span> <span class="n">language</span> <span class="n">modeling</span>
<span class="ow">or</span> <span class="n">how</span> <span class="n">to</span> <span class="n">write</span> <span class="ow">and</span> <span class="n">share</span> <span class="n">a</span> <span class="n">custom</span> <span class="n">model</span><span class="o">.</span>
</pre></div>
</div>
</li>
<li><p>CONCEPTUAL GUIDES:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>offers more discussion and explanation of the underlying concepts
    and ideas behind models, tasks, and the design philosophy of 🤗 Transformers.
</pre></div>
</div>
</li>
<li><p>API: describes all classes and functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MAIN</span> <span class="n">CLASSES</span>
    <span class="n">details</span> <span class="n">the</span> <span class="n">most</span> <span class="n">important</span> <span class="n">classes</span> <span class="n">like</span> <span class="n">configuration</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="ow">and</span> <span class="n">pipeline</span><span class="o">.</span>
<span class="n">MODELS</span>
    <span class="n">details</span> <span class="n">the</span> <span class="n">classes</span> <span class="ow">and</span> <span class="n">functions</span> <span class="n">related</span> <span class="n">to</span> <span class="n">each</span> <span class="n">model</span> <span class="n">implemented</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">library</span><span class="o">.</span>
<span class="n">INTERNAL</span>
    <span class="n">HELPERS</span> <span class="n">details</span> <span class="n">utility</span> <span class="n">classes</span> <span class="ow">and</span> <span class="n">functions</span> <span class="n">used</span> <span class="n">internally</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="get-started">
<h2>GET STARTED<a class="headerlink" href="#get-started" title="此标题的永久链接">¶</a></h2>
<section id="quick-tour">
<h3>Quick tour<a class="headerlink" href="#quick-tour" title="此标题的永久链接">¶</a></h3>
<p>安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span> <span class="n">datasets</span>
<span class="c1"># optional</span>
<span class="n">sentiment</span>
<span class="n">pip</span> <span class="n">install</span> <span class="n">tensorflow</span>
</pre></div>
</div>
<section id="pipeline">
<h4>Pipeline<a class="headerlink" href="#pipeline" title="此标题的永久链接">¶</a></h4>
<p>Task List:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Task</span>                         <span class="o">|</span> <span class="n">Modality</span>        <span class="o">|</span> <span class="n">Pipeline</span> <span class="n">identifier</span>                           <span class="o">|</span>
<span class="o">+==============================+=================+===============================================+</span>
<span class="o">|</span> <span class="n">Text</span> <span class="n">classification</span>          <span class="o">|</span> <span class="n">NLP</span>             <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">)</span>           <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Text</span> <span class="n">generation</span>              <span class="o">|</span> <span class="n">NLP</span>             <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;text-generation&quot;</span><span class="p">)</span>              <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Summarization</span>                <span class="o">|</span> <span class="n">NLP</span>             <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;summarization&quot;</span><span class="p">)</span>                <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Image</span> <span class="n">classification</span>         <span class="o">|</span> <span class="n">Computer</span> <span class="n">vision</span> <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;image-classification&quot;</span><span class="p">)</span>         <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Image</span> <span class="n">segmentation</span>           <span class="o">|</span> <span class="n">Computer</span> <span class="n">vision</span> <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;image-segmentation&quot;</span><span class="p">)</span>           <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Object</span> <span class="n">detection</span>             <span class="o">|</span> <span class="n">Computer</span> <span class="n">vision</span> <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;object-detection&quot;</span><span class="p">)</span>             <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Audio</span> <span class="n">classification</span>         <span class="o">|</span> <span class="n">Audio</span>           <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;audio-classification&quot;</span><span class="p">)</span>         <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Automatic</span> <span class="n">speech</span> <span class="n">recognition</span> <span class="o">|</span> <span class="n">Audio</span>           <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;automatic-speech-recognition&quot;</span><span class="p">)</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Visual</span> <span class="n">question</span> <span class="n">answering</span>    <span class="o">|</span> <span class="n">Multimodal</span>      <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;vqa&quot;</span><span class="p">)</span>                          <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Document</span> <span class="n">question</span> <span class="n">answering</span>  <span class="o">|</span> <span class="n">Multimodal</span>      <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;document-question-answering&quot;</span><span class="p">)</span>  <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
<span class="o">|</span> <span class="n">Image</span> <span class="n">captioning</span>             <span class="o">|</span> <span class="n">Multimodal</span>      <span class="o">|</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;image-to-text&quot;</span><span class="p">)</span>                <span class="o">|</span>
<span class="o">+------------------------------+-----------------+-----------------------------------------------+</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span><span class="p">)</span>
<span class="go">[{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998}]</span>
</pre></div>
</div>
<p>Example: iterate over an entire dataset of automatic speech:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="c1"># 语音识别pipeline(speech_recognizer)</span>
<span class="n">sr</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;automatic-speech-recognition&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;facebook/wav2vec2-base-960h&quot;</span><span class="p">)</span>

<span class="c1"># 载入数据</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Audio</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;PolyAI/minds14&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;en-US&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># 确保相同的 sampling rate</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast_column</span><span class="p">(</span><span class="s2">&quot;audio&quot;</span><span class="p">,</span> <span class="n">Audio</span><span class="p">(</span><span class="n">sampling_rate</span><span class="o">=</span><span class="n">sr</span><span class="o">.</span><span class="n">feature_extractor</span><span class="o">.</span><span class="n">sampling_rate</span><span class="p">))</span>

<span class="c1"># 执行task</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">sr</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">4</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">([</span><span class="n">d</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">result</span><span class="p">])</span>
</pre></div>
</div>
<p>Example: Use another model and tokenizer in the pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># 指定model和tokenizer</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;sentiment-analysis&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># 执行</span>
<span class="n">classifier</span><span class="p">(</span><span class="s2">&quot;Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="autoclass">
<h4>AutoClass<a class="headerlink" href="#autoclass" title="此标题的永久链接">¶</a></h4>
<p>AutoTokenizer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># Pass your text to the tokenizer:</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="s2">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">encoding</span><span class="p">)</span>
<span class="c1"># {</span>
<span class="c1">#         &#39;input_ids&#39;: [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],</span>
<span class="c1">#        &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="c1">#        &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span>
<span class="c1"># }</span>

<span class="c1"># accept a list of inputs, and pad and truncate the text to return a batch with uniform length</span>
<span class="n">pt_batch</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span><span class="p">,</span> <span class="s2">&quot;We hope you don&#39;t hate it.&quot;</span><span class="p">],</span>
    <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>AutoModel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For text (or sequence) classification, you should load `AutoModelForSequenceClassification`</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="c1"># pass your preprocessed batch of inputs directly to the model</span>
<span class="n">pt_outputs</span> <span class="o">=</span> <span class="n">pt_model</span><span class="p">(</span><span class="o">**</span><span class="n">pt_batch</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="c1"># outputs the final activations in the logits attribute</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pt_predictions</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pt_outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">pt_predictions</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0021</span><span class="p">,</span> <span class="mf">0.0018</span><span class="p">,</span> <span class="mf">0.0115</span><span class="p">,</span> <span class="mf">0.2121</span><span class="p">,</span> <span class="mf">0.7725</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2084</span><span class="p">,</span> <span class="mf">0.1826</span><span class="p">,</span> <span class="mf">0.1969</span><span class="p">,</span> <span class="mf">0.1755</span><span class="p">,</span> <span class="mf">0.2365</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SoftmaxBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<p>Save a model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pt_save_directory</span> <span class="o">=</span> <span class="s2">&quot;./pt_save_pretrained&quot;</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">pt_save_directory</span><span class="p">)</span>
<span class="n">pt_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">pt_save_directory</span><span class="p">)</span>

<span class="c1"># load</span>
<span class="n">pt_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./pt_save_pretrained&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="custom-model-builds">
<h4>Custom model builds<a class="headerlink" href="#custom-model-builds" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 使用AutoConfig加载要修改的预训练模型生成自定义配置</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>
<span class="n">my_config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># 使用AutoModel基于自定义配置创建模型</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>
<span class="n">my_model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">my_config</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="trainer">
<h4>Trainer<a class="headerlink" href="#trainer" title="此标题的永久链接">¶</a></h4>
<ol class="arabic">
<li><p>A PreTrainedModel or a torch.nn.Module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>TrainingArguments 包含可以更改的模型超参数，例如学习率、批量大小和要训练的周期数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>

<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;path/to/save/folder/&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-5</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>预处理类，如tokenizer, image processor, feature extractor, or processor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>加载数据集:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;rotten_tomatoes&quot;</span><span class="p">)</span>  <span class="c1"># doctest: +IGNORE_RESULT</span>
</pre></div>
</div>
</li>
<li><p>使用map应用整个数据集:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_dataset</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_dataset</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>使用DataCollatorWithPadding从数据集创建一批示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorWithPadding</span>
<span class="n">data_collator</span> <span class="o">=</span> <span class="n">DataCollatorWithPadding</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
<p>使用Trainer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Trainer</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">],</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">],</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">data_collator</span><span class="p">,</span>
<span class="p">)</span>  <span class="c1"># doctest: +SKIP</span>
</pre></div>
</div>
<p>开始训练:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="此标题的永久链接">¶</a></h3>
<ul>
<li><p>default install:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span>

<span class="c1"># 验证</span>
<span class="n">python</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;we love you&#39;))&quot;</span>
</pre></div>
</div>
</li>
<li><p>cpu版安装(install Transformers and a deep learning library in one line):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="s1">&#39;transformers[torch]&#39;</span>         <span class="c1"># 安装 🤗 Transformers 和 PyTorch</span>

<span class="n">pip</span> <span class="n">install</span> <span class="s1">&#39;transformers[tf-cpu]&#39;</span>        <span class="c1"># 安装 🤗 Transformers 和 TensorFlow 2.0</span>
</pre></div>
</div>
</li>
<li><p>源码安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">git</span><span class="o">+</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">transformers</span>


<span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">huggingface</span><span class="o">/</span><span class="n">transformers</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">transformers</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
<p>检查是否安装正确:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">c</span> <span class="s2">&quot;from transformers import pipeline; print(pipeline(&#39;sentiment-analysis&#39;)(&#39;we love you&#39;))&quot;</span>

<span class="c1"># 查看版本</span>
<span class="nb">print</span><span class="p">(</span><span class="n">transformers</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span>
</pre></div>
</div>
<p>附加模块:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="s1">&#39;transformers[audio]&#39;</span>
<span class="n">pip</span> <span class="n">install</span> <span class="s1">&#39;transformers[torch]&#39;</span>
<span class="n">pip</span> <span class="n">install</span> <span class="s1">&#39;transformers[tf-cpu]&#39;</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>环境变量<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h3>
<p>使用conda下载的模型文件地址:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;env_path&gt;/lib/pythonX.Y/site-packages/transformers/models
示例:
/home/username/miniconda/envs/myenv/lib/python3.7/site-packages/transformers/models


每个环境都有自己的模型文件副本,并且环境之间相互隔离。
可以通过设置`TRANSFORMERS_CACHE`环境变量来覆盖这一默认行为
</pre></div>
</div>
<section id="fetch-models-and-tokenizers-to-use-offline">
<h4>Fetch models and tokenizers to use offline<a class="headerlink" href="#fetch-models-and-tokenizers-to-use-offline" title="此标题的永久链接">¶</a></h4>
<section id="use-the-from-pretrained-and-save-pretrained-workflow">
<h5>Use the from_pretrained() and save_pretrained() workflow<a class="headerlink" href="#use-the-from-pretrained-and-save-pretrained-workflow" title="此标题的永久链接">¶</a></h5>
<ol class="arabic">
<li><p>Download your files ahead of time with PreTrainedModel.from_pretrained():</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/T0_3B&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/T0_3B&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Save your files to a specified directory with PreTrainedModel.save_pretrained():</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./your/path/bigscience_t0&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./your/path/bigscience_t0&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Now when you’re offline, reload your files with PreTrainedModel.from_pretrained() from the specified directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your/path/bigscience_t0&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your/path/bigscience_t0&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
<section id="programmatically-download-files-with-the-huggingface-hub-library">
<h5>Programmatically download files with the huggingface_hub library<a class="headerlink" href="#programmatically-download-files-with-the-huggingface-hub-library" title="此标题的永久链接">¶</a></h5>
<ol class="arabic">
<li><p>Install the huggingface_hub library in your virtual environment:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">huggingface_hub</span>
</pre></div>
</div>
</li>
<li><p>Use the hf_hub_download function to download a file to a specific path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>

<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;bigscience/T0_3B&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;config.json&quot;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s2">&quot;./your/path/bigscience_t0&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Once your file is downloaded and locally cached, specify it’s local path to load and use it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;./your/path/bigscience_t0/config.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
</section>
</section>
<section id="tutorials">
<h2>TUTORIALS<a class="headerlink" href="#tutorials" title="此标题的永久链接">¶</a></h2>
<section id="pipelines-for-inference">
<h3>Pipelines for inference<a class="headerlink" href="#pipelines-for-inference" title="此标题的永久链接">¶</a></h3>
<p>Start by creating a pipeline() and specify an inference task:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="s2">&quot;automatic-speech-recognition&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Pass your input text to the pipeline():</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">generator</span><span class="p">(</span><span class="s2">&quot;https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac&quot;</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;I HAVE A DREAM BUT ONE DAY THIS NATION WILL RISE UP LIVE UP THE TRUE MEANING OF ITS TREES&#39;</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="load-pretrained-instances-with-an-autoclass">
<h3>Load pretrained instances with an AutoClass<a class="headerlink" href="#load-pretrained-instances-with-an-autoclass" title="此标题的永久链接">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, BERT is an <strong>architecture</strong>, while bert-base-uncased is a <strong>checkpoint</strong>. Model is a general term that can mean either architecture or checkpoint.</p>
</div>
<section id="autotokenizer">
<h4>AutoTokenizer<a class="headerlink" href="#autotokenizer" title="此标题的永久链接">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model.</p>
</div>
<p>Load a tokenizer with AutoTokenizer.from_pretrained():</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>tokenize your input as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;In a hole in the ground there lived a hobbit.&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_input</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="go">{&#39;input_ids&#39;: [101, 1999, 1037, 4920, ...],</span>
<span class="go"> &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, ...],</span>
<span class="go"> &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, ...]}</span>
</pre></div>
</div>
<p>Return your input by decoding the input_ids:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
<span class="go">&quot;[CLS] in a hole in the ground there lived a hobbit.[SEP]&quot;</span>
<span class="go"># 说明: two special tokens</span>
<span class="go"># CLS: classifier</span>
<span class="go"># SEP: separator</span>
</pre></div>
</div>
</section>
<section id="autoimageprocessor">
<h4>AutoImageProcessor<a class="headerlink" href="#autoimageprocessor" title="此标题的永久链接">¶</a></h4>
<p>For vision tasks, an image processor processes the image into the correct input format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/vit-base-patch16-224&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="autofeatureextractor">
<h4>AutoFeatureExtractor<a class="headerlink" href="#autofeatureextractor" title="此标题的永久链接">¶</a></h4>
<p>For audio tasks, a feature extractor processes the audio signal the correct input format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoFeatureExtractor</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="autoprocessor">
<h4>AutoProcessor<a class="headerlink" href="#autoprocessor" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Multimodal tasks require a processor that combines two types of preprocessing tools.</p></li>
<li><p>For example, the <code class="docutils literal notranslate"><span class="pre">LayoutLMV2</span></code> model requires an image processor to handle images and a tokenizer to handle text; a processor combines both of them.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoProcessor</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;microsoft/layoutlmv2-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="automodel">
<h4>AutoModel<a class="headerlink" href="#automodel" title="此标题的永久链接">¶</a></h4>
<p>AutoModelFor classes let you load a pretrained model for a given task(sequence classification):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>reuse the same checkpoint to load an architecture for a different task(token classification):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForTokenClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Generally, we recommend using the AutoTokenizer class and the AutoModelFor class to load pretrained instances of models. This will ensure you load the correct architecture every time.</p>
</div>
</section>
</section>
<section id="preprocess-data">
<h3>Preprocess data<a class="headerlink" href="#preprocess-data" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format.</p></li>
<li><p>Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors.</p></li>
</ul>
<p>Transformers provides a set of preprocessing classes to help prepare your data for the model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. Text
    use `Tokenizer` to convert text into a sequence of tokens, and assemble them into tensors.
2. Speech and audio
    use `Feature` extractor to extract sequential features
        from audio waveforms and convert them into tensors.
3. Image inputs
    use `ImageProcessor` to convert images into tensors.
4. Multimodal inputs,
    use `Processor` to combine a tokenizer and a feature extractor or image processor.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p><code class="docutils literal notranslate"><span class="pre">AutoProcessor</span></code> always works and automatically chooses the correct class for the model you’re using, whether you’re using a tokenizer, image processor, feature extractor or processor.</p>
</div>
<section id="natural-language-processing">
<h4>Natural Language Processing<a class="headerlink" href="#natural-language-processing" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>The main tool for preprocessing textual data is a tokenizer.</p></li>
<li><p>A tokenizer splits text into tokens according to a set of rules.</p></li>
<li><p>The tokens are converted into numbers and then tensors, which become the model inputs.</p></li>
<li><p>Any additional inputs required by the model are added by the tokenizer.</p></li>
</ul>
<section id="pad">
<h5>Pad<a class="headerlink" href="#pad" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Sentences aren’t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape.</p></li>
<li><p>Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;But what about second breakfast?&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;Don&#39;t think he knows about second breakfast, Pip.&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;What about elevensies?&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="go">{&#39;input_ids&#39;: [[101, 1252, 1184, 1164, ..., 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">               [101, 1790, 112, 189, ..., 6462, 117, 21902, 1643, 119, 102],</span>
<span class="go">               [101, 1327, 1164, 545, ..., 0, 0, 0, 0, 0, 0, 0, 0]],</span>
<span class="go"> &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],</span>
<span class="go"> &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span>
<span class="go">                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}</span>
</pre></div>
</div>
</section>
<section id="truncation">
<h5>Truncation<a class="headerlink" href="#truncation" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>On the other end of the spectrum, sometimes a sequence may be too long for a model to handle.</p></li>
<li><p>In this case, you’ll need to truncate the sequence to a shorter length.</p></li>
<li><p>Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;But what about second breakfast?&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;Don&#39;t think he knows about second breakfast, Pip.&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;What about elevensies?&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="go">{&#39;input_ids&#39;: [[101, 1252, 1184, 1164, ..., 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">               [101, 1790, 112, 189, ..., 6462, 117, 21902, 1643, 119, 102],</span>
<span class="go">               [101, 1327, 1164, 545, ..., 0, 0, 0, 0, 0, 0, 0, 0]],</span>
<span class="go"> &#39;token_type_ids&#39;: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],</span>
<span class="go"> &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</span>
<span class="go">                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],</span>
<span class="go">                    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}</span>
</pre></div>
</div>
</section>
<section id="build-tensors">
<h5>Build tensors<a class="headerlink" href="#build-tensors" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Finally, you want the tokenizer to return the actual tensors that get fed to the model.</p></li>
<li><p>Set the return_tensors parameter to either pt for PyTorch, or tf for TensorFlow</p></li>
</ul>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;</span> <span class="n">batch_sentences</span> <span class="o">=</span> <span class="p">[</span>
<span class="o">&gt;&gt;</span>     <span class="s2">&quot;But what about second breakfast?&quot;</span><span class="p">,</span>
<span class="o">&gt;&gt;</span>     <span class="s2">&quot;Don&#39;t think he knows about second breakfast, Pip.&quot;</span><span class="p">,</span>
<span class="o">&gt;&gt;</span>     <span class="s2">&quot;What about elevensies?&quot;</span><span class="p">,</span>
<span class="o">&gt;&gt;</span> <span class="p">]</span>
<span class="o">&gt;&gt;</span> <span class="n">encoded_input</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">batch_sentences</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">encoded_input</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1252</span><span class="p">,</span> <span class="mi">1184</span><span class="p">,</span> <span class="mi">1164</span><span class="p">,</span> <span class="mi">1248</span><span class="p">,</span> <span class="mi">6462</span><span class="p">,</span> <span class="mi">136</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1790</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">189</span><span class="p">,</span> <span class="mi">1341</span><span class="p">,</span> <span class="mi">1119</span><span class="p">,</span> <span class="mi">3520</span><span class="p">,</span> <span class="mi">1164</span><span class="p">,</span> <span class="mi">1248</span><span class="p">,</span> <span class="mi">6462</span><span class="p">,</span> <span class="mi">117</span><span class="p">,</span> <span class="mi">21902</span><span class="p">,</span> <span class="mi">1643</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">],</span>
                      <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1327</span><span class="p">,</span> <span class="mi">1164</span><span class="p">,</span> <span class="mi">5450</span><span class="p">,</span> <span class="mi">23434</span><span class="p">,</span> <span class="mi">136</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span>
 <span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]),</span>
 <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                           <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])}</span>
</pre></div>
</div>
</section>
</section>
<section id="audio">
<h4>Audio<a class="headerlink" href="#audio" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>For audio tasks, you’ll need a <code class="docutils literal notranslate"><span class="pre">feature</span> <span class="pre">extractor</span></code> to prepare your dataset for the model.</p></li>
<li><p>The feature extractor is designed to extract features from raw audio data, and convert them into tensors.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Remember you should always resample your audio dataset’s sampling rate to match the sampling rate of the dataset used to pretrain a model!</p>
</div>
<p>获取数据:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Audio</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;PolyAI/minds14&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;en-US&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>

<span class="c1"># upsample the sampling rate to 16kHz:</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast_column</span><span class="p">(</span><span class="s2">&quot;audio&quot;</span><span class="p">,</span> <span class="n">Audio</span><span class="p">(</span><span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16_000</span><span class="p">))</span>
</pre></div>
</div>
<p>Load the feature extractor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoFeatureExtractor</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Pass the audio array to the feature extractor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">audio_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]]</span>
<span class="n">feature_extractor</span><span class="p">(</span><span class="n">audio_input</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>
</pre></div>
</div>
<section id="pading">
<h5>Pading<a class="headerlink" href="#pading" title="此标题的永久链接">¶</a></h5>
<p>查看数据长度:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">173398</span><span class="p">,)</span>

<span class="n">dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">106496</span><span class="p">,)</span>
</pre></div>
</div>
<p>补齐:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">preprocess_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">audio_arrays</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s2">&quot;array&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">]]</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span>
        <span class="n">audio_arrays</span><span class="p">,</span>
        <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
<span class="n">processed_dataset</span> <span class="o">=</span> <span class="n">preprocess_function</span><span class="p">(</span><span class="n">dataset</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>两次查看数据长度:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">100000</span><span class="p">,)</span>

<span class="n">dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;audio&quot;</span><span class="p">][</span><span class="s2">&quot;array&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">100000</span><span class="p">,)</span>
</pre></div>
</div>
</section>
</section>
<section id="computer-vision">
<h4>Computer vision<a class="headerlink" href="#computer-vision" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>For computer vision tasks, you’ll need an <code class="docutils literal notranslate"><span class="pre">image</span> <span class="pre">processor</span></code> to prepare your dataset for the model.</p></li>
<li><p>Image preprocessing consists of several steps that convert images into the input expected by the model.</p></li>
<li><p>These steps include but are not limited to resizing, normalizing, color channel correction, and converting images to tensors.</p></li>
</ul>
<p>载入数据:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;food101&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train[:100]&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>查看图片:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;image&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Load the image processor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/vit-base-patch16-224&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="image-augmentation">
<h5>image augmentation<a class="headerlink" href="#image-augmentation" title="此标题的永久链接">¶</a></h5>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>这儿用的是torchvision’s transforms module，还可以用其他图像增强方法，如: <a class="reference external" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb">Albumentations</a> 和 <a class="reference external" href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb">Kornia</a></p>
</div>
<p>resizing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">RandomResizedCrop</span><span class="p">,</span> <span class="n">ColorJitter</span><span class="p">,</span> <span class="n">Compose</span>

<span class="n">size</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">&quot;shortest_edge&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="s2">&quot;shortest_edge&quot;</span> <span class="ow">in</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span>
    <span class="k">else</span> <span class="p">(</span><span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">&quot;height&quot;</span><span class="p">],</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="s2">&quot;width&quot;</span><span class="p">])</span>
<span class="p">)</span>

<span class="c1"># 随机裁剪和变化颜色</span>
<span class="c1"># RandomResizedCrop会随机裁剪图片的区域。</span>
<span class="c1"># ColorJitter会随机改变图像的亮度、对比度等参数。</span>
<span class="n">_transforms</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span><span class="n">RandomResizedCrop</span><span class="p">(</span><span class="n">size</span><span class="p">),</span> <span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)])</span>
</pre></div>
</div>
<p>combines image augmentation and image preprocessing for a batch of images and generates pixel_values:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 对每个图像example应用_transforms</span>
<span class="c1"># 并将转换后的图像保存在example的pixel_values中</span>
<span class="k">def</span> <span class="nf">transforms</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">_transforms</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;RGB&quot;</span><span class="p">))</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]]</span>
    <span class="n">examples</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_processor</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">do_resize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">examples</span>
</pre></div>
</div>
<p>apply the transforms on the fly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transforms</span><span class="p">)</span>
</pre></div>
</div>
<p>The image has been randomly cropped and it’s color properties are different:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>dataset[0][“pixel_values”]每次结果不一样。原因是使用了``dataset.set_transform(transforms)``，每次遍历dataset时,这些随机操作都会重新应用,所以同一个样本经过增强之后的pixel_values就会有所不同。这也正是数据增强的目的,通过随机操作创造更多不同的训练样本,提高模型的泛化能力。总结来说,dataset[0]本身不变,但增强后pixel_values不同,是因为随机增强引起的。这对提高模型鲁棒性是有帮助的。</p>
</div>
</section>
<section id="id4">
<h5>Pading<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">):</span>
    <span class="n">pixel_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">encoding</span> <span class="o">=</span> <span class="n">image_processor</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">pixel_values</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">]</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;pixel_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoding</span><span class="p">[</span><span class="s2">&quot;pixel_mask&quot;</span><span class="p">]</span>
    <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<ul class="simple">
<li><p>在PyTorch中,collate_fn函数的作用是在使用DataLoader加载数据时对一个batch的数据进行预处理。</p></li>
<li><p>collate_fn会在每个batch被加载后执行,它接受一个batch的数据作为输入,并返回batch的数据作为输出。</p></li>
</ul>
<p>常见的使用collate_fn的场景有:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 当样本的数据格式不同时,collate_fn可以将其转换为相同格式。
    例如样本包括图像和文本,collate_fn可以将其转换为同样的张量格式。
- 当batch中的样本长度不同时,collate_fn可以通过padding将其补齐到相同长度。
    例如处理NLP任务中的文本数据。
- 对batch中的样本进行额外的预处理
    例如图像增强、文本tokenize等。
- 构建自定义的数据结构作为batch的输出
    例如为检测任务构建(images, targets)的结构。
- 在训练语音识别模型时,collate_fn可以将音频样本padding到相同长度,并构建长度变量等。
</pre></div>
</div>
</section>
</section>
</section>
<section id="fine-tune-a-pretrained-model">
<h3>Fine-tune a pretrained model<a class="headerlink" href="#fine-tune-a-pretrained-model" title="此标题的永久链接">¶</a></h3>
<p>安装包:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!pip install datasets transformers accelerate evaluate
</pre></div>
</div>
<p>load data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;yelp_review_full&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="mi">100</span><span class="p">]</span>
<span class="go">{&#39;label&#39;: 0,</span>
<span class="go"> &#39;text&#39;: &#39;My expectations for McDonal...&#39;}</span>
</pre></div>
</div>
<p>token:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-cased&quot;</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>取小部分数据以节省时间(可选):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">DatasetDict</span><span class="p">,</span> <span class="n">Dataset</span>

<span class="n">small_train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">small_test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
<span class="n">small_dataset</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">({</span>
    <span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">small_train_dataset</span><span class="p">,</span>
    <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">small_test_dataset</span>
<span class="p">})</span>
</pre></div>
</div>
<p>批处理token:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;max_length&quot;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">small_dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">small_tokenized_train_dataset</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">small_tokenized_test_dataset</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>
</pre></div>
</div>
<section id="train-with-pytorch-trainer">
<h4>Train with PyTorch Trainer<a class="headerlink" href="#train-with-pytorch-trainer" title="此标题的永久链接">¶</a></h4>
<section id="training-hyperparameters">
<h5>Training hyperparameters<a class="headerlink" href="#training-hyperparameters" title="此标题的永久链接">¶</a></h5>
<p>Specify where to save the checkpoints from your training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;test_trainer&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>monitor your evaluation metrics during fine-tuning(可选):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="n">training_args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;test_trainer&quot;</span><span class="p">,</span> <span class="n">evaluation_strategy</span><span class="o">=</span><span class="s2">&quot;epoch&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="evaluate">
<h5>Evaluate<a class="headerlink" href="#evaluate" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Trainer</span></code> does not automatically evaluate model performance during training.</p></li>
<li><p>You should add <code class="docutils literal notranslate"><span class="pre">compute_metrics</span></code> param to <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> object.</p></li>
</ul>
<p><a class="reference external" href="https://huggingface.co/docs/evaluate/index">Evaluate</a> library provides a simple accuracy function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>convert the predictions to logits (remember all 🤗 Transformers models return logits):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">eval_pred</span><span class="p">):</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">eval_pred</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id6">
<h5>Trainer<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h5>
<p>Create a Trainer object:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_args</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">small_tokenized_train_dataset</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">small_tokenized_eval_dataset</span><span class="p">,</span>
    <span class="n">compute_metrics</span><span class="o">=</span><span class="n">compute_metrics</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>fine-tune begin:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="train-in-native-pytorch">
<h4>Train in native PyTorch<a class="headerlink" href="#train-in-native-pytorch" title="此标题的永久链接">¶</a></h4>
<p>清除环境节省资源:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">del</span> <span class="n">model</span>
<span class="k">del</span> <span class="n">trainer</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</pre></div>
</div>
<p>manually postprocess tokenized_dataset to prepare it for training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="o">.</span><span class="n">remove_columns</span><span class="p">([</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
<span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>

<span class="c1"># Set the format of the dataset to return PyTorch tensors instead of lists:</span>
<span class="n">tokenized_datasets</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="s2">&quot;torch&quot;</span><span class="p">)</span>

<span class="n">small_tokenized_train_dataset</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span>
<span class="n">small_tokenized_test_dataset</span> <span class="o">=</span> <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span>
</pre></div>
</div>
<section id="dataloader">
<h5>DataLoader<a class="headerlink" href="#dataloader" title="此标题的永久链接">¶</a></h5>
<p>Create a DataLoader:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">small_tokenized_train_dataset</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">eval_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">small_tokenized_test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="optimizer-and-learning-rate-scheduler">
<h5>Optimizer and learning rate scheduler<a class="headerlink" href="#optimizer-and-learning-rate-scheduler" title="此标题的永久链接">¶</a></h5>
<p>Create an optimizer and learning rate scheduler to fine-tune the model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">AdamW</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">)</span>
</pre></div>
</div>
<p>Create the default learning rate scheduler from Trainer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">get_scheduler</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
<span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_scheduler</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span>
<span class="p">)</span>
</pre></div>
</div>
<p>specify device to use a GPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-loop">
<h5>Training loop<a class="headerlink" href="#training-loop" title="此标题的永久链接">¶</a></h5>
<p>基本的循环训练逻辑:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>       <span class="c1"># 前向传播</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>                <span class="c1"># 反向传播计算梯度</span>

        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>               <span class="c1"># 使用优化器更新参数</span>
        <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>            <span class="c1"># 使用学习率调度器更新学习率</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>          <span class="c1"># 清零优化器的梯度</span>
        <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id7">
<h5>Evaluate<a class="headerlink" href="#id7" title="此标题的永久链接">¶</a></h5>
<p>基本的模型评估逻辑:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>    <span class="c1"># 加载评估指标</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>                          <span class="c1"># 将模型设置为评估模式</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">eval_dataloader</span><span class="p">:</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>             <span class="c1"># 关闭autograd engine进行推理</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>      <span class="c1"># 模型前向传播计算</span>

    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>        <span class="c1"># 计算预测类别</span>
    <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>     <span class="c1"># 将预测结果和标签传入metric进行指标计算</span>

<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>                      <span class="c1"># 聚合批次结果,得到最终评估指标数量</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="train-with-a-script">
<h3>Train with a script<a class="headerlink" href="#train-with-a-script" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>本节主要展示了如何使用现成的脚本来直接实现相应的功能</p></li>
<li><p>主要如下面2个由社区贡献的脚本示例 <a class="reference external" href="https://github.com/huggingface/transformers/tree/main/examples/research_projects">research projects</a> 和 <a class="reference external" href="https://github.com/huggingface/transformers/tree/main/examples/legacy">legacy examples</a></p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>These scripts are not actively maintained and require a specific version of 🤗 Transformers that will most likely be incompatible with the latest version of the library.</p>
</div>
<p>运行脚本示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">examples</span><span class="o">/</span><span class="n">pytorch</span><span class="o">/</span><span class="n">summarization</span><span class="o">/</span><span class="n">run_summarization</span><span class="o">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">t5</span><span class="o">-</span><span class="n">small</span> \
    <span class="o">--</span><span class="n">do_train</span> \
    <span class="o">--</span><span class="n">do_eval</span> \
    <span class="o">--</span><span class="n">dataset_name</span> <span class="n">cnn_dailymail</span> \
    <span class="o">--</span><span class="n">dataset_config</span> <span class="s2">&quot;3.0.0&quot;</span> \
    <span class="o">--</span><span class="n">source_prefix</span> <span class="s2">&quot;summarize: &quot;</span> \
    <span class="o">--</span><span class="n">output_dir</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">tst</span><span class="o">-</span><span class="n">summarization</span> \
    <span class="o">--</span><span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">4</span> \
    <span class="o">--</span><span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">4</span> \
    <span class="o">--</span><span class="n">overwrite_output_dir</span> \
    <span class="o">--</span><span class="n">predict_with_generate</span>
</pre></div>
</div>
</section>
<section id="distributed-training-with-accelerate">
<h3>Distributed training with Accelerate<a class="headerlink" href="#distributed-training-with-accelerate" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>本节主要讲了一个分布式训练的工具: <code class="docutils literal notranslate"><span class="pre">Accelerate</span></code></p></li>
</ul>
<p>安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">accelerate</span>
</pre></div>
</div>
<section id="backward">
<h4>Backward<a class="headerlink" href="#backward" title="此标题的永久链接">¶</a></h4>
<p>使用 <code class="docutils literal notranslate"><span class="pre">Accelerate</span></code> 只需要做如下修改:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">+</span> <span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>
  <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">get_scheduler</span>

<span class="o">+</span> <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">()</span>

  <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">num_labels</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">)</span>

<span class="o">-</span> <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="o">+</span> <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
<span class="o">+</span>     <span class="n">train_dataloader</span><span class="p">,</span> <span class="n">eval_dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span>
<span class="o">+</span> <span class="p">)</span>

  <span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
  <span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
  <span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">get_scheduler</span><span class="p">(</span>
      <span class="s2">&quot;linear&quot;</span><span class="p">,</span>
      <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
      <span class="n">num_warmup_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
      <span class="n">num_training_steps</span><span class="o">=</span><span class="n">num_training_steps</span>
  <span class="p">)</span>

  <span class="n">progress_bar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">num_training_steps</span><span class="p">))</span>

  <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
      <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dataloader</span><span class="p">:</span>
<span class="o">-</span>         <span class="n">batch</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">batch</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
          <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="o">-</span>         <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">+</span>         <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
          <span class="n">lr_scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
          <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
          <span class="n">progress_bar</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="transformers-agent">
<h3>Transformers Agent<a class="headerlink" href="#transformers-agent" title="此标题的永久链接">¶</a></h3>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>Transformers Agent is an experimental API which is subject to change at any time. Results returned by the agents can vary as the APIs or underlying models are prone to change.</p>
</div>
<ul class="simple">
<li><p>building on the concept of tools and agents.</p></li>
<li><p>In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an agent to interpret natural language and to use these tools.</p></li>
</ul>
<section id="id8">
<h4>示例<a class="headerlink" href="#id8" title="此标题的永久链接">¶</a></h4>
<p>命令:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Caption the following image&quot;</span><span class="p">,</span> <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="p">)</span>
</pre></div>
</div>
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/B49pjD.png" src="https://img.zhaoweiguo.com/uPic/2023/08/B49pjD.png" />
<p>命令:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Read the following text out loud&quot;</span><span class="p">,</span> <span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/vtKK41.png" src="https://img.zhaoweiguo.com/uPic/2023/08/vtKK41.png" />
<p>命令:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="s2">&quot;In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?&quot;</span><span class="p">,</span>
    <span class="n">document</span><span class="o">=</span><span class="n">document</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/2cLcOq.png" src="https://img.zhaoweiguo.com/uPic/2023/08/2cLcOq.png" />
</section>
<section id="quickstart">
<h4>Quickstart<a class="headerlink" href="#quickstart" title="此标题的永久链接">¶</a></h4>
<p>安装:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">transformers</span><span class="p">[</span><span class="n">agents</span><span class="p">]</span>
</pre></div>
</div>
<p>logging in to have access to the Inference API:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">login</span>
<span class="n">login</span><span class="p">(</span><span class="s2">&quot;&lt;YOUR_TOKEN&gt;&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>instantiate the agent:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">HfAgent</span>

<span class="c1"># Starcoder</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">HfAgent</span><span class="p">(</span><span class="s2">&quot;https://api-inference.huggingface.co/models/bigcode/starcoder&quot;</span><span class="p">)</span>
<span class="c1"># StarcoderBase</span>
<span class="c1"># agent = HfAgent(&quot;https://api-inference.huggingface.co/models/bigcode/starcoderbase&quot;)</span>
<span class="c1"># OpenAssistant</span>
<span class="c1"># agent = HfAgent(url_endpoint=&quot;https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5&quot;)</span>

<span class="c1">## OpenAI</span>
<span class="c1"># pip install openai</span>
<span class="c1"># from transformers import OpenAiAgent</span>
<span class="c1"># agent = OpenAiAgent(model=&quot;text-davinci-003&quot;, api_key=&quot;&lt;your_api_key&gt;&quot;)</span>
</pre></div>
</div>
<section id="single-execution-run">
<h5>Single execution (run)<a class="headerlink" href="#single-execution-run" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Draw me a picture of rivers and lakes.&quot;</span><span class="p">)</span>


<span class="n">picture</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Generate a picture of rivers and lakes.&quot;</span><span class="p">)</span>
<span class="n">updated_picture</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Transform the image in `picture` to add an island to it.&quot;</span><span class="p">,</span> <span class="n">picture</span><span class="o">=</span><span class="n">picture</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="chat-based-execution-chat">
<h5>Chat-based execution (chat)<a class="headerlink" href="#chat-based-execution-chat" title="此标题的永久链接">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="s2">&quot;Generate a picture of rivers and lakes&quot;</span><span class="p">)</span>
<span class="n">agent</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="s2">&quot;Transform the picture so that there is a rock in there&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id9">
<h4>原理<a class="headerlink" href="#id9" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/1kTPz2.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/1kTPz2.jpg" />
</figure>
<section id="agents">
<h5>Agents<a class="headerlink" href="#agents" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>The “agent” here is a large language model, and we’re prompting it so that it has access to a specific set of tools.</p></li>
</ul>
</section>
<section id="tools">
<h5>Tools<a class="headerlink" href="#tools" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Tools are very simple: they’re a single function, with a name, and a description. We then use these tools’ descriptions to prompt the agent. Through the prompt, we show the agent how it would leverage tools to perform what was requested in the query.</p></li>
</ul>
</section>
</section>
<section id="resource">
<h4>Resource<a class="headerlink" href="#resource" title="此标题的永久链接">¶</a></h4>
<section id="a-curated-set-of-tools">
<h5>A curated set of tools<a class="headerlink" href="#a-curated-set-of-tools" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Document question answering: given a document (such as a PDF) in image format, answer a question on this document (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/donut">Donut</a>)</p></li>
<li><p>Text question answering: given a long text and a question, answer the question in the text (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/flan-t5">Flan_T5</a>)</p></li>
<li><p>Unconditional image captioning: Caption the image! (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/blip">BLIP</a>)</p></li>
<li><p>Image question answering: given an image, answer a question on this image (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/vilt">VILT</a>)</p></li>
<li><p>Image segmentation: given an image and a prompt, output the segmentation mask of that prompt (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/clipseg">CLIPSeg</a>)</p></li>
<li><p>Speech to text: given an audio recording of a person talking, transcribe the speech into text (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/whisper">Whisper</a>)</p></li>
<li><p>Text to speech: convert text to speech (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/speecht5">SpeechT5</a>)</p></li>
<li><p>Zero-shot text classification: given a text and a list of labels, identify to which label the text corresponds the most (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/bart">BART</a>)</p></li>
<li><p>Text summarization: summarize a long text in one or a few sentences (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/bart">BART</a>)</p></li>
<li><p>Translation: translate the text into a given language (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/nllb">NLLB</a>)</p></li>
</ul>
</section>
<section id="custom-tools">
<h5>Custom tools<a class="headerlink" href="#custom-tools" title="此标题的永久链接">¶</a></h5>
<ul class="simple">
<li><p>Text downloader: to download a text from a web URL</p></li>
<li><p>Text to image: generate an image according to a prompt, leveraging stable diffusion. <a class="reference external" href="https://huggingface.co/spaces/huggingface-tools/text-to-image">huggingface-tools/text-to-image</a></p></li>
<li><p>Image transformation: modify an image given an initial image and a prompt, leveraging instruct pix2pix stable diffusion</p></li>
<li><p>Text to video: generate a small video according to a prompt, leveraging damo-vilab</p></li>
</ul>
</section>
</section>
<section id="code-generation">
<h4>Code generation<a class="headerlink" href="#code-generation" title="此标题的永久链接">¶</a></h4>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Draw me a picture of rivers and lakes&quot;</span><span class="p">,</span> <span class="n">return_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="go">==Code generated by the agent==</span>
<span class="go">from transformers import load_tool</span>
<span class="go">image_generator = load_tool(&quot;huggingface-tools/text-to-image&quot;)</span>
<span class="go">image = image_generator(prompt=&quot;rivers and lakes&quot;)</span>
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Draw me a picture of the sea then transform the picture to add an island&quot;</span><span class="p">,</span> <span class="n">return_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="go">==Code generated by the agent==</span>
<span class="go">from transformers import load_tool</span>
<span class="go">image_transformer = load_tool(&quot;huggingface-tools/image-transformation&quot;)</span>
<span class="go">image_generator = load_tool(&quot;huggingface-tools/text-to-image&quot;)</span>
<span class="go">image = image_generator(prompt=&quot;a picture of the sea&quot;)</span>
<span class="go">image = image_transformer(image, prompt=&quot;an island&quot;)</span>
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">picture</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Generate a picture of rivers and lakes.&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">updated_picture</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="s2">&quot;Transform the image in `picture` to add an boat to it.&quot;</span><span class="p">,</span> <span class="n">picture</span><span class="o">=</span><span class="n">picture</span><span class="p">,</span> <span class="n">return_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="go">==Code generated by the agent==</span>
<span class="go">image = image_transformer(image=picture, prompt=&quot;a boat&quot;)</span>
</pre></div>
</div>
</section>
<section id="practice">
<h4>Practice<a class="headerlink" href="#practice" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj#scrollTo=Q9rx-nKzDpAW">https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj#scrollTo=Q9rx-nKzDpAW</a></p></li>
</ul>
</section>
</section>
</section>
<section id="task-guides">
<h2>TASK GUIDES<a class="headerlink" href="#task-guides" title="此标题的永久链接">¶</a></h2>
<section id="id11">
<h3>NATURAL LANGUAGE PROCESSING<a class="headerlink" href="#id11" title="此标题的永久链接">¶</a></h3>
<p>NLP:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Text</span> <span class="n">classification</span>
<span class="n">Token</span> <span class="n">classification</span>
  <span class="n">One</span> <span class="n">of</span> <span class="n">the</span> <span class="n">most</span> <span class="n">common</span> <span class="n">token</span> <span class="n">classification</span> <span class="n">tasks</span> <span class="ow">is</span> <span class="n">Named</span> <span class="n">Entity</span> <span class="n">Recognition</span> <span class="p">(</span><span class="n">NER</span><span class="p">)</span><span class="o">.</span>
  <span class="n">NER</span> <span class="n">attempts</span> <span class="n">to</span> <span class="n">find</span> <span class="n">a</span> <span class="n">label</span> <span class="k">for</span> <span class="n">each</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">sentence</span><span class="p">,</span>
    <span class="n">such</span> <span class="k">as</span> <span class="n">a</span> <span class="n">person</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="ow">or</span> <span class="n">org</span><span class="o">.</span>
<span class="n">Question</span> <span class="n">answering</span>
<span class="n">Causal</span> <span class="n">language</span> <span class="n">modeling</span>
  <span class="n">Causal</span> <span class="n">language</span> <span class="n">models</span> <span class="n">are</span> <span class="n">frequently</span> <span class="n">used</span> <span class="k">for</span> <span class="n">text</span> <span class="n">generation</span><span class="o">.</span>
  <span class="n">You</span> <span class="n">can</span> <span class="n">use</span> <span class="n">these</span> <span class="n">models</span> <span class="k">for</span> <span class="n">creative</span> <span class="n">applications</span>
    <span class="n">like</span> <span class="n">choosing</span> <span class="n">your</span> <span class="n">own</span> <span class="n">text</span> <span class="n">adventure</span> <span class="ow">or</span> <span class="n">an</span> <span class="n">intelligent</span> <span class="n">coding</span> <span class="n">assistant</span>
        <span class="n">like</span> <span class="n">Copilot</span> <span class="ow">or</span> <span class="n">CodeParrot</span><span class="o">.</span>
<span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span>
  <span class="n">it</span> <span class="n">predicts</span> <span class="n">a</span> <span class="n">masked</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">sequence</span><span class="p">,</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">model</span> <span class="n">can</span> <span class="n">attend</span> <span class="n">to</span> <span class="n">tokens</span> <span class="n">bidirectionally</span>
  <span class="n">it</span> <span class="ow">is</span> <span class="n">great</span> <span class="k">for</span> <span class="n">tasks</span> <span class="n">that</span> <span class="n">require</span> <span class="n">a</span> <span class="n">good</span> <span class="n">contextual</span> <span class="n">understanding</span> <span class="n">of</span> <span class="n">an</span> <span class="n">entire</span> <span class="n">sequence</span><span class="o">.</span>
  <span class="n">BERT</span> <span class="ow">is</span> <span class="n">an</span> <span class="n">example</span> <span class="n">of</span> <span class="n">a</span> <span class="n">masked</span> <span class="n">language</span> <span class="n">model</span><span class="o">.</span>
<span class="n">Translation</span>
<span class="n">Summarization</span>
<span class="n">Multiple</span> <span class="n">choice</span>
</pre></div>
</div>
</section>
<section id="id12">
<h3>AUDIO<a class="headerlink" href="#id12" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Audio</span> <span class="n">classification</span>
<span class="n">Automatic</span> <span class="n">speech</span> <span class="n">recognition</span>
</pre></div>
</div>
</section>
<section id="id13">
<h3>COMPUTER VISION<a class="headerlink" href="#id13" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Image classification
Semantic segmentation
  Semantic segmentation assigns a label or class to each individual pixel of an image.
  Common real-world applications of semantic segmentation include:
      training self-driving cars to identify pedestrians and important traffic information,
      identifying cells and abnormalities in medical imagery,
      monitoring environmental changes from satellite imagery.
Video classification
Object detection
  This task is commonly used in autonomous driving for detecting things
    like pedestrians, road signs, and traffic lights.
  Other applications include counting objects in images, image search, and more.
Zero-shot object detection
Zero-shot image classification
Depth estimation

说明:
语义分割需要处理所有像素,目标检测只处理感兴趣的目标区域。
语义分割侧重对整个场景全面理解,目标检测侧重检测特定感兴趣目标。
</pre></div>
</div>
</section>
<section id="multimodal">
<h3>MULTIMODAL<a class="headerlink" href="#multimodal" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Image</span> <span class="n">captioning</span>
<span class="n">Document</span> <span class="n">Question</span> <span class="n">Answering</span>
<span class="n">Text</span> <span class="n">to</span> <span class="n">speech</span>
</pre></div>
</div>
</section>
</section>
<section id="developer-guides">
<h2>DEVELOPER GUIDES<a class="headerlink" href="#developer-guides" title="此标题的永久链接">¶</a></h2>
<p>生成文本的模型包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPT2</span>
<span class="n">XLNet</span>
<span class="n">OpenAI</span> <span class="n">GPT</span>
<span class="n">CTRL</span>
<span class="n">TransformerXL</span>
<span class="n">XLM</span>
<span class="n">Bart</span>
<span class="n">T5</span>
<span class="n">GIT</span>
<span class="n">Whisper</span>
</pre></div>
</div>
<section id="transformers-notebooks-with-examples">
<h3>Transformers Notebooks with examples<a class="headerlink" href="#transformers-notebooks-with-examples" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/notebooks">https://huggingface.co/docs/transformers/notebooks</a></p></li>
</ul>
</section>
<section id="community-resources">
<h3>Community resources<a class="headerlink" href="#community-resources" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/docs/transformers/community">https://huggingface.co/docs/transformers/community</a></p></li>
</ul>
</section>
</section>
<section id="performance-and-scalability">
<h2>PERFORMANCE AND SCALABILITY<a class="headerlink" href="#performance-and-scalability" title="此标题的永久链接">¶</a></h2>
<p>Trainer supports four hyperparameter search backends currently:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optuna</span><span class="p">,</span> <span class="n">sigopt</span><span class="p">,</span> <span class="n">raytune</span> <span class="ow">and</span> <span class="n">wandb</span>
</pre></div>
</div>
</section>
<section id="conceptual-guides">
<h2>CONCEPTUAL GUIDES<a class="headerlink" href="#conceptual-guides" title="此标题的永久链接">¶</a></h2>
<section id="philosophy">
<h3>Philosophy<a class="headerlink" href="#philosophy" title="此标题的永久链接">¶</a></h3>
<p>three standard classes required to use each model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. configuration
2. models
3. a preprocessing class

     1) tokenizer for NLP(AutoTokenizer)
     2) image processor for vision(AutoImageProcessor)
     3) feature extractor for audio(AutoFeatureExtractor)
     4) processor for multimodal inputs(AutoProcessor)
</pre></div>
</div>
<p>On top of those three base classes, the library provides two APIs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">pipeline</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">quickly</span> <span class="n">using</span> <span class="n">a</span> <span class="n">model</span> <span class="k">for</span> <span class="n">inference</span> <span class="n">on</span> <span class="n">a</span> <span class="n">given</span> <span class="n">task</span>
<span class="mf">2.</span> <span class="n">Trainer</span>
    <span class="n">to</span> <span class="n">quickly</span> <span class="n">train</span> <span class="ow">or</span> <span class="n">fine</span><span class="o">-</span><span class="n">tune</span> <span class="n">a</span> <span class="n">PyTorch</span> <span class="n">model</span>
</pre></div>
</div>
<section id="main-concepts">
<h4>Main concepts<a class="headerlink" href="#main-concepts" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Model classes can be PyTorch models (torch.nn.Module), Keras models (tf.keras.Model) or JAX/Flax models (flax.linen.Module) that work with the pretrained weights provided in the library.</p></li>
<li><p>Configuration classes store the hyperparameters required to build a model (such as the number of layers and hidden size). You don’t always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).</p></li>
<li><p>Preprocessing classes convert the raw data into a format accepted by the model. A tokenizer stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. Image processors preprocess vision inputs, feature extractors preprocess audio inputs, and a processor handles multimodal inputs.</p></li>
</ul>
<p>All these classes have these three methods:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">from_pretrained</span><span class="p">()</span>
<span class="n">save_pretrained</span><span class="p">()</span>
<span class="n">push_to_hub</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="glossary">
<h3>Glossary<a class="headerlink" href="#glossary" title="此标题的永久链接">¶</a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">attention</span> <span class="n">mask</span>
<span class="n">autoencoding</span> <span class="n">models</span>
<span class="n">autoregressive</span> <span class="n">models</span>
<span class="n">backbone</span>
<span class="n">causal</span> <span class="n">language</span> <span class="n">modeling</span>
<span class="n">channel</span>
<span class="n">connectionist</span> <span class="n">temporal</span> <span class="n">classification</span> <span class="p">(</span><span class="n">CTC</span><span class="p">)</span>
<span class="n">convolution</span>
<span class="n">decoder</span> <span class="nb">input</span> <span class="n">IDs</span>
<span class="n">decoder</span> <span class="n">models</span>
<span class="n">encoder</span> <span class="n">models</span>
<span class="n">feature</span> <span class="n">extraction</span>
<span class="n">feed</span> <span class="n">forward</span> <span class="n">chunking</span>
<span class="n">finetuned</span> <span class="n">models</span>
<span class="n">head</span>
<span class="n">image</span> <span class="n">patch</span>
<span class="n">inference</span>
<span class="nb">input</span> <span class="n">IDs</span>
<span class="n">labels</span>
<span class="n">masked</span> <span class="n">language</span> <span class="n">modeling</span> <span class="p">(</span><span class="n">MLM</span><span class="p">)</span>
<span class="n">multimodal</span>
<span class="n">pipeline</span>
<span class="n">pixel</span> <span class="n">values</span>
<span class="n">pooling</span>
<span class="n">position</span> <span class="n">IDs</span>
<span class="n">representation</span> <span class="n">learning</span>
<span class="bp">self</span><span class="o">-</span><span class="n">attention</span>
<span class="bp">self</span><span class="o">-</span><span class="n">supervised</span> <span class="n">learning</span>
<span class="n">semi</span><span class="o">-</span><span class="n">supervised</span> <span class="n">learning</span>
<span class="n">sequence</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">sequence</span> <span class="p">(</span><span class="n">seq2seq</span><span class="p">)</span>
<span class="n">stride</span>
<span class="n">token</span>
<span class="n">token</span> <span class="n">Type</span> <span class="n">IDs</span>
<span class="n">transfer</span> <span class="n">learning</span>
</pre></div>
</div>
<figure class="align-default" id="id20">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/ZNMFdF.png" src="https://img.zhaoweiguo.com/uPic/2023/08/ZNMFdF.png" />
<figcaption>
<p><span class="caption-text">处理过程图: Tokenizer-&gt; Model -&gt; Post-Processing</span><a class="headerlink" href="#id20" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="how-transformers-solve-tasks">
<h3>How Transformers solve tasks<a class="headerlink" href="#how-transformers-solve-tasks" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Wav2Vec2</span></code> for audio classification and automatic speech recognition (ASR)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Vision</span> <span class="pre">Transformer</span> <span class="pre">(ViT)</span></code> and <code class="docutils literal notranslate"><span class="pre">ConvNeXT</span></code> for image classification</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DETR</span></code> for object detection</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Mask2Former</span></code> for image segmentation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GLPN</span></code> for depth estimation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BERT</span></code> for NLP tasks like text classification, token classification and question answering that use an encoder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GPT2</span></code> for NLP tasks like text generation that use a decoder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BART</span></code> for NLP tasks like summarization and translation that use an encoder-decoder</p></li>
</ul>
<figure class="align-default" id="id21">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/YtfN6S.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/YtfN6S.jpg" />
<figcaption>
<p><span class="caption-text">Vision Transformer</span><a class="headerlink" href="#id21" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id22">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/NiMpg0.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/NiMpg0.jpg" />
<figcaption>
<p><span class="caption-text">Object detection</span><a class="headerlink" href="#id22" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id23">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/3getUU.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/3getUU.jpg" />
<figcaption>
<p><span class="caption-text">Image segmentation</span><a class="headerlink" href="#id23" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id24">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/TU69J8.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/TU69J8.jpg" />
<figcaption>
<p><span class="caption-text">Depth estimation</span><a class="headerlink" href="#id24" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="the-transformer-model-family">
<h3>The Transformer model family<a class="headerlink" href="#the-transformer-model-family" title="此标题的永久链接">¶</a></h3>
<section id="id14">
<h4>Computer vision<a class="headerlink" href="#id14" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/RYzYvS.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/RYzYvS.jpg" />
</figure>
</section>
<section id="id15">
<h4>Natural language processing<a class="headerlink" href="#id15" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/FB5ONz.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/FB5ONz.jpg" />
</figure>
</section>
<section id="id16">
<h4>Audio<a class="headerlink" href="#id16" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/2MKAWt.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/2MKAWt.jpg" />
</figure>
</section>
<section id="id17">
<h4>Multimodal<a class="headerlink" href="#id17" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/tnCfmL.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/tnCfmL.jpg" />
</figure>
</section>
<section id="reinforcement-learning">
<h4>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="此标题的永久链接">¶</a></h4>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2023/08/eSDNPe.jpg" src="https://img.zhaoweiguo.com/uPic/2023/08/eSDNPe.jpg" />
</figure>
</section>
</section>
<section id="summary-of-the-tokenizers">
<h3>Summary of the tokenizers<a class="headerlink" href="#summary-of-the-tokenizers" title="此标题的永久链接">¶</a></h3>
<p>3 tokenization algorithms:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">word</span><span class="o">-</span><span class="n">based</span>
    <span class="n">very</span> <span class="n">large</span> <span class="n">vocabularies</span>
    <span class="n">large</span> <span class="n">quantity</span> <span class="n">of</span> <span class="n">out</span><span class="o">-</span><span class="n">of</span><span class="o">-</span><span class="n">vocabulary</span> <span class="n">tokens</span>
    <span class="n">loss</span> <span class="n">of</span> <span class="n">meaning</span> <span class="n">across</span> <span class="n">very</span> <span class="n">similar</span> <span class="n">words</span>
<span class="mf">2.</span> <span class="n">character</span><span class="o">-</span><span class="n">based</span>
    <span class="n">very</span> <span class="n">long</span> <span class="n">sequences</span>
    <span class="n">less</span> <span class="n">meaningful</span> <span class="n">individual</span> <span class="n">tokens</span>
<span class="mf">3.</span> <span class="n">subword</span><span class="o">-</span><span class="n">based</span>
    <span class="n">principles</span><span class="p">:</span>
        <span class="n">frequently</span> <span class="n">used</span> <span class="n">words</span> <span class="n">should</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">split</span> <span class="n">into</span> <span class="n">subwords</span>
        <span class="n">rare</span> <span class="n">words</span> <span class="n">should</span> <span class="n">be</span> <span class="n">decompose</span> <span class="n">into</span> <span class="n">meaningful</span> <span class="n">subwords</span>
</pre></div>
</div>
<p>Subword tokenization:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">Byte</span><span class="o">-</span><span class="n">Pair</span> <span class="n">Encoding</span> <span class="p">(</span><span class="n">BPE</span><span class="p">)</span>
    <span class="n">GPT</span><span class="o">-</span><span class="mi">2</span>
    <span class="n">RoBERTa</span>
<span class="mf">2.</span> <span class="n">WordPiece</span>
    <span class="n">BERT</span>
    <span class="n">DistilBERT</span>
    <span class="n">Electra</span>
<span class="mf">3.</span> <span class="n">Unigram</span><span class="o">+</span><span class="n">SentencePiece</span><span class="p">(</span><span class="n">适用于非空格分隔的语言</span><span class="p">)</span>
    <span class="n">XLNet</span>
    <span class="n">ALBERT</span>
    <span class="n">Marian</span>
    <span class="n">T5</span>
</pre></div>
</div>
</section>
</section>
<section id="api">
<h2>API<a class="headerlink" href="#api" title="此标题的永久链接">¶</a></h2>
<section id="main-classes">
<h3>MAIN CLASSES<a class="headerlink" href="#main-classes" title="此标题的永久链接">¶</a></h3>
<section id="id18">
<h4>Agents<a class="headerlink" href="#id18" title="此标题的永久链接">¶</a></h4>
<p>three types of Agents:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. `HfAgent` uses inference endpoints for opensource models
2. `LocalAgent` uses a model of your choice locally
3. `OpenAiAgent` uses OpenAI closed models
</pre></div>
</div>
</section>
<section id="auto-classes">
<h4>Auto Classes<a class="headerlink" href="#auto-classes" title="此标题的永久链接">¶</a></h4>
<p>Generic classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoConfig</span>
<span class="n">AutoModel</span>
<span class="n">AutoTokenizer</span>

<span class="n">AutoFeatureExtractor</span>
<span class="n">AutoImageProcessor</span>
<span class="n">AutoProcessor</span>
</pre></div>
</div>
<p>Generic pretraining classes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForPreTraining</span>
</pre></div>
</div>
<p>Natural Language Processing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForCausalLM</span>
<span class="n">AutoModelForMaskedLM</span>
<span class="n">AutoModelForMaskGeneration</span>
<span class="n">AutoModelForSeq2SeqLM</span>
<span class="n">AutoModelForSequenceClassification</span>
<span class="n">AutoModelForMultipleChoice</span>
<span class="n">AutoModelForNextSentencePrediction</span>
<span class="n">AutoModelForTokenClassification</span>
<span class="n">AutoModelForQuestionAnswering</span>
<span class="n">AutoModelForTextEncoding</span>
</pre></div>
</div>
<p>Computer vision:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForDepthEstimation</span>
<span class="n">AutoModelForImageClassification</span>
<span class="n">AutoModelForVideoClassification</span>
<span class="n">AutoModelForMaskedImageModeling</span>
<span class="n">AutoModelForObjectDetection</span>
<span class="n">AutoModelForImageSegmentation</span>
<span class="n">AutoModelForSemanticSegmentation</span>
<span class="n">AutoModelForInstanceSegmentation</span>
<span class="n">AutoModelForUniversalSegmentation</span>
<span class="n">AutoModelForZeroShotImageClassification</span>
<span class="n">AutoModelForZeroShotObjectDetection</span>
</pre></div>
</div>
<p>Audio:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForAudioClassification</span>
<span class="n">AutoModelForAudioFrameClassification</span>
<span class="n">AutoModelForCTC</span>
<span class="n">AutoModelForSpeechSeq2Seq</span>
<span class="n">AutoModelForAudioXVector</span>
</pre></div>
</div>
<p>Multimodal:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AutoModelForTableQuestionAnswering</span>
<span class="n">AutoModelForDocumentQuestionAnswering</span>
<span class="n">AutoModelForVisualQuestionAnswering</span>
<span class="n">AutoModelForVision2Seq</span>
</pre></div>
</div>
</section>
<section id="callbacks">
<h4>Callbacks<a class="headerlink" href="#callbacks" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>The main class that implements callbacks is TrainerCallback.</p></li>
<li><p>By default a Trainer will use the following callbacks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>`DefaultFlowCallback` which handles the default behavior for logging, saving and evaluation.
`PrinterCallback` or `ProgressCallback` to display progress and print the logs
    the first one is used if you deactivate tqdm through the TrainingArguments
    otherwise it’s the second one
...
</pre></div>
</div>
</li>
</ul>
</section>
<section id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="此标题的永久链接">¶</a></h4>
<p>日志默认Warning级别，可以调整成info级别:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">transformers</span>
<span class="n">transformers</span><span class="o">.</span><span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_info</span><span class="p">()</span>
</pre></div>
</div>
<p>环境变量设置:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TRANSFORMERS_VERBOSITY</span>
</pre></div>
</div>
<p>Usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers.utils</span> <span class="kn">import</span> <span class="n">logging</span>

<span class="n">logging</span><span class="o">.</span><span class="n">set_verbosity_info</span><span class="p">()</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="s2">&quot;transformers&quot;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;INFO&quot;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;WARN&quot;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Evaluate.html" class="btn btn-neutral float-right" title="7.1.6. Evaluate" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Datasets.html" class="btn btn-neutral" title="7.1.4. Datasets" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2024, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V1.7.17',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>