

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>model &mdash; 新溪-gordon V1.7.17 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="7.2. Papers with Code" href="../../Papers%20with%20Code.html" />
    <link rel="prev" title="evaluate" href="evaluate.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script src="../../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V1.7.17
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">AI</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../normal.html">1. 常用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/normal.html">1.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/AIGC.html">1.2. AIGC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/concept.html">1.3. 关键定义</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/ml.html">1.4. 机器学习machine learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/bi.html">1.5. BI(Business Intelligence)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/deep_learning.html">1.6. 深度学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/normal.html">1.6.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../normals/deep_learnings/history.html">1.6.2. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/monitor.html">1.7. monitor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/algorithm.html">1.8. 相关算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/tool.html">1.9. 工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../normals/question.html">1.10. 常见问题</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../theory.html">2. 理论</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/ReAct.html">2.1. ReAct框架</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/Reflection.html">2.2. Reflection反思</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/math.html">2.3. 数学</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/bag-of-words.html">2.4. bag-of-words</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/word2vec.html">2.5. Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/doc2vec.html">2.6. Doc2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/FastText.html">2.7. FastText</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/LDA.html">2.8. LDA-Latent Dirichlet Allocation(潜在狄利克雷分配)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/overfitting-underfitting.html">2.9. overfitting&amp;underfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/evaluate.html">2.10. evaluate评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/RAG.html">2.11. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/Agent.html">2.12. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/LLM.html">2.13. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../theories/prompt_engineering.html">2.14. Prompt Engineering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../LLM.html">3. 大模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/normal.html">3.1. 常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/normal.html">3.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/package.html">3.1.2. 依赖安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/encoder.html">3.1.3. 编码-解码器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/usage.html">3.1.4. 使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/normals/tmp.html">3.1.5. 临时</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/model.html">3.2. 著名模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/LLaMA.html">3.2.1. LLaMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatGLM.html">3.2.2. ChatGLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BERT.html">3.2.3. BERT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/GPT.html">3.2.4. GPT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/BART.html">3.2.5. BART</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/T5.html">3.2.6. T5</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/ChatRWKV.html">3.2.7. ChatRWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/models/Open-Assistant.html">3.2.8. Open-Assistant</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/finetune.html">3.3. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/fileformat.html">3.4. 文件格式</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/GGML.html">3.4.1. GGML系列文件格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/ONNX.html">3.4.2. ONNX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/normal.html">常用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/ONNX.html">ONNX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/onnxruntime.html">onnxruntime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../LLMs/fileformats/ONNXs/skl2onnx.html">skl2onnx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/fileformats/NCNN.html">3.4.3. NCNN</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/openai.html">3.5. 商业项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/normal.html">3.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/openais/openai.html">3.5.2. OpenAI</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../LLMs/prompt.html">3.6. Prompt 提示词</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_chinese.html">3.6.1. 中文</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/demo_english.html">3.6.2. English</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../LLMs/prompts/skill.html">3.6.3. 示例</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../NLP.html">4. NLP</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/normal.html">4.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/preprocess.html">4.2. 预处理</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/normal.html">4.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96.html">4.2.2. 关键词提取</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E5%88%86%E8%AF%8D.html">4.2.3. 分词</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.html">4.2.4. 情感分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA.html">4.2.5. 文本表示</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6.html">4.2.6. 注意力机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/preprocesses/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B.html">4.2.7. 语言模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/NER.html">4.3. NER-命名实体识别</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/normal.html">4.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/seq-label.html">4.3.2. 序列标注</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/BiLSTM%2BCRF.html">4.3.3. BiLSTM+CRF</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/NERs/history.html">4.3.4. 历史</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../NLPs/summary.html">4.4. 总结-摘要</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../NLPs/summarys/normal.html">4.4.1. 通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../library.html">5. 函数库</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/normal.html">5.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Image.html">5.2. Image图像处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Video.html">5.3. Video视频</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/IPython.html">5.4. IPython</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/normal.html">5.4.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/magic.html">5.4.2. 魔法命令 </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/IPythons/display.html">5.4.3. display函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Jupyter.html">5.5. Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/NumPy.html">5.6. NumPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/normal.html">5.6.1. 通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/Ndarray.html">5.6.2. Ndarray 对象</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/NumPys/function.html">5.6.3. 通用函数</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Pandas.html">5.7. Pandas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/normal.html">5.7.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_subset.html">5.7.2. 实例-subset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_analysis.html">5.7.3. 实例-统计分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_sql.html">5.7.4. 利用pandas实现SQL操作</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_default_value.html">5.7.5. 实例-缺失值的处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/example_multi_index.html">5.7.6. 多层索引的使用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/practice.html">5.7.7. 实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Pandas/practices/practice_2012ObamaElect.html">实践-2012年奥巴马总统连任选举</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_input_output.html">5.7.8. API-输入输出</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_General.html">5.7.9. API-General functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Series.html">5.7.10. API-Series</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_DataFrame.html">5.7.11. API-DataFrame</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Pandas/api_Index.html">5.7.12. API-index</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Matplotlib.html">5.8. Matplotlib</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/normal.html">5.8.1. 基本</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/install.html">5.8.2. 安装</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pyplot.html">5.8.3. pyplot </a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/matplotlib.patches.html">5.8.4. matplotlib.patches</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/example.html">5.8.5. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/plot.html">折线图plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/bar.html">条形图bar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/hist.html">直方图hist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/scatter.html">散点图scatter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/stackplot.html">面积图stackplot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/pie.html">饼图pie</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/box.html">箱型图box</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/Matplotlibs/examples/multi.html">多图合并multi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Matplotlibs/pylab.html">5.8.6. pylab子包</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/SciPy.html">5.9. SciPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/SciPys/normal.html">5.9.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/sklearn.html">5.10. sklearn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/normal.html">5.10.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/supervised.html">5.10.2. 监督学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../libraries/sklearns/superviseds/glm.html">广义线性模型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/sklearns/unsupervised.html">5.10.3. 无监督学习</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/statsmodels.html">5.11. statsmodels</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/OpenCV.html">5.12. OpenCV</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/normal.html">5.12.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/example.html">5.12.2. 实例</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/OpenCVs/struct.html">5.12.3. 代码类结构</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/Seaborn.html">5.13. Seaborn</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/Seaborns/normal.html">5.13.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/jieba.html">5.14. jieba中文分词</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/gensim.html">5.15. gensim: 文本主题建模和相似性分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/normal.html">5.15.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Core_Tutorials.html">5.15.2. Core Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/Tutorials.html">5.15.3. Tutorials: Learning Oriented Lessons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../libraries/gensims/How-to_Guides.html">5.15.4. How-to Guides: Solve a Problem</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../libraries/LAC.html">5.16. LAC-百度词法分析工具</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../framework.html">6. 学习框架</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/normal.html">6.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/mxnet.html">6.2. mxnet库</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/mxnets/ndarray.html">6.2.1. nd模块</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../frameworks/mxnets/ndarrays/ndarray.html">ndarray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../frameworks/mxnets/ndarrays/ndarray.random.html">ndarray.random</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/mxnets/gluon.html">6.2.2. gluon模块</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/mxnets/autograd.html">6.2.3. autograd模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/pytorch.html">6.3. PyTorch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/pytorchs/normal.html">6.3.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/pytorchs/nn.html">6.3.2. nn模块</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/tensorflow.html">6.4. tensorflow</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/Keras.html">6.5. Keras</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/Keras/normal.html">6.5.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../frameworks/Keras/demo.html">6.5.2. 实例</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../frameworks/Keras/demos/binary_classification.html">二分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../frameworks/Keras/demos/multiclass_classification.html">多分类问题</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../frameworks/Keras/demos/regression.html">回归问题</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../frameworks/other.html">6.6. 其他</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../website.html">7. 关键网站</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../../huggingface.html">7.1. huggingface</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../normal.html">7.1.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../huggingface_hub.html">7.1.2. Hugging Face Hub</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lib_python.html">7.1.3. Hub Python Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Datasets.html">7.1.4. Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Transformers.html">7.1.5. Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Evaluate.html">7.1.6. Evaluate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../PEFT.html">7.1.7. PEFT</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../collect.html">7.1.8. 收集</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="dataset.html">dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="resources.html">resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="evaluate.html">evaluate</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Papers%20with%20Code.html">7.2. Papers with Code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Kaggle.html">7.3. Kaggle</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ArXiv.html">7.4. ArXiv 学术论文预印本平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../video.html">7.5. 视频相关</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../normal.html">7.6. 通用</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../concept.html">8. 关键定义</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/LLM.html">8.1. LLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Sigmoid.html">8.2. Sigmoid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/RELU.html">8.3. ReLU(激活函数)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Leaky-ReLU.html">8.4. Leaky ReLU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/Tanh.html">8.5. Tanh</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/GELU.html">8.6. GELU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/ners/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B.html">8.7. HMM-隐马尔可夫模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/ners/WWM-%E5%85%A8%E8%AF%8DMask.html">8.8. WWM-Whole Word Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/ners/CRF-%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA.html">8.9. CRF-条件随机场</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/ners/MLE-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1.html">8.10. MLE-最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/dls/ANN.html">8.11. ANN(NN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/dls/DNN-%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.12. 深度神经网络(Deep Neural Network, DNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/dls/CNN-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.html">8.13. 卷积神经网络(Convolutional Neural Network, CNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/dls/RNN-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91.html">8.14. RNN: 循环神经网(Recurrent Neural Network, RNN)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/dls/LSTM-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86.html">8.15. LSTM: 长短时记忆(Long Short Term Memory, LSTM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/others/%E5%88%A4%E5%88%AB%E5%BC%8F%E6%A8%A1%E5%9E%8Bvs%E7%94%9F%E6%88%90%E5%BC%8F%E6%A8%A1%E5%9E%8B.html">8.16. 判别式模型vs生成式模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../concepts/others/Embedding%E6%A8%A1%E5%9E%8B.html">8.17. Embedding 模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../practice.html">9. 实践</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/OCR.html">9.1. OCR</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/OCRs/normal.html">9.1.1. 常用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../practices/AIML.html">9.2. AIML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/normal.html">9.2.1. 常用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../practices/AIMLs/spec.html">9.2.2. AIML 2.1 Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../opensource.html">10. 开源项目</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Agent.html">10.1. Agent</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/RAG.html">10.2. RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/normal.html">10.3. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/ui.html">10.4. UI界面</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/finetune.html">10.5. 调优</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/search.html">10.6. 搜索</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Engine.html">10.7. LLM Inference Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/LLM-Inference-Tool.html">10.8. 模型推理平台</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/Evaluate.html">10.9. LLM评估</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../opensources/platform.html">10.10. AI平台</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../dataset.html">11. 数据集</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/normal.html">11.1. 常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../datasets/chinese.html">11.2. 中文数据集</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../Evaluate.html">12. Evaluate评测</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/TruLens.html">12.1. TruLens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/Ragas.html">12.2. Ragas</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/DeepEval.html">12.3. DeepEval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../Evaluates/UpTrain.html">12.4. UpTrain</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../website.html"><span class="section-number">7. </span>关键网站</a> &raquo;</li>
        
          <li><a href="../../huggingface.html"><span class="section-number">7.1. </span>huggingface</a> &raquo;</li>
        
          <li><a href="../collect.html"><span class="section-number">7.1.8. </span>收集</a> &raquo;</li>
        
      <li>model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/websites/huggingfaces/collects/model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">model</a><ul>
<li><a class="reference internal" href="#multilingual-models">Multilingual models</a><ul>
<li><a class="reference internal" href="#xlm">XLM</a><ul>
<li><a class="reference internal" href="#xlm-clm-enfr-1024">xlm-clm-enfr-1024</a></li>
<li><a class="reference internal" href="#xlm-mlm-ende-1024">xlm-mlm-ende-1024</a></li>
</ul>
</li>
<li><a class="reference internal" href="#xlm-roberta">XLM-RoBERTa</a></li>
<li><a class="reference internal" href="#m2m100">M2M100</a><ul>
<li><a class="reference internal" href="#facebook-m2m100-418m">facebook/m2m100_418M</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#wav2vec2">wav2vec2</a><ul>
<li><a class="reference internal" href="#facebook-wav2vec2-base">facebook/wav2vec2-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#text-to-speech">Text to speech</a><ul>
<li><a class="reference internal" href="#suno-bark">suno/bark</a></li>
<li><a class="reference internal" href="#whisper">Whisper</a></li>
</ul>
</li>
<li><a class="reference internal" href="#imageclassification">ImageClassification</a><ul>
<li><a class="reference internal" href="#vit">ViT</a></li>
<li><a class="reference internal" href="#google-vit-base-patch16-224-in21k">google/vit-base-patch16-224-in21k</a></li>
</ul>
</li>
<li><a class="reference internal" href="#semanticsegmentation">SemanticSegmentation</a><ul>
<li><a class="reference internal" href="#nvidia-mit-b0">nvidia/mit-b0</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zeroshotobjectdetection">ZeroShotObjectDetection</a><ul>
<li><a class="reference internal" href="#owl-vit">Owl-ViT</a></li>
<li><a class="reference internal" href="#google-owlvit-base-patch32">google/owlvit-base-patch32</a></li>
</ul>
</li>
<li><a class="reference internal" href="#zeroshotimageclassification">ZeroShotImageClassification</a><ul>
<li><a class="reference internal" href="#openai-clip-vit-large-patch14">openai/clip-vit-large-patch14</a></li>
</ul>
</li>
<li><a class="reference internal" href="#objectdetection">ObjectDetection</a><ul>
<li><a class="reference internal" href="#detr">DETR</a></li>
<li><a class="reference internal" href="#facebook-detr-resnet-50">facebook/detr-resnet-50</a></li>
<li><a class="reference internal" href="#vinvino02-glpn-nyu">vinvino02/glpn-nyu</a></li>
<li><a class="reference internal" href="#microsoft-git-base">microsoft/git-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#videomae">VideoMAE</a><ul>
<li><a class="reference internal" href="#mcg-nju-videomae-base">MCG-NJU/videomae-base</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multimodal">MULTIMODAL</a><ul>
<li><a class="reference internal" href="#microsoft-layoutlmv2-base-uncased">microsoft/layoutlmv2-base-uncased</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="model">
<h1>model<a class="headerlink" href="#model" title="此标题的永久链接">¶</a></h1>
<section id="multilingual-models">
<h2>Multilingual models<a class="headerlink" href="#multilingual-models" title="此标题的永久链接">¶</a></h2>
<section id="xlm">
<h3>XLM<a class="headerlink" href="#xlm" title="此标题的永久链接">¶</a></h3>
<section id="xlm-clm-enfr-1024">
<h4>xlm-clm-enfr-1024<a class="headerlink" href="#xlm-clm-enfr-1024" title="此标题的永久链接">¶</a></h4>
<p>xlm-clm-enfr-1024是一个多语言预训练的语言模型,具有以下特征:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于XLM(Facebook的跨语言预训练语言模型),支持100种语言。
- 预训练使用英语和法语语料,可以处理英语和法语文本。
- 输入为文本序列,输出对应的词向量表示。
- 模型大小为1GB,包含1024个隐层单元。
- 提供了XLMTokenizer用于文本tokenize,以及XLMWithLMHeadModel作为模型实现。
- 可以进行Masked语言建模(MLM)、Next Sentence Prediction(NSP)等下游任务fine-tuning。
- 预训练目标是联合 masked language modeling 和翻译语言建模。
- 采用SentencePiece进行词元化,支持词汇表共享。
- 支持PyTorch框架,可以灵活集成。
- 预训练质量高,是英语和法语领域的通用语言表示模型。
</pre></div>
</div>
<p>总之,xlm-clm-enfr-1024是一个高质量的跨语言预训练模型,可以作为英法两语区域NLP任务的强大预训练语言模型基线。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">XLMTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlm-clm-enfr-1024&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XLMWithLMHeadModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;xlm-clm-enfr-1024&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>XLM with language embeddings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">ende</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">German</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">enfr</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">French</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">enro</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">Romanian</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">xnli15</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">XNLI</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="n">tlm</span><span class="o">-</span><span class="n">xnli15</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span> <span class="o">+</span> <span class="n">translation</span><span class="p">,</span> <span class="n">XNLI</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">clm</span><span class="o">-</span><span class="n">enfr</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Causal</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">French</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">clm</span><span class="o">-</span><span class="n">ende</span><span class="o">-</span><span class="mi">1024</span> <span class="p">(</span><span class="n">Causal</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="n">English</span><span class="o">-</span><span class="n">German</span><span class="p">)</span>
</pre></div>
</div>
<p>XLM without language embeddings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="mi">17</span><span class="o">-</span><span class="mi">1280</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">17</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">mlm</span><span class="o">-</span><span class="mi">100</span><span class="o">-</span><span class="mi">1280</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xlm-mlm-ende-1024">
<h4>xlm-mlm-ende-1024<a class="headerlink" href="#xlm-mlm-ende-1024" title="此标题的永久链接">¶</a></h4>
<p>xlm-mlm-ende-1024是一个多语言Mask语言模型(MLM),主要功能和特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 该模型由Facebook AI研究院在2019年提出,支持100种语言。
- 采用Transformer架构,并在编码器部分进行多语言预训练。
- 预训练目标是masked language modeling (MLM),即预测被遮挡的语言词汇。
- 支持1024个Transformer块,参数量约为570亿。
- 训练数据包含100种语言的维基百科文本数据,总计约250GB。
- 支持跨语言的表示学习,一个模型就可以处理100种语言。
- 通过在大规模多语言数据集上预训练,获得了强大的语言理解能力。
- xlm-mlm-ende-1024是一个通用的多语言语义表示模型,可应用于下游跨语言自然语言处理任务中。
- 开源发布后受到广泛关注,被视为多语言预训练模型的重要进展。
</pre></div>
</div>
</section>
</section>
<section id="xlm-roberta">
<h3>XLM-RoBERTa<a class="headerlink" href="#xlm-roberta" title="此标题的永久链接">¶</a></h3>
<p>The following XLM-RoBERTa models can be used for multilingual tasks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xlm</span><span class="o">-</span><span class="n">roberta</span><span class="o">-</span><span class="n">base</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
<span class="n">xlm</span><span class="o">-</span><span class="n">roberta</span><span class="o">-</span><span class="n">large</span> <span class="p">(</span><span class="n">Masked</span> <span class="n">language</span> <span class="n">modeling</span><span class="p">,</span> <span class="mi">100</span> <span class="n">languages</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="m2m100">
<h3>M2M100<a class="headerlink" href="#m2m100" title="此标题的永久链接">¶</a></h3>
<p>The following M2M100 models can be used for multilingual translation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">facebook</span><span class="o">/</span><span class="n">m2m100_418M</span> <span class="p">(</span><span class="n">Translation</span><span class="p">)</span>
<span class="n">facebook</span><span class="o">/</span><span class="n">m2m100_1</span><span class="mf">.2</span><span class="n">B</span> <span class="p">(</span><span class="n">Translation</span><span class="p">)</span>
</pre></div>
</div>
<section id="facebook-m2m100-418m">
<h4>facebook/m2m100_418M<a class="headerlink" href="#facebook-m2m100-418m" title="此标题的永久链接">¶</a></h4>
<p>facebook/m2m100_418M 是一个大型的多语言对多语言机器翻译模型，由 Facebook AI 研究院训练并开源。</p>
<p>主要特点:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>采用 Transformer 架构，基于 Fairseq 代码库实现。
训练数据包括 100 种语言对，总计 418 万对句子。覆盖语言非常广泛。
支持直接端到端多语言翻译，不需要中继语言。
性能较强，在 WMT、Flores 翻译测试集上表现良好。
模型大小为 1.1GB (半精度),inference 速度快。
开源且无需许可，可以自由使用在商业产品中。
预训练模型可直接下载使用，简单方便。
支持添加自定义词典来改进翻译质量。
可在 CPU、GPU 等硬件上部署，适合不同场景。
</pre></div>
</div>
<p>整体来说，这是一个非常强大且实用的多语言机器翻译模型，值得推荐使用。它的开源特性也使其易于集成到各种产品和服务中。</p>
<p>m2m100_418M是Facebook在2022年开源的一个多模态大模型,主要特征是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 采用Transformer作为模型架构,包含文本编码器和视觉编码器。
- 文本编码器采用T5结构,视觉编码器采用Vision Transformer结构。
- 模型参数量达到418亿,使用了大量训练数据,包括4000万张图像和1700万段文本。
- 训练目标是进行视觉问答,即根据图片内容来回答自然语言问题。
- 模型支持100种语言,可以进行跨语言的视觉问答。
- 开源的模型包括英文、德文、法文、意大利文、日文、韩文、简体中文、繁体中文等版本。
- m2m100展示了视觉与语言的多模态预训练的强大能力,是一种统一的多语言多模态模型。
- 该模型仍然有进一步提升的空间,后续工作将会在模型规模、训练数据、泛化能力等方面进行改进。
</pre></div>
</div>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">M2M100Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/m2m100_418M&quot;</span><span class="p">,</span> <span class="n">src_lang</span><span class="o">=</span><span class="s2">&quot;zh&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">M2M100ForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/m2m100_418M&quot;</span><span class="p">)</span>

<span class="n">chinese_text</span> <span class="o">=</span> <span class="s2">&quot;不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒.&quot;</span>
<span class="n">encoded_zh</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">chinese_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">generated_tokens</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">encoded_zh</span><span class="p">,</span> <span class="n">forced_bos_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_lang_id</span><span class="p">(</span><span class="s2">&quot;en&quot;</span><span class="p">))</span>

<span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="wav2vec2">
<h2>wav2vec2<a class="headerlink" href="#wav2vec2" title="此标题的永久链接">¶</a></h2>
<section id="facebook-wav2vec2-base">
<h3>facebook/wav2vec2-base<a class="headerlink" href="#facebook-wav2vec2-base" title="此标题的永久链接">¶</a></h3>
<p>facebook/wav2vec2-base是一个由Facebook AI研究院训练的音频表示模型,属于Wav2Vec2系列。该模型的主要特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 无监督预训练:在大规模语音数据上通过自监督学习获得语音表示,没有标签数据。
- 基于 transformers:模型结构基于transformers,包含时间和频域的卷积层,多层自注意力机制。
- 输出语音表示:模型可以直接输出语音的表征向量表示,包含丰富的语义信息。
- 小模型大小:相比BERT系列,wav2vec2-base模型大小更小,只有95M参数。
- 应用广泛:预训练表示可 Fine-tune 在各种下游语音任务,包括语音识别、音频分类等。
- 性能优异:在多个公开语音数据集上表现优于传统MFCC特征。
- 易于使用:提供了方便的API,可以快速应用到项目中。
</pre></div>
</div>
<p>总体来说,wav2vec2-base是一个非常强大的语音表示模型,具有预训练的优势,可以广泛地应用到语音领域的任务中,是音频领域很有价值的预训练模型之一。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># For Audio Classification</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">AutoFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForAudioClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>


<span class="c1"># For ASR</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCTC</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="text-to-speech">
<h2>Text to speech<a class="headerlink" href="#text-to-speech" title="此标题的永久链接">¶</a></h2>
<section id="suno-bark">
<h3>suno/bark<a class="headerlink" href="#suno-bark" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/suno-ai/bark">https://github.com/suno-ai/bark</a></p></li>
</ul>
<p>suno/bark是一个语音合成模型,具有以下特征:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于Facebook的BARK模型,可以生成高质量的英语语音。BARK采用流式设计,可以快速生成语音。
- 预训练模型suno/bark已在LibriTTS英语语料库上进行fine-tune,可以生成自然的英语语音。
- 输入为英文文本,输出为16kHz采样率的单声道语音波形。
- 模型大小仅有87MB,非常轻量级,适合在各种环境下部署。
- 支持PyTorch和TensorFlow两种框架,可以灵活地集成到不同的项目中。
- 提供了AutoProcessor和BarkModel两个类,可以通过简单的API进行语音合成。
- 预训练模型质量较高,合成语音清晰流畅,语调自然。
-  Open-source社区维护,后续会继续优化和完善模型。
</pre></div>
</div>
<p>总体来说,suno/bark是一个非常理想的英语TTS模型选择,可直接用于产品中,同时也为研究人员提供了一个优秀的语音合成基线。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;suno/bark&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BarkModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;suno/bark&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="whisper">
<h3>Whisper<a class="headerlink" href="#whisper" title="此标题的永久链接">¶</a></h3>
<p>Whisper 是一个面向文本的非常大规模的神经网络模型，由 OpenAI 于 2022 年 9 月开源。其主要特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>模型规模巨大，包含了 1000 亿参数，是目前公开的最大的参数量模型之一。
基于 Transformers 架构，采用了类似 GPT 的编码器 - 解码器结构。
可以直接对原始语音进行识别转录，也可生成人类语音。
训练数据集包含了 660,000 小时 (合 75 年) 的语音数据，涵盖多种语言。
采用了一些技术来控制生成内容的安全性。
拥有语音识别、语音合成、文字转语音、语音翻译等多种应用潜力。
目前还有一定局限，如音素错误、语法错误、生成可信信息等。
对未来 voice AI 技术产生重大影响，代表语音 AI 发展新方向。
研究引发了对大模型安全性、伦理规范等方面的广泛关注和讨论。
</pre></div>
</div>
<p>总体来说，Whisper 是一个划时代的大模型，预示着语音 AI 进入新的阶段，但也给社会带来新的挑战。</p>
</section>
</section>
<section id="imageclassification">
<h2>ImageClassification<a class="headerlink" href="#imageclassification" title="此标题的永久链接">¶</a></h2>
<section id="vit">
<h3>ViT<a class="headerlink" href="#vit" title="此标题的永久链接">¶</a></h3>
<p>Google发布的一个VISION Transformer(ViT)模型。</p>
</section>
<section id="google-vit-base-patch16-224-in21k">
<h3>google/vit-base-patch16-224-in21k<a class="headerlink" href="#google-vit-base-patch16-224-in21k" title="此标题的永久链接">¶</a></h3>
<p>这是一个在大规模图像数据集ImageNet-21k上预训练的Transformer模型,用于计算机视觉领域。其主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于Transformer:利用自注意力机制Capture全局信息。
- Patch分割:输入图片被分割为16x16的patches,每个patch生成一个向量。
- 大模型规模:Base模型包含86M参数。
- 高精度:top-1准确率可达83%。
- 预训练数据:使用了包含1400万张图像、21741分类的ImageNet-21k数据集。
- 易fine-tune:可以在下游任务中加入新的头部进行迁移Fine-tuning。
- 占用内存小:比 CNN 模型更高效。
</pre></div>
</div>
<p>这个ViT模型展示了Transformer在计算机视觉领域也可以取得非常强大的效果。可以用来进行图像分类、对象检测等任务。在下游任务中只需要加入新的分类头,就可以进行端到端的Fine-tuning,做出非常高精度的图像分类器。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/vit-base-patch16-224-in21k&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
     <span class="n">checkpoint</span><span class="p">,</span>
     <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span>
     <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
     <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
 <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="semanticsegmentation">
<h2>SemanticSegmentation<a class="headerlink" href="#semanticsegmentation" title="此标题的永久链接">¶</a></h2>
<section id="nvidia-mit-b0">
<h3>nvidia/mit-b0<a class="headerlink" href="#nvidia-mit-b0" title="此标题的永久链接">¶</a></h3>
<p>nvidia/mit-b0是一个针对语义分割预训练的Transformer模型,由Nvidia基于Megatron框架训练获得。其关键特征为:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 模型结构:基于Vision Transformer,使用分割注意力机制。
- 预训练任务:在ADE20K数据集上进行像素级预测进行预训练。
- 模型规模:基础版本mit-b0包含32M参数。
- 输入分辨率:高512,宽512。
- 速度快:基础版本可达到每秒处理8幅图像。
- 性能卓越:在多个语义分割数据集上达到或超越CNN模型的效果。
- 易于fine-tune:可在downstream任务中直接fine-tune。
- 可扩展性强:提供了小型到超大型的系列模型。
</pre></div>
</div>
<p>nvidia/mit-b0展示了Transformer模型在语义分割领域也可以达到极强的效果,甚至超过现有CNN模型。它可以作为通用的语义分割预训练模型,然后fine-tune到各种下游分割任务中,实现快速高效的语义分割。整体来说,该模型为Transformer在计算机视觉更广阔的应用提供了有力案例。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;nvidia/mit-b0&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">reduce_labels</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span> <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="zeroshotobjectdetection">
<h2>ZeroShotObjectDetection<a class="headerlink" href="#zeroshotobjectdetection" title="此标题的永久链接">¶</a></h2>
<section id="owl-vit">
<h3>Owl-ViT<a class="headerlink" href="#owl-vit" title="此标题的永久链接">¶</a></h3>
<p>Owl-ViT是一种改进的Vision Transformer (ViT) 模型架构,主要特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 提出了一种称为本地分组自注意力(Locally Grouped Self-Attention)的机制,对ViT的自注意力机制进行改进,使其对小尺寸图像也更有效。
- 将图像划分成较小的非重叠patch,然后在每个局部邻域内学习patch之间的关系。
- 引入了多尺度结构,不同级别的注意力模块负责不同的视野范围,既保留全局信息,也学习局部细节。
- 通过引入残差连接,缓解了自注意力模块堆叠导致的过拟合问题。
- 相比ViT,Owl-ViT的参数量和计算量更小,训练速度更快。
- 在多个视觉任务上性能优于ViT,尤其在小样本和低分辨率图像上。
</pre></div>
</div>
<p>总体来说,Owl-ViT对ViT进行了改进,使其更适合计算资源受限而数据不足的实际场景。它比ViT参数量减小60%以上,计算量减小70%以上,但性能优于ViT。Owl-ViT是一个轻量高效的ViT改进版本。</p>
</section>
<section id="google-owlvit-base-patch32">
<h3>google/owlvit-base-patch32<a class="headerlink" href="#google-owlvit-base-patch32" title="此标题的永久链接">¶</a></h3>
<p>google/owlvit-base-patch32是一个基于Owl-ViT模型架构的图像分类预训练模型,主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 发布者:Google。
- 模型结构:Owl-ViT-B,采用Vision Transformer的网络架构。
- 输入:32x32大小的图像patch,形状为[3, 32, 32]。
- 预训练数据集:JFT-300M数据集,包含3亿张图像。
- 预训练任务:图像分类,可以区分3000个类别。
- 模型参数量:8600万。
- 应用:适用于迁移学习,可以初始化下游任务的backbone。在图像分类、目标检测等任务上有很好的表现。
- 权重文件大小:336MB。
- 推理速度快,是实际部署的理想选择。
</pre></div>
</div>
<p>Owl-ViT改进了ViT的局部注意力机制,使其对小尺寸图像也适用。google/owlvit-base-patch32是一个计算成本低但效果强劲的图像分类预训练模型,适合在各种视觉任务中使用。它是一个高效实用的选择。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;google/owlvit-base-patch32&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForZeroShotObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;zero-shot-object-detection&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="zeroshotimageclassification">
<h2>ZeroShotImageClassification<a class="headerlink" href="#zeroshotimageclassification" title="此标题的永久链接">¶</a></h2>
<section id="openai-clip-vit-large-patch14">
<h3>openai/clip-vit-large-patch14<a class="headerlink" href="#openai-clip-vit-large-patch14" title="此标题的永久链接">¶</a></h3>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;openai/clip-vit-large-patch14&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForZeroShotImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">detector</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="s2">&quot;zero-shot-image-classification&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="objectdetection">
<h2>ObjectDetection<a class="headerlink" href="#objectdetection" title="此标题的永久链接">¶</a></h2>
<section id="detr">
<h3>DETR<a class="headerlink" href="#detr" title="此标题的永久链接">¶</a></h3>
<p>DETR(End-to-End Object Detection with Transformers)是Facebook AI研究院在2020年提出的使用Transformers进行端到端目标检测的新颖模型。其主要特点包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">整体网络结构端到端</span><span class="p">,</span><span class="n">无需NMS后处理</span>
<span class="o">-</span> <span class="n">利用Transformer直接预测目标类别和坐标</span>
<span class="o">-</span> <span class="n">训练过程更稳定</span><span class="p">,</span><span class="n">从零训练也可收敛</span>
</pre></div>
</div>
</section>
<section id="facebook-detr-resnet-50">
<h3>facebook/detr-resnet-50<a class="headerlink" href="#facebook-detr-resnet-50" title="此标题的永久链接">¶</a></h3>
<p>facebook/detr-resnet-50是一个基于DETR架构的目标检测预训练模型。该模型使用ResNet-50作为Backbone,在COCO数据集上预训练,主要参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="n">Backbone</span><span class="p">:</span> <span class="n">ResNet</span><span class="o">-</span><span class="mi">50</span>
<span class="o">-</span> <span class="n">Transformer</span> <span class="n">Encoder包含6层</span>
<span class="o">-</span> <span class="mi">80</span><span class="n">类对象检测</span>
<span class="o">-</span> <span class="n">输入图像尺寸为800</span>
</pre></div>
</div>
<p>facebook/detr-resnet-50展示了Transformer模型在目标检测任务上的有效性,端到端 learns 到了检测的能力。效果竞争力强,是目标检测领域的重要低设计模型之一。</p>
<p>facebook/detr-resnet-50是一个基于transformers的目标检测模型,其应用场景主要包括:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1. 通用目标检测:
该模型在COCO等通用目标检测数据集上效果优异,可以应用到需要检测日常物体的各种场景中,如视频监控、自动驾驶、图像搜索等。
2. 特定目标检测:
模型可以进行fine-tuning,适应特定目标检测任务,如质检、医疗等领域的特定物体检测。
3. 小数据集学习:
相比传统方法,DETR对数据量要求不高,适合小数据集的目标检测任务。
4. 端到端学习:
DETR通过端到端学习预测目标框,无需后处理,使得部署简单。适用于对系统流程敏感的应用。
5. 强化学习:
DETR的学习过程稳定,适合和强化学习算法相结合,实现模仿学习等功能。
6. 前沿研究:
DETR开辟了transformer在目标检测领域的新方向,可以进行各种改进的学术研究。
</pre></div>
</div>
<p>总之,facebook/detr-resnet-50在多种实际目标检测应用中都展示了较强的泛化能力,是目前较为先进的检测模型之一。其端到端学习方式也启发了后续一系列detection transformer的提出。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;facebook/detr-resnet-50&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span>
    <span class="n">id2label</span><span class="o">=</span><span class="n">id2label</span><span class="p">,</span>
    <span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span>
    <span class="n">ignore_mismatched_sizes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="vinvino02-glpn-nyu">
<h3>vinvino02/glpn-nyu<a class="headerlink" href="#vinvino02-glpn-nyu" title="此标题的永久链接">¶</a></h3>
<p>vinvino02/glpn-nyu是一个针对深度估计(monocular depth estimation)任务进行预训练的模型,主要特点如下:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 该模型基于GLPN架构,通过encoder-decoder结构进行单眼图像深度预测。
- 数据集方面,该模型是在NYU Depth V2数据集上预训练的,这是一个包含各种室内场景的RGB-D数据集。
- 输入是标准分辨率的RGB图像,输出是与输入分辨率相同的深度图。
- 模型主要组件包括:ResNet50作为encoder,U-Net作为decoder,还使用了红外线辅助训练。
- 在NYU Depth V2测试集上可以达到leading performance,mean relative error为0.131。
- 相比其他深度学习方法,该模型可以更好地学习纹理细节,recover更丰富的场景结构信息。
- 预训练模型大小约为47MB,可以快速加载使用或在自定义数据集上fine-tune。
- 可通过PyTorch Hub快速加载使用。
</pre></div>
</div>
<p>总而言之,vinvino02/glpn-nyu是一个轻量而高效的深度估计预训练模型,尤其适合在室内场景中进行单眼深度预测或相关下游任务的fine-tuning,值得尝试使用。</p>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;vinvino02/glpn-nyu&quot;</span>
<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># pipeline</span>
<span class="n">depth_estimator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;depth-estimation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="microsoft-git-base">
<h3>microsoft/git-base<a class="headerlink" href="#microsoft-git-base" title="此标题的永久链接">¶</a></h3>
<p>示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;microsoft/git-base&quot;</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="videomae">
<h2>VideoMAE<a class="headerlink" href="#videomae" title="此标题的永久链接">¶</a></h2>
<section id="mcg-nju-videomae-base">
<h3>MCG-NJU/videomae-base<a class="headerlink" href="#mcg-nju-videomae-base" title="此标题的永久链接">¶</a></h3>
<p>MCG-NJU/videomae-base是一个基于VideoMAE的视频分类预训练模型。VideoMAE是由MCG-NJU团队在2022年提出的用于视频理解的MASKED AUTOENCODER(MAE)模型。其特点是:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- 基于MAE框架,使用MASKING策略进行视频预训练。
- 输入是视频序列的打散帧(Shuffled Tokens)。
- 编码器采用TimeSformer结构。
- 可以在各类视频理解任务中进行微调使用。
- 先进的视频表示学习能力。
</pre></div>
</div>
<p>MCG-NJU/videomae-base是在大规模视频数据上预训练得到的VideoMAE基础模型,主要参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>- TimeSformer编码器包含21个阶段,107M参数。
- 在8900万视频上预训练,总时长超过9000小时。
- 预训练任务是预测视频帧顺序。
</pre></div>
</div>
<p>该模型展示了Transformer模型在视频领域的强大建模能力。 fine-tune后效果显著,特别是在小数据集上的低样本学习。是近期较为先进的视频理解预训练模型之一。</p>
</section>
</section>
<section id="multimodal">
<h2>MULTIMODAL<a class="headerlink" href="#multimodal" title="此标题的永久链接">¶</a></h2>
<section id="microsoft-layoutlmv2-base-uncased">
<h3>microsoft/layoutlmv2-base-uncased<a class="headerlink" href="#microsoft-layoutlmv2-base-uncased" title="此标题的永久链接">¶</a></h3>
<p>模型 “microsoft/layoutlmv2-base-uncased” 是由微软开发的一种预训练语言模型，属于 LayoutLMv2 模型系列。这个模型的主要目标是处理文档布局和文本信息的结合，以便更好地支持文档分析、信息提取和相关任务。</p>
<p>与传统的自然语言处理模型不同，LayoutLMv2 关注的是将文本与其在页面中的布局信息相结合，从而使模型能够更好地理解和处理具有多个文本区域、表格、图像和其他排版元素的文档。这对于诸如表格数据提取、文档分类、命名实体识别等任务非常有用。</p>
<p>“microsoft/layoutlmv2-base-uncased” 模型是一个预训练模型，使用了无大小写区分的文本（uncased text）作为输入。这意味着它对于大小写不敏感，能够处理大小写不同的文本。通过预训练，模型学会了从文本和布局信息中抽取有关上下文、语义和结构的特征。</p>
<p>使用这个预训练模型，您可以通过微调（fine-tuning）适应特定的任务，如文本分类、命名实体识别、表格数据提取等。模型的预训练能力使其能够在不同的文档分析任务中表现出色，因为它已经学会了从文本和布局信息中获取有价值的信息。</p>
<p>总之，”microsoft/layoutlmv2-base-uncased” 模型是一种专注于处理带有布局信息的文本的预训练模型，可用于各种文档分析和信息提取任务。</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../Papers%20with%20Code.html" class="btn btn-neutral float-right" title="7.2. Papers with Code" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="evaluate.html" class="btn btn-neutral" title="evaluate" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2024, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'V1.7.17',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>