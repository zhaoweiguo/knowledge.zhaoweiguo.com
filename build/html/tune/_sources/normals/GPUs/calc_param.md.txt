# æ¨¡å‹å‚æ•°è®¡ç®—

## ğŸ“Œ é€šç”¨è¯´æ˜

* åŸºæœ¬å‚æ•°ï¼š
    * H: hidden_size(éšè—å±‚å¤§å°)
    * L: num_layers(Transformerå±‚æ•°)
    * V: vocab_size(è¯è¡¨å¤§å°)
    * M: ffn_mult: FFN ä¸­é—´å±‚ä¸éšè—å±‚ä¹‹é—´çš„ç³»æ•°
        * ffn_dim: FFN ä¸­é—´å±‚çš„ç»´åº¦å¤§å°
            * ffn_dim = M Ã— H
        * è¯´æ˜
            * GPT ç³»åˆ—æ¨¡å‹ä¸­ ffn_mult é€šå¸¸ä¸º 4.0
            * LLaMA ç³»åˆ—ä¸­ä¸€èˆ¬ä½¿ç”¨ 3.5
            * ä¸€äº›è½»é‡åŒ–æ¨¡å‹å¯èƒ½ä¼šä½¿ç”¨æ›´å°çš„å€¼ï¼ˆå¦‚ 2.0ï¼‰
            * è¯´æ˜ï¼šä½†qwenå¥½åƒæ˜¯5.375, è€Œdeepseekæ˜¯2.6875

* Self Attention å‚æ•°:
    * å¤šå¤´æ³¨æ„åŠ›
        * head_dim: å¤šå¤´æ³¨æ„åŠ›çš„å•Head ç»´åº¦
        * num_heads: å¤šå¤´æ³¨æ„åŠ›çš„Headæ•°
        * è¯´æ˜ï¼š
            * H = head_dim * num_heads
    * K/V Cache
        * æ™®é€š(MHA)æƒ…å†µ
            * num_kv_heads: K/V çš„Headæ•°
            * kv_dim: K/V ç»´åº¦
                * = num_kv_heads Ã— head_dim
            * è¯´æ˜ï¼š
                * H = kv_dim * num_kv_heads
        * GQAçš„æƒ…å†µ
            * è¯´æ˜ï¼š
              * Q ä½¿ç”¨æ‰€æœ‰çš„ attention headsï¼ˆå¦‚ 32ï¼‰
              * K/V åªç”¨éƒ¨åˆ† headsï¼ˆå¦‚ 8ï¼‰
              * è¿™æ ·å°±é™ä½äº† KV-cache çš„å­˜å‚¨éœ€æ±‚å’Œè¯»å–å¼€é”€


## ğŸ“Œ ä¼°ç®—çº§å‚æ•°è®¡ç®—


### å…¬å¼

* é‚£ä¹ˆæ€»å‚æ•°(ä¸è€ƒè™‘GQA)è¿‘ä¼¼ä¸ºï¼š

```text
Total â‰ˆ V Ã— H                  ï¼ˆè¯åµŒå…¥ï¼‰
            + L Ã— 4HÂ²      ï¼ˆTransformerå±‚ Self-Attentionï¼‰
            + L Ã— 3MHÂ²     ï¼ˆTransformerå±‚ FFN)
       + V Ã— H                 ï¼ˆè¾“å‡ºå±‚ï¼‰
```

### ç¤ºä¾‹


* å‚æ•°
    * H = 4096
    * V = 32000
    * L = 32
    * ffn_dim = M\*H = 3.5 * 4096= 14336


* æ€»å‚æ•°

```
= 32000 * 4096 + 32 *(4*4096^2 + 3*4096^2*3.5) + 4096*32000
= 2 * 32000 * 4096 + 32 *(4*4096^2 + 3*4096^2*3.5)
= 2 * 32000 * 4096 + 32 * 4096^2 * (4 + 3*3.5)
= 2 * 32000 * 4096 + 32 * 4096^2 * 14.5
= 250M + 7.25B
= 7.5B
æ³¨æ„: 
    1. åªæœ‰åœ¨å­˜å‚¨æ—¶ï¼Œå•ä½Bæ‰ç”¨ 1024 * 1024 * 1024
    2. è®¡ç®—å‚æ•°é‡æ—¶ï¼Œå•ä½Båº”è¯¥æ˜¯1000*1000*1000
æ‰€ä»¥: 7.5B = 7.5 * (1024*1024*1024)/ (1000*1000*1000)
          â‰ˆ 8.053B
```


## ğŸ“Œ æ ‡å‡†å‚æ•°è®¡ç®—

### å‚æ•°è®¡ç®—å…¬å¼

#### æ€»å…¬å¼ç»“æ„
```
ä¸€ä¸ª LLM çš„æ€»å‚æ•°é‡ â‰ˆ
è¯åµŒå…¥å±‚ + Transformerå±‚(Attention + FFN) + è¾“å‡ºå±‚
```

#### 1.è¯åµŒå…¥å±‚ï¼ˆEmbeddingï¼‰

```text
è¯åµŒå…¥çŸ©é˜µå‚æ•°é‡ = V Ã— H
```


#### 2.Transformer Block Ã— N å±‚


##### A.æ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰

* å¤šå¤´æ³¨æ„åŠ›éœ€è¦ç”Ÿæˆ Qã€Kã€V ä»¥åŠä¸€ä¸ªè¾“å‡ºæ˜ å°„ï¼š
    * å¦‚æœæœ‰å¤šå¤´ attentionï¼Œé€šå¸¸ `head_dim Ã— num_heads = hidden_size`
    * åœ¨æœªä½¿ç”¨GQAæ—¶ï¼ŒAttentionå‚æ•°åªä¸hidden_sizeæœ‰å…³ï¼Œä¸head_dim Ã— num_headsæ— å…³ã€‚

```text
Attentionå‚æ•° = 3 Ã— H Ã— H  ï¼ˆQ, K, V æƒé‡ï¼‰
              + H Ã— H      ï¼ˆè¾“å‡ºæŠ•å½±ï¼‰
    = 4 Ã— HÂ²
```

* æ³¨æ„ï¼šä½¿ç”¨ GQA å‚è§åé¢


##### B.å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰

* é€šå¸¸ç»“æ„æ˜¯ï¼š

```text
hidden_size â†’ intermediate_size â†’ hidden_size
```

* ä¸” intermediate_size å¸¸ä¸ºï¼š

```text
intermediate_size = M Ã— H ï¼ˆå¦‚ 4x æˆ– 3.5xï¼‰
```

* FFN ç»“æ„æœ‰ä¸¤ç§æƒ…å†µï¼š
    * æ™®é€šç‰ˆ
        * FFN éƒ¨åˆ†ä¹˜ä»¥ 3
        * è®¡ç®—å…¬å¼
        ```text
        FFNå‚æ•°é‡ = H Ã— intermediate_size ï¼ˆç¬¬ä¸€å±‚ï¼‰
                + intermediate_size Ã— H ï¼ˆç¬¬äºŒå±‚ï¼‰
                = 2 Ã— H Ã— intermediate_size
        ```
    * ä½¿ç”¨ Gated FFNï¼ˆå¦‚ SwiGLUï¼‰ç»“æ„(å¦‚ LLaMA ç³»åˆ—)
        * ä½¿ç”¨ SwiGLUï¼Œæ‰€ä»¥ FFN éƒ¨åˆ†ä¹˜ä»¥ 3
        * å› ä¸ºï¼š
          * ç¬¬ä¸€å±‚ Linear è¾“å‡ºçš„ä¸æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè€Œæ˜¯ ä¸¤ä¸ª intermediate_size å‘é‡æ‹¼æ¥åœ¨ä¸€èµ·
          * æ‰€ä»¥ç¬¬ä¸€å±‚çš„å‚æ•°æ˜¯ï¼š `Linear1: hidden_size Ã— (2 Ã— intermediate_size)`
        * è®¡ç®—å…¬å¼
        ```text
        FFNå‚æ•°é‡ = 3 Ã— H Ã— intermediate_size
        = 3 Ã— H Ã— (M Ã— H)
        = 3 Ã— M Ã— H^2
```


##### C.æ€» Transformer å±‚å‚æ•°é‡

* å¯¹äºä½¿ç”¨ SwiGLU çš„ç»“æ„ï¼ˆå¦‚ LLaMAã€Mistralï¼‰ï¼Œæ¯å±‚ Transformer å±‚å‚æ•°é‡ï¼š

```text
params_per_layer = 4 Ã— hidden_sizeÂ²           â† Attentionéƒ¨åˆ†
                 + 3 Ã— hidden_size Ã— ffn_dim  â† FFNéƒ¨åˆ†
```

* L å±‚æ€»å‚æ•°é‡ï¼š

```text
transformer_params = L Ã— params_per_layer
```


#### 3.è¾“å‡ºå±‚ï¼ˆLM Headï¼‰

ç»“æ„ä¸ embedding ç±»ä¼¼ï¼š

```text
è¾“å‡ºå±‚å‚æ•° = vocab_size Ã— hidden_size
```

* å¦‚æœå…±äº«æƒé‡ï¼ˆweight tyingï¼Œè¾“å‡ºå±‚ = embedding æƒé‡ï¼‰ï¼Œè¿™éƒ¨åˆ†å‚æ•°å¯çœç•¥ã€‚




### ç¤ºä¾‹ï¼šMistral 7B å‚æ•°è®¡ç®—

* å‚æ•°
    * hidden_size = 4096
    * vocab_size = 32000
    * num_layers = 32
    * ffn_dim = 14336 = 3.5 Ã— hidden_size

* 1.è¯åµŒå…¥å±‚

```text
embedding = 32000 Ã— 4096 = 131M
```

* 2.Transformer æ€»å‚æ•°

```text
æ¯å±‚ = 4 Ã— 4096Â² + 3 Ã— 4096 Ã— 14336
    = 4096Â² Ã— (4 + 3 Ã— 3.5)
    = 4096Â² Ã— 14.5
    = 232M
32 å±‚ = 32 Ã— 232M = 7.25B
```

* å…¶ä¸­
    * Self-Attentionå‚æ•°ä¸º
    ```text
    æ¯å±‚ = 4 Ã— 4096Â²
        = 64M
    32å±‚ = 32 Ã— 64M = 2B
    ```

> å®é™…ä¸­ç¨å¤šï¼Œå› ä¸ºè¿˜æœ‰ LayerNorm ç­‰å°‘é‡åç½®é¡¹ï¼Œä½†å¯ä»¥å¿½ç•¥

* 3.è¾“å‡ºå±‚

```text
= 32000 Ã— 4096 = 131Mï¼ˆå¦‚æœä¸ weight tieï¼‰
```

* 4.æ€»è®¡ï¼š

```text
131M + 7.25B + 131M â‰ˆ 7.51B ï¼ˆä»…è¿‘ä¼¼ï¼‰
```

* ä½†æ˜¯å®é™… Mistral 7B æ˜¯ 7.2Bï¼Œå› ä¸ºï¼š
    * ä½¿ç”¨äº†å¤šç»„ Attentionï¼ˆgrouped QKVï¼‰
    * æ›´å¤šç»†èŠ‚ä¼˜åŒ–ï¼ˆRMSNormï¼Œbiasï¼Œrotary embedding ç­‰ï¼‰



## ğŸ“Œ ä½¿ç”¨GQAæ—¶çš„å‚æ•°è®¡ç®—

### å®šä¹‰ï¼šGQA æ˜¯ä»€ä¹ˆ

* GQA: Grouped Query Attention(åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)
* åœ¨æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼ŒMHAï¼‰ä¸­ï¼š
    * æ¯ä¸ª head éƒ½æœ‰è‡ªå·±çš„ä¸€å¥— Qï¼ˆæŸ¥è¯¢ï¼‰ã€Kï¼ˆé”®ï¼‰ã€Vï¼ˆå€¼ï¼‰ï¼š
      ```
      Q_head_i = X Ã— W_q_i
      K_head_i = X Ã— W_k_i
      V_head_i = X Ã— W_v_i
      ```
    * å‚æ•°è¯´æ˜

    | ç¬¦å·         | å«ä¹‰                 | ç»´åº¦ |
    | ---------- | ------------------ | ----------------------|
    | `X`        | å½“å‰ token çš„è¡¨ç¤ºï¼ˆè¾“å…¥å¼ é‡ï¼‰ | `[T, H]`       |
    | `W_q_i`    | ç¬¬ i ä¸ªå¤´çš„ Query æŠ•å½±çŸ©é˜µ | `[H, head_dim]`  |
    | `W_k_i`    | ç¬¬ i ä¸ªå¤´çš„ Key æŠ•å½±çŸ©é˜µ   | `[H, head_dim]`  |
    | `W_v_i`    | ç¬¬ i ä¸ªå¤´çš„ Value æŠ•å½±çŸ©é˜µ | `[H, head_dim]`  |
    | `Q_head_i` | ç¬¬ i ä¸ªå¤´çš„ Query å‘é‡   | `[T, head_dim]`   |
    | `K_head_i` | ç¬¬ i ä¸ªå¤´çš„ Key å‘é‡     | `[T, head_dim]`   |
    | `V_head_i` | ç¬¬ i ä¸ªå¤´çš„ Value å‘é‡   | `[T, head_dim]`   |

* è€Œåœ¨ GQA ä¸­ï¼š
    * æ¯ä¸ª head æœ‰è‡ªå·±çš„ Qï¼ˆQueryï¼‰
    * ä½†å¤šä¸ª head å…±äº«åŒä¸€ç»„ K å’Œ V

* ç¤ºä¾‹ï¼š
    * å¦‚æœ
        * æ¨¡å‹æ€»å…±æœ‰ 32 ä¸ª Attention heads
        * ä½¿ç”¨ GQA åˆ†æˆ **8 ç»„**
        * æ¯ç»„ 4 ä¸ªå¤´å…±äº«ä¸€å¥— K/V
    * é‚£ä¹ˆï¼š
        * æœ‰ 32 å¥— Q æƒé‡
        * ä½†åªæœ‰ **8 å¥— K æƒé‡** å’Œ **8 å¥— V æƒé‡**


| ç±»å‹      | Q æƒé‡æ•° | K/V æƒé‡æ•° | æ€»å‚æ•°é‡  |
| ------- | ----- | ------- | ---------------------------- |
| **MHA** | 32    | 32      | Q: 4096Ã—4096, K/V: 4096Ã—4096 |
| **GQA** | 32    | **8**   | Q: 4096Ã—4096, K/V: 4096Ã—1024 |
| **MQA** | 32    | **1**   | Q: 4096Ã—4096, K/V: 4096Ã—128  |



### ä¸ºä»€ä¹ˆä½¿ç”¨ GQAï¼Ÿ

1. **å‡å°‘å‚æ•°æ•°é‡**ï¼ˆå°¤å…¶é€‚åˆå¤§æ¨¡å‹ï¼‰
2. **èŠ‚çœæ˜¾å­˜å’Œå¸¦å®½**
3. **å¯¹æ¨¡å‹æ€§èƒ½å½±å“å¾ˆå°ï¼Œç”šè‡³æå‡ç¨³å®šæ€§**
4. **æ¯” MQAï¼ˆMulti-Query Attentionï¼‰æ›´çµæ´»**
   * MQA æ˜¯ GQA çš„æç«¯ç‰ˆæœ¬ï¼Œæ‰€æœ‰ head å…±äº«åŒä¸€å¥— K/Vï¼ˆåªæœ‰ 1 å¥—ï¼‰

### GQA åœ¨å‚æ•°è®¡ç®—ä¸­çš„ä½“ç°

* æ–°å¢å‚æ•°
    * n_head:Attention head æ•°
    * n_head_kv: K/Vçš„Attention head æ•°
    * group_size: å…±äº«ä¸€å¥— K/V Group çš„æ•°é‡
    * d_head: æ¯ä¸ª head çš„ç»´åº¦

* å‚æ•°å…³ç³»
    * H = d_head * n_head
    * n_head = n_head_kv * group_size

* å…¬å¼

```text
# å•å±‚ Self Attention å‚æ•°è®¡ç®—
= H*d_head*n_head + H*d_head*n_head_kv*2 + H*d_head * n_head)
= 2* H^2 + 2*H*d_head*n_head_kv
= 2*H^2 + 2H^2/group_size
= H^2 (2 + 2/group_size)
```



### ç¤ºä¾‹è®²è§£â€”â€”ä»¥ä¸Šé¢çš„ Mistral 7B ç¤ºä¾‹åˆ†æ

* è¯´æ˜ï¼šä¸å‰é¢ç¤ºä¾‹ç›¸æ¯”ï¼Œåªæœ‰è®¡ç®— Self Attention çš„å‚æ•°é‡ä¸åŒ
* å‚æ•°
    * H = 4096
    * n_head = 32
    * n_head_kv = 8
    * group_size = 4
    * d_head = 128
    * L = 32

* è®¡ç®—
```
å•å±‚Self Attention å‚æ•°é‡
    = H^2 (2 + 2/group_size)
    = (4096 Ã— 4096) Ã— (2 + 2/4)
    = 40M
32å±‚Self Attention å‚æ•°é‡
    = 32 * 40M
    = 1.25G
```

```note
ç›¸æ¯”ä¸ç”¨ GQA, self attention çš„å‚æ•°é‡ä» **2G** é™ä½åˆ° **1.25G**
```


## ç¤ºä¾‹-Qwen2.5-3B

### æ¨¡å‹ç»“æ„

```python
Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 2048)
    (layers): ModuleList(
      (0-35): 36 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
          (k_proj): Linear(in_features=2048, out_features=256, bias=True)
          (v_proj): Linear(in_features=2048, out_features=256, bias=True)
          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (up_proj): Linear(in_features=2048, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=2048, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((2048,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
)
```

### å‚æ•°åˆ†æ

* H: 2048
* V: 151936
* L: 36
* ffn_hidden: 11008
* M: 11008/2048=5.375
* K/Vçš„ out_features: 256
* æŸ¥ config.json æ–‡ä»¶
    * num_heads = 16
    * num_KV_heads = 2  (GQA æœºåˆ¶ï¼Œè¡¨ç¤ºåªç”¨ 2 ä¸ª key/value å¤´ï¼ŒèŠ‚çœ KV-cache å¼€é”€)
* åˆ™
    * head_dim = 2048 / 16 = 128  (æ¯ä¸ª head æ˜¯ 128 ç»´)
    * num_group = 256 / 2 = 128
    * group_size: ä¸€ä¸ªkvç»„æœ‰8(=16/2)ä¸ªquery head


### å‚æ•°è®¡ç®—

* è¯åµŒå…¥å±‚: 151936 Ã— 2048 = 311,205,888 = 296.75M
* Transformer Block
    * Attention æ¨¡å—: L * H^2 (2 + 2/group_size)
        * = 36 * 2048^2 * (2 + 2/2)
        * = 324M
    * FFN æ¨¡å—: L Ã— 3 Ã— M Ã— H^2
        * = 36 Ã— 3 Ã— 5.375 Ã— 2048^2
        * = 2322M
* è¾“å‡ºå±‚: 151936 Ã— 2048 = 296.75M
* æ€»è®¡ç®—: 2322M + 324M +296.75M*2
    * = 3.16G














