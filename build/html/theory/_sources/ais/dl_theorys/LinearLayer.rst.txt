Linear Layer
############

* linear layer 也叫 feedforward or fully connected layer

为什么叫 Feedforward Layer
==========================

前馈网络的结构特点：

    单向信息流动：信息从输入层流向隐藏层，再到输出层，始终是单向的，没有循环或反馈路径

* 它只负责将输入通过权重矩阵变换后传递到下一层，没有反馈连接。
* 没有回环结构（如 RNN 的反馈）或自连接（如循环网络中的自激励机制）。


* Feedforward 强调的是 信息的前向流动：信息从输入层传到输出层，沿固定方向传播，没有循环。

.. code-block:: python

    # 输入、输出结构相同
    self.layers = nn.Sequential(
        nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
        GELU(),
        nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"]),
    )







为什么叫 Fully Connected Layer
==============================

* 连接方式：全连接
* 每个输入与每个输出相连：在线性层中，每个输入神经元与每个输出神经元都有权重连接，形成了全连接的网络结构。对于输入维度为 𝑛 和输出维度为 𝑚 的线性层，其权重矩阵是 𝑚×𝑛 大小，每个输入对每个输出都有一条连接。
* 信息融合：全连接层将所有输入组合在一起，通过权重和偏置调整，形成一个新的特征表示。这种连接方式可以捕获全局的信息，而不像卷积层那样局限于局部区域。


* Fully Connected 强调的是 输入与输出的连接方式：每个输入神经元与每个输出神经元都连接，权重矩阵 𝑊 包含所有这些连接的权重值。



应用场景与作用
==============


特征变换与映射::

    把输入数据从一个空间线性变换到另一个空间。
    通常用于从隐藏层提取高级特征，或生成最终的分类/回归输出。

优点::

    表达能力强，适用于各种任务。
    能够融合全局信息，适合用作网络的最后几层。

缺点::

    参数多（尤其在大规模输入情况下）。
    容易过拟合（需要正则化技术如 Dropout 或 L2 正则化）。




















