损失函数-分类-KL 散度(KL Loss)
##############################


* Kullback-Leibler Divergence
* Kullback-Leibler Divergence (KL散度)，又称为相对熵，是信息论中的一个概念，用于衡量两个概率分布之间的差异。在机器学习中，它常用于评估模型预测分布与真实分布之间的距离。



.. math::

    \begin{array}{l}
    L = \sum_i{P(i)log\frac{P(i)}{Q(i)}}\\
    \\
    离散分布的 KL散度：\\
    D_{\text{KL}}(P || Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}\\
    \\
    连续分布的 KL散度：\\
    D_{\text{KL}}(P || Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx\\
    \\
    P(x) 是真实分布（或目标分布）\\
    𝑄(𝑥) 是近似分布（或模型分布）\\
    𝐷_{KL}(𝑃∣∣𝑄) 表示  𝑃 和 𝑄 之间的“信息损失”
    \end{array}


* 优点：衡量两个概率分布之间的差异。
* 应用场景：分类任务中需要比较概率分布。



直观意义
========

信息损失的度量： KL散度衡量了如果用 𝑄 代替 P，会损失多少信息量::

    D_kl(P∣∣Q) 越小，表示 Q 越接近 P。
    D_kl(P∣∣Q)=0 表示 P=Q。

非对称性::

    D_kl(P∣∣Q) != D_kl(Q∣∣P)，
    这意味着它不是一种真正的“距离”，因为不满足对称性。



KL散度与交叉熵的关系
====================


* KL散度与交叉熵和熵的关系：:math:`D_{\text{KL}}(P || Q) = H(P, Q) - H(P)`
* 其中
* 交叉熵 H(P,Q) :math:`H(P, Q) = -\sum_{x} P(x) \log Q(x)`
* 熵 H(P) :math:`H(P) = -\sum_{x} P(x) \log P(x)`






变体或替代方法
==============

* 反向KL散度： 使用 :math:`D_{\text{KL}}(Q||P)` ，适用于某些需要强调 Q(x) 为 0 的场景。

* Jensen-Shannon Divergence (JS散度)： 是 KL散度的对称变体，定义为 :math:`D_{\text{JS}}(P || Q) = \frac{1}{2} D_{\text{KL}}(P || M) + \frac{1}{2} D_{\text{KL}}(Q || M)`
    * 其中 M 为 P 和 Q 的均值分布：:math:`M = \frac{1}{2}(P+Q)`

* Wasserstein距离： 用于生成对抗网络（GAN）中的分布对比，解决了 KL散度在某些场景下的数值问题。



























