损失函数-分类-负对数似然损失NLL Loss
####################################

* 负对数似然损失（Negative Log-Likelihood Loss, NLL Loss） 
* 一种常用的损失函数，特别是在分类任务中（如逻辑回归或神经网络的分类问题）。
* 本质是衡量模型预测的概率分布与真实标签分布之间的不匹配程度。

核心思想
========

* 负对数似然损失的目标是最小化负的对数似然值, 等价于最大化模型的预测概率。
* 假设模型输出的概率分布  :math:`p(y \mid x)`  反映输入  x  对应标签  y  的预测概率, 则负对数似然损失可以表示为:

.. math::

    \mathrm{NLL}=-\sum_{i=1}^{N} \log p\left(y_{i} \mid x_{i}\right)


- 其中:
    -  N  是样本数
    -  :math:`y_{i}`  是第  i  个样本的真实标签
    -  :math:`p\left(y_{i} \mid x_{i}\right)`  是模型预测的第  i  个样本的真实类别的概率

.. warning:: 【注意】torch是通过 ``torch.nll_loss`` 函数与 ``torch.log_softmax`` 函数一起实现的。


特性
====

* 对数形式：由于对数的性质，它会将低概率预测的损失放大，从而惩罚错误分类，提高模型对低概率样本的敏感度。
* 梯度友好：对数函数的平滑性使其在优化时容易计算梯度，从而提高训练稳定性。

示例
====

* 假设我们有一个模型的 softmax 输出为 ``[0.1,0.7,0.2]`` ，真实类别标签索引为 1（第二个类别）
* 则负对数似然损失为::

    NLL=−log(0.7)


优点与不足
==========

优点::

    对于概率分布的建模效果好。
    适合处理类别不平衡问题（可以通过设置 class weights 进行调节）。

不足::

    对异常值敏感，可能需要对输入进行正则化处理。
    对于预测分布过于集中的情况，可能导致梯度消失。





























