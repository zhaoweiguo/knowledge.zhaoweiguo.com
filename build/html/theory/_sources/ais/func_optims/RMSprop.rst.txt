RMSprop
#######

* RMSProp（Root Mean Square Propagation）
* RMSProp 是一种自适应学习率优化算法，用于解决标准梯度下降在处理非平稳目标函数（如深度学习任务）时的困难问题。它通过对梯度平方的指数加权平均动态调整学习率，特别适用于稀疏数据和非平稳损失函数。


核心思想
========

* RMSProp 的核心思想是对每个参数的梯度平方进行移动平均，进而调整学习率。
* 它的主要目标是::

    避免学习率因为梯度幅度过大而爆炸。
    解决梯度较小参数更新缓慢的问题。

* 通过动态调整学习率，RMSProp 平衡了收敛速度和稳定性。



公式
====

.. math::

    \begin{array}{l}
    1. 梯度平方的移动平均：\\
    E\left[g^{2}\right]_{t}=\beta E\left[g^{2}\right]_{t-1}+(1-\beta) g_{t}^{2} \\
    2. 参数更新 \\
    \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E\left[g^{2}\right]_{t}}+\epsilon} g_{t} \\
    \end{array}


* β 是衰减率（通常取 0.9）。
* η：学习率。
* :math:`g_t` ：当前梯度。
* :math:`E[g^2]_t` ：梯度平方的移动平均值。
* ϵ：一个小常数（通常是 10^−8 ），防止除零。


.. note:: 与标准 SGD 相比，RMSProp 为每个参数动态调整学习率，使得梯度较大的参数学习率更小，梯度较小的参数学习率更大。


优缺点
======

优点::

    自适应学习率：
        根据梯度平方的移动平均值动态调整学习率，无需手动调节。

    收敛更快：
        在非平稳目标函数（如深度学习任务）中表现稳定，并能快速收敛。

    适用于稀疏数据：
        RMSProp 在梯度稀疏的场景中表现良好，如 NLP 和推荐系统。

    易于实现：
        简单的公式和易调参数使其成为深度学习中的常用优化器。


局限性::

    依赖超参数：
        尽管 𝛽 和 𝜂 的默认值（如 0.9 和 0.001）适用大多数任务，但特定任务可能需要手动调整。

    泛化能力较弱：
        RMSProp 的正则化效果不如 SGD，可能导致泛化能力不足。







