WWM-Whole Word Masking
========================

* 全词Mask 或整词Mask，是谷歌在 2019 年 5 月 31 日发布的一项 BERT 的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于 WordPiece 的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被 mask。 在全词Mask 中，如果一个完整的词的部分 WordPiece 子词被 mask，则同属该词的其他部分也会被 mask，即全词Mask。
* 谷歌官方发布的 BERT-base, Chinese 中，中文是以字为粒度进行切分，没有考虑到传统 NLP 中的中文分词（CWS）。 我们将全词 Mask 的方法应用在了中文中，使用了中文维基百科（包括简体和繁体）进行训练，并且使用了哈工大 LTP 作为分词工具，即对组成同一个 词的汉字全部进行 Mask。——https://github.com/ymcui/Chinese-BERT-wwm

全词Mask 的生成样例::

    说明              样例
    原始文本           用语言模型来预测下一个词的 probability。
    分词文本           用 语言 模型 来 预测 下 一个 词 的 probability 。
    原始 Mask 输入     用 语 言 [MASK] 型 来 [MASK] 测 下 一 个 词 的 pro [MASK] ##lity
    全词 Mask 输入     用 语 言 [MASK] [MASK] 来 [MASK] [MASK] 下 一 个 词 的 [MASK] [MASK] [MASK]








