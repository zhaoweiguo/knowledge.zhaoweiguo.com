Embedding 模型
##############

* Embedding 模型在自然语言处理（NLP）和机器学习中的作用主要是将高维度、稀疏的离散数据（如单词、短语或其他类别）转换为低维度、稠密的实数向量。这些实数向量（embedding）能够捕捉到数据之间的语义关系和模式，从而使得计算和分析更加高效和有效。


常见的Embedding模型包括::

    Word2Vec：将单词表示为向量，通过上下文窗口进行训练。
    GloVe：基于全局词共现矩阵进行训练，生成词向量。
    FastText：扩展了Word2Vec，通过考虑子词信息提高表示能力。
    BERT：基于Transformer架构的预训练模型，可以捕捉上下文依赖关系。



向量
====

* 向量是指在数学中具有一定大小和方向的量，文本、图片、音视频等非结构化数据， 通过机器学习/深度学习模型 Embedding 提取出来的“特征” 用数学中的向量来表示。


特征向量
========

* 特征向量是包含事物重要特征的向量。大家比较熟知的一个特征向量是 RGB（红-绿-蓝）色彩，每种颜色都可以通过对红(R)、绿(G)、蓝(B)三种颜色的比例来得到，这样一个特征向量可以描述为：颜色 = [红，绿，蓝]。对于一个像素点，我们可以用数组 [255, 255, 255] 表示白色，用数组 [0, 0, 0] 表示黑色，这里 [255, 255, 255]、[0, 0, 0] 可以认为是该像素点的特征向量。


Embedding
=========

* 通过深度学习神经网络提取非结构化数据里的内容和语义，把图片、视频等变成特征向量，这个过程叫Embedding。


向量相似度检索
==============

* 相似度检索是指将目标对象与数据库中数据进行比对，并召回最相似的结果。
* 同理，向量相似度检索是一种基于向量空间模型的检索方法，用于计算和比较两个向量之间的相似度，返回最相似的向量数据。如果两条向量十分相似，意味着他们所代表的源数据（例如图片）也十分相似。

向量相似度检索算法
==================

* 内积（ip）: 全称是 Inner Product，内积，该算法基于向量的内积，即两个元素的对应元素相乘并求和的结果计算相似度，内积值越大相似度越高。
* 欧几里得距离（L2）: 欧几里得距离，它计算两个向量的欧几里得空间距离，欧式距离越小相似度越高。
* 余弦相似度（cosine）: 余弦相似度（Cosine Similarity），也称为余弦距离（Cosine Distance），用于计算两个高维向量的夹角余弦值从而衡量向量相似度，夹角余弦值越小表示两向量的夹角越大，则两个向量差异越大。






























