Kaggle Learn
############






Intermediate Machine Learning
=============================



Missing Values
--------------

There are many ways data can end up with missing values. For example::

    A 2 bedroom house won't include a value for the size of a third bedroom.
    A survey respondent may choose not to share his income.

* Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with missing values. So you'll need to choose one of the strategies below.
* Three Approaches

1) A Simple Option: Drop Columns with Missing Values
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/03/ssXZZf.png

    Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!) information with this approach.


2) A Better Option: Imputation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/03/2svo87.png

    Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each column.

The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would get from dropping the column entirely.


.. note:: why did imputation perform better than dropping the columns? The training data has 10864 rows and 12 columns, where three columns contain missing data. For each column, less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better.

.. note:: Given that there are so few missing values in the dataset, we'd expect imputation to perform better than dropping columns entirely. However, we see that dropping columns performs slightly better! While this can probably partially be attributed to noise in the dataset, another potential explanation is that the imputation method is not a great match to this dataset. That is, maybe instead of filling in the mean value, it makes more sense to set every missing value to a value of 0, to fill in the most frequently encountered value, or to use some other method. 




3) An Extension To Imputation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/03/B8ntBJ.png

    In this approach, we impute the missing values, as before. And, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.



Imputation is the standard approach, and it usually works well. However, imputed values may be systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. In that case, your model would make better predictions by considering which values were originally missing.


Categorical Variables
---------------------

A categorical variable takes only a limited number of values.

* Consider a survey that asks how often you eat breakfast and provides four options: "Never", "Rarely", "Most days", or "Every day". In this case, the data is categorical, because responses fall into a fixed set of categories.
* If people responded to a survey about which what brand of car they owned, the responses would fall into categories like "Honda", "Toyota", and "Ford". In this case, the data is also categorical.
* Three Approaches




1) Drop Categorical Variables
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This approach will only work well if the columns did not contain useful information.


2) Ordinal Encoding
^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/03/6VimYm.png

    Ordinal encoding assigns each unique value to a different integer.

* This approach assumes an ordering of the categories: "Never" (0) < "Rarely" (1) < "Most days" (2) < "Every day" (3).

* This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables. For tree-based models (like decision trees and random forests), you can expect ordinal encoding to work well with ordinal variables.

* OrdinalEncoder 是 scikit-learn 库中的一个预处理工具，用于将分类特征转换为整数编码。它主要用于处理具有有序类别的特征，即特征的分类值具有一定的顺序关系。




3) One-Hot Encoding
^^^^^^^^^^^^^^^^^^^


.. figure:: https://img.zhaoweiguo.com/uPic/2024/03/ITSWrC.png

    One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.

* In the original dataset, "Color" is a categorical variable with three categories: "Red", "Yellow", and "Green". The corresponding one-hot encoding contains one column for each possible value, and one row for each row in the original dataset. Wherever the original value was "Red", we put a 1 in the "Red" column; if the original value was "Yellow", we put a 1 in the "Yellow" column, and so on.

* In contrast to ordinal encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g., "Red" is neither more nor less than "Yellow"). We refer to categorical variables without an intrinsic ranking as nominal variables.

* One-hot encoding generally does not perform well if the categorical variable takes on a large number of values (i.e., you generally won't use it for variables taking more than 15 different values).

* OneHotEncoder 是 scikit-learn 库中的一个预处理工具，用于将分类特征转换为独热编码（One-Hot Encoding）。它将每个分类特征的每个类别转换为一个新的特征，并使用 0 和 1 来表示是否存在该类别。


.. note:: In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.




Pipelines
---------

* Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.

Many data scientists hack together models without pipelines, but pipelines have some important benefits. Those include::

    1. Cleaner Code: Accounting for data at each step of preprocessing can get messy.
    2. Fewer Bugs: There are fewer opportunities to misapply a step or forget a preprocessing step.
    3. Easier to Productionize: pipelines can help to transition a model from a prototype to something deployable at scale.
    4. More Options for Model Validation:

示例::

    # Preprocessing for numerical data
    numerical_transformer = SimpleImputer(strategy='constant')

    # Preprocessing for categorical data
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Bundle preprocessing for numerical and categorical data
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_cols),
            ('cat', categorical_transformer, categorical_cols)
        ])







































