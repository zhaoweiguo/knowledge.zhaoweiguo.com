åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ (Dive into Deep Learning)
#######################################


* è‹±æ–‡ç‰ˆ: https://d2l.ai/
* GitHubå¼€æºåœ°å€: https://github.com/d2l-ai/d2l-en
* ä¸­æ–‡ç‰ˆ: https://zh.d2l.ai/
* GitHubå¼€æºåœ°å€: https://github.com/d2l-ai/d2l-zh
* å‘å¸ƒäº2021å¹´(å¤§æ¨¡å‹çœŸæ­£å…´èµ·å‰)


å‰è¨€
====


* ç¬¬ 1 éƒ¨åˆ†ï¼šåŸºç¡€çŸ¥è¯†å’Œé¢„å¤‡çŸ¥è¯†ã€‚
    * ç¬¬ 1 èŠ‚ æ˜¯æ·±åº¦å­¦ä¹ çš„ç®€ä»‹ã€‚
    * ç¬¬ 2 èŠ‚ï¼Œæˆ‘ä»¬å°†å¿«é€Ÿå¸¦æ‚¨äº†è§£å®è·µæ·±åº¦å­¦ä¹ æ‰€éœ€çš„å…ˆå†³æ¡ä»¶ï¼Œä¾‹å¦‚å¦‚ä½•å­˜å‚¨å’Œæ“ä½œæ•°æ®ï¼Œä»¥åŠå¦‚ä½•åº”ç”¨åŸºäºçº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†å’Œæ¦‚ç‡çš„åŸºæœ¬æ¦‚å¿µçš„å„ç§æ•°å€¼è¿ç®—ã€‚
    * ç¬¬ 3 èŠ‚åˆ°ç¬¬ 5 èŠ‚æ¶µç›–æ·±åº¦å­¦ä¹ ä¸­æœ€åŸºæœ¬çš„æ¦‚å¿µå’ŒæŠ€æœ¯ï¼ŒåŒ…æ‹¬å›å½’å’Œåˆ†ç±»ï¼›çº¿æ€§æ¨¡å‹ï¼›å¤šå±‚æ„ŸçŸ¥å™¨ï¼›ä»¥åŠè¿‡åº¦æ‹Ÿåˆå’Œæ­£åˆ™åŒ–ã€‚

* ç¬¬ 2 éƒ¨åˆ†ï¼šç°ä»£æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚ 
    * ç¬¬ 6 èŠ‚æè¿°äº†å…³é”®çš„è®¡ç®—æ·±åº¦å­¦ä¹ ç³»ç»Ÿçš„ç»„æˆéƒ¨åˆ†ï¼Œå¹¶ä¸ºæˆ‘ä»¬çš„æ›´å¤æ‚æ¨¡å‹çš„åç»­å®ç°ã€‚
    * ç¬¬ 7 æ¡å’Œç¬¬ 8 æ¡å­˜åœ¨ å·ç§¯ç¥ç»ç½‘ç»œ (CNN) æ˜¯æ„æˆ å¤§å¤šæ•°ç°ä»£è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„æ”¯æŸ±ã€‚
    * ç¬¬ 9 èŠ‚å’Œç¬¬ 10 èŠ‚ä»‹ç» å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)ï¼Œåˆ©ç”¨é¡ºåºæ¨¡å‹ æ•°æ®ä¸­çš„ï¼ˆä¾‹å¦‚ï¼Œæ—¶é—´ï¼‰ç»“æ„ï¼Œé€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†å’Œæ—¶é—´åºåˆ—é¢„æµ‹ã€‚
    * ç¬¬ 11 èŠ‚ï¼Œæˆ‘ä»¬æè¿°äº†ä¸€ç±»ç›¸å¯¹è¾ƒæ–°çš„æ¨¡å‹ï¼ŒåŸºäºæ‰€è°“çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒå·²ç»å–ä»£ RNN æˆä¸ºå¤§å¤šæ•°è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ä¸»å¯¼æ¶æ„ã€‚è¿™äº›éƒ¨åˆ†å°†å¸¦æ‚¨å¿«é€Ÿäº†è§£æ·±åº¦å­¦ä¹ ä»ä¸šè€…å¹¿æ³›ä½¿ç”¨çš„æœ€å¼ºå¤§ã€æœ€é€šç”¨çš„å·¥å…·ã€‚

* ç¬¬ 3 éƒ¨åˆ†ï¼šå¯æ‰©å±•æ€§ã€æ•ˆç‡å’Œåº”ç”¨ç¨‹åºã€‚
    * åœ¨ç¬¬ 12 ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å‡ ç§ç”¨äºè®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¸¸è§ä¼˜åŒ–ç®—æ³•ã€‚
    * åœ¨ç¬¬ 13 ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†å½±å“æ·±åº¦å­¦ä¹ ä»£ç è®¡ç®—æ€§èƒ½çš„å‡ ä¸ªå…³é”®å› ç´ ã€‚
    * åœ¨ç¬¬ 14 ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯´æ˜æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸»è¦åº”ç”¨ã€‚
    * åœ¨ç¬¬ 15 ç« å’Œç¬¬ 16 ç« ä¸­ï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†å¦‚ä½•é¢„è®­ç»ƒè¯­è¨€è¡¨ç¤ºæ¨¡å‹å¹¶å°†å…¶åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚


Notation
--------

Numerical Objects:
^^^^^^^^^^^^^^^^^^

* x: a scalar
* **x**: a vector
* **X**: a matrix
* X: a general tensor
* **I**: the identity matrix (of some given dimension), i.e., a square matrix with 1 on all diagonal entries and 0 on all off-diagonals
* :math:`x_i, [x]_i`, : the :math:`i^{th}` element of vector 
* :math:`x_{ij}, x_{i,j}, [X]_{ij}, [X]_{i,j}`: the element of matrix **X** at row i and column j.


Set Theory
^^^^^^^^^^

- X: a set
-  :math:`\mathbb{Z}`  : the set of integers
-  :math:`\mathbb{Z}^{+}` : the set of positive integers
-  :math:`\mathbb{R}`  : the set of real numbers
-  :math:`\mathbb{R}^{n}`  : the set of  n-dimensional vectors of real numbers
-  :math:`\mathbb{R}^{a \times b}`  : The set of matrices of real numbers with  a  rows and  b  columns
-  :math:`|\mathcal{X}|`  : cardinality (number of elements) of set  :math:`\mathcal{X}` 
-  :math:`\mathcal{A} \cup \mathcal{B}`  : union of sets  :math:`\mathcal{A}`  and  :math:`\mathcal{B}`
-  :math:`\mathcal{A} \cap \mathcal{B}`  : intersection of sets  :math:`\mathcal{A}`  and  :math:`\mathcal{B}`
-  :math:`\mathcal{A} \backslash \mathcal{B}`  : set subtraction of  :math:`\mathcal{B}`  from  :math:`\mathcal{A}`  (contains only those elements of  :math:`\mathcal{A}`  that do not belong to  :math:`\mathcal{B}`  )

Functions and Operators
^^^^^^^^^^^^^^^^^^^^^^^

-  :math:`f(\cdot)` :  a function
-  :math:`\log (\cdot)`  : the natural logarithm (base  e  )
-  :math:`\log _{2}(\cdot)`  : logarithm to base 2
-  :math:`\exp (\cdot)`  : the exponential function
-  :math:`\mathbf{1}(\cdot)`  : the indicator function; evaluates to 1 if the boolean argument is true, and 0 otherwise
-  :math:`\mathbf{1}_{\mathcal{X}}(z)`  : the set-membership indicator function; evaluates to 1 if the element  z  belongs to the set  \mathcal{X}  and 0 otherwise
-  :math:`(\cdot)^{\top}`  : transpose of a vector or a matrix
-  :math:`\mathbf{X}^{-1}`  : inverse of matrix  \mathbf{X} 
-  :math:`\odot`  : Hadamard (elementwise) product
-  :math:`[\cdot, . ]` : concatenation 
-  :math:`\|\cdot\|_{p}: \ell_{p}`  norm
-  :math:`\|\cdot\|: \ell_{2}`  norm
-  :math:`\langle\mathbf{x}, \mathbf{y}\rangle` : inner (dot) product of vectors  :math:`\mathbf{x}`  and  :math:`\mathbf{y}`
-  :math:`\sum`  : summation over a collection of elements
-  :math:`\Pi`  : product over a collection of elements
- =: an equality asserted as a definition of the symbol on the left-hand side

Calculus
^^^^^^^^

-  :math:`\frac{d y}{d x}`  : derivative of  y  with respect to  x 
-  :math:`\frac{\partial y}{\partial x}`  : partial derivative of  y  with respect to  x 
-  :math:`\nabla_{\mathrm{x}} y`  : gradient of  y  with respect to  x
-  :math:`\int_{a}^{b} f(x) d x`  : definite integral of  f  from  a  to  b  with respect to  x 
-  :math:`\int f(x) d x`  : indefinite integral of  f  with respect to  x 


Probability and Information Theory
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

-  X  : a random variable
-  P  : a probability distribution
-  :math:`X \sim P`  : the random variable  X  follows distribution  P 
-  :math:`X \sim N(Î¼,Ïƒ^2 )` :éšæœºå˜é‡ X æœä»å‡å€¼ä¸º Î¼ï¼Œæ–¹å·®ä¸º Ïƒ^2  çš„é«˜æ–¯åˆ†å¸ƒ(æ­£æ€åˆ†å¸ƒ)
-  :math:`\epsilon \sim \mathcal{N}\left(0, 0.01^{2}\right)` : éšæœºå˜é‡ Ïµ æœä»ä¸€ä¸ªé«˜æ–¯ï¼ˆæ­£æ€ï¼‰åˆ†å¸ƒï¼Œå…¶å‡å€¼ï¼ˆmeanï¼‰ä¸º 0ï¼Œæ–¹å·®ï¼ˆvarianceï¼‰ä¸º :math:`0.01 ^ 2`
-  P(X=x)  : the probability assigned to the event where random variable  X  takes value  x 
-  :math:`P(X \mid Y)`  : the conditional probability distribution of  X  given  Y 
-  :math:`p(\cdot)`  : a probability density function (PDF) associated with distribution  P 
-  E[X]  : expectation of a random variable  X 
-  :math:`X \perp Y`  : random variables  X  and  Y  are independent
-  :math:`X \perp Y \mid Z`  : random variables  X  and  Y  are conditionally independent given  Z 
-  :math:`\sigma_{X}`  : standard deviation of random variable  X 
-  :math:`\operatorname{Var}(X)`  : variance of random variable  X , equal to  \sigma_{X}^{2} 
-  :math:`\operatorname{Cov}(X, Y)`  : covariance of random variables  X  and  Y 

é¢å¤–çš„
^^^^^^

-  :math:`\rho(X, Y)`  : the Pearson correlation coefficient between  X  and  Y , equals :math:`\frac{\operatorname{Cov}(X, Y)}{\sigma_{X} \sigma_{Y}}`
-  H(X) : entropy of random variable  X
-  H(P, Q): cross-entropy from P to Q
-  :math:`D_{\mathrm{KL}}(P \| Q)`  : the KL-divergence (or relative entropy) from distribution  Q  to distribution  P 
- :math:`P(y=i) \propto \exp o_{i}` : è¡¨ç¤ºç±»åˆ« y æ˜¯ i çš„æ¦‚ç‡ä¸ :math:`o^i` çš„æŒ‡æ•°å‡½æ•°æˆæ­£æ¯”(è¿™å„¿ :math:`o^i` é€šå¸¸æŒ‡çš„æ˜¯æ¨¡å‹è¾“å‡ºçš„æœªç»å½’ä¸€åŒ–çš„å¯¹æ•°å‡ ç‡ï¼ˆlogitsï¼‰ï¼Œå³æ¨¡å‹å¯¹äºæ¯ä¸ªç±»åˆ«çš„åŸå§‹é¢„æµ‹å€¼)
- :math:`R` : è¡¨ç¤ºé£é™©æˆ–è¯¯å·®ï¼Œè¡¨ç¤ºæ³›åŒ–è¯¯å·® (Generalization Error) 
- :math:`R_{\text{emp}}` : è¡¨ç¤º ç»éªŒé£é™© (Empirical Risk)ï¼Œä¹Ÿç§°ä¸º è®­ç»ƒè¯¯å·® (Training Error)




Part 1: Basics and Preliminaries
================================

1. Introduction
---------------

Key Components
^^^^^^^^^^^^^^

* Data
* Model
* Function that quantifies how well (or badly) the model is doing
* Algorithm to adjust the modelâ€™s parameters to optimize the objective function.

::

    ç‰¹å¾ï¼ˆåå˜é‡æˆ– è¾“å…¥ï¼‰-----> æ ‡ç­¾ï¼ˆæˆ–ç›®æ ‡ï¼‰
    features (covariates or inputs) -----> label (or target)

Kinds of Machine Learning Problems
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Supervised Learning
"""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/IJaPfO.png

    Fig. 1.3.1 Supervised learning.

* Regression
* Classification
* Tagging
* Search(e.g. PageRank)
* Recommender Systems
* Sequence Learning
    * Tagging and Parsing(æ ‡è®°å’Œè§£æ): å¦‚è¯æ€§ï¼ˆPoSï¼‰æ ‡è®°, å‘½åå®ä½“è¯†åˆ«
    * Automatic Speech Recognition(è‡ªåŠ¨è¯­éŸ³è¯†åˆ«)
    * Text to Speech(æ–‡å­—è½¬è¯­éŸ³)
    * Machine Translation(æœºå™¨ç¿»è¯‘)



.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/rOICXJ.png

    [Tagging]å½“åˆ†ç±»å™¨é‡åˆ°è¿™ç§å›¾åƒæ—¶ï¼Œæˆ‘ä»¬è‡ªå·±å°±ä¼šé‡åˆ°éº»çƒ¦ã€‚å­¦ä¹ é¢„æµ‹ä¸äº’æ–¥çš„ç±»çš„é—®é¢˜ç§°ä¸ºå¤šæ ‡ç­¾åˆ†ç±»ã€‚è‡ªåŠ¨æ ‡è®°é—®é¢˜é€šå¸¸æœ€å¥½ç”¨å¤šæ ‡ç­¾åˆ†ç±»æ¥æè¿°ã€‚





Unsupervised and Self-Supervised Learning
"""""""""""""""""""""""""""""""""""""""""

* æ— ç›‘ç£å­¦ä¹ çš„è¿›ä¸€æ­¥å‘å±•æ˜¯: è‡ªæˆ‘ç›‘ç£å­¦ä¹ 
* è‡ªæˆ‘ç›‘ç£å­¦ä¹ ï¼šåˆ©ç”¨æŸäº›æ–¹é¢çš„æŠ€æœ¯ï¼Œä½¿ç”¨æœªæ ‡è®°çš„æ•°æ®æä¾›ç›‘ç£ã€‚
* å¯¹äºæ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ¨¡å‹ é€šè¿‡ä½¿ç”¨å®ƒä»¬é¢„æµ‹éšæœºå±è”½çš„å•è¯æ¥â€œå¡«ç©ºâ€ å¤§è¯­æ–™åº“ä¸­çš„å‘¨å›´å•è¯ï¼ˆä¸Šä¸‹æ–‡ï¼‰ï¼Œæ— éœ€ä»»ä½•æ ‡è®°å·¥ä½œ
* å¯¹äºå›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒæ¨¡å‹ å‘Šè¯‰åŒä¸€å›¾åƒçš„ä¸¤ä¸ªè£å‰ªåŒºåŸŸä¹‹é—´çš„ç›¸å¯¹ä½ç½®ï¼ŒåŸºäºå›¾åƒçš„å‰©ä½™éƒ¨åˆ†æ¥é¢„æµ‹å›¾åƒçš„è¢«é®æŒ¡éƒ¨åˆ†ï¼Œæˆ–è€…é¢„æµ‹ä¸¤ä¸ªç¤ºä¾‹æ˜¯å¦æ˜¯åŒä¸€åº•å±‚å›¾åƒçš„å˜åŠ¨ç‰ˆæœ¬ã€‚

Interacting with an Environment
"""""""""""""""""""""""""""""""

* å‰é¢çš„ç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ éƒ½ä¼šé¢„å…ˆè·å–å¤§é‡æ•°æ®ï¼Œç„¶åå¯åŠ¨æ¨¡å¼è¯†åˆ«æœºå™¨ï¼Œè€Œæ— éœ€å†æ¬¡ä¸ç¯å¢ƒäº¤äº’ã€‚
* å› ä¸ºæ‰€æœ‰çš„å­¦ä¹ éƒ½æ˜¯åœ¨ç®—æ³•ä¸ç¯å¢ƒæ–­å¼€è¿æ¥ä¹‹åè¿›è¡Œçš„ï¼Œæ‰€ä»¥è¿™æœ‰æ—¶è¢«ç§°ä¸ºç¦»çº¿å­¦ä¹ 

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/K3Ulip.png

    Fig. 1.3.6 Collecting data for supervised learning from an environment.


Reinforcement Learning
""""""""""""""""""""""

* å¼ºåŒ–å­¦ä¹ ç»™å‡ºäº†ä¸€ä¸ªéå¸¸ç¬¼ç»Ÿçš„é—®é¢˜æè¿°ï¼Œå…¶ä¸­ä»£ç†é€šè¿‡ä¸€ç³»åˆ—æ—¶é—´æ­¥éª¤ä¸ç¯å¢ƒè¿›è¡Œäº¤äº’ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œä»£ç†éƒ½ä¼šä»ç¯å¢ƒä¸­æ¥æ”¶ä¸€äº›è§‚å¯Ÿç»“æœï¼Œå¹¶ä¸”å¿…é¡»é€‰æ‹©éšåä¼ è¾“çš„æ“ä½œ é€šè¿‡æŸç§æœºåˆ¶ï¼ˆæœ‰æ—¶ç§°ä¸º æ‰§è¡Œå™¨ï¼‰ï¼Œå½“æ¯æ¬¡å¾ªç¯ä¹‹åï¼Œä»£ç†æ”¶åˆ°æ¥è‡ªç¯å¢ƒçš„å¥–åŠ±ã€‚ç„¶åï¼Œä»£ç†æ¥æ”¶åç»­è§‚å¯Ÿï¼Œå¹¶é€‰æ‹©åç»­æ“ä½œï¼Œä¾æ­¤ç±»æ¨ã€‚å¼ºåŒ–å­¦ä¹ ä»£ç†çš„è¡Œä¸ºå—ç­–ç•¥æ§åˆ¶ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä¸€ä¸ª æ”¿ç­–åªæ˜¯å°†ç¯å¢ƒè§‚å¯Ÿæ˜ å°„åˆ°è¡ŒåŠ¨çš„å‡½æ•°ã€‚å¼ºåŒ–å­¦ä¹ çš„ç›®æ ‡æ˜¯äº§ç”Ÿå¥½çš„æ”¿ç­–ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/Tqv5o7.png


* å¼ºåŒ–å­¦ä¹ æ¡†æ¶çš„é€šç”¨æ€§æ€ä¹ˆå¼ºè°ƒéƒ½ä¸ä¸ºè¿‡ã€‚ä¸€èˆ¬çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜æœ‰ä¸€ä¸ªéå¸¸é€šç”¨çš„è®¾ç½®ã€‚è¡ŒåŠ¨ä¼šå½±å“éšåçš„è§‚å¯Ÿã€‚ä»…å½“å¥–åŠ±ä¸æ‰€é€‰æ“ä½œç›¸å¯¹åº”æ—¶æ‰ä¼šè§‚å¯Ÿåˆ°å¥–åŠ±ã€‚

* å½“ç¯å¢ƒè¢«å……åˆ†è§‚å¯Ÿæ—¶ï¼Œæˆ‘ä»¬å°†å¼ºåŒ–å­¦ä¹ é—®é¢˜ç§°ä¸º``é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov decision process)``
* å½“çŠ¶æ€ä¸ä¾èµ–äºå…ˆå‰çš„åŠ¨ä½œæ—¶ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸º ``ä¸Šä¸‹æ–‡å¼ºç›—é—®é¢˜(contextual bandit problem)``
* å½“æ²¡æœ‰çŠ¶æ€ï¼Œåªæœ‰ä¸€ç»„åˆå§‹å¥–åŠ±æœªçŸ¥çš„å¯ç”¨åŠ¨ä½œæ—¶ï¼Œæˆ‘ä»¬å°±ä¼šé‡åˆ°ç»å…¸çš„ ``å¤šè‡‚è€è™æœºé—®é¢˜(multi-armed bandit problem)``


Roots
^^^^^

* å¯¹äºä¸€ç³»åˆ—ä¸åŒçš„æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œæ·±åº¦å­¦ä¹  å­¦ä¹ ä¸ºä»–ä»¬çš„è§£å†³æ–¹æ¡ˆæä¾›äº†å¼ºå¤§çš„å·¥å…·ã€‚è™½ç„¶å¾ˆå¤šæ·± å­¦ä¹ æ–¹æ³•æ˜¯æœ€è¿‘çš„å‘æ˜ï¼Œå­¦ä¹ èƒŒåçš„æ ¸å¿ƒæ€æƒ³ å‡ ä¸ªä¸–çºªä»¥æ¥ï¼Œäººä»¬ä¸€ç›´åœ¨ç ”ç©¶æ•°æ®ã€‚äº‹å®ä¸Šï¼Œäººç±»å·²ç»æŒæ¡äº† æ¸´æœ›åˆ†ææ•°æ®å¹¶é¢„æµ‹æœªæ¥çš„ç»“æœï¼Œå¹¶ä¸”å®ƒ è¿™ç§æ„¿æœ›æ˜¯è®¸å¤šè‡ªç„¶ç§‘å­¦çš„æ ¹æº æ•°å­¦ã€‚ä¸¤ä¸ªä¾‹å­æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œä»¥ é›…å„å¸ƒÂ·ä¼¯åŠªåˆ©ï¼ˆJacob Bernoulliï¼Œ1655-1705ï¼‰ ï¼Œä»¥åŠå¡å°”Â·å¼—é‡Œå¾·é‡Œå¸ŒÂ·é«˜æ–¯ï¼ˆCarl Friedrich Gaussï¼Œ1777-1855ï¼‰å‘ç°çš„é«˜æ–¯åˆ†å¸ƒã€‚

* éšç€æ•°æ®çš„å¯ç”¨æ€§å’Œæ”¶é›†ï¼Œç»Ÿè®¡æ•°æ®çœŸæ­£èµ·é£ã€‚å®ƒçš„å…ˆé©±ä¹‹ä¸€ç½—çº³å¾·Â·è´¹å¸Œå°”ï¼ˆRonald Fisherï¼Œ1890-1962ï¼‰å¯¹å…¶ç†è®ºåŠå…¶åœ¨é—ä¼ å­¦ä¸­çš„åº”ç”¨åšå‡ºäº†é‡å¤§è´¡çŒ®ã€‚ä»–çš„è®¸å¤šç®—æ³•ï¼ˆä¾‹å¦‚çº¿æ€§åˆ¤åˆ«åˆ†æï¼‰å’Œæ¦‚å¿µï¼ˆä¾‹å¦‚è´¹èˆå°”ä¿¡æ¯çŸ©é˜µï¼‰ä»ç„¶åœ¨ç°ä»£ç»Ÿè®¡å­¦çš„åŸºç¡€ä¸­å æœ‰é‡è¦åœ°ä½ã€‚Fisher äº 1936 å¹´å‘å¸ƒçš„ Iris æ•°æ®é›†æœ‰æ—¶ä»ç”¨äºæ¼”ç¤ºæœºå™¨å­¦ä¹ ç®—æ³•ã€‚
* å¯¹æœºå™¨å­¦ä¹ çš„å…¶ä»–å½±å“æ¥è‡ªå…‹åŠ³å¾·Â·é¦™å†œï¼ˆ1916-2001ï¼‰çš„ä¿¡æ¯è®ºå’Œè‰¾ä¼¦Â·å›¾çµï¼ˆ1912-1954ï¼‰æå‡ºçš„è®¡ç®—ç†è®ºã€‚
* è¿›ä¸€æ­¥çš„å½±å“æ¥è‡ªç¥ç»ç§‘å­¦å’Œå¿ƒç†å­¦ã€‚æ¯•ç«Ÿï¼Œ äººç±»æ˜æ˜¾è¡¨ç°å‡ºæ™ºèƒ½è¡Œä¸ºã€‚å¾ˆå¤šå­¦è€…éƒ½é—®è¿‡ æ˜¯å¦å¯ä»¥è§£é‡Šå¹¶å¯èƒ½å¯¹è¿™ç§èƒ½åŠ›è¿›è¡Œé€†å‘å·¥ç¨‹ã€‚ ç¬¬ä¸€ä¸ªå—ç”Ÿç‰©å­¦å¯å‘çš„ç®—æ³•æ˜¯ç”± å”çº³å¾·Â·èµ«å¸ƒ (1904â€“1985) ã€‚åœ¨ä»–çš„å¼€åˆ›æ€§è‘—ä½œã€Šè¡Œä¸ºçš„ç»„ç»‡ã€‹ï¼ˆ Hebbï¼Œ1949 ï¼‰ä¸­ï¼Œä»–å‡è®¾ç¥ç»å…ƒé€šè¿‡æ­£å¼ºåŒ–è¿›è¡Œå­¦ä¹ ã€‚è¿™è¢«ç§°ä¸ºèµ«å¸ƒå­¦ä¹ è§„åˆ™ã€‚è¿™äº›æƒ³æ³•å¯å‘äº†åæ¥çš„å·¥ä½œï¼Œä¾‹å¦‚ç½—æ£®å¸ƒæ‹‰ç‰¹çš„æ„ŸçŸ¥å™¨å­¦ä¹ ç®—æ³•ï¼Œå¹¶ä¸ºå½“ä»Šæ·±åº¦å­¦ä¹ çš„è®¸å¤šéšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•å¥ å®šäº†åŸºç¡€ï¼šå¼ºåŒ–æœŸæœ›çš„è¡Œä¸ºå¹¶å‡å°‘ä¸è‰¯è¡Œä¸ºï¼Œä»¥è·å¾—ç¥ç»ç½‘ç»œä¸­å‚æ•°çš„è‰¯å¥½è®¾ç½®ã€‚
* ç¥ç»ç½‘ç»œçš„åå­—æ¥æºäºç”Ÿç‰©å­¦çµæ„Ÿã€‚ä¸€ä¸ªå¤šä¸–çºªä»¥æ¥ï¼ˆå¯ä»¥è¿½æº¯åˆ° 1873 å¹´ Alexander Bain å’Œ 1890 å¹´ James Sherrington çš„æ¨¡å‹ï¼‰ï¼Œç ”ç©¶äººå‘˜ä¸€ç›´è¯•å›¾ç»„è£…ç±»ä¼¼äºç›¸äº’ä½œç”¨ç¥ç»å…ƒç½‘ç»œçš„è®¡ç®—ç”µè·¯ã€‚



The Road to Deep Learning
^^^^^^^^^^^^^^^^^^^^^^^^^
.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/MeU9NJ.png

    è¡¨1.5.1 æ•°æ®é›†vsè®¡ç®—æœºå†…å­˜å’Œè®¡ç®—èƒ½åŠ›



* æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡çš„æœ€ä½³ç»“åˆç‚¹ ä»ï¼ˆå¹¿ä¹‰ï¼‰çº¿æ€§æ¨¡å‹å’Œæ ¸æ–¹æ³•è½¬å‘æ·±åº¦ç¥ç»ç½‘ç»œ ç½‘ç»œã€‚è¿™ä¹Ÿæ˜¯å¾ˆå¤šä¸­æµç ¥æŸ±çš„åŸå› ä¹‹ä¸€ æ·±åº¦å­¦ä¹ ï¼Œä¾‹å¦‚å¤šå±‚æ„ŸçŸ¥å™¨ ï¼ˆ McCulloch å’Œ Pittsï¼Œ1943 ï¼‰ ï¼Œå·ç§¯ç¥ç»ç½‘ç»œ ï¼ˆ LeCunç­‰ï¼Œ1998 ï¼‰ ï¼Œé•¿çŸ­æœŸè®°å¿† ï¼ˆ Hochreiter å’Œ Schmidhuberï¼Œ1997 ï¼‰å’Œ Q-Learning ï¼ˆ Watkins å’Œ Dayanï¼Œ1992 ï¼‰ 


* ä¸‹é¢åˆ—ä¸¾äº†å¸®åŠ©ç ”ç©¶äººå‘˜åœ¨è¿‡å»åå¹´ä¸­å–å¾—å·¨å¤§è¿›æ­¥çš„æƒ³æ³•
    * æ–°çš„å®¹é‡æ§åˆ¶æ–¹æ³•ï¼Œå¦‚dropout (Srivastava et al., 2014)ï¼Œæœ‰åŠ©äºå‡è½»è¿‡æ‹Ÿåˆçš„å±é™©ã€‚è¿™æ˜¯é€šè¿‡åœ¨æ•´ä¸ªç¥ç»ç½‘ç»œä¸­åº”ç”¨å™ªå£°æ³¨å…¥ (Bishop, 1995) æ¥å®ç°çš„ï¼Œå‡ºäºè®­ç»ƒç›®çš„ï¼Œç”¨éšæœºå˜é‡æ¥ä»£æ›¿æƒé‡ã€‚
    * æ³¨æ„åŠ›æœºåˆ¶è§£å†³äº†å›°æ‰°ç»Ÿè®¡å­¦ä¸€ä¸ªå¤šä¸–çºªçš„é—®é¢˜ï¼šå¦‚ä½•åœ¨ä¸å¢åŠ å¯å­¦ä¹ å‚æ•°çš„æƒ…å†µä¸‹å¢åŠ ç³»ç»Ÿçš„è®°å¿†å’Œå¤æ‚æ€§ã€‚
    * å¤šé˜¶æ®µè®¾è®¡ã€‚ä¾‹å¦‚ï¼Œå­˜å‚¨å™¨ç½‘ç»œ (Sukhbaatar et al., 2015) å’Œç¥ç»ç¼–ç¨‹å™¨-è§£é‡Šå™¨ (Reed and De Freitas, 2015)ã€‚å®ƒä»¬å…è®¸ç»Ÿè®¡å»ºæ¨¡è€…æè¿°ç”¨äºæ¨ç†çš„è¿­ä»£æ–¹æ³•ã€‚
    * ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (Goodfellow et al., 2014) ã€‚ä¼ ç»Ÿæ¨¡å‹ä¸­ï¼Œå¯†åº¦ä¼°è®¡å’Œç”Ÿæˆæ¨¡å‹çš„ç»Ÿè®¡æ–¹æ³•ä¾§é‡äºæ‰¾åˆ°åˆé€‚çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯è¿‘ä¼¼çš„ï¼‰å’ŒæŠ½æ ·ç®—æ³•ã€‚å› æ­¤ï¼Œè¿™äº›ç®—æ³•åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°ç»Ÿè®¡æ¨¡å‹å›ºæœ‰çµæ´»æ€§çš„é™åˆ¶ã€‚ç”Ÿæˆå¼å¯¹æŠ—æ€§ç½‘ç»œçš„å…³é”®åˆ›æ–°æ˜¯ç”¨å…·æœ‰å¯å¾®å‚æ•°çš„ä»»æ„ç®—æ³•ä»£æ›¿é‡‡æ ·å™¨ã€‚ç„¶åå¯¹è¿™äº›æ•°æ®è¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—é‰´åˆ«å™¨ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªåŒæ ·æœ¬æµ‹è¯•ï¼‰ä¸èƒ½åŒºåˆ†å‡æ•°æ®å’ŒçœŸå®æ•°æ®ã€‚é€šè¿‡ä½¿ç”¨ä»»æ„ç®—æ³•ç”Ÿæˆæ•°æ®çš„èƒ½åŠ›ï¼Œå®ƒä¸ºå„ç§æŠ€æœ¯æ‰“å¼€äº†å¯†åº¦ä¼°è®¡çš„å¤§é—¨ã€‚


2. Preliminaries
----------------

survival skills::

    1) techniques for storing and manipulating data; 
    2) libraries for ingesting and preprocessing data from a variety of sources; 
    3) knowledge of the basic linear algebraic operations that we apply to high-dimensional data elements; 
    4) just enough calculus to determine which direction to adjust each parameter in order to decrease the loss function; 
    5) the ability to automatically compute derivatives so that you can forget much of the calculus you just learned; 
    6) some basic fluency in probability, our primary language for reasoning under uncertainty; and 
    7) some aptitude for finding answers in the official documentation when you get stuck.

2.1 Data Manipulation
^^^^^^^^^^^^^^^^^^^^^

* å¹¿æ’­æœºåˆ¶
* ç´¢å¼•å’Œåˆ‡ç‰‡
* è½¬æ¢ä¸ºå…¶ä»–Pythonå¯¹è±¡::

    X = torch.tensor([[ 0,  1,  2,  3],
                         [ 4,  5,  6,  7],
                         [ 8,  9, 10, 11]])
    A = X.numpy()
    B = torch.tensor(A)
    type(A), type(B)
    # (numpy.ndarray, torch.Tensor)


    # å°†å¤§å°ä¸º1çš„å¼ é‡è½¬æ¢ä¸ºPythonæ ‡é‡
    a = torch.tensor([3.5])
    a, a.item(), float(a), int(a)
    # (array([3.5]), 3.5, 3.5, 3)



2.2. Data Preprocessing
^^^^^^^^^^^^^^^^^^^^^^^

* è¯»å–æ•°æ®é›†
* å¤„ç†ç¼ºå¤±å€¼
* è½¬æ¢ä¸ºå¼ é‡æ ¼å¼::

    import torch
    X = torch.tensor(inputs.to_numpy(dtype=float))
    y = torch.tensor(outputs.to_numpy(dtype=float))
    X, y



2.3. Linear Algebra(çº¿æ€§ä»£æ•°)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* Scalars(æ ‡é‡)
* Vectors(å‘é‡)
* Matrices(çŸ©é˜µ)
* Tensors(å¼ é‡)


Hadamard product
""""""""""""""""

* The elementwise product of two matrices is called their Hadamard product (denoted  :math:`\odot`  ).
* two matrices  :math:`\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}`

.. math::

     \mathbf{A} \odot \mathbf{B}= \left[\begin{array}{cccc}
         a_{11} b_{11} & a_{12} b_{12} & \ldots & a_{1 n} b_{1 n} \\ 
         a_{21} b_{21} & a_{22} b_{22} & \ldots & a_{2 n} b_{2 n} \\ 
         \vdots & \vdots & \ddots & \vdots \\ 
         a_{m 1} b_{m 1} & a_{m 2} b_{m 2} & \ldots & a_{m n} b_{m n} \\
     \end{array}\right] .


::

    A = torch.arange(6, dtype=torch.float32).reshape(2, 3)
    B = A.clone()  # Assign a copy of A to B by allocating new memory
    A * B
    tensor([[ 0.,  1.,  4.],
            [ 9., 16., 25.]])



ç‚¹ç§¯(Dot Product)
"""""""""""""""""

::

    x = torch.arange(3, dtype=torch.float32)
    y = torch.ones(3, dtype = torch.float32)
    x, y
    # (tensor([0., 1., 2.]), tensor([1., 1., 1.]))

    torch.dot(x, y), torch.sum(x * y)
    # (tensor(3.), tensor(3.))
    # è¿‡ç¨‹:
    # 0*1 + 1*1 + 2*1 = 3


çŸ©é˜µ-å‘é‡ç§¯(Matrixâ€“Vector Products)
"""""""""""""""""""""""""""""""""""

.. math::

    \mathbf{A}=\left[\begin{array}{c}
    \mathbf{a}_{1}^{\top} \\
    \mathbf{a}_{2}^{\top} \\
    \vdots \\
    \mathbf{a}_{m}^{\top} \\
    \end{array}\right]

* where each  :math:`\mathbf{a}_{i}^{\top} \in \mathbb{R}^{n}`  is a row vector representing the  :math:`i^{\text {th }}`  row of the matrix  :math:`\mathbf{A}` .
* The matrix-vector product  :math:`\mathbf{A x}`  is simply a column vector of length  m , whose  :math:`i^{\text {th }}`  element is the dot product  :math:`\mathbf{a}_{i}^{\top} \mathbf{x}`

.. math::

    \mathbf{A} \mathbf{x}=\left[\begin{array}{c}
    \mathbf{a}_{1}^{\top} \\
    \mathbf{a}_{2}^{\top} \\
    \vdots \\
    \mathbf{a}_{m}^{\top} \\
    \end{array}\right] \mathbf{x}=\left[\begin{array}{c}
    \mathbf{a}_{1}^{\top} \mathbf{x} \\
    \mathbf{a}_{2}^{\top} \mathbf{x} \\
    \vdots \\
    \mathbf{a}_{m}^{\top} \mathbf{x} \\
    \end{array}\right]

::

    A = torch.arange(6).reshape(2, 3)
    # tensor([[0, 1, 2],
    #         [3, 4, 5]])
    x = torch.arange(3)
    # tensor([0, 1, 2])

    A.shape, x.shape
    # (torch.Size([2, 3]), torch.Size([3])

    torch.mv(A, x), A@x
    # tensor([ 5., 14.]), tensor([ 5., 14.]))
    # è¿‡ç¨‹:
    # torch.dot([0, 1, 2], [0, 1, 2]) = 0*0+1*1+2*2=5
    # torch.dot([3, 4, 5], [0, 1, 2]) = 0*3+1*4+2*5=14



çŸ©é˜µ-çŸ©é˜µä¹˜æ³•(Matrixâ€“Matrix Multiplication)
"""""""""""""""""""""""""""""""""""""""""""

* Say that we have two matrices  :math:`\mathbf{A} \in \mathbb{R}^{n \times k}`  and  :math:`\mathbf{B} \in \mathbb{R}^{k \times m}` .

.. math::

    \mathbf{A}=\left[\begin{array}{cccc}
    a_{11} & a_{12} & \cdots & a_{1 k} \\
    a_{21} & a_{22} & \cdots & a_{2 k} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{n 1} & a_{n 2} & \cdots & a_{n k} \\
    \end{array}\right], \quad \mathbf{B}=\left[\begin{array}{cccc}
    b_{11} & b_{12} & \cdots & b_{1 m} \\
    b_{21} & b_{22} & \cdots & b_{2 m} \\
    \vdots & \vdots & \ddots & \vdots \\
    b_{k 1} & b_{k 2} & \cdots & b_{k m}
    \end{array}\right] .


* Let  :math:`\mathbf{a}_{i}^{\top} \in \mathbb{R}^{k}`  denote the row vector representing the  :math:`i^{\text {th }}`  row of the matrix  :math:`\mathbf{A}`  and let  :math:`\mathbf{b}_{j} \in \mathbb{R}^{k}`  denote the column vector from the  :math:`j^{\text {th }}`  column of the matrix B:

.. math::

    \mathbf{A}=\left[\begin{array}{c}
    \mathbf{a}_{1}^{\top} \\
    \mathbf{a}_{2}^{\top} \\
    \vdots \\
    \mathbf{a}_{n}^{\top}
    \end{array}\right], \quad \mathbf{B}=\left[\begin{array}{llll}
    \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m}
    \end{array}\right]


To form the matrix product  :math:`\mathbf{C} \in \mathbb{R}^{n \times m}` , we simply compute each element  :math:`c_{i j}`  as the dot product between the  :math:`i^{\text {th }}`  row of  :math:`\mathbf{A}`  and the  :math:`j^{\text {th }}`  column of  :math:`\mathbf{B}` , i.e.,  :math:`\mathbf{a}_{i}^{\top} \mathbf{b}_{j}`  :

.. math::

    \mathbf{C}=\mathbf{A B}=\left[\begin{array}{c}
    \mathbf{a}_{1}^{\top} \\
    \mathbf{a}_{2}^{\top} \\
    \vdots \\
    \mathbf{a}_{n}^{\top}
    \end{array}\right]\left[\begin{array}{llll}
    \mathbf{b}_{1} & \mathbf{b}_{2} & \cdots & \mathbf{b}_{m}
    \end{array}\right]=\left[\begin{array}{cccc}
    \mathbf{a}_{1}^{\top} \mathbf{b}_{1} & \mathbf{a}_{1}^{\top} \mathbf{b}_{2} & \cdots & \mathbf{a}_{1}^{\top} \mathbf{b}_{m} \\
    \mathbf{a}_{2}^{\top} \mathbf{b}_{1} & \mathbf{a}_{2}^{\top} \mathbf{b}_{2} & \cdots & \mathbf{a}_{2}^{\top} \mathbf{b}_{m} \\
    \vdots & \vdots & \ddots & \vdots \\
    \mathbf{a}_{n}^{\top} \mathbf{b}_{1} & \mathbf{a}_{n}^{\top} \mathbf{b}_{2} & \cdots & \mathbf{a}_{n}^{\top} \mathbf{b}_{m}
    \end{array}\right]


* æˆ‘ä»¬å¯ä»¥å°†çŸ©é˜µ-çŸ©é˜µä¹˜æ³• **AB** çœ‹ä½œç®€å•åœ°æ‰§è¡Œ m æ¬¡çŸ©é˜µ-å‘é‡ç§¯ï¼Œå¹¶å°†ç»“æœæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå½¢æˆä¸€ä¸ª n*m çŸ©é˜µã€‚ 
* åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬åœ¨Aå’ŒBä¸Šæ‰§è¡ŒçŸ©é˜µä¹˜æ³•ã€‚ è¿™é‡Œçš„Aæ˜¯ä¸€ä¸ª5è¡Œ4åˆ—çš„çŸ©é˜µï¼ŒBæ˜¯ä¸€ä¸ª4è¡Œ3åˆ—çš„çŸ©é˜µã€‚ ä¸¤è€…ç›¸ä¹˜åï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä¸€ä¸ª5è¡Œ3åˆ—çš„çŸ©é˜µã€‚

::

    A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
    tensor([[ 0.,  1.,  2.,  3.],
            [ 4.,  5.,  6.,  7.],
            [ 8.,  9., 10., 11.],
            [12., 13., 14., 15.],
            [16., 17., 18., 19.]])
    B = torch.ones(4, 3)
    tensor([[1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.],
            [1., 1., 1.]])
    torch.mm(A, B)
    # è¾“å‡º
    tensor([[ 6.,  6.,  6.],
            [22., 22., 22.],
            [38., 38., 38.],
            [54., 54., 54.],
            [70., 70., 70.]])



.. note:: ``çŸ©é˜µ-çŸ©é˜µä¹˜æ³•(matrixâ€“matrix multiplication)`` å¯ä»¥ç®€å•åœ°ç§°ä¸º ``çŸ©é˜µä¹˜æ³•(matrix multiplication)`` ï¼Œä¸åº”ä¸ ``Hadamardç§¯(Hadamard product)`` æ··æ·†ã€‚


2.4. Calculus(å¾®ç§¯åˆ†)
^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/Ovvwmt.png

    é€¼è¿‘æ³•å°±æ˜¯ç§¯åˆ†(integral calculus)çš„èµ·æº

* å¾®åˆ†(differential calculus)è¢«å‘æ˜å‡ºæ¥ã€‚ åœ¨å¾®åˆ†å­¦æœ€é‡è¦çš„åº”ç”¨æ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œå³è€ƒè™‘å¦‚ä½•æŠŠäº‹æƒ…åšåˆ°æœ€å¥½

å°†æ‹Ÿåˆæ¨¡å‹çš„ä»»åŠ¡åˆ†è§£ä¸ºä¸¤ä¸ªå…³é”®é—®é¢˜::

    1. ä¼˜åŒ–(optimization): ç”¨æ¨¡å‹æ‹Ÿåˆè§‚æµ‹æ•°æ®çš„è¿‡ç¨‹
    2. æ³›åŒ–(generalization): æ•°å­¦åŸç†å’Œå®è·µè€…çš„æ™ºæ…§ï¼Œèƒ½å¤ŸæŒ‡å¯¼æˆ‘ä»¬ç”Ÿæˆå‡ºæœ‰æ•ˆæ€§è¶…å‡ºç”¨äºè®­ç»ƒçš„æ•°æ®é›†æœ¬èº«çš„æ¨¡å‹

Derivatives and Differentiation(å¯¼æ•°å’Œå¾®åˆ†)
"""""""""""""""""""""""""""""""""""""""""""

* Put simply, a ``derivative`` is the rate of change in a function with respect to changes in its arguments. Derivatives can tell us how rapidly a loss function would increase or decrease were we to increase or decrease each parameter by an infinitesimally small amount.
* ç®€è€Œè¨€ä¹‹ï¼Œå¯¹äºæ¯ä¸ªå‚æ•°ï¼Œ å¦‚æœæˆ‘ä»¬æŠŠè¿™ä¸ªå‚æ•°å¢åŠ æˆ–å‡å°‘ä¸€ä¸ªæ— ç©·å°çš„é‡ï¼Œå¯ä»¥çŸ¥é“æŸå¤±ä¼šä»¥å¤šå¿«çš„é€Ÿåº¦å¢åŠ æˆ–å‡å°‘

* å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡½æ•° :math:`f: \mathbb{R} \rightarrow \mathbb{R}` , å…¶è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯æ ‡é‡ã€‚å¦‚æœ  f  çš„å¯¼æ•°å­˜åœ¨, è¿™ä¸ªæé™è¢«å®šä¹‰ä¸º

.. math::

    f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}

* å¦‚æœ :math:`f^{\prime}(a)` å­˜åœ¨ï¼Œåˆ™ç§° ``f`` åœ¨ ``a`` å¤„æ˜¯å¯å¾®ï¼ˆdifferentiableï¼‰çš„
* å¦‚æœ ``f`` åœ¨ä¸€ä¸ªåŒºé—´å†…çš„æ¯ä¸ªæ•°ä¸Šéƒ½æ˜¯å¯å¾®çš„ï¼Œåˆ™æ­¤å‡½æ•°åœ¨æ­¤åŒºé—´ä¸­æ˜¯å¯å¾®çš„
* å¯ä»¥å°†ä¸Šé¢å…¬å¼ä¸­çš„å¯¼æ•° :math:`f^{\prime}(a)` è§£é‡Šä¸º ``f(x)`` ç›¸å¯¹äº ``x`` çš„ç¬æ—¶ï¼ˆinstantaneousï¼‰å˜åŒ–ç‡
* æ‰€è°“çš„ç¬æ—¶å˜åŒ–ç‡æ˜¯åŸºäº ``x`` ä¸­çš„å˜åŒ– ``h`` ï¼Œä¸” ``h`` æ¥è¿‘ ``0``


* å¯¼æ•°çš„å‡ ä¸ªç­‰ä»·ç¬¦å·ï¼š

.. math::

    f^{\prime}(x)=y^{\prime}=\frac{d y}{d x}=\frac{d f}{d x}=\frac{d}{d x} f(x)=D f(x)=D_{x} f(x),


å…¶ä¸­ç¬¦å·  :math:`\frac{d}{d x}`  å’Œ  **D**  æ˜¯å¾®åˆ†è¿ç®—ç¬¦, è¡¨ç¤ºå¾®åˆ†æ“ä½œã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è§„åˆ™æ¥å¯¹å¸¸è§å‡½æ•°æ±‚å¾®åˆ†ï¼š

    -  :math:`D C=0`  (  C  æ˜¯ä¸€ä¸ªå¸¸æ•°)
    -  :math:`D x^{n}=n x^{n-1}`  (å¹‚å¾‹ï¼ˆpower ruleï¼‰,  n  æ˜¯ä»»æ„å®æ•°ï¼‰
    -  :math:`D e^{x}=e^{x}`
    -  :math:`D \ln (x)=1 / x`

* ä¸ºäº†å¾®åˆ†ä¸€ä¸ªç”±ä¸€äº›å¸¸è§å‡½æ•°ç»„æˆçš„å‡½æ•°, ä¸‹é¢çš„ä¸€äº›æ³•åˆ™æ–¹ä¾¿ä½¿ç”¨ã€‚å‡è®¾å‡½æ•°  ``f``  å’Œ  ``g``  éƒ½æ˜¯å¯å¾®çš„,  ``C``  æ˜¯ä¸€ä¸ªå¸¸æ•°, åˆ™:
* å¸¸æ•°ç›¸ä¹˜æ³•åˆ™

.. math::

    \frac{d}{d x}[C f(x)]=C \frac{d}{d x} f(x),


* åŠ æ³•æ³•åˆ™

.. math::

    \frac{d}{d x}[f(x)+g(x)]=\frac{d}{d x} f(x)+\frac{d}{d x} g(x)

ä¹˜æ³•æ³•åˆ™

.. math::

    \frac{d}{d x}[f(x) g(x)]=f(x) \frac{d}{d x}[g(x)]+g(x) \frac{d}{d x}[f(x)],


é™¤æ³•æ³•åˆ™

.. math::

    \frac{d}{d x}\left[\frac{f(x)}{g(x)}\right]=\frac{g(x) \frac{d}{d x}[f(x)]-f(x) \frac{d}{d x}[g(x)]}{[g(x)]^{2}} .


* ç°åœ¨æˆ‘ä»¬å¯ä»¥åº”ç”¨ä¸Šè¿°å‡ ä¸ªæ³•åˆ™æ¥è®¡ç®— :math:`u=f(x)=3x^2-4x`
* :math:`u^{\prime}=f^{\prime}(x)=3 \frac{d}{d x} x^{2}-4 \frac{d}{d x}x = 6x-4`
* ä»¤  x=1 , æˆ‘ä»¬æœ‰  :math:`u^{\prime}=2`
* å½“  x=1  æ—¶, æ­¤å¯¼æ•°ä¹Ÿæ˜¯æ›²çº¿  u=f(x)  åˆ‡çº¿çš„æ–œç‡ã€‚


Partial Derivatives(åå¯¼æ•°)
"""""""""""""""""""""""""""

* åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå‡½æ•°é€šå¸¸ä¾èµ–äºè®¸å¤šå˜é‡ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†å¾®åˆ†çš„æ€æƒ³æ¨å¹¿åˆ°å¤šå…ƒå‡½æ•°ï¼ˆmultivariate functionï¼‰ä¸Š
* è®¾  :math:`y=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)`  æ˜¯ä¸€ä¸ªå…·æœ‰  n  ä¸ªå˜é‡çš„å‡½æ•°ã€‚  y  å…³äºç¬¬  i  ä¸ªå‚æ•°  :math:`x_{i}`  çš„åå¯¼æ•°ï¼ˆpartial derivativeï¼‰ä¸º:

.. math::

    \frac{\partial y}{\partial x_{i}}=
        \lim_{h \rightarrow 0} \frac{f\left(x_{1}, \ldots, x_{i-1}, x_{i}+h, x_{i+1}, \ldots, x_{n}\right)-f\left(x_{1}, \ldots, x_{i}, \ldots, x_{n}\right)}{h}


* ä¸ºäº†è®¡ç®—  :math:`\frac{\partial y}{\partial x_{i}}` , æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†  :math:`x_{1}, \ldots, x_{i-1}, x_{i+1}, \ldots, x_{n}`  çœ‹ä½œå¸¸æ•°, å¹¶è®¡ç®—  y  å…³äº  :math:`x_{i}`  çš„å¯¼æ•°ã€‚å¯¹äºåå¯¼æ•°çš„è¡¨ç¤º, ä»¥ä¸‹æ˜¯ç­‰ä»·çš„:

.. math::

    \frac{\partial y}{\partial x_{i}}=\frac{\partial f}{\partial x_{i}}=f_{x_{i}}=f_{i}=D_{i} f=D_{x_{i}} f .



Gradients(æ¢¯åº¦)
"""""""""""""""

* æˆ‘ä»¬å¯ä»¥è¿ç»“ä¸€ä¸ªå¤šå…ƒå‡½æ•°å¯¹å…¶æ‰€æœ‰å˜é‡çš„åå¯¼æ•°, ä»¥å¾—åˆ°è¯¥å‡½æ•°çš„æ¢¯åº¦ï¼ˆgradientï¼‰å‘é‡ã€‚
* å…·ä½“è€Œè¨€ï¼Œè®¾å‡½æ•°  :math:`f: \mathbb{R}^{n} \rightarrow \mathbb{R}`  çš„è¾“å…¥æ˜¯ä¸€ä¸ª  n  ç»´å‘é‡  :math:`\mathbf{x}=\left[x_{1}, x_{2}, \ldots, x_{n}\right]^{\top}` , å¹¶ä¸”è¾“å‡ºæ˜¯ä¸€ä¸ªæ ‡é‡ã€‚
* å‡½æ•°  :math:`f(\mathbf{x})`  ç›¸å¯¹äº  :math:`\mathbf{x}`  çš„æ¢¯åº¦æ˜¯ä¸€ä¸ªåŒ…å«  n  ä¸ªåå¯¼æ•°çš„å‘é‡:

.. math::

    \nabla_{\mathbf{x}} f(\mathbf{x})=\left[\frac{\partial f(\mathbf{x})}{\partial x_{1}}, \frac{\partial f(\mathbf{x})}{\partial x_{2}}, \ldots, \frac{\partial f(\mathbf{x})}{\partial x_{n}}\right]^{\top}


* å…¶ä¸­  :math:`\nabla_{\mathbf{x}} f(\mathbf{x})`  é€šå¸¸åœ¨æ²¡æœ‰æ­§ä¹‰æ—¶è¢«  :math:`\nabla f(\mathbf{x})`  å–ä»£ã€‚
* å‡è®¾ x ä¸º  n  ç»´å‘é‡, åœ¨å¾®åˆ†å¤šå…ƒå‡½æ•°æ—¶ç»å¸¸ä½¿ç”¨ä»¥ä¸‹è§„åˆ™:
    - å¯¹äºæ‰€æœ‰  :math:`\mathbf{A} \in \mathbb{R}^{m \times n}`  ï¼Œéƒ½æœ‰  :math:`\nabla_{\mathbf{x}} \mathbf{A} \mathbf{x}=\mathbf{A}^{\top}`
    - å¯¹äºæ‰€æœ‰  :math:`\mathbf{A} \in \mathbb{R}^{n \times m}`  ï¼Œéƒ½æœ‰  :math:`\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A}=\mathbf{A}`
    - å¯¹äºæ‰€æœ‰  :math:`\mathbf{A} \in \mathbb{R}^{n \times n}`  ï¼Œéƒ½æœ‰  :math:`\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{A} \mathbf{x}=\left(\mathbf{A}+\mathbf{A}^{\top}\right) \mathbf{x}` 
    -  :math:`\nabla_{\mathbf{x}}\|\mathbf{x}\|^{2}=\nabla_{\mathbf{x}} \mathbf{x}^{\top} \mathbf{x}=2 \mathbf{x}`

* :math:`|\mathbf{x}\|^{2} = \mathbf{x}^{\top} \mathbf{x}` æ˜¯å‘é‡ **ğ‘¥** çš„äºŒèŒƒæ•°å¹³æ–¹
* åŒæ ·ï¼Œå¯¹äºä»»ä½•çŸ©é˜µ  :math:`\mathbf{X}`  ï¼Œéƒ½æœ‰  :math:`\nabla_{\mathbf{X}}\|\mathbf{X}\|_{F}^{2}=2 \mathbf{X}` ï¼Œå…¶ä¸­ :math:`\|\mathbf{X}\|_{F}` æ˜¯ **çŸ©é˜µ Frobenius èŒƒæ•°**




2.5. Automatic Differentiation(è‡ªåŠ¨å¾®åˆ†)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* æ·±åº¦å­¦ä¹ æ¡†æ¶é€šè¿‡è‡ªåŠ¨è®¡ç®—å¯¼æ•°ï¼Œå³è‡ªåŠ¨å¾®åˆ†ï¼ˆautomatic differentiationï¼‰æ¥åŠ å¿«æ±‚å¯¼ã€‚ å®é™…ä¸­ï¼Œæ ¹æ®è®¾è®¡å¥½çš„æ¨¡å‹ï¼Œç³»ç»Ÿä¼šæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆcomputational graphï¼‰ï¼Œ æ¥è·Ÿè¸ªè®¡ç®—æ˜¯å“ªäº›æ•°æ®é€šè¿‡å“ªäº›æ“ä½œç»„åˆèµ·æ¥äº§ç”Ÿè¾“å‡ºã€‚ è‡ªåŠ¨å¾®åˆ†ä½¿ç³»ç»Ÿèƒ½å¤Ÿéšååå‘ä¼ æ’­æ¢¯åº¦ã€‚ è¿™é‡Œï¼Œåå‘ä¼ æ’­ï¼ˆbackpropagateï¼‰æ„å‘³ç€è·Ÿè¸ªæ•´ä¸ªè®¡ç®—å›¾ï¼Œå¡«å……å…³äºæ¯ä¸ªå‚æ•°çš„åå¯¼æ•°ã€‚

.. note:: grad can be implicitly created only for scalar outputs æ¢¯åº¦é»˜è®¤ç»™æ ‡é‡è¾“å‡ºåˆ›å»ºï¼Œå°±æ˜¯è¯´ y åº”è¯¥æ˜¯ä¸ªæ ‡é‡

.. code-block:: python

    import torch
    x = torch.arange(4.0)       #  tensor([0., 1., 2., 3.])
    x.requires_grad_(True)      # ç­‰ä»·äºx=torch.arange(4.0,requires_grad=True)
    print(f"====1: {x.grad}")   # None
    y = 2 * torch.dot(x, x)
    print(f"====2: y:{y}")      # tensor(28., grad_fn=<MulBackward0>)
    y.backward()
    print(f"====3: x.grad:{x.grad}")    # tensor([ 0.,  4.,  8., 12.])


    # åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šç´¯ç§¯æ¢¯åº¦ï¼Œæˆ‘ä»¬éœ€è¦æ¸…é™¤ä¹‹å‰çš„å€¼
    x.grad.zero_()
    y = x.sum()  # grad can be implicitly created only for scalar outputs
    y.backward()
    x.grad          # tensor([1., 1., 1., 1.])


Backward for Non-Scalar Variables
"""""""""""""""""""""""""""""""""

* åœ¨æ•°å­¦ä¸­ï¼Œå½“ ğ‘¦ æ˜¯ä¸€ä¸ªå‘é‡ï¼Œğ‘¥ ä¹Ÿæ˜¯ä¸€ä¸ªå‘é‡æ—¶ï¼Œğ‘¦ å¯¹ ğ‘¥ çš„å¯¼æ•°æ˜¯ä¸€ä¸ª Jacobian çŸ©é˜µã€‚
    * Jacobian çŸ©é˜µçš„æ¯ä¸ªå…ƒç´ è¡¨ç¤º ğ‘¦ çš„æ¯ä¸ªåˆ†é‡å¯¹ ğ‘¥ çš„æ¯ä¸ªåˆ†é‡çš„åå¯¼æ•°ã€‚
    * å¦‚æœ ğ‘¦ å’Œ ğ‘¥ çš„ç»´åº¦éƒ½å¾ˆé«˜ï¼Œæ±‚å¯¼çš„ç»“æœä¼šæ˜¯ä¸€ä¸ªæ›´é«˜é˜¶çš„å¼ é‡ã€‚
* ä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä¸éœ€è¦ç›´æ¥è®¡ç®— Jacobian çŸ©é˜µï¼Œè€Œæ˜¯å¸Œæœ›å°†ç»“æœè¿›è¡Œæ±‡æ€»ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå’Œ ğ‘¥ å½¢çŠ¶ç›¸åŒçš„å‘é‡ï¼ˆå³æ¢¯åº¦ï¼‰

PyTorch çš„å¤„ç†æ–¹å¼::

    å¦‚æœå¯¹ éæ ‡é‡å¼ é‡ç›´æ¥è°ƒç”¨ .backward()ï¼Œä¼šæŠ¥é”™ã€‚
        RuntimeError: grad can be implicitly created only for scalar outputs
    å› ä¸ºæ¡†æ¶æ— æ³•è‡ªåŠ¨å†³å®šå¦‚ä½•å°†éæ ‡é‡å¤„ç†æˆæ ‡é‡ã€‚
    æˆ‘ä»¬éœ€è¦æä¾›ä¸€ä¸ªå‘é‡ï¼ˆé€šå¸¸ç§°ä¸º gradient å‚æ•°ï¼‰ï¼Œæ¥å‘Šè¯‰ PyTorchå¦‚ä½•æ±‡æ€»æ¢¯åº¦
        y = x * x  # å‡è®¾ y æ˜¯ä¸€ä¸ªå‘é‡
        y.backward(gradient=torch.ones(len(y)))
    å®é™…ä¸Šæ›´å¿«çš„æ–¹å¼æ˜¯ç›´æ¥å¯¹ ğ‘¦ æ±‚å’Œåå†è°ƒç”¨ .backward()
        y.sum().backward()

ç¤ºä¾‹::

    >>> x
    tensor([0., 1., 2., 3.], requires_grad=True)

    >>> x.grad.zero_()
    >>> y = x * x
    >>> y.sum().backward()
    >>> x.grad
    tensor([0., 2., 4., 6.])
    # è¯´æ˜
    #   y = x1*x1 + x2*x2 + ... + xi*xi
    #   å¯¼æ•°: [2x1, 2x2, ..., 2xi]
    #   å³: [0, 2, 4, 6]


    >>> x.grad.zero_()
    >>> y = x * x
    >>> y.mean().backward()
    >>> x.grad
    tensor([0.0000, 0.5000, 1.0000, 1.5000])
    # è¯´æ˜
    #   y = 1/i(x1*x1 + x2*x2 + ... + xi*xi)
    #   å¯¼æ•°: 1/i([2x1, 2x2, ..., 2xi])
    #   å³: 1/4([0, 2, 4, 6])
    #       [0.0000, 0.5000, 1.0000, 1.5000]


Detaching Computation
"""""""""""""""""""""

* æœ‰æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å°†æŸäº›è®¡ç®—ç§»åŠ¨åˆ°è®°å½•çš„è®¡ç®—å›¾ä¹‹å¤–ã€‚ 
* ä¾‹å¦‚ï¼Œå‡è®¾yæ˜¯ä½œä¸ºxçš„å‡½æ•°è®¡ç®—çš„ï¼Œè€Œzåˆ™æ˜¯ä½œä¸ºyå’Œxçš„å‡½æ•°è®¡ç®—çš„ã€‚ 
* æƒ³è®¡ç®—zå…³äºxçš„æ¢¯åº¦ï¼Œä½†ç”±äºæŸç§åŸå› ï¼Œå¸Œæœ›å°†yè§†ä¸ºä¸€ä¸ªå¸¸æ•°ï¼Œ å¹¶ä¸”åªè€ƒè™‘åˆ°xåœ¨yè¢«è®¡ç®—åå‘æŒ¥çš„ä½œç”¨ã€‚


.. code-block:: python

    x=torch.arange(4.0,requires_grad=True)
    x
    # tensor([0., 1., 2., 3.], requires_grad=True)

    # æ¢¯åº¦è®¡ç®—åˆ†ç¦»y
    y = x * x
    u = y.detach()
    z = u * x
    z.sum().backward()
    x.grad
    # tensor([0., 1., 4., 9.])

    # æ¢¯åº¦è®¡ç®—ä¸åˆ†ç¦»y
    x.grad.zero_()
    z=y * x
    z.sum().backward()
    x.grad
    # tensor([ 0.,  3., 12., 27.])

Gradients and Python Control Flow
"""""""""""""""""""""""""""""""""

* ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†çš„ä¸€ä¸ªå¥½å¤„æ˜¯ï¼š å³ä½¿æ„å»ºå‡½æ•°çš„è®¡ç®—å›¾éœ€è¦é€šè¿‡Pythonæ§åˆ¶æµï¼ˆä¾‹å¦‚ï¼Œæ¡ä»¶ã€å¾ªç¯æˆ–ä»»æ„å‡½æ•°è°ƒç”¨ï¼‰ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥è®¡ç®—å¾—åˆ°çš„å˜é‡çš„æ¢¯åº¦
* ç¤ºä¾‹-whileå¾ªç¯çš„è¿­ä»£æ¬¡æ•°å’Œifè¯­å¥çš„ç»“æœéƒ½å–å†³äºè¾“å…¥açš„å€¼::

    def f(a):
        b = a * 2
        while b.norm() < 1000:
            b = b * 2
        if b.sum() > 0:
            c = b
        else:
            c = 100 * b
        return c

è®¡ç®—æ¢¯åº¦::

    a = torch.randn(size=(), requires_grad=True)    # tensor(0.0412, requires_grad=True)
    d = f(a)                                        # tensor(1350.7505, grad_fn=<MulBackward0>)
    d.backward()
    a.grad
    # tensor(32768.)
    a.grad == d / a     # ä¸ç®¡æ€ä¹ˆç®—ï¼Œå¯¹açš„æ¢¯åº¦å°±æ˜¯é™¤aå¤–çš„å¸¸æ•°,å› ä¸ºf(a)=å¸¸æ•°*a
    # tensor(True)



2.6 Probability and Statistics
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


åŸºæœ¬æ¦‚ç‡è®º
""""""""""

* åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œæˆ‘ä»¬æŠŠä»æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½å–æ ·æœ¬çš„è¿‡ç¨‹ç§°ä¸ºæŠ½æ ·ï¼ˆsamplingï¼‰


æ¦‚ç‡è®ºå…¬ç†
++++++++++

* åœ¨å¤„ç†éª°å­æ·å‡ºç¤ºä¾‹æ—¶ï¼Œæˆ‘ä»¬å°†é›†åˆ ``S={1,2,3,4,5,6}`` ç§°ä¸º **æ ·æœ¬ç©ºé—´ï¼ˆsample spaceï¼‰æˆ–ç»“æœç©ºé—´ï¼ˆoutcome spaceï¼‰** ï¼Œ å…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ç»“æœï¼ˆoutcomeï¼‰ã€‚ 
* **äº‹ä»¶ï¼ˆeventï¼‰** æ˜¯ä¸€ç»„ç»™å®šæ ·æœ¬ç©ºé—´çš„éšæœºç»“æœã€‚ ä¾‹å¦‚ï¼Œâ€œçœ‹åˆ°5â€ï¼ˆ ``{5}`` ï¼‰å’Œâ€œçœ‹åˆ°å¥‡æ•°â€ï¼ˆ ``{1,3,5}`` ï¼‰éƒ½æ˜¯æ·å‡ºéª°å­çš„æœ‰æ•ˆäº‹ä»¶ã€‚ æ³¨æ„ï¼Œå¦‚æœä¸€ä¸ªéšæœºå®éªŒçš„ç»“æœåœ¨ :math:`\mathcal{A}` ä¸­ï¼Œåˆ™äº‹ä»¶ :math:`\mathcal{A}` å·²ç»å‘ç”Ÿã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæŠ•æ·å‡º 3 ç‚¹ï¼Œå› ä¸º :math:`3 \in {1,3,5}` ï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œâ€œçœ‹åˆ°å¥‡æ•°â€çš„äº‹ä»¶å‘ç”Ÿäº†ã€‚
* **æ¦‚ç‡ï¼ˆprobabilityï¼‰** å¯ä»¥è¢«è®¤ä¸ºæ˜¯å°†é›†åˆæ˜ å°„åˆ°çœŸå®å€¼çš„å‡½æ•°ã€‚åœ¨ç»™å®šçš„æ ·æœ¬ç©ºé—´  :math:`\mathcal{S}`  ä¸­, äº‹ä»¶  :math:`\mathcal{A}`  çš„æ¦‚ç‡, è¡¨ç¤ºä¸º  :math:`P(\mathcal{A})` , æ»¡è¶³ä»¥ä¸‹å±æ€§ï¼š
    - å¯¹äºä»»æ„äº‹ä»¶  :math:`\mathcal{A}` , å…¶æ¦‚ç‡ä»ä¸ä¼šæ˜¯è´Ÿæ•°, å³  :math:`P(\mathcal{A}) \geq 0` ;
    - æ•´ä¸ªæ ·æœ¬ç©ºé—´çš„æ¦‚ç‡ä¸º 1 , å³ :math:`P(\mathcal{S})=1` ;
    - å¯¹äºäº’æ–¥ï¼ˆmutually exclusiveï¼‰äº‹ä»¶ï¼ˆå¯¹äºæ‰€æœ‰  :math:`i \neq j`  éƒ½æœ‰  :math:`\mathcal{A}_{i} \cap \mathcal{A}_{j}=\emptyset`  ï¼‰çš„ä»»æ„ä¸€ä¸ªå¯æ•°åºåˆ— :math:`\mathcal{A}_{1}, \mathcal{A}_{2}, \ldots` ï¼Œåºåˆ—ä¸­ä»»æ„ä¸€ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ç­‰äºå®ƒä»¬å„è‡ªå‘ç”Ÿçš„æ¦‚ç‡ä¹‹å’Œ, å³  :math:`P\left(\bigcup_{i=1}^{\infty} \mathcal{A}_{i}\right)=\sum_{i=1}^{\infty} P\left(\mathcal{A}_{i}\right)`  ã€‚

* ä¸Šé¢è¿™ä¸ªå°±æ˜¯æ¦‚ç‡è®ºçš„å…¬ç†ï¼Œç”±ç§‘å°”è«æˆˆç½—å¤«äº1933å¹´æå‡º

éšæœºå˜é‡
++++++++

* éšæœºå˜é‡ï¼ˆrandom variableï¼‰
* :math:`P(\mathcal{X} = a)` æˆ‘ä»¬åŒºåˆ†äº†éšæœºå˜é‡ :math:`\mathcal{X}` å’Œè¿™ä¸ªéšæœºå˜é‡å¯ä»¥é‡‡å–çš„å€¼ï¼ˆä¾‹å¦‚aï¼‰
* ä¸ºäº†ç®€åŒ–ç¬¦å·
* ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥å°† ``P(X)`` è¡¨ç¤ºä¸ºéšæœºå˜é‡ ``X`` ä¸Šçš„åˆ†å¸ƒï¼ˆdistributionï¼‰ï¼š åˆ†å¸ƒå‘Šè¯‰æˆ‘ä»¬ ``X`` è·å¾—æŸä¸€å€¼çš„æ¦‚ç‡ï¼›å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•ç”¨ ``P(a)`` è¡¨ç¤ºéšæœºå˜é‡å–å€¼ a çš„æ¦‚ç‡
* ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯ä»¥å°† ``P(1<=X<=3)`` è¡¨ç¤ºäº‹ä»¶ ``{1<=X<=3}`` çš„æ¦‚ç‡ï¼›å¦ä¸€æ–¹é¢ï¼Œ ``P(1<=X<=3)`` è¡¨ç¤ºéšæœºå˜é‡ ``X`` ä» `{1,2,3}` ä¸­å–å€¼çš„æ¦‚ç‡

* æ³¨æ„ï¼šç¦»æ•£ï¼ˆdiscreteï¼‰éšæœºå˜é‡ï¼ˆå¦‚éª°å­çš„æ¯ä¸€é¢ï¼‰ å’Œè¿ç»­ï¼ˆcontinuousï¼‰éšæœºå˜é‡ï¼ˆå¦‚äººçš„ä½“é‡å’Œèº«é«˜ï¼‰ä¹‹é—´å­˜åœ¨å¾®å¦™çš„åŒºåˆ«


å¤„ç†å¤šä¸ªéšæœºå˜é‡
""""""""""""""""

è”åˆæ¦‚ç‡
++++++++

* è”åˆæ¦‚ç‡ï¼ˆjoint probabilityï¼‰  ``P(A=a, B=b)`` : ç»™å®šä»»æ„å€¼  a  å’Œ  b , è”åˆæ¦‚ç‡å¯ä»¥å›ç­”:  A=a  å’Œ  B=b  åŒæ—¶æ»¡è¶³çš„æ¦‚ç‡æ˜¯å¤šå°‘?
* è¯·æ³¨æ„, å¯¹äºä»»ä½•  a  å’Œ  b  çš„å–å€¼,  :math:`P(A=a, B=b) \leq P(A=a)`  ã€‚ è¿™ç‚¹æ˜¯ç¡®å®šçš„, å› ä¸ºè¦åŒæ—¶å‘ç”Ÿ  ``A=a  å’Œ  B=b`` , ``A=a``  å°±å¿…é¡»å‘ç”Ÿ

æ¡ä»¶æ¦‚ç‡
++++++++

* è”åˆæ¦‚ç‡çš„ä¸ç­‰å¼å¸¦ç»™æˆ‘ä»¬ä¸€ä¸ªæœ‰è¶£çš„æ¯”ç‡ï¼š :math:`0 \leq \frac{P(A=a, B=b)}{P(A=a)} \leq 1` 
* æˆ‘ä»¬ç§°è¿™ä¸ªæ¯”ç‡ä¸ºæ¡ä»¶æ¦‚ç‡ï¼ˆconditional probabilityï¼‰, å¹¶ç”¨  :math:`P(B=b \mid A=a)`  è¡¨ç¤ºå®ƒï¼šåœ¨ ``A=b`` å‰æä¸‹  ``B=b``  çš„æ¦‚ç‡ã€‚

è´å¶æ–¯å®šç†
++++++++++

* ä½¿ç”¨æ¡ä»¶æ¦‚ç‡çš„å®šä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºç»Ÿè®¡å­¦ä¸­æœ€æœ‰ç”¨çš„æ–¹ç¨‹ä¹‹ä¸€ï¼š **è´å¶æ–¯å®šç†ï¼ˆBayes' theoremï¼‰**
* æ ¹æ® **ä¹˜æ³•æ³•åˆ™ï¼ˆmultiplication ruleï¼‰** å¯å¾—åˆ°  :math:`P(A, B)=P(B \mid A) P(A)` 
* æ ¹æ®å¯¹ç§°æ€§, å¯å¾—åˆ°  :math:`P(A, B)=P(A \mid B) P(B)`  ã€‚å‡è®¾  ``P(B)>0`` , æ±‚è§£å…¶ä¸­ä¸€ä¸ªæ¡ä»¶å˜é‡, æˆ‘ä»¬å¾—åˆ°

.. math::

    P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}

* P(A, B)  æ˜¯ä¸€ä¸ªè”åˆåˆ†å¸ƒï¼ˆjoint distributionï¼‰
* P(A \mid B)  æ˜¯ä¸€ä¸ªæ¡ä»¶åˆ†å¸ƒï¼ˆconditional distributionï¼‰

è¾¹é™…åŒ–
++++++

* è¾¹é™…åŒ–(marginalization)ï¼šä¸ºäº†èƒ½è¿›è¡Œäº‹ä»¶æ¦‚ç‡æ±‚å’Œ, æˆ‘ä»¬éœ€è¦ ``æ±‚å’Œæ³•åˆ™ (sum rule)`` , å³  B  çš„æ¦‚ç‡ç›¸å½“äºè®¡ç®—  A  çš„æ‰€æœ‰å¯èƒ½é€‰æ‹©, å¹¶å°†æ‰€æœ‰é€‰æ‹©çš„è”åˆæ¦‚ç‡èšåˆåœ¨ä¸€èµ·:

.. math::

    P(B)=\sum_{A} P(A, B)


* è¾¹é™…åŒ–ç»“æœçš„æ¦‚ç‡æˆ–åˆ†å¸ƒç§°ä¸ºè¾¹é™…æ¦‚ç‡ï¼ˆmarginal probabilityï¼‰ æˆ–è¾¹é™…åˆ†å¸ƒï¼ˆmarginal distributionï¼‰ã€‚


ç‹¬ç«‹æ€§
++++++

* å¦ä¸€ä¸ªæœ‰ç”¨å±æ€§æ˜¯ä¾èµ–ï¼ˆdependenceï¼‰ä¸ç‹¬ç«‹ï¼ˆindependenceï¼‰ã€‚
* å¦‚æœä¸¤ä¸ªéšæœºå˜é‡  A  å’Œ  B  æ˜¯ç‹¬ç«‹çš„ï¼Œæ„å‘³ç€äº‹ä»¶  A  çš„å‘ç”Ÿè·Ÿ  B  äº‹ä»¶çš„å‘ç”Ÿæ— å…³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç»Ÿè®¡å­¦å®¶é€šå¸¸å°†è¿™ä¸€ç‚¹è¡¨è¿°ä¸º  :math:`A \perp B` 
* æ ¹æ®è´å¶æ–¯å®šç†ï¼Œé©¬ä¸Šå°±èƒ½åŒæ ·å¾—åˆ°  :math:`P(A \mid B)=P(A)` 
* åœ¨æ‰€æœ‰å…¶ä»–æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç§°  A  å’Œ  B  ä¾èµ–ã€‚
* æ¯”å¦‚ï¼Œä¸¤æ¬¡è¿ç»­æŠ›å‡ºä¸€ä¸ªéª°å­çš„äº‹ä»¶æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç¯å¼€å…³çš„ä½ç½®å’Œæˆ¿é—´çš„äº®åº¦å¹¶ä¸æ˜¯ï¼ˆå› ä¸ºå¯èƒ½å­˜åœ¨ç¯æ³¡åæ‰ã€ç”µæºæ•…éšœï¼Œæˆ–è€…å¼€å…³æ•…éšœï¼‰

* å¦‚æœ A  å’Œ  B  æ˜¯ç‹¬ç«‹çš„ï¼Œåˆ™ :math:`P(A \mid B)=\frac{P(A, B)}{P(B)}=P(A)`  ç­‰ä»·äº  :math:`P(A, B)=P(A) P(B)` =ã€‹ç»“è®ºï¼šå½“ä¸”ä»…å½“ä¸¤ä¸ªéšæœºå˜é‡æ˜¯ç‹¬ç«‹çš„ï¼Œä¸¤ä¸ªéšæœºå˜é‡çš„è”åˆåˆ†å¸ƒæ˜¯å…¶å„è‡ªåˆ†å¸ƒçš„ä¹˜ç§¯
* åŒæ ·åœ°, ç»™å®šå¦ä¸€ä¸ªéšæœºå˜é‡  C  æ—¶, ä¸¤ä¸ªéšæœºå˜é‡  A  å’Œ  B  æ˜¯æ¡ä»¶ç‹¬ç«‹çš„ï¼ˆconditionally independentï¼‰ï¼Œæœ‰ :math:`P(A, B \mid C)=P(A \mid C) P(B \mid C)`
* è¿™ä¸ªæƒ…å†µè¡¨ç¤ºä¸º  :math:`A \perp B \mid C`

åº”ç”¨ç¤ºä¾‹
++++++++

+---------------------------+-----+------+
| æ¡ä»¶æ¦‚ç‡                  | H=1 | H=0  |
+===========================+=====+======+
| :math:`P(D_1 = 1 \mid H)` | 1   | 0.01 |
+---------------------------+-----+------+
| :math:`P(D_1 = 0 \mid H)` | 0   | 0.99 |
+---------------------------+-----+------+

* å¦‚æœ :math:`P(H=1) = 0.0015`
* è¿ç”¨è¾¹é™…åŒ–å’Œä¹˜æ³•æ³•åˆ™æ¥ç¡®å®š

.. math::

    \begin{aligned}
    &P(D_1 = 1) \\
    =& P(D_1=1, H=0) + P(D_1=1, H=1)  \\
    =& P(D_1=1 \mid H=0) P(H=0) + P(D_1=1 \mid H=1) P(H=1) \\
    =& 0.01 * (1-0.0015) + 1*0.0015
    =& 0.011485.
    \end{aligned}

* äºæ˜¯

.. math::

    \begin{aligned}
        &P(H = 1 \mid D_1 = 1)\\
        =& \frac{P(D_1=1 \mid H=1) P(H=1)}{P(D_1=1)} \\
        =& (1*0.0015)/0.011485
        =& 0.1306 
    \end{aligned}

* ç¬¬2æ¬¡çš„æµ‹è¯•æ¦‚ç‡

+---------------------------+------+------+
| æ¡ä»¶æ¦‚ç‡                  | H=1  | H=0  |
+===========================+======+======+
| :math:`P(D_2 = 1 \mid H)` | 0.98 | 0.03 |
+---------------------------+------+------+
| :math:`P(D_2 = 0 \mid H)` | 0.02 | 0.97 |
+---------------------------+------+------+

.. math::

    \begin{aligned}
    &P(D_1 = 1, D_2 = 1 \mid H = 0) \\
    =& P(D_1 = 1 \mid H = 0) P(D_2 = 1 \mid H = 0)  \\
    =& 0.01*0.03
    =& 0.0003,
    \end{aligned}

.. math::

    \begin{aligned}
    &P(D_1 = 1, D_2 = 1 \mid H = 1) \\
    =& P(D_1 = 1 \mid H = 1) P(D_2 = 1 \mid H = 1)  \\
    =& 1*0.98
    =& 0.98.
    \end{aligned}

* ç°åœ¨æˆ‘ä»¬å¯ä»¥åº”ç”¨è¾¹é™…åŒ–å’Œä¹˜æ³•è§„åˆ™ï¼š

.. math::

    \begin{aligned}
    &P(D_1 = 1, D_2 = 1) \\
    =& P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\
    =& P(D_1 = 1, D_2 = 1 \mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \mid H = 1)P(H=1)\\
    =& 0.0003*(1-0.0015) + 0.98*0.0015
    =& 0.00176955.
    \end{aligned}


* æœ€åï¼Œé‰´äºå­˜åœ¨ä¸¤æ¬¡é˜³æ€§æ£€æµ‹ï¼Œæ‚£è€…æ‚£æœ‰è‰¾æ»‹ç—…çš„æ¦‚ç‡ä¸º

.. math::

    \begin{aligned}
    &P(H = 1 \mid D_1 = 1, D_2 = 1)\\
    =& \frac{P(D_1 = 1, D_2 = 1 \mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)} \\
    =& 0.98*0.0015/0.00176955
    =& 0.8307.
    \end{aligned}


æœŸæœ›å’Œæ–¹å·®
""""""""""

* å‡è®¾æŸé¡¹æŠ•èµ„æœ‰ï¼š
    * 50% çš„æ¦‚ç‡ä¼šå¤±è´¥
    * 40% çš„æ¦‚ç‡å®ƒå¯èƒ½æä¾› 2å€å›æŠ¥
    * 10% çš„æ¦‚ç‡å®ƒå¯èƒ½ä¼šæä¾› 10 å€å›æŠ¥ ã€‚
    * è®¡ç®—é¢„æœŸå›æŠ¥ï¼Œæˆ‘ä»¬æ€»ç»“äº†æ‰€æœ‰å›æŠ¥ï¼Œå°†æ¯ä¸ªå›æŠ¥ä¹˜ä»¥å®ƒä»¬å‘ç”Ÿçš„æ¦‚ç‡ã€‚
    * æœŸæœ›= ``0.5*0 + 0.4*2 + 0.1*10``
    * å› æ­¤ é¢„æœŸå›æŠ¥ç‡ä¸º1.8

ä¸€ä¸ªéšæœºå˜é‡ X çš„ **æœŸæœ›(expectation)/å¹³å‡å€¼(average)** è¡¨ç¤ºä¸º

.. math::

    E[X] = E_{x \sim P}[x] =  \sum_{x} x P(X = x).


* å½“å‡½æ•° ``f(x)`` çš„è¾“å…¥æ˜¯ä»åˆ†å¸ƒ ``P`` ä¸­æŠ½å–çš„éšæœºå˜é‡æ—¶ï¼Œ ``f(x)`` çš„æœŸæœ›å€¼ä¸º

.. math::

    E_{x \sim P}[f(x)] = \sum_x f(x) P(x)

ä¸€ä¸ªéšæœºå˜é‡ X çš„ **å¯†åº¦**



* åœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›è¡¡é‡éšæœºå˜é‡ **X** ä¸å…¶æœŸæœ›å€¼çš„åç½®ã€‚è¿™å¯ä»¥é€šè¿‡æ–¹å·®æ¥é‡åŒ–

.. math::

    \mathrm{Var}[X] = E\left[(X - E[X])^2\right] =
    E[X^2] - E[X]^2

* æ–¹å·®çš„å¹³æ–¹æ ¹è¢«ç§°ä¸º **æ ‡å‡†å·®(standard deviation)**
* éšæœºå˜é‡å‡½æ•°çš„æ–¹å·®è¡¡é‡çš„æ˜¯ï¼šå½“ä»è¯¥éšæœºå˜é‡åˆ†å¸ƒä¸­é‡‡æ ·ä¸åŒå€¼ x æ—¶ï¼Œ
* å‡½æ•°å€¼åç¦»è¯¥å‡½æ•°çš„æœŸæœ›çš„ç¨‹åº¦ï¼š

.. math::

    \mathrm{Var}[f(x)] = E\left[\left(f(x) - E[f(x)]\right)^2\right]


3. Linear Neural Networks for Regression
----------------------------------------

3.1. Linear Regression
^^^^^^^^^^^^^^^^^^^^^^

Basics
""""""

Model
+++++

* åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨é«˜ç»´æ•°æ®é›†ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ç´§å‡‘çš„çº¿æ€§ä»£æ•°è¡¨ç¤ºæ³•ä¼šæ›´æ–¹ä¾¿
* å½“æˆ‘ä»¬çš„è¾“å…¥ç”± d ç‰¹å¾ç»„æˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªç‰¹å¾åˆ†é…ä¸€ä¸ªç´¢å¼•ï¼ˆåœ¨ 1 å’Œ d ä¹‹é—´ï¼‰å¹¶è¡¨è¾¾æˆ‘ä»¬çš„é¢„æµ‹ :math:`\hat{y}`

.. math::

    \hat{y} = w_1x_1 + \cdot + w_dx_d + b


* å°†æ‰€æœ‰ **ç‰¹å¾** æ”¶é›†åˆ°å‘é‡ :math:`\mathbf{x} \in \mathbb{R}^d` ä¸­ï¼Œå¹¶å°†æ‰€æœ‰ **æƒé‡** æ”¶é›†åˆ°å‘é‡ :math:`\mathbf{w} \in \mathbb{R}^d` ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ **x** å’Œ **w** å‘é‡çš„ç‚¹ç§¯ç®€æ´çš„è¡¨è¾¾

.. math::

    \hat{y} = \mathbf{w}^{\top}\mathbf{x} + b

* é€šè¿‡è®¾è®¡çŸ©é˜µ :math:`\mathbf{X} \in \mathbb{R}^{n \times d}` å¼•ç”¨ n ä¸ªç¤ºä¾‹çš„æ•´ä¸ªæ•°æ®é›†çš„ç‰¹å¾å¾ˆæ–¹ä¾¿ã€‚è¿™é‡Œï¼Œ :math:`\mathbf{X}` åŒ…å«æ¯ä¸ªç¤ºä¾‹(è¡Œ)å’Œæ¯ä¸ªåŠŸèƒ½(åˆ—)ã€‚å¯¹äºç‰¹å¾é›†åˆ :math:`\mathbf{X}` ï¼Œé¢„æµ‹ :math:`\hat{y} \in \mathbb{R}^n` å¯ä»¥é€šè¿‡çŸ©é˜µå‘é‡ç§¯è¡¨ç¤º

.. math::

    \hat{y} = \mathbf{X}\mathbf{w} + b

Loss Function
+++++++++++++

* æŸå¤±å‡½æ•°é‡åŒ–ç›®æ ‡çš„å®é™…å€¼å’Œé¢„æµ‹å€¼ä¹‹é—´çš„è·ç¦»ã€‚æŸå¤±é€šå¸¸æ˜¯ä¸€ä¸ªéè´Ÿæ•°ï¼Œå…¶ä¸­å€¼è¶Šå°è¶Šå¥½ï¼Œå®Œç¾çš„é¢„æµ‹ä¼šå¯¼è‡´æŸå¤±ä¸º 0ã€‚
* å¯¹äºå›å½’é—®é¢˜ï¼Œæœ€å¸¸è§çš„æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹è¯¯å·®ã€‚
* å¯¹ç¤ºä¾‹ i çš„é¢„æµ‹ä¸º :math:`\hat{y}^{(i)}` ä¸”ç›¸åº”çš„çœŸå®æ ‡ç­¾ä¸º :math:`y^{(i)}` æ—¶ï¼Œå¹³æ–¹è¯¯å·®ç”±ä¸‹å¼ç»™å‡ºï¼š

.. math::

    l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)}-y^{(i)}\right)^2


æ³¨æ„ï¼Œç”±äºå…¶äºŒæ¬¡æ–¹å½¢å¼ï¼Œä¼°è®¡  :math:`\hat{y}^{(i)}` å’Œç›®æ ‡ :math:`y^{(i)}` ä¹‹é—´çš„å·¨å¤§å·®å¼‚ä¼šå¯¼è‡´å¯¹æŸå¤±çš„å½±å“æ›´å¤§ï¼ˆè¿™ç§äºŒæ¬¡æ–¹çš„ç‰¹æ€§å¯èƒ½æ˜¯ä¸€æŠŠåŒåˆƒå‰‘ï¼›è™½ç„¶å®ƒé¼“åŠ±æ¨¡å‹ä»¥é¿å…å¤§é”™è¯¯ï¼Œä¹Ÿå¯èƒ½å¯¼è‡´å¯¹å¼‚å¸¸æ•°æ®è¿‡åº¦æ•æ„Ÿï¼‰ã€‚ä¸ºäº†è¡¡é‡ 
n ä¸ªç¤ºä¾‹çš„æ•°æ®é›†ä¸Šçš„æ•´ä½“æ¨¡å‹è´¨é‡ï¼Œæˆ‘ä»¬åªéœ€å¯¹è®­ç»ƒé›†ä¸Šçš„æŸå¤±è¿›è¡Œå¹³å‡å³å¯

.. math::

    L(\mathbf{w}, b)=\frac{1}{n} \sum_{i=1}^{n} l^{(i)}(\mathbf{w}, b)
                    =\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \left(\hat{y}^{(i)}-y^{(i)}\right)^{2}
                    =\frac{1}{n} \sum_{i=1}^{n} \frac{1}{2}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)^{2}


è®­ç»ƒæ¨¡å‹æ—¶, æˆ‘ä»¬å¯»æ±‚èƒ½å¤Ÿæœ€å°åŒ–æ‰€æœ‰è®­ç»ƒç¤ºä¾‹çš„æ€»æŸå¤±çš„å‚æ•°  :math:`\left(\mathbf{w}^{*}, b^{*}\right)` :

.. math::

    \mathbf{w}^{*}, b^{*}=\underset{\mathbf{w}, b}{\operatorname{argmin}} L(\mathbf{w}, b) .

Analytic Solution(è§£æè§£)
+++++++++++++++++++++++++

* çº¿æ€§å›å½’çš„è§£å¯ä»¥ç”¨ä¸€ä¸ªå…¬å¼ç®€å•åœ°è¡¨è¾¾å‡ºæ¥ï¼Œ è¿™ç±»è§£å«ä½œè§£æè§£ï¼ˆanalytical solutionï¼‰ã€‚
* çº¿æ€§å›å½’çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ç»„å‚æ•° **w** å’Œåç½® bï¼Œä½¿å¾—é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„è¯¯å·®æœ€å°åŒ–ã€‚ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œå¯ä»¥å°†åç½®é¡¹ b åˆå¹¶åˆ°å‚æ•°å‘é‡ **w** ä¸­ï¼Œæ–¹æ³•æ˜¯åœ¨è®¾è®¡çŸ©é˜µ **X** çš„æ¯ä¸€è¡Œæœ«å°¾æ·»åŠ ä¸€ä¸ª1ï¼Œä»è€Œå°†åç½®è§†ä¸ºæƒé‡çš„ä¸€éƒ¨åˆ†ã€‚


.. note::  åƒçº¿æ€§å›å½’è¿™æ ·çš„ç®€å•é—®é¢˜å­˜åœ¨è§£æè§£ï¼Œä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„é—®é¢˜éƒ½å­˜åœ¨è§£æè§£ã€‚ è§£æè§£å¯ä»¥è¿›è¡Œå¾ˆå¥½çš„æ•°å­¦åˆ†æï¼Œä½†è§£æè§£å¯¹é—®é¢˜çš„é™åˆ¶å¾ˆä¸¥æ ¼ï¼Œå¯¼è‡´å®ƒæ— æ³•å¹¿æ³›åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ é‡Œã€‚


Minibatch Stochastic Gradient Descent
+++++++++++++++++++++++++++++++++++++

* æ¢¯åº¦ä¸‹é™ï¼ˆgradient descentï¼‰ï¼šå®ƒé€šè¿‡ä¸æ–­åœ°åœ¨æŸå¤±å‡½æ•°é€’å‡çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°æ¥é™ä½è¯¯å·®ã€‚æ¢¯åº¦ä¸‹é™æœ€ç®€å•çš„ç”¨æ³•æ˜¯è®¡ç®—æŸå¤±å‡½æ•°ï¼ˆæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬çš„æŸå¤±å‡å€¼ï¼‰ å…³äºæ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼ˆåœ¨è¿™é‡Œä¹Ÿå¯ä»¥ç§°ä¸ºæ¢¯åº¦ï¼‰
* å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆminibatch stochastic gradient descentï¼‰ï¼šåœ¨æ¯æ¬¡éœ€è¦è®¡ç®—æ›´æ–°çš„æ—¶å€™éšæœºæŠ½å–ä¸€å°æ‰¹æ ·æœ¬çš„æ¢¯åº¦ä¸‹é™ã€‚ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™æ˜¯å› ä¸ºæ¢¯åº¦ä¸‹é™åœ¨å®é™…ä¸­çš„æ‰§è¡Œå¯èƒ½ä¼šéå¸¸æ…¢ï¼šåŸå› æ˜¯åœ¨æ¯ä¸€æ¬¡æ›´æ–°å‚æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»éå†æ•´ä¸ªæ•°æ®é›†ã€‚

åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆéšæœºæŠ½æ ·ä¸€ä¸ªå°æ‰¹é‡ :math:`\mathcal{B}` ï¼Œ å®ƒæ˜¯ç”±å›ºå®šæ•°é‡çš„è®­ç»ƒæ ·æœ¬ç»„æˆçš„ã€‚ ç„¶åï¼Œæˆ‘ä»¬è®¡ç®—å°æ‰¹é‡çš„å¹³å‡æŸå¤±å…³äºæ¨¡å‹å‚æ•°çš„å¯¼æ•°ï¼ˆä¹Ÿå¯ä»¥ç§°ä¸ºæ¢¯åº¦ï¼‰ã€‚ æœ€åï¼Œæˆ‘ä»¬å°†æ¢¯åº¦ä¹˜ä»¥ä¸€ä¸ªé¢„å…ˆç¡®å®šçš„æ­£æ•° :math:`\eta` ï¼Œå¹¶ä»å½“å‰å‚æ•°çš„å€¼ä¸­å‡æ‰ã€‚

* æˆ‘ä»¬ç”¨ä¸‹é¢çš„æ•°å­¦å…¬å¼æ¥è¡¨ç¤ºè¿™ä¸€æ›´æ–°è¿‡ç¨‹ï¼ˆ  :math:`\partial`  è¡¨ç¤ºåå¯¼æ•°ï¼‰ï¼š

.. math::

    (\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w}, b)} l^{(i)}(\mathbf{w}, b) .


æ€»ç»“ä¸€ä¸‹ï¼Œç®—æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼šï¼ˆ1ï¼‰åˆå§‹åŒ–æ¨¡å‹å‚æ•°çš„å€¼ï¼Œå¦‚éšæœºåˆå§‹åŒ–ï¼›ï¼ˆ2ï¼‰ä»æ•°æ®é›†ä¸­éšæœºæŠ½å–å°æ‰¹é‡æ ·æœ¬ä¸”åœ¨è´Ÿæ¢¯åº¦çš„æ–¹å‘ä¸Šæ›´æ–°å‚æ•°ï¼Œå¹¶ä¸æ–­è¿­ä»£è¿™ä¸€æ­¥éª¤ã€‚å¯¹äºå¹³æ–¹æŸå¤±å’Œä»¿å°„å˜æ¢ï¼Œæˆ‘ä»¬å¯ä»¥æ˜ç¡®åœ°å†™æˆå¦‚ä¸‹å½¢å¼:

.. math::

    \begin{aligned}
    \mathbf{w} & \leftarrow \mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b)=\mathbf{w}-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right) \\
    b & \leftarrow b-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{b} l^{(i)}(\mathbf{w}, b)=b-\frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(\mathbf{w}^{\top} \mathbf{x}^{(i)}+b-y^{(i)}\right)
    \end{aligned}

.. note:: æ›´è‰°å·¨çš„ä»»åŠ¡æ˜¯æ‰¾åˆ°èƒ½å¤Ÿå¯¹ä»¥å‰æœªè§è¿‡çš„æ•°æ®è¿›è¡Œå‡†ç¡®é¢„æµ‹çš„å‚æ•°ï¼Œè¿™ä¸€æŒ‘æˆ˜ç§°ä¸ºæ³›åŒ–ã€‚The more formidable task is to find parameters that lead to accurate predictions on previously unseen data, a challenge called generalization.

Predictions(é¢„æµ‹)
+++++++++++++++++

* ç»™å®šæ¨¡å‹ :math:`\hat{\mathbf{w}}^{\top}\mathbf{x} + \hat{b}` ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¯¹æ–°ç¤ºä¾‹è¿›è¡Œé¢„æµ‹(æœ‰æ—¶ä¹Ÿç§°æ¨ç†)


Vectorization for Speed
"""""""""""""""""""""""

* ä½¿ç”¨torchå‘é‡åº“æ¯”ç›´æ¥ä½¿ç”¨forå¾ªç¯è¦å¿«3ä¸ªæ•°é‡çº§

The Normal Distribution and Squared Loss
""""""""""""""""""""""""""""""""""""""""

* æ­£æ€åˆ†å¸ƒçš„å…¬å¼

.. math::

    f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}

å…¶ä¸­::

    Î¼ï¼šå‡å€¼ï¼ˆmeanï¼‰ï¼Œè¡¨ç¤ºåˆ†å¸ƒçš„ä¸­å¿ƒã€‚
    ğœ^2 ï¼šæ–¹å·®ï¼ˆvarianceï¼‰ï¼Œè¡¨ç¤ºåˆ†å¸ƒçš„å®½åº¦ï¼Œåæ˜ æ•°æ®çš„ç¦»æ•£ç¨‹åº¦

    ğœ è¶Šå¤§ï¼Œåˆ†å¸ƒè¶Šå®½ã€è¶Šå¹³
    ğœ è¶Šå°ï¼Œåˆ†å¸ƒè¶Šçª„ã€è¶Šå°–

Linear Regression as a Neural Network
"""""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/LjmKJJ.png

    ç”±æ ‘çªï¼ˆdendritesï¼Œè¾“å…¥ç»ˆç«¯ï¼‰ã€ ç»†èƒæ ¸ï¼ˆnucleusï¼ŒCPUï¼‰ç»„æˆçš„ç”Ÿç‰©ç¥ç»å…ƒå›¾ç‰‡ã€‚ è½´çªï¼ˆaxonï¼Œè¾“å‡ºçº¿ï¼‰å’Œè½´çªç«¯å­ï¼ˆaxon terminalï¼Œè¾“å‡ºç«¯å­ï¼‰ é€šè¿‡çªè§¦ï¼ˆsynapseï¼‰ä¸å…¶ä»–ç¥ç»å…ƒè¿æ¥ã€‚consisting of ``dendrites`` (input terminals), the ``nucleus`` (CPU), the ``axon`` (output wire), and the ``axon terminals`` (output terminals), enabling connections to other neurons via ``synapses``.


.. note:: è¿‡ç¨‹ï¼šæ¥è‡ªå…¶ä»–ç¥ç»å…ƒï¼ˆæˆ–ç¯å¢ƒä¼ æ„Ÿå™¨ï¼‰çš„ä¿¡æ¯ :math:`x_i` åœ¨æ ‘çªä¸­è¢«æ¥æ”¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œè¯¥ä¿¡æ¯é€šè¿‡çªè§¦æƒé‡ :math:`w_i` è¿›è¡ŒåŠ æƒï¼Œç¡®å®šè¾“å…¥çš„æ•ˆæœï¼Œä¾‹å¦‚é€šè¿‡äº§å“ :math:`x_i w_i` æ¿€æ´»æˆ–æŠ‘åˆ¶ã€‚æ¥è‡ªå¤šä¸ªæºçš„åŠ æƒè¾“å…¥åœ¨æ ¸ä¸­èšåˆä¸ºåŠ æƒå’Œ :math:`y=\sum_i{x_i w_i} + b` ï¼Œå¯èƒ½é€šè¿‡å‡½æ•° :math:`\sigma(y)` è¿›è¡Œä¸€äº›éçº¿æ€§åå¤„ç†ã€‚ç„¶åï¼Œè¯¥ä¿¡æ¯é€šè¿‡è½´çªå‘é€åˆ°è½´çªæœ«ç«¯ï¼Œåœ¨é‚£é‡Œåˆ°è¾¾ç›®çš„åœ°ï¼ˆä¾‹å¦‚è‚Œè‚‰ç­‰æ‰§è¡Œå™¨ï¼‰ï¼Œæˆ–è€…é€šè¿‡æ ‘çªé¦ˆé€åˆ°å¦ä¸€ä¸ªç¥ç»å…ƒã€‚



3.2. Object-Oriented Design for Implementation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

å®ç°äº†å‡ ä¸ªå¯¹è±¡ç±»::

    HyperParameters
    ProgressBoard
    Module
    DataModule
    Trainer


3.3. Synthetic Regression Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* åˆ†åˆ«ä»‹ç»äº†ä½¿ç”¨ **ç”Ÿæˆæ•°æ®é›†** å’Œ **è¯»å–æ•°æ®é›†**
* è¿˜ä»‹ç»äº†ä½¿ç”¨ ``torch.utils.data.TensorDataset`` å’Œ ``torch.utils.data.DataLoader`` çš„ç®€æ´å®ç°

* å®ç°æ•°æ®åŠ è½½å™¨å¯¹è±¡ç±»::

    SyntheticRegressionData


3.4. Linear Regression Implementation from Scratch
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ä»å¤´å¼€å§‹å®ç°æ•´ä¸ªæ–¹æ³•ï¼ŒåŒ…æ‹¬ï¼ˆiï¼‰æ¨¡å‹ï¼› (ii) æŸå¤±å‡½æ•°ï¼› (iii) å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼› (iv) å°†æ‰€æœ‰è¿™äº›éƒ¨åˆ†æ‹¼æ¥åœ¨ä¸€èµ·çš„è®­ç»ƒå‡½æ•°ã€‚

3.4.1. Defining the Model
"""""""""""""""""""""""""

.. code-block:: python

    # ä»å¹³å‡å€¼ä¸º 0ã€æ ‡å‡†å·®ä¸º 0.01 çš„æ­£æ€åˆ†å¸ƒä¸­æŠ½å–éšæœºæ•°æ¥åˆå§‹åŒ–æƒé‡
    # é­”æ³•æ•°å­— 0.01 åœ¨å®è·µä¸­é€šå¸¸æ•ˆæœå¾ˆå¥½
    class LinearRegressionScratch(d2l.Module):  #@save
        """The linear regression model implemented from scratch."""
        def __init__(self, num_inputs, lr, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)
            self.b = torch.zeros(1, requires_grad=True)

        # ç”Ÿæˆçš„ forward æ–¹æ³•
        def forward(self, X):
            return torch.matmul(X, self.w) + self.b

3.4.2. Defining the Loss Function
"""""""""""""""""""""""""""""""""

è¿”å›å°æ‰¹é‡ä¸­æ‰€æœ‰ç¤ºä¾‹çš„å¹³å‡æŸå¤±å€¼(ä½¿ç”¨å¹³æ–¹æŸå¤±å‡½æ•°):

.. code-block:: python

    @d2l.add_to_class(LinearRegressionScratch)  #@save
    def loss(self, y_hat, y):
        l = (y_hat - y) ** 2 / 2
        return l.mean()

3.4.3. Defining the Optimization Algorithm
""""""""""""""""""""""""""""""""""""""""""

SGD(éšæœºæ¢¯åº¦ä¸‹é™) ä¼˜åŒ–å™¨:

.. code-block:: python

    class SGD(d2l.HyperParameters):  #@save
        """Minibatch stochastic gradient descent."""
        def __init__(self, params, lr):
            self.save_hyperparameters()

        def step(self):
            for param in self.params:
                param -= self.lr * param.grad

        def zero_grad(self):
            for param in self.params:
                if param.grad is not None:
                    param.grad.zero_()


å®šä¹‰ configure_optimizers æ–¹æ³•ï¼Œå®ƒè¿”å› SGD ç±»çš„å®ä¾‹:

.. code-block:: python

    @d2l.add_to_class(LinearRegressionScratch)  #@save
    def configure_optimizers(self):
        return SGD([self.w, self.b], self.lr)


3.4.4. Training
"""""""""""""""

* æ‰§è¡Œä»¥ä¸‹å¾ªç¯ï¼ˆå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼‰
    * Initialize parameters  :math:`(\mathbf{w}, b)`
    * Repeat until done
        * Compute gradient :math:`\mathbf{g} \leftarrow \partial_{(\mathbf{w}, b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l\left(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b\right)`
        * Update parameters :math:`(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\eta \mathbf{g}`


.. .. math::

..     \begin{array}{l}
..     \text { Initialize parameters} (\mathbf{w}, b) \\
..     \text { Repeat until done} \\
..         \begin{array}{l}
..         \text { Compute gradient } \mathbf{g} \leftarrow \partial_{(\mathbf{w}, b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l\left(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b\right) \\
..         \text { Update parameters }(\mathbf{w}, b) \leftarrow(\mathbf{w}, b)-\eta \mathbf{g} \\
..         \end{array}
..     \end{array}

* åœ¨æ¯ä¸ª epoch ä¼ é€’ä¸€æ¬¡éªŒè¯æ•°æ®åŠ è½½å™¨æ¥æµ‹é‡æ¨¡å‹æ€§èƒ½

.. code-block:: python

    @d2l.add_to_class(d2l.Trainer)  #@save
    def prepare_batch(self, batch):
        return batch

    @d2l.add_to_class(d2l.Trainer)  #@save
    def fit_epoch(self):
        self.model.train()
        for batch in self.train_dataloader:
            loss = self.model.training_step(self.prepare_batch(batch))
            self.optim.zero_grad()
            with torch.no_grad():
                loss.backward()
                if self.gradient_clip_val > 0:  # To be discussed later
                    self.clip_gradients(self.gradient_clip_val, self.model)
                self.optim.step()
            self.train_batch_idx += 1
        if self.val_dataloader is None:
            return
        self.model.eval()
        for batch in self.val_dataloader:
            with torch.no_grad():
                self.model.validation_step(self.prepare_batch(batch))
            self.val_batch_idx += 1

ä½¿ç”¨å­¦ä¹ ç‡ ``lr=0.03`` è®­ç»ƒæ¨¡å‹å¹¶è®¾ç½® ``max_epochs=3`` ::

    model = LinearRegressionScratch(2, lr=0.03)
    data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)
    trainer = d2l.Trainer(max_epochs=3)
    trainer.fit(model, data)

    with torch.no_grad():
        print(f'error in estimating w: {data.w - model.w.reshape(data.w.shape)}')
        print(f'error in estimating b: {data.b - model.b}')
    # error in estimating w: tensor([ 0.1408, -0.1493])
    # error in estimating b: tensor([0.2130])





3.5. Concise Implementation of Linear Regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

3.5.1. Defining the Model
"""""""""""""""""""""""""

.. code-block:: python

    class LinearRegression(d2l.Module):  #@save
        """The linear regression model implemented with high-level APIs."""
        def __init__(self, lr):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.LazyLinear(1)
            self.net.weight.data.normal_(0, 0.01)
            self.net.bias.data.fill_(0)

3.5.2. Defining the Loss Function
"""""""""""""""""""""""""""""""""

.. code-block:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def loss(self, y_hat, y):
        fn = nn.MSELoss()
        return fn(y_hat, y)


3.5.3. Defining the Optimization Algorithm
""""""""""""""""""""""""""""""""""""""""""

.. code-block:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def configure_optimizers(self):
        return torch.optim.SGD(self.parameters(), self.lr)

3.5.4. Training
"""""""""""""""

.. code-block:: python

    model = LinearRegression(lr=0.03)
    data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)
    trainer = d2l.Trainer(max_epochs=3)
    trainer.fit(model, data)

* ä¼°è®¡å‚æ•°ä¸å…¶çœŸå®çš„å¯¹åº”å‚æ•°çš„å¯¹æ¯”

.. code-block:: python

    @d2l.add_to_class(LinearRegression)  #@save
    def get_w_b(self):
        return (self.net.weight.data, self.net.bias.data)
    w, b = model.get_w_b()

    print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')
    print(f'error in estimating b: {data.b - b}')


3.6. Generalization
^^^^^^^^^^^^^^^^^^^

3.6.1. Training Error and Generalization Error
""""""""""""""""""""""""""""""""""""""""""""""

* è®­ç»ƒè¯¯å·®è¡¨ç¤ºä¸ºæ€»å’Œ(è®­ç»ƒæ•°æ®é›†ä¸Šè®¡ç®—çš„ç»Ÿè®¡é‡)
* è®­ç»ƒè¯¯å·®æ˜¯åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—çš„è¯¯å·®ï¼Œæ˜¯ä¸€ä¸ªç»Ÿè®¡é‡ã€‚
* å®ƒåæ˜ æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æ‹Ÿåˆç¨‹åº¦ã€‚

.. math::

    R_{\mathrm{emp}}[\mathbf{X}, \mathbf{y}, f]=\frac{1}{n} \sum_{i=1}^{n} l\left(\mathbf{x}^{(i)}, y^{(i)}, f\left(\mathbf{x}^{(i)}\right)\right)\\

* æ³›åŒ–è¯¯å·®åˆ™è¡¨ç¤ºä¸ºç§¯åˆ†(integral)
* æ³›åŒ–è¯¯å·®æ˜¯åœ¨çœŸå®åˆ†å¸ƒä¸Šçš„è¯¯å·®ï¼Œæ˜¯ä¸€ä¸ªæœŸæœ›ã€‚
* æ³›åŒ–è¯¯å·®æ˜¯å¯¹æ— é™å¤šæ•°æ®æ ·æœ¬çš„æœŸæœ›ã€‚
* æ³›åŒ–è¯¯å·®æ˜¯å¯¹åŸºç¡€åˆ†å¸ƒçš„é¢„æœŸ: å¯ä»¥å°†æ³›åŒ–é”™è¯¯è§†ä¸ºå¦‚æœæ‚¨å°†æ¨¡å‹åº”ç”¨äºä»åŒä¸€åŸºç¡€æ•°æ®åˆ†å¸ƒä¸­æå–çš„æ— é™é™„åŠ æ•°æ®ç¤ºä¾‹æµ

.. math::

    R[p, f]=E_{(\mathbf{x}, y) \sim P}[l(\mathbf{x}, y, f(\mathbf{x}))]=\iint l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) d \mathbf{x} d y


* æœ‰é—®é¢˜çš„æ˜¯ï¼Œæˆ‘ä»¬æ°¸è¿œæ— æ³•å‡†ç¡®è®¡ç®—æ³›åŒ–è¯¯å·® :math:`R` 
* çœŸå®æ•°æ®åˆ†å¸ƒ :math:`p(\mathbf{x}, y)` æ˜¯æœªçŸ¥çš„ï¼Œæˆ‘ä»¬æ— æ³•ç›´æ¥å¾—åˆ°çœŸå®åˆ†å¸ƒã€‚
* æ— æ³•è·å–æ— é™æ•°æ®ï¼Œåªèƒ½åœ¨æœ‰é™çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°ã€‚
* å› æ­¤ï¼Œæ³›åŒ–è¯¯å·®åªèƒ½é€šè¿‡æµ‹è¯•é›†è¿‘ä¼¼ä¼°è®¡ï¼Œè€Œéç²¾ç¡®è®¡ç®—ã€‚
* åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¿…é¡»é€šè¿‡å°†æˆ‘ä»¬çš„æ¨¡å‹åº”ç”¨åˆ°ä¸€ä¸ªç‹¬ç«‹çš„æµ‹è¯•é›†æ¥ä¼°è®¡æ³›åŒ–è¯¯å·®ï¼Œè¯¥æµ‹è¯•é›†ç”±éšæœºé€‰æ‹©çš„ç¤ºä¾‹ :math:`Xâ€˜` å’Œä»æˆ‘ä»¬çš„è®­ç»ƒé›†ä¸­ä¿ç•™çš„æ ‡ç­¾ :math:`yâ€˜` ç»„æˆã€‚è¿™åŒ…æ‹¬å°†ç”¨äºè®¡ç®—ç»éªŒè®­ç»ƒè¯¯å·®çš„ç›¸åŒå…¬å¼åº”ç”¨äºæµ‹è¯•é›† :math:`Xâ€˜, yâ€˜` ã€‚

.. note:: è¯·æ³¨æ„ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„æ¨¡å‹æ˜ç¡®å–å†³äºè®­ç»ƒé›†çš„é€‰æ‹©ï¼Œå› æ­¤è®­ç»ƒè¯¯å·®é€šå¸¸æ˜¯å¯¹åŸºç¡€æ€»ä½“çœŸå®è¯¯å·®çš„æœ‰åä¼°è®¡ã€‚æ³›åŒ–çš„æ ¸å¿ƒé—®é¢˜æ˜¯æˆ‘ä»¬ä»€ä¹ˆæ—¶å€™åº”è¯¥æœŸæœ›æˆ‘ä»¬çš„è®­ç»ƒè¯¯å·®æ¥è¿‘æ€»ä½“è¯¯å·®ï¼ˆä»¥åŠæ³›åŒ–è¯¯å·®ï¼‰ã€‚


.. note:: ã€æ€»ç»“ã€‘è®­ç»ƒè¯¯å·®æ˜¯å¯¹è®­ç»ƒæ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ï¼Œè€Œæ³›åŒ–è¯¯å·®åæ˜ æ¨¡å‹åœ¨çœŸå®åˆ†å¸ƒä¸Šçš„è¡¨ç°ã€‚æ³›åŒ–è¯¯å·®æ— æ³•ç›´æ¥è®¡ç®—ï¼Œä½†å¯ä»¥é€šè¿‡æµ‹è¯•é›†ä¼°è®¡ã€‚æ³›åŒ–çš„æ ¸å¿ƒé—®é¢˜æ˜¯å¦‚ä½•ä½¿è®­ç»ƒè¯¯å·®ä¸æ³›åŒ–è¯¯å·®å°½å¯èƒ½æ¥è¿‘ï¼Œä»è€Œç¡®ä¿æ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°è‰¯å¥½ã€‚


3.6.2. Underfitting or Overfitting?
"""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/AhEHKr.png

    Fig. 3.6.1 Influence of model complexity on underfitting and overfitting.Â¶

3.6.3. Model Selection
""""""""""""""""""""""

* é€šå¸¸ï¼Œæˆ‘ä»¬åªæœ‰åœ¨è¯„ä¼°äº†å¤šä¸ªä¸åŒæ–¹é¢ï¼ˆä¸åŒçš„æ¶æ„ã€è®­ç»ƒç›®æ ‡ã€é€‰å®šçš„ç‰¹å¾ã€æ•°æ®é¢„å¤„ç†ã€å­¦ä¹ ç‡ç­‰ï¼‰çš„æ¨¡å‹åæ‰é€‰æ‹©æœ€ç»ˆæ¨¡å‹ã€‚åœ¨ä¼—å¤šæ¨¡å‹ä¸­è¿›è¡Œé€‰æ‹©è¢«æ°å½“åœ°ç§°ä¸ºæ¨¡å‹é€‰æ‹©ã€‚
* ã€Cross-Validationã€‘å½“è®­ç»ƒæ•°æ®ç¨€ç¼ºæ—¶ï¼Œæˆ‘ä»¬ç”šè‡³å¯èƒ½æ— æ³•æä¾›è¶³å¤Ÿçš„æ•°æ®æ¥æ„æˆé€‚å½“çš„éªŒè¯é›†ã€‚æ­¤é—®é¢˜çš„ä¸€ç§æµè¡Œè§£å†³æ–¹æ¡ˆæ˜¯é‡‡ç”¨ K æŠ˜å äº¤å‰éªŒè¯ã€‚è¿™é‡Œï¼ŒåŸå§‹è®­ç»ƒæ•°æ®è¢«åˆ†æˆ K ä¸ªä¸é‡å çš„å­é›†ã€‚ç„¶åæ‰§è¡Œæ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ K æ¬¡ï¼Œæ¯æ¬¡å¯¹ K-1 å­é›†è¿›è¡Œè®­ç»ƒå¹¶åœ¨ä¸åŒçš„å­é›†ï¼ˆè¯¥è½®ä¸­æœªç”¨äºè®­ç»ƒçš„å­é›†ï¼‰ä¸Šè¿›è¡ŒéªŒè¯ã€‚æœ€åï¼Œé€šè¿‡å¯¹ K å®éªŒç»“æœè¿›è¡Œå¹³å‡æ¥ä¼°è®¡è®­ç»ƒå’ŒéªŒè¯è¯¯å·®ã€‚


3.6.4. Summary
""""""""""""""

* ç»éªŒæ³•åˆ™
    * ä½¿ç”¨éªŒè¯é›†ï¼ˆæˆ– K-æŠ˜å äº¤å‰éªŒè¯, k-fold cross-validationï¼‰è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼›
    * æ›´å¤æ‚çš„æ¨¡å‹é€šå¸¸éœ€è¦æ›´å¤šçš„æ•°æ®ï¼›
    * å¤æ‚æ€§çš„ç›¸å…³æ¦‚å¿µåŒ…æ‹¬å‚æ•°çš„æ•°é‡å’Œå®ƒä»¬å…è®¸é‡‡ç”¨çš„å€¼çš„èŒƒå›´ï¼›
    * åœ¨å…¶ä»–æ¡ä»¶ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œæ›´å¤šçš„æ•°æ®å‡ ä¹æ€»æ˜¯èƒ½å¸¦æ¥æ›´å¥½çš„æ¦‚æ‹¬ï¼›
    * åœ¨è®¨è®ºæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æ—¶ï¼Œé€šå¸¸å‡è®¾è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰çš„ã€‚å¦‚æœæ”¾å®½è¿™ä¸€å‡è®¾ï¼Œå…è®¸è®­ç»ƒå’Œæµ‹è¯•æœŸé—´çš„æ•°æ®åˆ†å¸ƒå‘ç”Ÿå˜åŒ–ï¼ˆå³åˆ†å¸ƒæ¼‚ç§»ï¼‰ï¼Œé‚£ä¹ˆåœ¨æ²¡æœ‰å…¶ä»–ï¼ˆå¯èƒ½æ›´å¼±çš„ï¼‰å‡è®¾çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— æ³•å¯¹æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›åšå‡ºä»»ä½•ä¿è¯ã€‚

* ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰å‡è®¾ï¼šæ˜¯è®¸å¤šç»Ÿè®¡å­¦ä¹ ç†è®ºçš„åŸºç¡€ã€‚å®ƒå‡å®šè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ¥è‡ªç›¸åŒçš„åˆ†å¸ƒï¼Œä¸”å„ä¸ªæ ·æœ¬ä¹‹é—´ç›¸äº’ç‹¬ç«‹ã€‚
* åœ¨è¿™ç§å‡è®¾ä¸‹ï¼Œå¯ä»¥æ¨å¯¼å‡ºæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¡¨ç°ï¼ˆæ³›åŒ–èƒ½åŠ›ï¼‰ã€‚ç„¶è€Œï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„åˆ†å¸ƒå¯èƒ½ä¸åŒï¼ˆç§°ä¸ºåˆ†å¸ƒæ¼‚ç§»ï¼‰ï¼Œè¿™è¿åäº†IIDå‡è®¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¼ ç»Ÿçš„æ³›åŒ–ç†è®ºå¯èƒ½ä¸å†é€‚ç”¨ï¼Œéœ€è¦å¼•å…¥æ–°çš„å‡è®¾æˆ–æ–¹æ³•æ¥åˆ†æå’Œä¿è¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚



3.7. Weight Decay
^^^^^^^^^^^^^^^^^

* our first **regularization** technique

.. note:: å‚è§ï¼š ``ã€çŸ¥è¯†ä½“ç³»ã€‘æƒé‡è¡°å‡(L2æ­£åˆ™åŒ–)``


3.7.1. Norms and Weight Decay
"""""""""""""""""""""""""""""

* æƒé‡è¡°å‡ä¸æ˜¯ç›´æ¥æ“çºµå‚æ•°çš„æ•°é‡ï¼Œè€Œæ˜¯é€šè¿‡é™åˆ¶å‚æ•°å¯ä»¥å–çš„å€¼æ¥è¿›è¡Œæ“ä½œã€‚
* åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸï¼Œæƒé‡è¡°å‡é€šå¸¸è¢«ç§°ä¸ºL2æ­£åˆ™åŒ–ã€‚å®ƒæ˜¯ä¸€ç§é€šè¿‡é™åˆ¶æ¨¡å‹å‚æ•°çš„å–å€¼èŒƒå›´æ¥é˜²æ­¢è¿‡æ‹Ÿåˆçš„æŠ€æœ¯ã€‚
    * ä¸ç›´æ¥å‡å°‘å‚æ•°æ•°é‡ä¸åŒï¼Œæƒé‡è¡°å‡é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ å‚æ•°å€¼çš„å¹³æ–¹å’Œä½œä¸ºæƒ©ç½šé¡¹ï¼Œé¼“åŠ±æ¨¡å‹å­¦ä¹ è¾ƒå°çš„æƒé‡ï¼Œä»è€Œé™ä½æ¨¡å‹å¤æ‚åº¦ã€‚
    * è¿™ç§æ–¹æ³•çš„ç›´è§‚åŠ¨æœºæ˜¯ï¼šåœ¨æ‰€æœ‰å‡½æ•°ä¸­ï¼Œæ’ç­‰äºé›¶çš„å‡½æ•° ``f=0`` è¢«è®¤ä¸ºæ˜¯æœ€ç®€å•çš„ã€‚å› æ­¤ï¼Œå¯ä»¥é€šè¿‡å‚æ•°å€¼åç¦»é›¶çš„ç¨‹åº¦æ¥è¡¡é‡å‡½æ•°çš„å¤æ‚åº¦ã€‚
    * ç„¶è€Œï¼Œå¦‚ä½•ç²¾ç¡®åœ°åº¦é‡å‡½æ•°ä¸é›¶ä¹‹é—´çš„è·ç¦»å¹¶æ²¡æœ‰å”¯ä¸€çš„ç­”æ¡ˆã€‚äº‹å®ä¸Šï¼Œæ•°å­¦ä¸­çš„æŸäº›åˆ†æ”¯ï¼ˆå¦‚æ³›å‡½åˆ†æå’ŒBanachç©ºé—´ç†è®ºï¼‰ä¸“é—¨ç ”ç©¶æ­¤ç±»é—®é¢˜ã€‚


.. note:: ã€æˆ‘çš„ç†è§£ã€‘å‰é¢è¯´äº†ã€Œå¤æ‚æ€§çš„ç›¸å…³æ¦‚å¿µåŒ…æ‹¬å‚æ•°çš„æ•°é‡å’Œå®ƒä»¬å…è®¸é‡‡ç”¨çš„å€¼çš„èŒƒå›´ã€è¿™å„¿è¯´äº†ã€Œæƒé‡è¡°å‡ä¸æ˜¯ç›´æ¥æ“çºµå‚æ•°çš„æ•°é‡ï¼Œè€Œæ˜¯é€šè¿‡é™åˆ¶å‚æ•°å¯ä»¥å–çš„å€¼æ¥è¿›è¡Œæ“ä½œã€ã€‚æ‰€ä»¥ **æƒé‡è¡°å‡æ˜¯é€šè¿‡é™ä½å‚æ•°çš„å–å€¼èŒƒå›´æ¥é™ä½æ¨¡å‹çš„å¤æ‚åº¦** ã€‚

* ã€from gptã€‘å½“æƒé‡ç‰¹åˆ«å¤§æ—¶ï¼Œæ¨¡å‹ä¼šå€¾å‘äºå»è®°ä½æ•°æ®çš„æ¯ä¸€ä¸ªç»†èŠ‚ï¼ˆåŒ…æ‹¬å™ªå£°å’Œéšæœºæ€§ï¼‰ï¼Œè¿™æ ·å®ƒçš„â€œå¤æ‚åº¦â€å°±ä¼šå˜å¾—å¾ˆé«˜ã€‚ä½†ç°å®ä¸–ç•Œçš„æ•°æ®å¾€å¾€åŒ…å«å™ªå£°ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦åˆ°ä¸»è¦è§„å¾‹ï¼Œè€Œä¸æ˜¯æ‰€æœ‰ç»†èŠ‚ã€‚
* ã€from gptã€‘è¾ƒå°çš„æƒé‡æ„å‘³ç€æ¨¡å‹ä¸èƒ½å¯¹æ•°æ®ä¸­çš„æ¯ä¸€ä¸ªå°ç‰¹å¾éƒ½è¿‡åº¦â€œè®°ä½â€ï¼Œåªèƒ½å­¦ä¹ åˆ°å¤§ä½“è§„å¾‹ï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆã€‚
* ã€from gptã€‘è¾ƒå°çš„æƒé‡é™ä½äº†æ¨¡å‹å¯¹æ•°æ®çš„æ•æ„Ÿåº¦ï¼Œè®©æ¨¡å‹çš„è¡Œä¸ºæ›´â€œå¹³æ»‘â€ï¼Œæ›´å°‘å—åˆ°å™ªå£°çš„å¹²æ‰°ï¼Œå› æ­¤é™ä½äº†æ¨¡å‹çš„å¤æ‚åº¦ã€‚è¿™ç§æ–¹æ³•é€šè¿‡é™åˆ¶æ¨¡å‹çš„â€œè¡¨è¾¾èƒ½åŠ›â€æ¥å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°æ³›åŒ–ã€‚ä½ å¯ä»¥æŠŠè¾ƒå°çš„æƒé‡æƒ³è±¡æˆç»™æ¨¡å‹å¸¦ä¸Šäº†â€œå®‰å…¨å¸½â€ï¼Œä¸è®©å®ƒå¤ªéšæ„åœ°å¯¹æ•°æ®åšå‡ºè¿‡åº¦ååº”ã€‚


.. note:: åœ¨å®è·µä¸­ï¼Œæƒé‡è¡°å‡å·²æˆä¸ºè®­ç»ƒå‚æ•°åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹æ—¶æœ€å¹¿æ³›ä½¿ç”¨çš„æ­£åˆ™åŒ–æŠ€æœ¯ä¹‹ä¸€ã€‚é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ L2æ­£åˆ™é¡¹ï¼Œæ¨¡å‹çš„æƒé‡è¢«è¿«å‡å°ï¼Œä»è€Œé™åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ï¼Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šå‡å°‘äº†æ¨¡å‹è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚

* ã€å°ç»“ã€‘æƒé‡è¡°å‡ä½œä¸ºä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œé€šè¿‡é™åˆ¶æ¨¡å‹å‚æ•°çš„å¤§å°ï¼Œå¸®åŠ©æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚

* æ–°çš„æŸå¤±å‡½æ•°

.. math::

    L_{reg}(\mathbf{w}, b) = L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|


* L2æ­£åˆ™åŒ–å›å½’çš„å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™çš„ **æƒé‡æ›´æ–°**

.. math::

    \begin{aligned}
    \mathbf{w} & \leftarrow \left(1- \eta\lambda \right) \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)
    \end{aligned}



3.7.2. High-Dimensional Linear Regression
"""""""""""""""""""""""""""""""""""""""""

.. math::

    y=0.05+\sum_{i=1}^{d} 0.01 x_{i}+\epsilon \\
    \text { where } \epsilon \sim \mathcal{N}\left(0, 0.01^{2}\right) .

.. code-block:: python

    class Data(d2l.DataModule):
        def __init__(self, num_train, num_val, num_inputs, batch_size):
            self.save_hyperparameters()
            n = num_train + num_val
            self.X = torch.randn(n, num_inputs)
            noise = torch.randn(n, 1) * 0.01
            w, b = torch.ones((num_inputs, 1)) * 0.01, 0.05
            self.y = torch.matmul(self.X, w) + b + noise

        def get_dataloader(self, train):
            i = slice(0, self.num_train) if train else slice(self.num_train, None)
            return self.get_tensorloader([self.X, self.y], train, i)


3.7.3. Implementation from Scratch
""""""""""""""""""""""""""""""""""

* ä»å¤´å¼€å§‹å®ç°æƒé‡è¡°å‡

3.7.3.1. Defining L2 Norm Penalty
+++++++++++++++++++++++++++++++++

* æœ€æ–¹ä¾¿çš„æ–¹æ³•æ˜¯å°†æ‰€æœ‰é¡¹å¹³æ–¹å¹¶æ±‚å’Œ

::

    def l2_penalty(w):
        return (w ** 2).sum() / 2


3.7.3.2. Defining the Model
+++++++++++++++++++++++++++

* å”¯ä¸€çš„å˜åŒ–æ˜¯æˆ‘ä»¬çš„æŸå¤±ç°åœ¨åŒ…æ‹¬äº†æƒ©ç½šé¡¹ã€‚

.. code-block:: python

    class WeightDecayScratch(d2l.LinearRegressionScratch):
        def __init__(self, num_inputs, lambd, lr, sigma=0.01):
            super().__init__(num_inputs, lr, sigma)
            self.save_hyperparameters()

        def loss(self, y_hat, y):
            return (super().loss(y_hat, y) +
                    self.lambd * l2_penalty(self.w))

åœ¨åŒ…å« 20 ä¸ªç¤ºä¾‹çš„è®­ç»ƒé›†ä¸Šæ‹Ÿåˆæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶åœ¨åŒ…å« 100 ä¸ªç¤ºä¾‹çš„éªŒè¯é›†ä¸Šå¯¹å…¶è¿›è¡Œè¯„ä¼°ï¼ˆè¯´æ˜ï¼šè¿™å„¿æ˜¯æƒ³è¯´åœ¨è¾ƒå°‘çš„è®­ç»ƒé›†ä¸Šé€‚åˆç”¨æƒé‡è¡°å‡ï¼‰:

.. code-block:: python

    data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)
    trainer = d2l.Trainer(max_epochs=10)

    def train_scratch(lambd):
        model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)
        model.board.yscale='log'
        trainer.fit(model, data)
        print('L2 norm of w:', float(l2_penalty(model.w)))

3.7.3.3. Training without Regularization
++++++++++++++++++++++++++++++++++++++++

::

    train_scratch(0)
    # L2 norm of w: 0.009948714636266232


3.7.3.4. Using Weight Decay
+++++++++++++++++++++++++++

::

    train_scratch(3)
    # L2 norm of w: 0.0017270983662456274


3.7.4. Concise Implementation
"""""""""""""""""""""""""""""

* é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorch åŒæ—¶è¡°å‡æƒé‡å’Œåå·®ï¼Œä½†æˆ‘ä»¬å¯ä»¥é…ç½®ä¼˜åŒ–å™¨æ ¹æ®ä¸åŒçš„ç­–ç•¥å¤„ç†ä¸åŒçš„å‚æ•°ã€‚
* åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªä¸ºæƒé‡ï¼ˆ net.weight å‚æ•°ï¼‰è®¾ç½® weight_decay ï¼Œå› æ­¤åå·®ï¼ˆ net.bias å‚æ•°ï¼‰ä¸ä¼šè¡°å‡ã€‚

.. code-block:: python

    class WeightDecay(d2l.LinearRegression):
        def __init__(self, wd, lr):
            super().__init__(lr)
            self.save_hyperparameters()
            self.wd = wd   # weight_decay

        def configure_optimizers(self):
            return torch.optim.SGD([
                {'params': self.net.weight, 'weight_decay': self.wd},
                {'params': self.net.bias}], lr=self.lr)

è¿™ä¸ªç‰ˆæœ¬è¿è¡Œé€Ÿåº¦æ›´å¿«ï¼Œæ›´å®¹æ˜“å®ç°::

    model = WeightDecay(wd=3, lr=0.01)
    model.board.yscale='log'
    trainer.fit(model, data)

    print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))
    # L2 norm of w: 0.013779522851109505

3.7.5. Summary
""""""""""""""

* æ­£åˆ™åŒ–æ˜¯å¤„ç†è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ–¹æ³•ã€‚
* ç»å…¸æ­£åˆ™åŒ–æŠ€æœ¯åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹ï¼ˆè®­ç»ƒæ—¶ï¼‰ä»¥é™ä½å­¦ä¹ æ¨¡å‹çš„å¤æ‚æ€§ã€‚
* ä¿æŒæ¨¡å‹ç®€å•çš„ä¸€ç§ç‰¹æ®Šé€‰æ‹©æ˜¯ä½¿ç”¨ L2 æƒ©ç½šã€‚è¿™å¯¼è‡´å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ›´æ–°æ­¥éª¤ä¸­çš„æƒé‡è¡°å‡ã€‚
* åœ¨å®è·µä¸­ï¼Œæƒé‡è¡°å‡åŠŸèƒ½æ˜¯åœ¨æ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä¼˜åŒ–å™¨ä¸­æä¾›çš„ã€‚åœ¨åŒä¸€è®­ç»ƒå¾ªç¯ä¸­ï¼Œä¸åŒçš„å‚æ•°é›†å¯ä»¥æœ‰ä¸åŒçš„æ›´æ–°è¡Œä¸ºã€‚


4. Linear Neural Networks for Classification
--------------------------------------------

4.1. Softmax Regression
^^^^^^^^^^^^^^^^^^^^^^^

4.1.1. Classification
"""""""""""""""""""""

* ç»Ÿè®¡å­¦å®¶å¾ˆä¹…ä»¥å‰å°±å‘æ˜äº†ä¸€ç§è¡¨ç¤ºåˆ†ç±»æ•°æ®çš„ç®€å•æ–¹æ³•ï¼šone-hot ç¼–ç ã€‚ one-hot ç¼–ç æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå…¶åˆ†é‡ä¸ç±»åˆ«ä¸€æ ·å¤šã€‚ä¸ç‰¹å®šå®ä¾‹ç±»åˆ«ç›¸å¯¹åº”çš„ç»„ä»¶è®¾ç½®ä¸º 1ï¼Œæ‰€æœ‰å…¶ä»–ç»„ä»¶è®¾ç½®ä¸º 0ã€‚

4.1.1.1. Linear Model
+++++++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/CbhiVl.png

    Fig. 4.1.1 Softmax regression is a single-layer neural network.

æ›´ç®€æ´çš„è¡¨ç¤ºæ³•:

.. math::

    \mathbf{o} = \mathbf{Wx} + \mathbf{b}

4.1.1.2. The Softmax
++++++++++++++++++++

* æœªè§„èŒƒåŒ–çš„é¢„æµ‹ o ä¸èƒ½ç›´æ¥è§†ä½œè¾“å‡ºçš„åŸå› ï¼š

    - æ²¡æœ‰é™åˆ¶è¿™äº›è¾“å‡ºæ•°å­—çš„æ€»å’Œä¸º1
    - è¾“å‡ºå¯èƒ½ä¸ºè´Ÿå€¼

* å®ç°æ­¤ç›®æ ‡ï¼ˆå¹¶ç¡®ä¿éè´Ÿæ€§ï¼‰çš„ä¸€ç§æ–¹æ³•æ˜¯ ä½¿ç”¨æŒ‡æ•°å‡½æ•° :math:`P(y=i) \propto \exp o_{i}` ã€‚è¿™ç¡®å®æ»¡è¶³äº†æ¡ä»¶ç±»åˆ«æ¦‚ç‡éšç€ :math:`o_i` å¢åŠ è€Œå¢åŠ çš„è¦æ±‚ï¼Œå®ƒæ˜¯å•è°ƒçš„ï¼Œå¹¶ä¸”æ‰€æœ‰æ¦‚ç‡éƒ½æ˜¯éè´Ÿçš„ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥è½¬æ¢è¿™äº›å€¼ï¼Œä½¿å®ƒä»¬ç›¸åŠ ä¸º ``1`` ï¼šå°†æ¯ä¸ªé™¤ä»¥å®ƒä»¬çš„æ€»å’Œã€‚è¿™ä¸ªè¿‡ç¨‹ç§°ä¸º **æ ‡å‡†åŒ–**

.. math::

    \hat{\mathbf{y}}=\operatorname{softmax}(\mathbf{o}) \quad \\
    \text { where } \quad \hat{y}_{i}=\frac{\exp \left(o_{i}\right)}{\sum_{j} \exp \left(o_{j}\right)} \\

* è¯´æ˜ï¼šå‘é‡ :math:`\mathbf{o}` çš„æœ€å¤§åæ ‡å¯¹åº”äºé¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ :math:`\hat{\mathbf{y}}` ä¸­æœ€å¯èƒ½çš„ç±»åˆ«ã€‚ 
* æ­¤å¤–ï¼Œç”±äº softmax æ“ä½œä¼šä¿ç•™è¾“å…¥ä¹‹é—´çš„æ’åºå…³ç³»ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶ä¸éœ€è¦çœŸæ­£è®¡ç®— softmax çš„ç»“æœï¼Œå°±å¯ä»¥ç¡®å®šå“ªä¸ªç±»åˆ«è¢«åˆ†é…äº†æœ€é«˜çš„æ¦‚ç‡ã€‚
* æ‰€ä»¥å¦‚æœé€‰æ‹©æœ€æœ‰å¯èƒ½çš„ç±»åˆ«çš„è¯ï¼Œå¯ä»¥çœç•¥ softmax æ­¥éª¤ï¼Œå³ï¼š

.. math::

    \underset{j}{\operatorname{argmax}} \hat{y}_{j}=\underset{j}{\operatorname{argmax}} o_{j} .


4.1.1.3. Vectorization
++++++++++++++++++++++

* ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡å¹¶ä¸”å……åˆ†åˆ©ç”¨GPU, æˆ‘ä»¬é€šå¸¸ä¼šå¯¹å°æ‰¹é‡æ ·æœ¬çš„æ•°æ®æ‰§è¡ŒçŸ¢é‡åŒ–è®¡ç®—(vectorize calculations)ã€‚
* å‡è®¾æˆ‘ä»¬è¯»å–äº†ä¸€ä¸ªæ‰¹é‡çš„æ ·æœ¬  :math:`\mathbf{X}` , å…¶ä¸­ç‰¹å¾ç»´åº¦ï¼ˆè¾“å…¥æ•°é‡ï¼‰ä¸º  d , æ‰¹é‡å¤§å°ä¸º  n  ã€‚
* æ­¤å¤–, å‡è®¾æˆ‘ä»¬åœ¨è¾“å‡ºä¸­æœ‰  q  ä¸ªç±»åˆ«ã€‚
* é‚£ä¹ˆå°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾ä¸º  :math:`\mathbf{X} \in \mathbb{R}^{n \times d}` , æƒé‡ä¸º  :math:`\mathbf{W} \in \mathbb{R}^{d \times q}` , åç½®ä¸º  :math:`\mathbf{b} \in \mathbb{R}^{1 \times q}`  
* softmaxå›å½’çš„çŸ¢é‡è®¡ç®—è¡¨è¾¾å¼ä¸º:

.. math::

    \begin{array}{l}
    \mathbf{O}=\mathbf{X W}+\mathbf{b} \\
    \hat{\mathbf{Y}}=\operatorname{softmax}(\mathbf{O})
    \end{array}




4.1.2. Loss Function
""""""""""""""""""""

4.1.2.1. Log-Likelihood(å¯¹æ•°ä¼¼ç„¶)
+++++++++++++++++++++++++++++++++

* softmax å‡½æ•°ç»™æˆ‘ä»¬ä¸€ä¸ªå‘é‡ :math:`\hat{\mathbf{y}}` ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶è§†ä¸ºâ€œå¯¹ç»™å®šä»»æ„è¾“å…¥ :math:`\mathbf{x}` çš„æ¯ä¸ªç±»çš„æ¡ä»¶æ¦‚ç‡â€ã€‚
* ä¾‹å¦‚ :math:`\hat{y}_1 = P(y=\textrm{cat} \mid \mathbf{x})` ã€‚
* å‡è®¾å¯¹äºå…·æœ‰ç‰¹å¾çš„æ•°æ®é›† :math:`\mathbf{X}` å¯¹åº”çš„æ ‡ç­¾ :math:`\mathbf{Y}` ï¼ˆå³æ•´ä¸ªæ•°æ®é›† :math:`{\mathbf \{X, Y\}}` ï¼‰å…·æœ‰ n ä¸ªæ ·æœ¬ã€‚
* å…¶ä¸­ç´¢å¼• i çš„æ ·æœ¬ç”±ï¼šç‰¹å¾å‘é‡ :math:`\mathbf{x}^{(i)}` å’Œä½¿ç”¨ one-hot ç¼–ç çš„æ ‡ç­¾å‘é‡ :math:`\mathbf{y}^{(i)}` è¡¨ç¤ºã€‚
* è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„è”åˆæ¦‚ç‡(å‡è®¾æ¯ä¸ªæ ‡ç­¾ :math:`ğ‘¦^{(ğ‘–)}` éƒ½æ˜¯ç‹¬ç«‹ä»æ¡ä»¶åˆ†å¸ƒ :math:`ğ‘ƒ(ğ‘¦âˆ£ğ‘¥^{(ğ‘–)})` ä¸­æŠ½å–çš„)ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹å¯¹æ•´ä¸ªæ•°æ®é›†çš„é¢„æµ‹èƒ½åŠ›ï¼š

.. math::

    P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).

* ç›®æ ‡ï¼šæˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ– ``ğ‘ƒ(ğ‘Œâˆ£ğ‘‹)`` ï¼Œè¿™å®é™…ä¸Šæ˜¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„ç›®æ ‡ã€‚
* æ ¹æ®æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œæˆ‘ä»¬æœ€å¤§åŒ– :math:`P(\mathbf{Y} | \mathbf{X})` ï¼Œç›¸å½“äºæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶

.. math::

    -\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
    = \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),

è¿™å…¶ä¸­ä»»æ„ä¸€å¯¹æ ‡ç­¾ :math:`\mathbf{y}` å’Œæ¨¡å‹é¢„æµ‹ :math:`\hat{\mathbf{y}}` åœ¨ q ä¸ªåˆ†ç±»ä¸Šï¼ŒæŸå¤±å‡½æ•° l æ˜¯

.. math::

    l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j

.. note:: æ³¨æ„è¿™å„¿çš„ **y** æ˜¯ one-hot ç¼–ç 

* ä¸Šé¢è¿™ä¸ªæŸå¤±å‡½æ•°é€šå¸¸è¢«å« **äº¤å‰ç†µæŸå¤±ï¼ˆcross-entropy lossï¼‰** 


4.1.2.2. Softmax and Cross-Entropy Loss
+++++++++++++++++++++++++++++++++++++++

1. äº¤å‰ç†µæŸå¤±çš„æ¨å¯¼è¿‡ç¨‹
~~~~~~~~~~~~~~~~~~~~~~~

1) softmax è¾“å‡ºçš„å½¢å¼ï¼š

.. math::

    \hat{\mathbf{y}} = \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)} \\
    å…¶ä¸­ o_j æ˜¯æ¨¡å‹å¯¹ç±»åˆ« ğ‘— çš„åŸå§‹è¾“å‡º (logits)



2) å°† softmax ä»£å…¥äº¤å‰ç†µæŸå¤±çš„å®šä¹‰ï¼š


.. math::

    \begin{aligned}
    l(\mathbf{y}, \hat{\mathbf{y}}) & =-\sum_{j=1}^{q} y_{j} \log \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)}
    \end{aligned}

3) æ‹†åˆ†å¯¹æ•°ï¼š

.. math::

    l(\mathbf{y}, \hat{\mathbf{y}}) =\sum_{j=1}^{q} y_{j} \log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j}




4) ç”±äºæ ‡ç­¾ ğ‘¦ æ˜¯ one-hot ç¼–ç æˆ–æ¦‚ç‡åˆ†å¸ƒï¼Œæ ‡ç­¾å‘é‡çš„æ‰€æœ‰å…ƒç´ åŠ èµ·æ¥æ€»æ˜¯ç­‰äº 1ï¼Œæ‰€ä»¥å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–ï¼š

.. math::

    l(\mathbf{y}, \hat{\mathbf{y}}) =\log \sum_{k=1}^{q} \exp \left(o_{k}\right)-\sum_{j=1}^{q} y_{j} o_{j} .


2. æ¢¯åº¦æ¨å¯¼ (åå‘ä¼ æ’­çš„æ ¸å¿ƒ)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~


å¯¹ä»»ä½• :math:`o_{j}` æ±‚å¯¼, æˆ‘ä»¬å¾—åˆ°:

.. math::

    \partial_{o_{j}} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp \left(o_{j}\right)}{\sum_{k=1}^{q} \exp \left(o_{k}\right)}-y_{j}

ç®€åŒ–ï¼š

.. math::

    \partial_{o_{j}} l(\mathbf{y}, \hat{\mathbf{y}}) =\operatorname{softmax}(\mathbf{o})_{j}-y_{j}


* ç›´è§‚ç†è§£ï¼š
    * :math:`softmax(ğ‘œ_ğ‘—)` æ˜¯æ¨¡å‹å¯¹ç±»åˆ« ğ‘— é¢„æµ‹çš„æ¦‚ç‡ã€‚
    * :math:`ğ‘¦_ğ‘—` æ˜¯çœŸå®æ ‡ç­¾ï¼ˆone-hotç¼–ç ï¼‰ï¼Œå¦‚æœ ğ‘— æ˜¯çœŸå®ç±»åˆ«ï¼Œåˆ™ :math:`ğ‘¦_ğ‘—=1` ï¼Œå¦åˆ™ :math:`ğ‘¦_ğ‘—=0`
    * æ¢¯åº¦è¡¨ç¤ºæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ï¼Œå³è¯¯å·®ä¿¡å·

3. æ›´ä¸€èˆ¬çš„æƒ…å†µ: æ ‡ç­¾åˆ†å¸ƒä¸ºæ¦‚ç‡åˆ†å¸ƒ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* é€šå¸¸æˆ‘ä»¬å‡è®¾æ ‡ç­¾æ˜¯ one-hot ç¼–ç çš„ï¼Œå¦‚ (0,0,1)ã€‚ä½†åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œæ ‡ç­¾å¯èƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œå¦‚ (0.1,0.2,0.7)
* è¿™ç§æƒ…å†µä¸‹ï¼Œäº¤å‰ç†µæŸå¤±çš„è®¡ç®—æ–¹å¼ä¸å˜ï¼š

.. math::

    l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j

* å”¯ä¸€çš„åŒºåˆ«æ˜¯ ğ‘¦_ğ‘— ä¸å†æ˜¯ 0 æˆ– 1ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¦‚ç‡å€¼ã€‚è¿™ç§å½¢å¼æ›´å…·ä¸€èˆ¬æ€§ï¼Œå…è®¸æ¨¡å‹å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼Œå¦‚çŸ¥è¯†è’¸é¦æˆ–å¤šæ ‡ç­¾åˆ†ç±»é—®é¢˜ã€‚


4. äº¤å‰ç†µæŸå¤±çš„æ„ä¹‰: ä¿¡æ¯è®ºè§£é‡Š
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* äº¤å‰ç†µæŸå¤±å¯ä»¥ä»ä¿¡æ¯è®ºè§’åº¦ç†è§£ï¼š
    * å®ƒè¡¡é‡äº†çœŸå®åˆ†å¸ƒ ğ‘¦ ä¸æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ :math:`\hat{ğ‘¦}` ä¹‹é—´çš„å·®å¼‚ã€‚
    * ç›´è§‚ç†è§£ï¼šæ¨¡å‹è¶Šå‡†ç¡®ï¼Œäº¤å‰ç†µæŸå¤±è¶Šå°ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒè¶Šæ¥è¿‘ã€‚

* ä¸¾ä¾‹è¯´æ˜ï¼š
    * å¦‚æœçœŸå®åˆ†å¸ƒæ˜¯ (0,0,1)ï¼Œæ¨¡å‹é¢„æµ‹ (0.1,0.2,0.7)ï¼ŒæŸå¤±è¾ƒå°ã€‚
    * å¦‚æœæ¨¡å‹é¢„æµ‹ä¸º (0.7,0.2,0.1)ï¼ŒæŸå¤±è¾ƒå¤§ï¼Œå› ä¸ºæ¨¡å‹è¾“å‡ºåç¦»çœŸå®ç±»åˆ«æ›´è¿œã€‚

5. å…³é”®ä¿¡æ¯æ€»ç»“
~~~~~~~~~~~~~~~

* äº¤å‰ç†µæŸå¤±çš„æ¨å¯¼æ¥è‡ªæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œé€šè¿‡ softmax å’Œå¯¹æ•°è¿ç®—å¾—å‡ºã€‚
* æ¢¯åº¦çš„å½¢å¼æ˜¯æ¨¡å‹é¢„æµ‹æ¦‚ç‡ä¸çœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®è·ï¼Œè¿™æ˜¯æ¨¡å‹å‚æ•°æ›´æ–°çš„æ ¸å¿ƒã€‚
* ä¿¡æ¯è®ºè§’åº¦è§£é‡Šï¼šäº¤å‰ç†µæŸå¤±è¡¡é‡æ¨¡å‹å¯¹çœŸå®æ ‡ç­¾çš„ç¼–ç æ•ˆç‡ï¼Œæ¨¡å‹è¶Šå‡†ç¡®ï¼Œç¼–ç ä»£ä»·è¶Šå°ã€‚
* æ³›åŒ–æ€§ï¼šäº¤å‰ç†µæŸå¤±ä¸ä»…é€‚ç”¨äº one-hot æ ‡ç­¾ï¼Œè¿˜å¯ä»¥å¤„ç†æ¦‚ç‡æ ‡ç­¾ï¼Œé€‚åº”æ›´å¤æ‚çš„ä»»åŠ¡ã€‚



4.1.3. Information Theory Basics
""""""""""""""""""""""""""""""""

* Information theory(ä¿¡æ¯è®º) deals with the problem of encoding, decoding, transmitting, and manipulating information (also known as data).

4.1.3.1. Entropy(ç†µ)
++++++++++++++++++++

* ä¿¡æ¯è®ºçš„æ ¸å¿ƒæ€æƒ³æ˜¯é‡åŒ–æ•°æ®ä¸­çš„ä¿¡æ¯å†…å®¹ã€‚
* åœ¨ä¿¡æ¯è®ºä¸­ï¼Œè¯¥æ•°å€¼è¢«ç§°ä¸ºåˆ†å¸ƒ P çš„ç†µï¼ˆentropyï¼‰ã€‚
* å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹ç¨‹å¾—åˆ°ï¼š

.. math::

    H[P] = \sum_j{-P(j)logP(j)}

* One of the fundamental theorems of information theory states that in order to encode data drawn randomly from the distribution P, we need at least  ``H[P]â€œnatsâ€`` to encode it (Shannon, 1948).
* If you wonder what a â€œnatâ€ is, it is the equivalent of bit but when using a code with base e rather than one with base 2. Thus, one nat is :math:`\frac{1}{log(2)} \approx 1.44` bit.

4.1.3.2. Surprisal(æƒŠè®¶åº¦)
++++++++++++++++++++++++++

1. å‹ç¼©ä¸é¢„æµ‹çš„è”ç³»
~~~~~~~~~~~~~~~~~~~

* å‹ç¼©(compression)ä¸é¢„æµ‹(prediction)çš„å…³ç³»
* æ ¸å¿ƒè§‚ç‚¹ï¼šå¦‚æœä¸€ä¸ªæ•°æ®æµå¾ˆå®¹æ˜“é¢„æµ‹ï¼Œé‚£ä¹ˆå®ƒä¹Ÿå¾ˆå®¹æ˜“å‹ç¼©ã€‚
* ä¾‹å­ï¼šä¸¾ä¸€ä¸ªæç«¯çš„ä¾‹å­ï¼Œæµä¸­çš„æ¯ä¸ªæ ‡è®°å§‹ç»ˆé‡‡ç”¨ç›¸åŒçš„å€¼ã€‚
* è§£é‡Šï¼š
    * å®¹æ˜“é¢„æµ‹ï¼šç”±äºæ•°æ®æœ‰å¾ˆå¼ºçš„è§„å¾‹æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å‡†ç¡®åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªç¬¦å·æ˜¯ä»€ä¹ˆã€‚
    * å®¹æ˜“å‹ç¼©ï¼šå‹ç¼©ç®—æ³•åªéœ€è®°å½•è¿™ä¸ªè§„å¾‹ï¼Œè€Œä¸ç”¨ä¼ è¾“å¤§é‡å†—ä½™æ•°æ®ã€‚
* ç»“è®ºï¼š
    * â€œæ˜“é¢„æµ‹â€ âŸ¹ â€œæ˜“å‹ç¼©â€
    * â€œéš¾é¢„æµ‹â€ âŸ¹ â€œéš¾å‹ç¼©â€

2. é¢„æµ‹å¤±è´¥ä¸â€œæƒŠè®¶åº¦â€
~~~~~~~~~~~~~~~~~~~~~


* è§£é‡Šï¼šå½“ä¸€ä¸ªä½æ¦‚ç‡äº‹ä»¶å‘ç”Ÿæ—¶ï¼Œæˆ‘ä»¬ä¼šæ„Ÿåˆ°â€œæƒŠè®¶â€ã€‚
* ç¤ºä¾‹ï¼šæ·éª°å­ï¼Œç»“æœæ˜¯ 7ã€‚è¿™ä¼šéå¸¸ä»¤äººæƒŠè®¶ï¼Œå› ä¸º ğ‘ƒ(7)=0
* é‡åŒ–æƒŠè®¶åº¦-å…‹åŠ³å¾·Â·é¦™å†œ (Claude Shannon) æå‡ºäº†ä¸€ä¸ªå…¬å¼æ¥è¡¡é‡è¿™ç§æƒŠè®¶ç¨‹åº¦(Surprisal)ï¼š

.. math::

    æƒŠè®¶åº¦(surprisal) = \log \frac{1}{P(j)} = -\log P(j)

* æ¦‚ç‡è¶Šå°ï¼ŒæƒŠè®¶åº¦è¶Šå¤§ã€‚
* å¦‚æœ ``ğ‘ƒ(ğ‘—)=1`` ï¼Œå³äº‹ä»¶å¿…ç„¶å‘ç”Ÿï¼ŒæƒŠè®¶åº¦ä¸º 0
* å¦‚æœ ``ğ‘ƒ(ğ‘—)=0.01`` ï¼ŒæƒŠè®¶åº¦è¾ƒå¤§ã€‚


3. ç†µ: æœŸæœ›çš„æƒŠè®¶åº¦
~~~~~~~~~~~~~~~~~~~

* å®šä¹‰ï¼šç†µ (Entropy) æ˜¯æ‰€æœ‰å¯èƒ½äº‹ä»¶çš„â€œå¹³å‡æƒŠè®¶åº¦â€ï¼š

.. math::

    H(p) = - \sum_j{P(j)logP(j)}


* è§£é‡Šï¼šç†µè¡¡é‡äº†ä¸€ä¸ªç³»ç»Ÿçš„â€œä¸ç¡®å®šæ€§â€ã€‚å¦‚æœç³»ç»Ÿçš„ç†µå¾ˆé«˜ï¼Œè¯´æ˜äº‹ä»¶åˆ†å¸ƒå¾ˆåˆ†æ•£ï¼Œéš¾ä»¥é¢„æµ‹ã€‚
* æç«¯ä¾‹å­ï¼š
    * å¦‚æœä¸€ä¸ªäº‹ä»¶æ€»æ˜¯å‘ç”Ÿ (æ¦‚ç‡ä¸º 1)ï¼Œç†µä¸º 0ï¼ˆå®Œå…¨å¯é¢„æµ‹ï¼‰ã€‚
    * å¦‚æœæ‰€æœ‰äº‹ä»¶æ¦‚ç‡å‡ç­‰ï¼Œç†µè¾¾åˆ°æœ€å¤§ï¼ˆæœ€ä¸ç¡®å®šï¼Œæœ€éš¾é¢„æµ‹ï¼‰ã€‚

4. äº¤å‰ç†µ: é¢„æµ‹ä¸çœŸå®åˆ†å¸ƒçš„å·®è·
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ä¸ºä»€ä¹ˆäº¤å‰ç†µæ˜¯æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µè¡¡é‡æ¨¡å‹é¢„æµ‹åˆ†å¸ƒ :math:`\hat{y}` å’ŒçœŸå®åˆ†å¸ƒ ğ‘¦ ä¹‹é—´çš„å·®å¼‚ï¼š
    * å¦‚æœæ¨¡å‹é¢„æµ‹ä¸çœŸå®åˆ†å¸ƒæ¥è¿‘ï¼Œäº¤å‰ç†µè¾ƒå°ã€‚
    * å¦‚æœæ¨¡å‹é¢„æµ‹è¿œç¦»çœŸå®åˆ†å¸ƒï¼Œäº¤å‰ç†µè¾ƒå¤§ã€‚

5. ç›´è§‚ä¾‹å­
~~~~~~~~~~~

* å‡è®¾
* çœŸå®æ¦‚ç‡åˆ†å¸ƒ y=(0,0,0,0,0,1) è¡¨ç¤ºåªä¼šæ·å‡º 6
* æ¨¡å‹é¢„æµ‹åˆ†å¸ƒä¸º :math:`\hat{y} =(0.1,0.1,0.1,0.1,0.1,0.5)`
* è®¡ç®—äº¤å‰ç†µæŸå¤±ï¼š

.. math::

    H(y, \hat{y}) = - \sum_{j=1}^6{y_j log{\hat{y}_j}} \\
        - log(0.5) = 0.693



4.1.3.3. Cross-Entropy Revisited
++++++++++++++++++++++++++++++++

* ç»“åˆä¸Šé¢çš„æƒŠè®¶åº¦(Surprisal)çš„ç†è§£
* å¯ä»¥æŠŠ **ç†µH(P)** çœ‹æˆ ``ä¸€ä¸ªçŸ¥é“çœŸå®æ¦‚ç‡çš„äººåœ¨ç»å†æ¦‚ç‡äº‹ä»¶æ—¶çš„æƒŠè®¶åº¦(Surprisal)``
    * ç›´è§‚ç†è§£ï¼š
    * å¦‚æœæˆ‘ä»¬å¯¹æ•°æ®çš„åˆ†å¸ƒéå¸¸äº†è§£ (å³ ğ‘ƒ æ˜¯æˆ‘ä»¬é¢„æµ‹çš„åˆ†å¸ƒ)ï¼Œé‚£ä¹ˆç†µå°±æ˜¯æˆ‘ä»¬å¯¹æœªæ¥äº‹ä»¶çš„â€œå¹³å‡æƒŠè®¶åº¦â€ã€‚
    * è¿™æ˜¯æœ€ä½³å‹ç¼©çš„ç†è®ºæé™ã€‚
* é‚£ä¹ˆ **äº¤å‰ç†µH(P, Q)** æè¿°çš„æ˜¯æˆ‘ä»¬ç”¨ä¸»è§‚æ¦‚ç‡åˆ†å¸ƒ ğ‘„ é¢„æµ‹çœŸå®åˆ†å¸ƒ ğ‘ƒ æ•°æ®æ—¶çš„å¹³å‡æƒŠè®¶åº¦ã€‚
    * ç›´è§‚è§£é‡Šï¼š
    * çœŸå®åˆ†å¸ƒ ğ‘ƒ è¡¨ç¤ºå®é™…å‘ç”Ÿçš„æƒ…å†µï¼Œè€Œæ¨¡å‹é¢„æµ‹ ğ‘„ ä»£è¡¨æˆ‘ä»¬å¯¹æ•°æ®çš„â€œä¸»è§‚ç†è§£â€ã€‚
    * å¦‚æœæ¨¡å‹ ğ‘„ åç¦»äº†çœŸå®åˆ†å¸ƒ ğ‘ƒï¼Œæˆ‘ä»¬åœ¨çœ‹åˆ°çœŸå®æ•°æ®æ—¶ä¼šæ„Ÿåˆ°æ›´â€œæƒŠè®¶â€ã€‚

* ã€ç¤ºä¾‹ã€‘
* çœŸå®åˆ†å¸ƒ ğ‘ƒï¼šéª°å­æ·å‡ºæ¯ä¸ªé¢çš„æ¦‚ç‡å‡ä¸º ğ‘ƒ(ğ‘—)=1/6
* æ¨¡å‹åˆ†å¸ƒ ğ‘„ (é¢„æµ‹åˆ†å¸ƒ)ï¼šæ¨¡å‹é”™è¯¯åœ°è®¤ä¸ºéª°å­æ·å‡º6çš„æ¦‚ç‡æ˜¯ ğ‘„(6)=0.5ï¼Œå…¶ä»–é¢çš„æ¦‚ç‡æ˜¯ ğ‘„(ğ‘—)=0.1ã€‚
* è®¡ç®—ç†µå’Œäº¤å‰ç†µï¼š
    * ç†µ: :math:`H(P)=-\sum_{j=1}^6{\frac{1}{6}log{\frac{1}{6}}} = log6`
    * äº¤å‰ç†µ:  :math:`H(P, Q) = -\sum_{j=1}^6{\frac{1}{6}log{Q(j)}} = -\frac{5}{6}log{0.1} -\frac{1}{6}log{0.5} = \frac{5}{6}log10 + \frac{1}{6}log2`

::

    # ç†µ(1.7918)
    torch.log(torch.tensor(6))

    # äº¤å‰ç†µ(2.0343)
    -5/6 * torch.log(torch.tensor(0.1)) -1/6 * torch.log(torch.tensor(0.5))
    5/6 * torch.log(torch.tensor(10)) + 1/6 * torch.log(torch.tensor(2))


.. note:: ã€è§£è¯»ã€‘å½“ ğ‘ƒ=ğ‘„(æ¨¡å‹é¢„æµ‹å‡†ç¡®) æ—¶ï¼Œäº¤å‰ç†µè¾¾åˆ°æœ€å°å€¼ï¼Œæ­¤æ—¶ï¼šğ»(ğ‘ƒ,ğ‘ƒ)=ğ»(ğ‘ƒ) â‡ï¸=ã€‹ä¹Ÿå°±æ˜¯è¯´ï¼šçœŸå®åˆ†å¸ƒå’Œé¢„æµ‹åˆ†å¸ƒä¸€è‡´æ—¶ï¼Œäº¤å‰ç†µç­‰äºç†µã€‚

* ä»¥ä»ä¸¤ä¸ªè§’åº¦ç†è§£äº¤å‰ç†µåˆ†ç±»ç›®æ ‡å‡½æ•°ï¼š
    * æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„æ•°æ®çš„ä¼¼ç„¶ (likelihood)ï¼›
    * æœ€å°åŒ–æ¨¡å‹å¯¹çœŸå®æ ‡ç­¾çš„æƒŠè®¶åº¦ï¼ˆå³å‡å°‘ç¼–ç æ ‡ç­¾æ‰€éœ€çš„æ¯”ç‰¹æ•°ï¼‰ã€‚


4.2. The Image Classification Dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ä¸»è¦è®²äº† torchvision çš„åŸºæœ¬ä½¿ç”¨


4.3. The Base Classification Model
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

4.3.1. The Classifier Class
"""""""""""""""""""""""""""

å®šä¹‰Classifierç±»

.. code-block:: python

    class Classifier(d2l.Module):  #@save
        """The base class of classification models."""
        def validation_step(self, batch):
            Y_hat = self(*batch[:-1])
            self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)
            self.plot('acc', self.accuracy(Y_hat, batch[-1]), train=False)

ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œåœ¨å°æ‰¹é‡ä¸Šè¿è¡Œ

.. code-block:: python

    @d2l.add_to_class(d2l.Module)  #@save
    def configure_optimizers(self):
        return torch.optim.SGD(self.parameters(), lr=self.lr)


4.3.2. Accuracy
"""""""""""""""

.. code-block:: python

    @d2l.add_to_class(Classifier)  #@save
    def accuracy(self, Y_hat, Y, averaged=True):
        """Compute the number of correct predictions."""
        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
        preds = Y_hat.argmax(axis=1).type(Y.dtype)
        compare = (preds == Y.reshape(-1)).type(torch.float32)
        return compare.mean() if averaged else compare



4.4. Softmax Regression Implementation from Scratch
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

4.4.1. The Softmax
""""""""""""""""""

è®¡ç®— softmax éœ€è¦ä¸‰ä¸ªæ­¥éª¤::

    i) æ¯ä¸€é¡¹æ±‚å¹‚
    ii) æ¯è¡Œæ±‚å’Œä»¥è®¡ç®—æ¯ä¸ªç¤ºä¾‹çš„å½’ä¸€åŒ–å¸¸æ•°
    iii) å°†æ¯ä¸€è¡Œé™¤ä»¥å…¶å½’ä¸€åŒ–å¸¸æ•°ï¼Œç¡®ä¿ç»“æœæ€»å’Œä¸º 1

4.4.2. The Model
""""""""""""""""

.. code-block:: python

    class SoftmaxRegressionScratch(d2l.Classifier):
        def __init__(self, num_inputs, num_outputs, lr, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.W = torch.normal(0, sigma, size=(num_inputs, num_outputs),
                                  requires_grad=True)
            self.b = torch.zeros(num_outputs, requires_grad=True)

        def parameters(self):
            return [self.W, self.b]

ç½‘ç»œå¦‚ä½•å°†æ¯ä¸ªè¾“å…¥æ˜ å°„åˆ°è¾“å‡º:

.. code-block:: python

    @d2l.add_to_class(SoftmaxRegressionScratch)
    def forward(self, X):
        X = X.reshape((-1, self.W.shape[0]))
        return softmax(torch.matmul(X, self.W) + self.b)


4.4.3. The Cross-Entropy Loss
"""""""""""""""""""""""""""""

* åˆ›å»ºäº†ç¤ºä¾‹æ•°æ®y_hatå…¶ä¸­åŒ…å« 2 ä¸ªç¤ºä¾‹ é¢„æµ‹ 3 ä¸ªç±»åˆ«çš„æ¦‚ç‡åŠå…¶ç›¸åº”çš„æ ‡ç­¾ y

.. code-block:: python

    y = torch.tensor([0, 2])
    y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
    y_hat[[0, 1], y]
    # tensor([0.1000, 0.5000])


å¯¹æ‰€é€‰æ¦‚ç‡çš„å¯¹æ•°è¿›è¡Œå¹³å‡æ¥å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼š

.. code-block:: python

    def cross_entropy(y_hat, y):
        y2 = y_hat[list(range(len(y_hat))), y]   # tensor([0.1000, 0.5000])
        return -torch.log(y2).mean()
    cross_entropy(y_hat, y)
    # tensor(1.4979)

* å®šä¹‰æŸå¤±å‡½æ•°::

    @d2l.add_to_class(SoftmaxRegressionScratch)
    def loss(self, y_hat, y):
        return cross_entropy(y_hat, y)

4.4.4. Training
"""""""""""""""

.. code-block:: python

    data = d2l.FashionMNIST(batch_size=256)
    model = SoftmaxRegressionScratch(num_inputs=784, num_outputs=10, lr=0.1)
    trainer = d2l.Trainer(max_epochs=10)
    trainer.fit(model, data)

4.4.5. Prediction
"""""""""""""""""

.. code-block:: python

    X, y = next(iter(data.val_dataloader()))
    preds = model(X).argmax(axis=1)
    preds.shape
    # torch.Size([256])

æˆ‘ä»¬å¯¹é”™è¯¯æ ‡è®°çš„å›¾åƒæ›´æ„Ÿå…´è¶£ã€‚é€šè¿‡å°†å®ƒä»¬çš„å®é™…æ ‡ç­¾ï¼ˆæ–‡æœ¬è¾“å‡ºçš„ç¬¬ä¸€è¡Œï¼‰ä¸æ¨¡å‹çš„é¢„æµ‹ï¼ˆæ–‡æœ¬è¾“å‡ºçš„ç¬¬äºŒè¡Œï¼‰è¿›è¡Œæ¯”è¾ƒæ¥å¯è§†åŒ–å®ƒä»¬::

    wrong = preds.type(y.dtype) != y
    X, y, preds = X[wrong], y[wrong], preds[wrong]
    labels = [a+'\n'+b for a, b in zip(
        data.text_labels(y), data.text_labels(preds))]
    data.visualize([X, y], labels=labels)

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/o23wtV.png



4.5. Concise Implementation of Softmax Regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

4.5.1. Defining the Model
"""""""""""""""""""""""""

* å†…ç½®çš„__call__æ–¹æ³•å°±ä¼šè°ƒç”¨forward

.. code-block:: python

    class SoftmaxRegression(d2l.Classifier):  #@save
        """The softmax regression model."""
        def __init__(self, num_outputs, lr):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(nn.Flatten(),
                                     nn.LazyLinear(num_outputs))
        def forward(self, X):
            return self.net(X)

4.5.2. Softmax Revisited
""""""""""""""""""""""""

* åŸå§‹ softmax è®¡ç®—æ–¹å¼åœ¨å®é™…å®ç°ä¸­å¯èƒ½å­˜åœ¨æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
    * ä¸Šæº¢ (Overflow)ï¼šå¦‚æœ :math:`o_k` éå¸¸å¤§ï¼Œ:math:`\exp(o_k)` å¯èƒ½è¶…å‡ºè®¡ç®—æœºèƒ½è¡¨ç¤ºçš„æœ€å¤§å€¼ï¼Œå¯¼è‡´æº¢å‡ºã€‚
    * ä¸‹æº¢ (Underflow)ï¼šå¦‚æœæ‰€æœ‰ :math:`o_k` éƒ½éå¸¸å° (è´Ÿæ•°å¾ˆå¤§)ï¼Œ :math:`\exp(o_k)` ä¼šè¶‹è¿‘äº 0ï¼Œå¯èƒ½å¯¼è‡´ä¸‹æº¢ã€‚
* æ•°å€¼ä¸ç¨³å®šæ€§çš„å…·ä½“åŸå› 
    *  è®¡ç®—æœºè¡¨ç¤ºæµ®ç‚¹æ•°çš„èŒƒå›´æœ‰é™ã€‚ä¾‹å¦‚ï¼Œå•ç²¾åº¦æµ®ç‚¹æ•°çš„è¡¨ç¤ºèŒƒå›´çº¦ä¸º :math:`10^{-38}` åˆ° :math:`10^{38}`
    *  å¦‚æœæœ€å¤§çš„ :math:`o_k` è¶…å‡ºåŒºé—´ ``[-90, 90]``ï¼Œç»“æœå°±ä¼šå˜å¾—ä¸ç¨³å®šã€‚
* è§£å†³æ–¹æ¡ˆ
    * æ ¸å¿ƒæ€æƒ³ï¼šä¸ºäº†é¿å…æº¢å‡ºæˆ–ä¸‹æº¢ï¼Œå¯ä»¥é€šè¿‡å¹³ç§» logitsï¼Œä½¿å¾—æœ€å¤§çš„ logits å˜ä¸º 0ï¼Œä»è€Œè®©æ‰€æœ‰ logits éƒ½ä½äºä¸€ä¸ªè¾ƒå°çš„èŒƒå›´å†…ã€‚
    * å…·ä½“æ–¹æ³•ï¼š
        * è®¾ :math:`\bar{o}=\max_k{o_k}`
        * ä»æ‰€æœ‰ logits ä¸­å‡å» :math:`\bar{o}` ï¼š
        * :math:`\hat{y}_{j}=\frac{\exp \left(o_{j}-\bar{o}\right)}{\sum_{k} \exp \left(o_{k}-\bar{o}\right)}`
    * åˆ†æ
        * é˜²æ­¢ä¸Šæº¢ï¼šå› ä¸º :math:`\exp(0) = 1` ï¼Œè€Œ :math:`\exp(è´Ÿæ•°)` çš„å€¼å§‹ç»ˆä»‹äº ``(0, 1]``
        * é˜²æ­¢ä¸‹æº¢ï¼šå¦‚æœ :math:`o_j - \bar{o}` éå¸¸å°ï¼Œ :math:`\exp(o_j - \bar{o})` å¯èƒ½è¶‹è¿‘ 0ï¼Œä½†ä¸ä¼šæº¢å‡ºï¼Œæœ€å¤šå¯¼è‡´ :math:`\hat y_j = 0`
        * ä½†æˆ‘ä»¬å¯ä»¥åˆ©ç”¨ softmax å’Œäº¤å‰ç†µçš„ç»„åˆï¼Œé¿å…ç›´æ¥è®¡ç®— :math:`\hat{y}_j` ï¼Œè€Œæ˜¯è®¡ç®—ï¼š :math:`log{\hat{y}_i} = o_j - \bar{o} - log{\sum_k{\exp(o_k - \bar{o})}}`

* æ¨ç†è¿‡ç¨‹

.. math::

    \begin{array}{l}
    \hat{y}_{j}=\frac{\exp \left(o_{j}-\bar{o}\right)}{\sum_{k} \exp \left(o_{k}-\bar{o}\right)} \\
    \text{ä¸¤è¾¹æ±‚å¯¹æ•°=>}  \\
    \log \hat{y}_i = o_j - \bar{o} - log{\sum_k{\exp (o_k - \bar{o})}}
    \end{array}

.. code-block:: python

    @d2l.add_to_class(d2l.Classifier)  #@save
    def loss(self, Y_hat, Y, averaged=True):
        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))
        Y = Y.reshape((-1,))
        return F.cross_entropy(
            Y_hat, Y, reduction='mean' if averaged else 'none')

4.5.3. Training
"""""""""""""""

.. code-block:: python

    data = d2l.FashionMNIST(batch_size=256)
    model = SoftmaxRegression(num_outputs=10, lr=0.1)
    trainer = d2l.Trainer(max_epochs=10)
    trainer.fit(model, data)

4.6. Generalization in Classification
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

4.6.1. The Test Set
"""""""""""""""""""

1. ç»éªŒè¯¯å·® (Empirical Error)
+++++++++++++++++++++++++++++

å…¬å¼è§£æï¼š

.. math::

    \epsilon_\mathcal{D}(f) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)})

* å…¶ä¸­
* :math:`\epsilon_\mathcal{D}(f)` : åœ¨æµ‹è¯•é›† :math:`\mathcal{D}` ä¸Šçš„åˆ†ç±»è¯¯å·®
* :math:`\mathbf{1}(\cdot)` æ˜¯æŒ‡ç¤ºå‡½æ•° (indicator function)ï¼Œ :math:`\mathbf{1}(\text{condition})` çš„å€¼åªæœ‰ä¸¤ä¸ªå¯èƒ½ï¼šå¦‚æœæ¡ä»¶ä¸ºçœŸï¼Œ :math:`\mathbf{1}(\text{condition}) = 1` ;å¦‚æœæ¡ä»¶ä¸º å‡ï¼Œ :math:`\mathbf{1}(\text{condition}) = 0`
* :math:`\mathbf{1}(f(\mathbf{x}^{(i)}) \neq y^{(i)})` ï¼šå¦‚æœé¢„æµ‹ :math:`f(\mathbf{x}^{(i)})` ä¸çœŸå®æ ‡ç­¾ :math:`y^{(i)}` ä¸ä¸€è‡´ï¼Œåˆ™è¾“å‡º **1**ï¼Œå¦åˆ™è¾“å‡º **0**
* å«ä¹‰ï¼šæ¨¡å‹åœ¨æµ‹è¯•é›† :math:`\mathcal{D}` ä¸Šé¢„æµ‹é”™è¯¯çš„æ¯”ä¾‹ï¼Œå³æ¨¡å‹åœ¨å®é™…æ•°æ®ä¸Šçš„è¡¨ç°ã€‚


* æŒ‡ç¤ºå‡½æ•°çš„ç›´è§‚ç†è§£ï¼š :math:`\mathbf{1}` ç›¸å½“äºä¸€ä¸ªå¼€å…³ï¼Œç”¨æ¥åˆ¤æ–­æ˜¯å¦æ»¡è¶³æŸä¸ªæ¡ä»¶ï¼šæ»¡è¶³æ¡ä»¶å°±â€œæ‰“å¼€â€ (1)ï¼Œä¸æ»¡è¶³æ¡ä»¶å°±â€œå…³é—­â€ (0)ã€‚åœ¨è¯¯å·®è®¡ç®—ä¸­ï¼Œå®ƒå¸®åŠ©ç»Ÿè®¡æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„é”™è¯¯æ ·æœ¬æ•°é‡ã€‚



2. æ€»ä½“è¯¯å·® (Population Error)
++++++++++++++++++++++++++++++

å…¬å¼è§£æï¼š

.. math::

    \epsilon(f) =  E_{(\mathbf{x}, y) \sim P} \mathbf{1}(f(\mathbf{x}) \neq y) \\
    = \int\int \mathbf{1}(f(\mathbf{x}) \neq y) p(\mathbf{x}, y) \;d\mathbf{x} dy

* å…¶ä¸­
* :math:`\epsilon(f)` ï¼šæ¨¡å‹åœ¨çœŸå®æ•°æ®åˆ†å¸ƒä¸‹çš„æœŸæœ›è¯¯å·®ï¼Œæ˜¯ç†æƒ³çŠ¶æ€ä¸‹æ¨¡å‹çœŸæ­£çš„è¯¯å·®ã€‚
* :math:`p(\mathbf{x}, y)` ï¼šæ•°æ®åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚
* å«ä¹‰ï¼šæ¨¡å‹åœ¨æ•´ä¸ªæ½œåœ¨æ•°æ®åˆ†å¸ƒä¸­åˆ†ç±»é”™è¯¯çš„æœŸæœ›æ¦‚ç‡ã€‚
* é—®é¢˜ï¼šæ— æ³•ç›´æ¥è®¡ç®—ï¼Œå› ä¸ºçœŸå®åˆ†å¸ƒ :math:`p(\mathbf{x}, y)` é€šå¸¸æœªçŸ¥ã€‚


* ç”±äºæµ‹è¯•é›† :math:`\mathcal{D}` åœ¨ç»Ÿè®¡ä¸Šä»£è¡¨äº†æ½œåœ¨æ€»ä½“ï¼Œæˆ‘ä»¬å¯ä»¥å°† :math:`\epsilon_\mathcal{D}(f)` è§†ä¸ºæ€»ä½“è¯¯å·® :math:`\epsilon(f)` çš„ç»Ÿè®¡ä¼°è®¡é‡ã€‚
* æ­¤å¤–ï¼Œç”±äºæˆ‘ä»¬æ„Ÿå…´è¶£çš„é‡ :math:`\epsilon(f)` æ˜¯éšæœºå˜é‡ :math:`\mathbf{1}(f(X) \neq Y)` çš„æœŸæœ›å€¼ï¼Œå¯¹åº”çš„ä¼°è®¡é‡ :math:`\epsilon_\mathcal{D}(f)` æ˜¯æ ·æœ¬å‡å€¼ï¼Œå› æ­¤ä¼°è®¡æ€»ä½“è¯¯å·®å…¶å®æ˜¯ä¸€ä¸ªç»å…¸çš„å‡å€¼ä¼°è®¡é—®é¢˜ã€‚




3. ä¸­å¿ƒæé™å®šç† (CLT) å’Œè¯¯å·®æ”¶æ•›é€Ÿåº¦
++++++++++++++++++++++++++++++++++++

* æˆ‘ä»¬å…³å¿ƒçš„éšæœºå˜é‡ :math:`\mathbf{1}(f(X) \neq Y)` åªèƒ½å– 0 å’Œ 1 ä¸¤ä¸ªå€¼ï¼Œå› æ­¤æ˜¯ä¸€ä¸ªä¼¯åŠªåˆ©éšæœºå˜é‡ï¼Œå…¶å‚æ•°è¡¨ç¤ºè¯¥å˜é‡å–å€¼ä¸º 1 çš„æ¦‚ç‡ã€‚
* Bernoulli åˆ†å¸ƒçš„éšæœºå˜é‡å•ä¸ªæ ·æœ¬è¯¯å·®çš„æ–¹å·®æ˜¯ï¼š :math:`\sigma^2=\epsilon(f)(1-\epsilon(f))`
* è™½ç„¶ :math:`\epsilon(f)` æœ€åˆæ˜¯æœªçŸ¥çš„ï¼Œä½†æˆ‘ä»¬çŸ¥é“å®ƒä¸ä¼šå¤§äº 1ã€‚è¿›ä¸€æ­¥åˆ†æè¿™ä¸ªå‡½æ•°ä¼šå‘ç°ï¼Œå½“ :math:`\epsilon(f) \approx 0.5` æ—¶æ–¹å·®æœ€å¤§ï¼Œè€Œå½“ :math:`\epsilon(f)` æ¥è¿‘ 0 æˆ– 1 æ—¶æ–¹å·®è¾ƒå°ã€‚è¿™è¡¨æ˜ï¼Œä¼°è®¡é‡ :math:`\epsilon_\mathcal{D}(f)` çš„æ¸è¿‘æ ‡å‡†å·®ä¸ä¼šè¶…è¿‡ï¼š :math:`\sqrt{\frac{0.25}{N}}`


* ä¸­å¿ƒæé™å®šç†è¡¨æ˜ï¼Œå½“æ ·æœ¬é‡ :math:`n \to \infty` æ—¶ï¼Œæµ‹è¯•è¯¯å·® :math:`\epsilon_\mathcal{D}(f)` å°†ä»¥é€Ÿç‡ :math:`\mathcal{O}(1/\sqrt{n})` æ”¶æ•›åˆ°çœŸå®è¯¯å·® :math:`\epsilon(f)`
* ç›´è§‚å«ä¹‰ï¼š
    * æƒ³æŠŠæµ‹è¯•è¯¯å·®å‡å°‘ä¸€åŠï¼Œéœ€è¦ 4 å€çš„æ ·æœ¬é‡ã€‚
    * å¦‚æœè¦å°†è¯¯å·®å‡å°‘ 100 å€ï¼Œéœ€è¦ 10,000 å€çš„æ ·æœ¬é‡ã€‚
    * ä¾‹å¦‚ï¼Œå¦‚æœå¸Œæœ›è¯¯å·®çš„ä¼°è®¡ç²¾ç¡®åˆ° ``Â±0.01`` ï¼Œå¤§çº¦éœ€è¦ 2,500 ä¸ªæ ·æœ¬ã€‚
        * Bernoulli åˆ†å¸ƒçš„éšæœºå˜é‡å•ä¸ªæ ·æœ¬è¯¯å·®çš„æ–¹å·®æ˜¯ï¼š :math:`\sigma^2=\epsilon(f)(1-\epsilon(f))`
        * æ–¹å·® :math:`\sigma^2` åœ¨ :math:`\epsilon(f) = 0.5` æ—¶è¾¾åˆ°æœ€å¤§å€¼ 0.25
        * å¦‚æœå¸Œæœ›è¯¯å·®æ³¢åŠ¨åœ¨ $Â±0.01$ ä»¥å†…ï¼Œåˆ™éœ€è¦æ»¡è¶³
        * :math:`\sqrt{\frac{0.25}{n}}=0.01`
        * è§£å¾— n=2500

* é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™ç§ :math:`\mathcal{O}(1/\sqrt{n})` çš„é€Ÿç‡æ˜¯ç»Ÿè®¡å­¦ä¸­æˆ‘ä»¬èƒ½æœŸæœ›çš„æœ€ä¼˜é€Ÿç‡ã€‚
* ä¸­å¿ƒæé™å®šç†çš„æ ¸å¿ƒæ€æƒ³ï¼šä¸­å¿ƒæé™å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼Œæ— è®ºæ€»ä½“åˆ†å¸ƒå¦‚ä½•ï¼Œå¦‚æœä»æ€»ä½“ä¸­æŠ½å–å¤§é‡ç‹¬ç«‹åŒåˆ†å¸ƒçš„éšæœºæ ·æœ¬ï¼Œå¹¶è®¡ç®—æ ·æœ¬å‡å€¼ï¼Œè¿™ä¸ªæ ·æœ¬å‡å€¼çš„åˆ†å¸ƒå°†è¿‘ä¼¼æœä»æ­£æ€åˆ†å¸ƒ (Normal Distribution)ï¼Œåªè¦æ ·æœ¬æ•°é‡è¶³å¤Ÿå¤§ã€‚



4. Hoeffding ä¸ç­‰å¼å’Œæœ‰é™æ ·æœ¬è¯¯å·®ç•Œ
+++++++++++++++++++++++++++++++++++

* å‰é¢çš„åˆ†æä¸»è¦é’ˆå¯¹æ¸è¿‘æƒ…å†µï¼Œå³éšç€æ ·æœ¬æ•°é‡è¶‹è¿‘æ— ç©·æ—¶çš„è¡Œä¸ºã€‚
* ç„¶è€Œï¼Œå¹¸è¿çš„æ˜¯ï¼Œç”±äºæˆ‘ä»¬çš„éšæœºå˜é‡æ˜¯æœ‰ç•Œçš„ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ Hoeffding (1963) æå‡ºçš„ä¸€ä¸ªä¸ç­‰å¼å¾—åˆ°æœ‰é™æ ·æœ¬ä¸‹çš„æœ‰æ•ˆç•Œé™ï¼š

.. math::

    P(\epsilon_\mathcal{D}(f) - \epsilon(f) \geq t) < \exp\left( - 2n t^2 \right)

* ä¸ºç¡®ä¿åœ¨ 95% ç½®ä¿¡æ°´å¹³ä¸‹ï¼Œ :math:`\epsilon_\mathcal{D}(f)` ä¸ :math:`\epsilon(f)` ä¹‹é—´çš„è·ç¦»ä¸è¶…è¿‡ 0.01ï¼Œæˆ‘ä»¬éœ€è¦çš„æœ€å°æ ·æœ¬é‡å¤§çº¦ä¸º 15,000ï¼Œç•¥å¤šäºæ¸è¿‘åˆ†æå¾—å‡ºçš„ 10,000ã€‚
* è¿™ç§è¶‹åŠ¿åœ¨ç»Ÿè®¡å­¦ä¸­æ™®éå­˜åœ¨ã€‚é€‚ç”¨äºæœ‰é™æ ·æœ¬çš„ä¿è¯é€šå¸¸æ¯”æ¸è¿‘åˆ†ææ›´ä¿å®ˆä¸€äº›ã€‚ç„¶è€Œï¼Œè¿™ä¸¤è€…ç»™å‡ºçš„æ•°å€¼å·®è·ä¸å¤§ï¼Œåæ˜ å‡ºæ¸è¿‘åˆ†æåœ¨å®é™…åº”ç”¨ä¸­ä»ç„¶å…·æœ‰ç›¸å½“çš„å‚è€ƒä»·å€¼ï¼Œå³ä½¿å®ƒä»¬æ— æ³•æä¾›å®Œå…¨çš„ä¿è¯ã€‚


* è§£é‡Šï¼šè¿™æ¡ä¸ç­‰å¼æä¾›äº†åœ¨æœ‰é™æ ·æœ¬ä¸‹çš„è¯¯å·®ä¼°è®¡ã€‚
* t è¡¨ç¤ºå…è®¸è¯¯å·®çš„å®¹å¿èŒƒå›´ã€‚
* å½“ ``t = 0.01`` ï¼ˆå³å¸Œæœ›è¯¯å·®åœ¨ ``Â±0.01`` èŒƒå›´å†…ï¼‰æ—¶ï¼Œéœ€è¦çº¦ 15,000 ä¸ªæ ·æœ¬ï¼Œæ¯”ä¸­å¿ƒæé™å®šç†ä¼°è®¡çš„ 10,000 æ ·æœ¬ç•¥å¤šã€‚
* ç»“è®ºï¼šæœ‰é™æ ·æœ¬ä¸‹çš„è¯¯å·®ä¼°è®¡æ¯”æ— ç©·æ ·æœ¬ä¸‹ç•¥ä¿å®ˆï¼Œä½†ä¸¤è€…å·®è·ä¸å¤§ï¼Œè¡¨æ˜ä¸­å¿ƒæé™å®šç†æä¾›äº†å¾ˆå¥½çš„ä¼°è®¡ã€‚


4.6.2. Test Set Reuse
"""""""""""""""""""""

* åˆ†æï¼šæµ‹è¯•é›†é‡ç”¨é—®é¢˜ä¸é£é™©

æ ¸å¿ƒè§‚ç‚¹
+++++++++++

* æµ‹è¯•é›†æ˜¯æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°çš„åŸºå‡†ï¼Œä½†é‡ç”¨æµ‹è¯•é›†å¯èƒ½å¸¦æ¥ä¸¥é‡çš„é—®é¢˜ï¼Œä¸»è¦æ¶‰åŠåˆ°ï¼š
    * å‡å‘ç°ç‡ï¼ˆFalse Discovery Rateï¼‰é—®é¢˜
    * è‡ªé€‚åº”è¿‡æ‹Ÿåˆï¼ˆAdaptive Overfittingï¼‰é£é™©

1) å‡å‘ç°ç‡é—®é¢˜
+++++++++++++++

* èƒŒæ™¯ï¼šåœ¨è¯„ä¼°æ¨¡å‹ :math:`f_1` ä¹‹åï¼Œç”¨æˆ·å¯èƒ½ç»§ç»­å¼€å‘æ–°æ¨¡å‹ :math:`f_2, f_3, ..., f_k` ï¼Œå¹¶åœ¨ç›¸åŒçš„æµ‹è¯•é›†ä¸Šè¯„ä¼°å®ƒä»¬çš„æ€§èƒ½ã€‚
* é£é™©ï¼š
    * æ¯æ¬¡æ¨¡å‹è¯„ä¼°éƒ½å­˜åœ¨ 5% çš„è¯¯å¯¼é£é™©ï¼ˆç½®ä¿¡æ°´å¹³95%ï¼‰ã€‚
    * å¦‚æœåœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šè¯„ä¼° k ä¸ªæ¨¡å‹ï¼Œå³ä½¿æ¯ä¸ªæ¨¡å‹ç‹¬ç«‹åœ°æœ‰95%ç½®ä¿¡åº¦ï¼Œæ•´ä½“å‡ºç°è‡³å°‘ä¸€ä¸ªè¯¯å¯¼ç»“æœçš„æ¦‚ç‡å¤§å¤§å¢åŠ ã€‚
    * ä¸¾ä¾‹ï¼šå½“ :math:`k = 20` æ—¶ï¼Œè‡³å°‘ä¸€ä¸ªæ¨¡å‹è¯¯å¯¼çš„æ¦‚ç‡ :math:`= 1 - (0.95)^{20} \approx 64%`ã€‚
    * å½±å“ï¼šé”™è¯¯çš„æ¨¡å‹å¯èƒ½è¢«è¯¯é€‰ä¸ºæœ€ä½³æ¨¡å‹ï¼Œå¯¼è‡´å®é™…æ€§èƒ½ä¸ä½³ã€‚

2) è‡ªé€‚åº”è¿‡æ‹Ÿåˆ
+++++++++++++++

* èƒŒæ™¯ï¼šå¦‚æœæ¨¡å‹ :math:`f_2` æ˜¯åœ¨è§‚å¯Ÿ :math:`f_1` çš„æµ‹è¯•é›†ç»“æœåè®¾è®¡çš„ï¼Œé‚£ä¹ˆ :math:`f_2` çš„æ€§èƒ½å·²å—åˆ°æµ‹è¯•é›†ä¿¡æ¯çš„å½±å“ã€‚
* é£é™©ï¼š
    * æµ‹è¯•é›†åœ¨è¯„ä¼° :math:`f_2` æ—¶å·²ä¸å†æ˜¯çœŸæ­£çš„â€œæœªçŸ¥æ•°æ®â€ï¼Œä½¿å¾—æ¨¡å‹è¯„ä¼°çš„ç»“æœåä¹è§‚ã€‚
    * è¿™ç ´åäº†æœºå™¨å­¦ä¹ æ¨¡å‹è¯„ä¼°çš„æ ¸å¿ƒåŸåˆ™ï¼Œå³æ¨¡å‹ä¸èƒ½â€œè§è¿‡â€æµ‹è¯•é›†æ•°æ®ã€‚
    * ä¾‹å­ï¼šåœ¨ Kaggle æ¯”èµ›ä¸­ï¼Œå¦‚æœå¤šæ¬¡åœ¨ç§æœ‰æµ‹è¯•é›†ä¸Šæäº¤æ¨¡å‹å¹¶è°ƒæ•´å‚æ•°ï¼Œæœ€ç»ˆçš„æ¨¡å‹å¯èƒ½åªæ˜¯åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œè€Œåœ¨çœŸå®åœºæ™¯ä¸­è¡¨ç°è¾ƒå·®ã€‚
* æœ¬è´¨ï¼šæ¨¡å‹ä¸æ–­æ ¹æ®æµ‹è¯•é›†åé¦ˆä¼˜åŒ–ï¼Œæµ‹è¯•é›†é€æ¸é€€åŒ–ä¸ºè®­ç»ƒé›†çš„å»¶ä¼¸ï¼Œæ— æ³•æœ‰æ•ˆåæ˜ æ¨¡å‹çš„çœŸå®æ³›åŒ–èƒ½åŠ›ã€‚

ç¼“è§£ç­–ç•¥ä¸å®è·µå»ºè®®
++++++++++++++++++

1) é¿å…é‡å¤ä½¿ç”¨åŒä¸€æµ‹è¯•é›†ã€‚ç­–ç•¥ï¼šæ„å»ºå¤šä¸ªç‹¬ç«‹çš„æµ‹è¯•é›†ï¼Œæ¯è½®è¯„ä¼°åå°†æ—§æµ‹è¯•é›†é™çº§ä¸ºéªŒè¯é›†ï¼Œé¿å…åå¤ä½¿ç”¨åŒä¸€æ‰¹æ•°æ®ã€‚
2) è€ƒè™‘å¤šé‡å‡è®¾æ£€éªŒã€‚æ–¹æ³•ï¼šåœ¨è¯„ä¼°å¤šä¸ªæ¨¡å‹æ—¶ï¼Œé‡‡ç”¨ **Bonferroniæ ¡æ­£** ç­‰æ–¹æ³•é™ä½å‡å‘ç°ç‡ã€‚ä¾‹å¦‚ï¼Œå¯¹äº k ä¸ªæ¨¡å‹è¯„ä¼°ï¼Œå°†ç½®ä¿¡æ°´å¹³è°ƒæ•´ä¸º :math:`1 - \frac{0.05}{k}` ï¼Œç¡®ä¿æ•´ä½“è¯¯å¯¼æ¦‚ç‡ç»´æŒåœ¨ 5% å·¦å³ã€‚
3) é™åˆ¶å¯¹æµ‹è¯•é›†çš„è®¿é—®é¢‘ç‡ã€‚å®è·µï¼šè®¾ç½®æ˜ç¡®çš„æµ‹è¯•é›†è®¿é—®æ¬¡æ•°ä¸Šé™ï¼ˆå¦‚æœ€å¤š3æ¬¡ï¼‰ï¼Œä¸¥æ ¼è®°å½•æ¯æ¬¡è®¿é—®ç›®çš„ã€‚åœ¨é‡å¤§æ¨¡å‹è¯„ä¼°å‰ï¼Œå°½é‡å‡å°‘å¯¹æµ‹è¯•é›†çš„æ¥è§¦ï¼Œä»…åœ¨æœ€ç»ˆæ¨¡å‹å‡†å¤‡å‘å¸ƒå‰ä½¿ç”¨æµ‹è¯•é›†ã€‚
4) åŠ å¤§æ•°æ®é›†è§„æ¨¡ã€‚ç†ç”±ï¼šå¤§æ•°æ®é›†æ›´èƒ½æŠµæŠ—è¿‡æ‹Ÿåˆé£é™©ï¼Œå³ä½¿æœ‰ä¸€å®šç¨‹åº¦çš„ä¿¡æ¯æ³„éœ²ï¼Œå¤§è§„æ¨¡æ•°æ®ä»èƒ½æä¾›å¯é è¯„ä¼°ã€‚



4.6.3. Statistical Learning Theory
""""""""""""""""""""""""""""""""""

æ ¸å¿ƒè§‚ç‚¹
++++++++

* æ¨¡å‹æ³›åŒ–çš„æ ¹æœ¬éš¾é¢˜åœ¨äºå¦‚ä½•ä¿è¯è®­ç»ƒè¯¯å·®æ¥è¿‘çœŸå®è¯¯å·®ã€‚
* è§£å†³è·¯å¾„ï¼šé€šè¿‡æ•°å­¦å·¥å…·ï¼ˆå¦‚VCç»´åº¦ï¼‰å»ºç«‹æ³›åŒ–è¯¯å·®çš„ä¸Šç•Œï¼Œé‡åŒ–æ¨¡å‹å¤æ‚æ€§ä¸æ•°æ®æ ·æœ¬æ•°é‡ä¹‹é—´çš„å…³ç³»ã€‚
* ç›®æ ‡ï¼šå®ç°ä¸€è‡´æ”¶æ•›æ€§ï¼Œç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¯¯å·®å·®è·åœ¨å¯æ§èŒƒå›´å†…ã€‚

ä¸»è¦é—®é¢˜æ‹†è§£
++++++++++++

* 1) æ³›åŒ–é—®é¢˜çš„æœ¬è´¨ï¼šâ€œæµ‹è¯•é›†æ˜¯æˆ‘ä»¬å”¯ä¸€çš„å‚è€ƒâ€ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½è¯„ä¼°ä¾èµ–äºæµ‹è¯•é›†ï¼Œä½†æµ‹è¯•é›†çš„ç»“æœä»…èƒ½åæ˜ äº‹åæ³›åŒ–èƒ½åŠ›ï¼Œæ— æ³•æä¾›äº‹å‰æ³›åŒ–ä¿è¯ã€‚å›°éš¾ç‚¹ï¼šå³ä½¿ä¸€ä¸ªæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä¹Ÿæ— æ³•ä¿è¯ä¸‹ä¸€ä¸ªæ¨¡å‹ï¼ˆf_2, f_3, ...ï¼‰èƒ½æŒç»­æ³›åŒ–ã€‚
* 2) æ³›åŒ–è¯¯å·®ä¸æ ·æœ¬è¯¯å·®çš„å·®è·ï¼šæ ¸å¿ƒé—®é¢˜ï¼šç»éªŒè¯¯å·® :math:`\epsilon_\mathcal{S}` æ¥è¿‘çœŸå®è¯¯å·® :math:`\epsilon` å—ï¼Ÿå¦‚æœæ¨¡å‹ä»…åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°ä¼˜ç§€ï¼Œä½†åœ¨æµ‹è¯•é›†æˆ–çœŸå®æ•°æ®ä¸Šæ•ˆæœä¸ä½³ï¼Œå°±å‘ç”Ÿäº†è¿‡æ‹Ÿåˆã€‚
* 3) æ¨¡å‹ç±» :math:`\mathcal{F}` çš„å¤æ‚æ€§ï¼šæŒ‘æˆ˜ï¼šå¦‚ä½•åœ¨å¤æ‚æ¨¡å‹ç±»ä¸­æŒ‘é€‰æ—¢èƒ½æ‹Ÿåˆè®­ç»ƒé›†åˆèƒ½æ³›åŒ–çš„æ¨¡å‹ï¼Ÿçº¿æ€§åˆ†ç±»å™¨é€šå¸¸æ³›åŒ–è‰¯å¥½ï¼Œä½†å¤æ‚çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆå‡½æ•°é›†åˆéå¸¸å¤§ï¼Œ$|\mathcal{F}| = \infty$ï¼‰æ›´å®¹æ˜“è¿‡æ‹Ÿåˆã€‚

è§£å†³æ€è·¯:ä¸€è‡´æ”¶æ•›æ€§ä¸VCç»´åº¦
+++++++++++++++++++++++++++

1) ä¸€è‡´æ”¶æ•›æ€§(Uniform Convergence)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* ç›®æ ‡ï¼šç¡®ä¿æ‰€æœ‰æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒçœŸå®åˆ†å¸ƒä¸Šçš„è¯¯å·®æ”¶æ•›åˆ°åŒä¸€ä¸ªå°èŒƒå›´å†…ã€‚
* å®šä¹‰ï¼šå¯¹äºæ¨¡å‹ç±»ä¸­çš„æ‰€æœ‰æ¨¡å‹ :math:`f \in \mathcal{F}` ï¼Œå¸Œæœ›ä»¥é«˜æ¦‚ç‡ä¿è¯ï¼š

.. math::

    |\epsilon(f)-\epsilon_{\mathcal{S}}(f)| < \alpha \quad \text { (for all } f \in \mathcal{F})

* å…¶ä¸­ :math:`\alpha` æ˜¯è¯¯å·®ç•Œé™
* æŒ‘æˆ˜ï¼š
    * è¿‡äºçµæ´»çš„æ¨¡å‹ç±»ï¼ˆå¦‚è®°å¿†æœºï¼Œèƒ½è®°ä½è®­ç»ƒé›†ä¸Šæ‰€æœ‰æ•°æ®ä½†æ³›åŒ–æ€§æå·®ï¼‰å¾ˆéš¾æ»¡è¶³ä¸€è‡´æ”¶æ•›æ€§ã€‚
    * è¿‡äºåˆšæ€§çš„æ¨¡å‹ç±»åˆ™é£é™©åœ¨äºæ¬ æ‹Ÿåˆï¼Œéš¾ä»¥æ•æ‰è®­ç»ƒæ•°æ®çš„è§„å¾‹ã€‚
* å¹³è¡¡ï¼šå­¦ä¹ ç†è®ºçš„ç›®æ ‡æ˜¯åœ¨ **æ¨¡å‹çµæ´»æ€§ï¼ˆé«˜æ–¹å·®ï¼‰å’Œæ¨¡å‹åˆšæ€§ï¼ˆé«˜åå·®ï¼‰** ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ç‚¹ã€‚


2) Vapnik-Chervonenkis (VC) ç»´åº¦
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* VC ç»´åº¦ï¼šè¡¡é‡æ¨¡å‹ç±»çš„å¤æ‚æ€§ï¼Œåæ˜ æ¨¡å‹æ‹Ÿåˆä»»æ„æ•°æ®ç‚¹çš„èƒ½åŠ›ã€‚
* VC ç»´åº¦æä¾›äº†ä¸€ç§é‡åŒ–æ¨¡å‹ç±»å¤æ‚æ€§çš„æ–¹æ³•ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½è¿‡äºä¿å®ˆã€‚



ç°å®æ„ä¹‰ä¸åº”ç”¨
++++++++++++++

1) ç»éªŒè¯¯å·® vs. æ³›åŒ–è¯¯å·®
    * å°æ•°æ®åœºæ™¯ï¼šè®­ç»ƒè¯¯å·®ä½å¹¶ä¸ä»£è¡¨æ¨¡å‹èƒ½æ³›åŒ–ï¼Œå¯èƒ½æ˜¯è¿‡æ‹Ÿåˆã€‚
    * å¤§æ•°æ®åœºæ™¯ï¼šéšç€æ•°æ®é‡ ``n`` å¢åŠ ï¼Œç»éªŒè¯¯å·®é€æ¸æ”¶æ•›åˆ°çœŸå®è¯¯å·®ã€‚

2) å·¥ç¨‹å®è·µ
    * æ·±åº¦å­¦ä¹ ï¼šå¤æ‚æ¨¡å‹å¾€å¾€éœ€è¦å¤§é‡æ ·æœ¬ï¼Œå³ä½¿è®­ç»ƒé›†è¯¯å·®æ¥è¿‘ 0ï¼Œä¹Ÿä¸èƒ½ç®€å•å‡è®¾æ³›åŒ–è¯¯å·®å¾ˆä½ã€‚
    * è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸï¼šé«˜é£é™©åœºæ™¯ä¸‹é€šå¸¸é‡‡ç”¨æ›´å¤§çš„æ•°æ®é›†å’Œå¤šé‡äº¤å‰éªŒè¯ï¼Œé™ä½æ³›åŒ–è¯¯å·®çš„ä¸ç¡®å®šæ€§ã€‚


4.7. Environment and Distribution Shift
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* åˆ†å¸ƒæ¼‚ç§»ï¼ˆDistribution Shiftï¼‰

4.7.1. Types of Distribution Shift
""""""""""""""""""""""""""""""""""

* å¼•è¨€ï¼š
    * æå‡ºåˆ†å¸ƒæ¼‚ç§»çš„æ¦‚å¿µï¼ŒæŒ‡å‡ºè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®å¯èƒ½æ¥è‡ªä¸åŒçš„åˆ†å¸ƒï¼Œç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ã€‚
    * å¼ºè°ƒåœ¨ç¼ºä¹å…³äºåˆ†å¸ƒå…³ç³»çš„å‡è®¾ä¸‹ï¼Œé²æ£’åˆ†ç±»å™¨çš„å­¦ä¹ æ˜¯ä¸å¯èƒ½çš„ã€‚
    * é€šè¿‡äºŒåˆ†ç±»é—®é¢˜ï¼ˆçŒ«ç‹—åˆ†ç±»ï¼‰å¼•å‡ºä¸€ä¸ªæç«¯ä¾‹å­ï¼šå¦‚æœè¾“å…¥åˆ†å¸ƒä¿æŒä¸å˜ï¼Œä½†æ ‡ç­¾å®Œå…¨åè½¬ï¼Œå°†æ— æ³•åŒºåˆ†åˆ†å¸ƒæ˜¯å¦å‘ç”Ÿå˜åŒ–ã€‚
* è½¬æŠ˜ï¼š
    * è¯´æ˜åœ¨é€‚å½“å‡è®¾ä¸‹ï¼Œå¯ä»¥æ£€æµ‹åˆ†å¸ƒæ¼‚ç§»ï¼Œå¹¶å¯èƒ½åŠ¨æ€è°ƒæ•´æ¨¡å‹ä»¥æå‡æ€§èƒ½ã€‚

* æ ¸å¿ƒæ€æƒ³
    * åˆ†å¸ƒæ¼‚ç§»ä¸å¯é¿å…ï¼Œä½†åœ¨åˆç†å‡è®¾ä¸‹ï¼Œå¯ä»¥æ£€æµ‹å’Œé€‚åº”æ¼‚ç§»ã€‚
    * å…³é”®æ˜¯ç†è§£æ¼‚ç§»æ¥æºï¼ˆç‰¹å¾å˜åŒ–ã€æ ‡ç­¾å˜åŒ–æˆ–æ ‡ç­¾å®šä¹‰å˜åŒ–ï¼‰ï¼Œå¹¶é€‰æ‹©åˆé€‚çš„ç®—æ³•æ¥åº”å¯¹ã€‚
    * åå˜é‡æ¼‚ç§»æ˜¯æœ€å¸¸ç ”ç©¶çš„æ–¹å‘ï¼Œå› ä¸ºç‰¹å¾åˆ†å¸ƒå˜åŒ–ç›´è§‚ä¸”æ›´æ˜“è¢«ç›‘æµ‹åˆ°ã€‚
    * æ ‡ç­¾æ¼‚ç§»æ›´å…·æŒ‘æˆ˜ï¼Œé€šå¸¸éœ€è¦åœ¨ä½ç»´æ ‡ç­¾ç©ºé—´ä¸­æ“ä½œï¼Œè€Œä¸æ˜¯ç›´æ¥åœ¨é«˜ç»´ç‰¹å¾ç©ºé—´ä¸­å¤„ç†æ¼‚ç§»ã€‚
    * æ¦‚å¿µæ¼‚ç§»æœ€å¤æ‚ï¼Œé€šå¸¸ä¾èµ–å¤–éƒ¨çŸ¥è¯†æˆ–å…ƒå­¦ä¹ æ–¹æ³•æ¥é€æ¸é€‚åº”ã€‚


4.7.1.1. Covariate Shift
++++++++++++++++++++++++

* å®šä¹‰ï¼š ç‰¹å¾çš„åˆ†å¸ƒ :math:`p(\mathbf{x})` å‘ç”Ÿå˜åŒ–ï¼Œä½†æ ‡ç­¾æ¡ä»¶åˆ†å¸ƒ :math:`P(y \mid \mathbf{x})` ä¿æŒä¸å˜ã€‚
* ç¤ºä¾‹ï¼šè®­ç»ƒé›†æ˜¯å®ç‰©ç…§ç‰‡ï¼Œæµ‹è¯•é›†æ˜¯å¡é€šå›¾åƒï¼Œç‰¹å¾åˆ†å¸ƒï¼ˆå›¾ç‰‡é£æ ¼ï¼‰å˜äº†ï¼Œä½†çŒ«ç‹—çš„æœ¬è´¨å®šä¹‰ä¸å˜ã€‚
* è§£é‡Šï¼šåå˜é‡æ¼‚ç§»å¸¸è§äºå› æœå…³ç³»ä¸­ï¼Œç‰¹å¾ :math:`\mathbf{x}` å½±å“æ ‡ç­¾ y ã€‚éœ€è¦é‡ç‚¹å…³æ³¨æ¨¡å‹å¦‚ä½•é€‚åº”æ–°åˆ†å¸ƒçš„ç‰¹å¾ã€‚


4.7.1.2. Label Shift
++++++++++++++++++++

* å®šä¹‰ï¼š æ ‡ç­¾åˆ†å¸ƒ :math:`P(y)` å‘ç”Ÿå˜åŒ–ï¼Œä½†ç±»æ¡ä»¶ç‰¹å¾åˆ†å¸ƒ :math:`P(\mathbf{x} \mid y)` ä¿æŒä¸å˜ã€‚
* ç¤ºä¾‹ï¼šä¸åŒç–¾ç—…çš„æ‚£ç—…ç‡å˜åŒ–ï¼Œä½†ç–¾ç—…è¡¨ç°å‡ºçš„ç—‡çŠ¶ä¸å˜ã€‚
* è§£é‡Šï¼š
    * æ ‡ç­¾æ¼‚ç§»é€šå¸¸å‡ºç°åœ¨æ ‡ç­¾ y å½±å“ç‰¹å¾ :math:`\mathbf{x}` çš„å› æœå…³ç³»ä¸­ã€‚
    * æ“ä½œæ ‡ç­¾çš„æ¨¡å‹ï¼ˆä½ç»´ï¼‰é€šå¸¸æ›´æ˜“å¤„ç†è¿™ç§æƒ…å†µï¼Œè€Œæ“ä½œç‰¹å¾çš„æ¨¡å‹ï¼ˆé«˜ç»´ï¼‰éš¾åº¦è¾ƒå¤§ã€‚

4.7.1.3. Concept Shift
++++++++++++++++++++++

* å®šä¹‰ï¼š æ ‡ç­¾æœ¬èº«çš„å®šä¹‰å‘ç”Ÿå˜åŒ–ï¼Œå³ :math:`P(y \mid \mathbf{x})` å˜åŒ–ã€‚
* ç¤ºä¾‹ï¼š
    * ä¸åŒåœ°åŒºå¯¹åŒä¸€ç§è½¯é¥®æ–™æœ‰ä¸åŒç§°å‘¼ï¼ˆå¦‚ "pop" å’Œ "soda"ï¼‰ã€‚
    * ç–¾ç—…è¯Šæ–­æ ‡å‡†æˆ–æ—¶å°šè¶‹åŠ¿éšæ—¶é—´å’Œåœ°åŸŸå˜åŒ–ã€‚
* è§£é‡Šï¼š
    * æ¦‚å¿µæ¼‚ç§»éš¾ä»¥å¯Ÿè§‰ï¼Œå› ä¸ºæ ‡ç­¾å®šä¹‰å¯èƒ½éšæ—¶é—´æˆ–åœ°ç†ä½ç½®é€æ¸å˜åŒ–ã€‚
    * åœ¨è‡ªç„¶è¯­è¨€å¤„ç†æˆ–æœºå™¨ç¿»è¯‘ä¸­ï¼Œæ¦‚å¿µæ¼‚ç§»å°¤ä¸ºæ˜æ˜¾ã€‚



4.7.2. Examples of Distribution Shift
"""""""""""""""""""""""""""""""""""""

4.7.2.1. Medical Diagnostics
++++++++++++++++++++++++++++

* åŒ»ç–—è¯Šæ–­ï¼ˆMedical Diagnosticsï¼‰
* èƒŒæ™¯ï¼šç›®æ ‡æ˜¯å¼€å‘ç™Œç—‡æ£€æµ‹ç®—æ³•ï¼Œä½¿ç”¨å¥åº·äººå’Œç—…äººçš„è¡€æ¶²æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚
* é—®é¢˜ï¼šç”±äºå¥åº·ç”·æ€§æ ·æœ¬éš¾ä»¥æ”¶é›†ï¼Œåˆ›ä¸šå…¬å¸é€‰æ‹©äº†å¤§å­¦ç”Ÿè¡€æ ·ä½œä¸ºå¯¹ç…§ç»„ã€‚
* ç»“æœï¼šåˆ†ç±»å™¨å¯ä»¥è½»æ¾åŒºåˆ†å¥åº·å’Œç—…äººç¾¤ä½“ï¼Œä½†è¿™æ˜¯å› ä¸ºå¤§å­¦ç”Ÿå’Œè€å¹´ç—…äººä¹‹é—´å­˜åœ¨å¤§é‡æ— å…³å˜é‡ï¼ˆå¹´é¾„ã€æ¿€ç´ æ°´å¹³ã€ç”Ÿæ´»æ–¹å¼ç­‰ï¼‰å·®å¼‚ï¼Œè€Œéç–¾ç—…ç›¸å…³ç‰¹å¾ã€‚
* æœ¬è´¨ï¼šæç«¯åå˜é‡æ¼‚ç§»ï¼ˆCovariate Shiftï¼‰ï¼Œå¥åº·å¯¹ç…§ç»„å’ŒçœŸå®ç—…äººç¾¤ä½“ç‰¹å¾å­˜åœ¨å·¨å¤§å·®å¼‚ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œè¡¨ç°ä¸ä½³ã€‚
* å¯ç¤ºï¼šæ•°æ®é‡‡æ ·è¿‡ç¨‹å¿…é¡»åŒ¹é…çœŸå®åº”ç”¨ç¯å¢ƒã€‚ä¸èƒ½ä»…ä¸ºè§£å†³æ•°æ®ç¨€ç¼ºé—®é¢˜éšæ„é€‰æ‹©ä¸å…·ä»£è¡¨æ€§çš„æ ·æœ¬ã€‚

4.7.2.2. Self-Driving Cars
++++++++++++++++++++++++++

* èƒŒæ™¯ï¼šå…¬å¸å¸Œæœ›ç”¨æ¸¸æˆå¼•æ“åˆæˆæ•°æ®è®­ç»ƒé“è·¯æ¢æµ‹å™¨ï¼Œä»¥å‡å°‘æ ‡æ³¨æ•°æ®çš„æˆæœ¬ã€‚
* é—®é¢˜ï¼šåœ¨å¼•æ“æµ‹è¯•æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œä½†åœ¨çœŸå®ç¯å¢ƒä¸­å®Œå…¨å¤±è´¥ã€‚
* åŸå› ï¼šé“è·¯çº¹ç†åœ¨æ¸¸æˆå¼•æ“ä¸­è¿‡äºç®€å•ï¼Œå¹¶ä¸”æ‰€æœ‰é“è·¯éƒ½ä½¿ç”¨ç›¸åŒçº¹ç†ã€‚æ¨¡å‹å­¦ä¹ åˆ°çš„æ˜¯çº¹ç†å·®å¼‚ï¼Œè€ŒéçœŸæ­£çš„é“è·¯ç‰¹å¾ã€‚
* ç±»ä¼¼æ¡ˆä¾‹ï¼šç¾å†›æ›¾è¯•å›¾é€šè¿‡èˆªæ‹ç…§ç‰‡è®­ç»ƒå¦å…‹æ¢æµ‹å™¨ï¼Œä½†æ¨¡å‹å®é™…ä¸Šåªæ˜¯å­¦ä¼šäº†åŒºåˆ†æ—©æ™¨å’Œä¸­åˆçš„æ ‘å½±å·®å¼‚ã€‚
* æœ¬è´¨ï¼šæ¦‚å¿µæ¼‚ç§»æˆ–åå˜é‡æ¼‚ç§»ï¼Œ æ¨¡å‹å­¦åˆ°äº†é”™è¯¯ç‰¹å¾ï¼Œå¯¼è‡´çœŸå®åœºæ™¯ä¸‹è¡¨ç°ä¸ä½³ã€‚
* å¯ç¤ºï¼šåˆæˆæ•°æ®éœ€å°½é‡è´´è¿‘ç°å®ã€‚é¿å…è®­ç»ƒæ•°æ®å’Œå®é™…åº”ç”¨åœºæ™¯é—´å­˜åœ¨ä¸¥é‡å·®è·ã€‚éœ€è¦æ··åˆå¤šç§çœŸå®å’Œåˆæˆæ•°æ®æ¥æºï¼Œä»¥å‡å°‘æ¨¡å‹å¯¹æ— å…³ç‰¹å¾çš„ä¾èµ–ã€‚


4.7.2.3. Nonstationary Distributions
++++++++++++++++++++++++++++++++++++

* éå¹³ç¨³åˆ†å¸ƒï¼ˆNonstationary Distributionsï¼‰
* å®šä¹‰ï¼š åˆ†å¸ƒéšæ—¶é—´ç¼“æ…¢å˜åŒ–ï¼Œæ¨¡å‹æœªèƒ½åŠæ—¶æ›´æ–°ã€‚
* å…¸å‹æ¡ˆä¾‹ï¼š
    * å¹¿å‘Šæ¨¡å‹æœªåŠæ—¶æ›´æ–°ï¼Œæ–°è®¾å¤‡ï¼ˆå¦‚iPadï¼‰æ¨å‡ºåæœªçº³å…¥è®­ç»ƒï¼Œæ¨¡å‹å¤±æ•ˆã€‚
    * åƒåœ¾é‚®ä»¶è¿‡æ»¤å™¨è¿‡æ—¶ï¼Œæ–°å‹åƒåœ¾é‚®ä»¶é€ƒè¿‡æ£€æµ‹ã€‚
    * äº§å“æ¨èç³»ç»Ÿæ»åï¼Œä»æ¨èåœ£è¯å¸½ï¼Œæœªèƒ½é€‚åº”å­£èŠ‚å˜åŒ–ã€‚
* æœ¬è´¨ï¼šåˆ†å¸ƒç¼“æ…¢æ¼‚ç§»ï¼Œ éšç€æ—¶é—´æ¨ç§»æ¨¡å‹æ€§èƒ½é€æ¸ä¸‹é™ã€‚
* å¯ç¤ºï¼šæ¨¡å‹éœ€è¦å®šæœŸæ›´æ–°å’Œé‡æ–°è®­ç»ƒ ä»¥é€‚åº”ç¯å¢ƒå˜åŒ–ã€‚å¼•å…¥åœ¨çº¿å­¦ä¹ æœºåˆ¶ï¼Œä½¿æ¨¡å‹å¯ä»¥æŒç»­å­¦ä¹ æ–°æ•°æ®ã€‚

4.7.3. Correction of Distribution Shift
"""""""""""""""""""""""""""""""""""""""

* æœ¬èŠ‚æ˜¯æ¯”è¾ƒé«˜çº§çš„åŠŸèƒ½ï¼Œä¸ç†è§£ä¹Ÿä¸å½±å“åé¢ç« èŠ‚çš„å­¦ä¹ 

4.7.3.1. Empirical Risk and Risk
++++++++++++++++++++++++++++++++

* é£é™©å®šä¹‰ä¸è®­ç»ƒç›®æ ‡ï¼šè®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä½¿æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æŸå¤±æœ€å°åŒ–ã€‚

1. ç»éªŒé£é™© (Empirical Risk)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. math::

    \operatorname{minimize}_{f} \frac{1}{n} \sum_{i=1}^{n} l\left(f\left(\mathbf{x}_{i}\right), y_{i}\right)

* ç»éªŒé£é™©æœ€å°åŒ–, å³åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šè®¡ç®—æŸå¤±å¹¶æœ€å°åŒ–å¹³å‡æŸå¤±ã€‚

2. çœŸå®é£é™© (True Risk)
~~~~~~~~~~~~~~~~~~~~~~~

.. math::

    E_{p(\mathbf{x}, y)}[l(f(\mathbf{x}), y)]=\iint l(f(\mathbf{x}), y) p(\mathbf{x}, y) d \mathbf{x} d y

* çœŸå®é£é™©è€ƒè™‘çš„æ˜¯æ•´ä¸ªæ•°æ®åˆ†å¸ƒ :math:`p(\mathbf{x}, y)` ï¼Œä½†å®é™…æƒ…å†µä¸­æ— æ³•è·å–æ•´ä¸ªåˆ†å¸ƒï¼Œæ‰€ä»¥åªèƒ½ä½¿ç”¨ç»éªŒé£é™©è¿‘ä¼¼æœ€å°åŒ–çœŸå®é£é™©ã€‚


4.7.3.2. Covariate Shift Correction
+++++++++++++++++++++++++++++++++++

* å®šä¹‰ï¼šç‰¹å¾çš„åˆ†å¸ƒ :math:`p(\mathbf{x})` å‘ç”Ÿå˜åŒ–ï¼Œä½†æ¡ä»¶åˆ†å¸ƒ :math:`p(y|\mathbf{x})` ä¿æŒä¸å˜ã€‚
* é—®é¢˜ï¼šè®­ç»ƒæ•°æ®æ¥è‡ªæºåˆ†å¸ƒ :math:`q(\mathbf{x})` ï¼Œä½†æµ‹è¯•æ•°æ®æ¥è‡ªç›®æ ‡åˆ†å¸ƒ :math:`p(\mathbf{x})` ã€‚å¦‚æœæºåˆ†å¸ƒå’Œç›®æ ‡åˆ†å¸ƒä¸åŒï¼Œæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¯èƒ½å¾ˆå·®ã€‚
* è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡é‡åŠ æƒ (Re-weighting) æŠ€æœ¯è°ƒæ•´è®­ç»ƒæ•°æ®çš„æƒé‡ï¼Œä½¿å…¶æ›´ç¬¦åˆç›®æ ‡åˆ†å¸ƒã€‚

.. math::

    \begin{aligned}
    \int\int l(f(\mathbf{x}), y) p(y \mid \mathbf{x})p(\mathbf{x}) \;d\mathbf{x}dy =
    \int\int l(f(\mathbf{x}), y) q(y \mid \mathbf{x})q(\mathbf{x})\frac{p(\mathbf{x})}{q(\mathbf{x})} \;d\mathbf{x}dy.
    \end{aligned}

* å³ï¼Œå°†æ¯ä¸ªæ ·æœ¬çš„æƒé‡ä¹˜ä»¥ :math:`\beta_i = \frac{p(\mathbf{x}_i)}{q(\mathbf{x}_i)}` ï¼Œä»è€Œæ ¡æ­£åå˜é‡æ¼‚ç§»ã€‚

* å®é™…æ“ä½œæ­¥éª¤ï¼š
    * è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ï¼ŒåŒºåˆ†ç›®æ ‡åˆ†å¸ƒ :math:`p(\mathbf{x})`  å’Œæºåˆ†å¸ƒ :math:`q(\mathbf{x})`  çš„æ ·æœ¬ã€‚
    * ä½¿ç”¨é€»è¾‘å›å½’è®¡ç®— :math:`\beta_i = \exp(h(\mathbf{x}_i))` ï¼Œå¾—åˆ°æ ¡æ­£æƒé‡ã€‚
    * åœ¨æ¨¡å‹è®­ç»ƒæ—¶ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬ :math:`(\mathbf{x}_i, y_i)`  ä¹˜ä»¥ :math:`\beta_i`  è¿›è¡ŒåŠ æƒç»éªŒé£é™©æœ€å°åŒ–ã€‚



4.7.3.3. Label Shift Correction
+++++++++++++++++++++++++++++++

* è§£å†³æ–¹æ¡ˆï¼šé€šè¿‡è®¡ç®—æ ‡ç­¾åˆ†å¸ƒ p(y) å’Œ q(y) çš„æ¯”å€¼ :math:`\beta_i = \frac{p(y_i)}{q(y_i)}` è¿›è¡Œæ ¡æ­£ã€‚
* å…¬å¼æ¨å¯¼ï¼š

.. math::

    \begin{aligned}
    \int\int l(f(\mathbf{x}), y) p(\mathbf{x} \mid y)p(y) \;d\mathbf{x}dy =
    \int\int l(f(\mathbf{x}), y) q(\mathbf{x} \mid y)q(y)\frac{p(y)}{q(y)} \;d\mathbf{x}dy.
    \end{aligned}


4.7.3.4. Concept Shift Correction
+++++++++++++++++++++++++++++++++

* é—®é¢˜ï¼šä¾‹å¦‚ï¼Œä»åŒºåˆ†çŒ«å’Œç‹—è½¬å˜ä¸ºåŒºåˆ†ç™½è‰²å’Œé»‘è‰²åŠ¨ç‰©ï¼Œè¿™ç§å˜åŒ–å¾ˆéš¾é€šè¿‡ç®€å•çš„æ–¹æ³•æ ¡æ­£ã€‚
* è§£å†³æ–¹æ¡ˆï¼š
    * å¯¹äºæ¸å˜çš„æ¼‚ç§»ï¼Œå¯ä»¥åœ¨ç°æœ‰æ¨¡å‹ä¸Šè¿›è¡Œå°‘é‡æ›´æ–°ï¼Œè€Œéä»å¤´è®­ç»ƒæ–°æ¨¡å‹ã€‚
    * å¯¹äºå‰§çƒˆçš„æ¼‚ç§»ï¼Œé€šå¸¸éœ€è¦é‡æ–°æ”¶é›†æ•°æ®å’Œæ ‡ç­¾ï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
* å®é™…åº”ç”¨ç¤ºä¾‹ï¼š
    * å¹¿å‘Šæ¨èï¼šç”¨æˆ·å…´è¶£å˜åŒ–ï¼Œæ–°äº§å“ä¸Šçº¿ã€‚
    * äº¤é€šæ‘„åƒå¤´ï¼šé•œå¤´è€åŒ–å¯¼è‡´å›¾åƒè´¨é‡ä¸‹é™ã€‚
    * æ–°é—»æ¨èï¼šæ–°é—»å†…å®¹ä¸æ–­æ›´æ–°ï¼Œæ–°äº‹ä»¶å‡ºç°ã€‚
    * é€šè¿‡æŒç»­å­¦ä¹  (Continual Learning) æˆ–è¿ç§»å­¦ä¹  (Transfer Learning) åº”å¯¹æ¦‚å¿µæ¼‚ç§»ã€‚


4.7.4. A Taxonomy of Learning Problems
""""""""""""""""""""""""""""""""""""""

4.7.4.1. Batch Learning
+++++++++++++++++++++++

* æ¦‚å¿µï¼šåœ¨æ‰¹é‡å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬æ‹¥æœ‰ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒæ•°æ®é›† :math:`{(\mathbf{x}_1, y_1), \dots, (\mathbf{x}_n, y_n)}`
    * æ¨¡å‹ :math`f(\mathbf{x})` æ˜¯åœ¨æ‰€æœ‰æ•°æ®éƒ½å·²çŸ¥çš„æƒ…å†µä¸‹è®­ç»ƒå®Œæˆçš„ã€‚
    * è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹éƒ¨ç½²åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œä¸å†è¿›è¡Œæ›´æ–°ï¼ˆé™¤éæœ‰é‡å¤§é”™è¯¯æˆ–ç‰¹æ®Šæƒ…å†µï¼‰ã€‚
* ç¤ºä¾‹ï¼šè®­ç»ƒä¸€ä¸ªçŒ«ç‹—åˆ†ç±»å™¨ï¼Œç”¨äºæ™ºèƒ½çŒ«é—¨ã€‚å½“æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å®‰è£…åœ¨å®¢æˆ·å®¶ä¸­åï¼Œå®ƒä¸ä¼šå†æ”¹å˜æˆ–å­¦ä¹ æ–°çš„æ•°æ®ã€‚
* ç‰¹ç‚¹ï¼š
    * è®­ç»ƒå’Œæ¨ç†æ˜¯åˆ†å¼€çš„ã€‚
    * é€‚ç”¨äºé™æ€ã€ä¸ç»å¸¸å˜åŒ–çš„ä»»åŠ¡ã€‚
    * è®­ç»ƒæ•°æ®ä¸æœªæ¥æ•°æ®åˆ†å¸ƒä¸€è‡´æ—¶æ•ˆæœæœ€å¥½ã€‚

4.7.4.2. Online Learning
++++++++++++++++++++++++

* æ¦‚å¿µï¼šæ•°æ®é€ä¸ªåˆ°è¾¾ï¼Œæ¨¡å‹éœ€è¦é€æ­¥å­¦ä¹ ï¼Œæ¯æ¬¡æ¥æ”¶ä¸€ä¸ªæ ·æœ¬ :math:`(\mathbf{x}_i, y_i)`
    * åœ¨è§‚å¯Ÿåˆ°æ ‡ç­¾ :math:`y_i` ä¹‹å‰ï¼Œæ¨¡å‹å…ˆåŸºäº :math:`\mathbf{x}_i` ç»™å‡ºé¢„æµ‹ã€‚
    * åœ¨å¾—åˆ°æ ‡ç­¾åï¼Œæ¨¡å‹æ ¹æ®æŸå¤±è¿›è¡Œæ›´æ–°ï¼Œé€æ¸å˜å¾—æ›´å¥½ã€‚
* ç¤ºä¾‹ï¼šè‚¡ç¥¨ä»·æ ¼é¢„æµ‹ï¼šæ¯å¤©é¢„æµ‹ç¬¬äºŒå¤©çš„è‚¡ä»·ï¼Œç­‰åˆ°å®é™…ä»·æ ¼å‡ºæ¥åï¼Œå†æ›´æ–°æ¨¡å‹ï¼Œè°ƒæ•´é¢„æµ‹æ–¹å¼ã€‚
* æµç¨‹ï¼š
    * ä½¿ç”¨æ¨¡å‹ :math:`f_t` å¯¹æ–°çš„æ•°æ® :math:`\mathbf{x}_t` è¿›è¡Œé¢„æµ‹ã€‚
    * è§‚å¯ŸçœŸå®æ ‡ç­¾ :math:`y_t` å¹¶è®¡ç®—æŸå¤±ã€‚
    * æ›´æ–°æ¨¡å‹ :math:`f_{t+1}` ä»¥æ”¹è¿›ä¸‹ä¸€æ¬¡é¢„æµ‹ã€‚

.. math::

    f_t \rightarrow \mathbf{x}_t \rightarrow f_t(\mathbf{x}_t) \rightarrow y_t \rightarrow l(y_t, f_t(\mathbf{x}_t)) \rightarrow f_{t+1}

* ç‰¹ç‚¹ï¼š
    * æŒç»­å­¦ä¹ å’Œæ›´æ–°æ¨¡å‹
    * é€‚ç”¨äºç¯å¢ƒåŠ¨æ€å˜åŒ–ã€æ•°æ®ä¸æ–­æµå…¥çš„åœºæ™¯
    * å¯åº”å¯¹æ¦‚å¿µæ¼‚ç§»ï¼ˆconcept shiftï¼‰


4.7.4.3. Bandits
++++++++++++++++

* æ¦‚å¿µï¼š
    * å¤šè‡‚è€è™æœºæ˜¯ä¸€ç±»ç‰¹æ®Šçš„åœ¨çº¿å­¦ä¹ é—®é¢˜
    * ä¸åŒäºè¿ç»­å‚æ•°æ¨¡å‹ï¼ˆå¦‚ç¥ç»ç½‘ç»œï¼‰ï¼ŒBandit åªæœ‰æœ‰é™ä¸ªåŠ¨ä½œæˆ–é€‰æ‹©
    * ç›®æ ‡æ˜¯æ‰¾åˆ°æ”¶ç›Šæœ€é«˜çš„â€œæ‹‰æ†â€æˆ–åŠ¨ä½œ
* ç¤ºä¾‹ï¼š
    * åœ¨çº¿å¹¿å‘Šæ¨èï¼šåœ¨å¤šä¸ªå¹¿å‘Šä¸­é€‰æ‹©ä¸€ä¸ªå±•ç¤ºï¼Œè§‚å¯Ÿç‚¹å‡»ç‡ï¼Œä¸æ–­è°ƒæ•´é€‰æ‹©ç­–ç•¥ã€‚
* ç‰¹ç‚¹ï¼š
    * åªéœ€åœ¨æœ‰é™é€‰é¡¹ä¸­è¿›è¡Œå†³ç­–
    * é€šå¸¸å…·æœ‰è¾ƒå¼ºçš„ç†è®ºä¿è¯å’Œä¼˜åŒ–ç­–ç•¥
    * ç®—æ³•æ›´ç®€å•ï¼Œä½†é—®é¢˜èŒƒå›´æ›´ç‹­çª„

4.7.4.4. Control
++++++++++++++++

* æ¦‚å¿µï¼š
    * ç¯å¢ƒä¼šè®°ä½æ¨¡å‹çš„å†³ç­–ï¼Œä¸‹ä¸€æ¬¡çš„è§‚æµ‹å€¼ä¾èµ–äºä¹‹å‰çš„è¡Œä¸ºã€‚
    * ä¸ä¸€å®šæ˜¯å¯¹æŠ—æ€§çš„ï¼Œä½†æ¨¡å‹çš„è¡Œä¸ºä¼šå½±å“æœªæ¥çŠ¶æ€ã€‚
* ç¤ºä¾‹ï¼š
    * å’–å•¡æœºçš„æ¸©åº¦æ§åˆ¶å™¨ï¼šæ˜¯å¦ç»§ç»­åŠ çƒ­å–å†³äºå½“å‰æ¸©åº¦ä»¥åŠä¹‹å‰çš„åŠ çƒ­çŠ¶æ€ã€‚
    * æ–°é—»æ¨èç³»ç»Ÿï¼šç”¨æˆ·æ˜¯å¦ç‚¹å‡»æ–°é—»å–å†³äºä¹‹å‰æ¨èçš„å†…å®¹ã€‚
* ç‰¹ç‚¹ï¼š
    * æ¨¡å‹éœ€è¦è®°å¿†å’Œè€ƒè™‘è¿‡å»çš„è¡Œä¸ºã€‚
    * ç»å¸¸ä½¿ç”¨æ§åˆ¶ç†è®ºæ–¹æ³•ï¼Œå¦‚ PID æ§åˆ¶å™¨ã€‚
    * å¯ç”¨äºç¯å¢ƒäº¤äº’å¼å†³ç­–é—®é¢˜ã€‚



4.7.4.5. Reinforcement Learning
+++++++++++++++++++++++++++++++

* æ¦‚å¿µï¼š
    * å¼ºåŒ–å­¦ä¹ æ˜¯åœ¨å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œå†³ç­–çš„æ›´é«˜çº§å½¢å¼ã€‚
    * ç¯å¢ƒå¯èƒ½æ˜¯åˆä½œçš„ï¼ˆä¾‹å¦‚å¤šç©å®¶åˆä½œæ¸¸æˆï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯ç«äº‰çš„ï¼ˆå¦‚è±¡æ£‹ã€å›´æ£‹ï¼‰ã€‚
    * æ¨¡å‹éœ€è¦é€šè¿‡ä¸ç¯å¢ƒäº¤äº’ï¼Œä¸æ–­å­¦ä¹ ä»¥æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±ã€‚
* ç¤ºä¾‹ï¼š
    * æ¸¸æˆ AIï¼šåœ¨è±¡æ£‹ã€å›´æ£‹æˆ–ç”µå­ç«æŠ€æ¸¸æˆä¸­ï¼Œè‡ªä¸»å­¦ä¹ å¯¹æŠ—ç­–ç•¥ã€‚
    * è‡ªåŠ¨é©¾é©¶ï¼šå…¶ä»–è½¦è¾†çš„è¡Œä¸ºä¼šå—åˆ°è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„å½±å“ã€‚
* ç‰¹ç‚¹ï¼š
    * é€‚ç”¨äºå¤æ‚ã€æœ‰è®°å¿†çš„åŠ¨æ€ç¯å¢ƒã€‚
    * å¼ºè°ƒé•¿æœŸç­–ç•¥å’Œç´¯ç§¯å¥–åŠ±ã€‚
    * ç¯å¢ƒçš„åé¦ˆï¼ˆå¥–åŠ±æˆ–æƒ©ç½šï¼‰å†³å®šæ¨¡å‹æ›´æ–°æ–¹å¼ã€‚

4.7.4.6. Considering the Environment
++++++++++++++++++++++++++++++++++++

* æ ¸å¿ƒæ€æƒ³ï¼š
    * ä¸åŒç¯å¢ƒä¸‹ï¼Œæ¨¡å‹çš„è¡¨ç°å’Œç­–ç•¥å¯èƒ½å®Œå…¨ä¸åŒ
    * åœ¨é™æ€ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„ç­–ç•¥å¯èƒ½åœ¨åŠ¨æ€ç¯å¢ƒä¸­å¤±æ•ˆ
    * ç¯å¢ƒçš„å˜åŒ–é€Ÿåº¦å’Œæ–¹å¼ï¼Œå†³å®šäº†éœ€è¦ä½¿ç”¨å“ªç§å­¦ä¹ æ–¹æ³•
* ç¤ºä¾‹ï¼š
    * é‡‘èå¸‚åœºï¼šå¥—åˆ©æœºä¼šä¸€æ—¦è¢«å‘ç°å¹¶åˆ©ç”¨ï¼Œå¸‚åœºä¼šè¿…é€Ÿè°ƒæ•´ï¼Œå¥—åˆ©æœºä¼šæ¶ˆå¤±ã€‚
    * æ¨èç³»ç»Ÿï¼šç”¨æˆ·å…´è¶£éšæ—¶é—´å˜åŒ–ï¼Œéœ€è¦åŠ¨æ€æ›´æ–°æ¨¡å‹ã€‚
* è§£å†³æ–¹æ³•ï¼š
    * ç¼“æ…¢å˜åŒ–çš„ç¯å¢ƒï¼š
        * çº¦æŸæ¨¡å‹çš„æ›´æ–°é€Ÿåº¦ï¼Œä½¿å…¶ç¼“æ…¢é€‚åº”ç¯å¢ƒå˜åŒ–ã€‚
    * å¿«é€Ÿä½†å¶å°”å˜åŒ–çš„ç¯å¢ƒï¼š
        * åœ¨ç¯å¢ƒçªç„¶å˜åŒ–æ—¶å…è®¸æ¨¡å‹è¿…é€Ÿè°ƒæ•´ï¼Œä½†æ—¥å¸¸å˜åŒ–è¾ƒå°‘ã€‚

æ€»ç»“
++++

* æ‰¹é‡å­¦ä¹ ï¼šé™æ€ã€ä¸€æ¬¡æ€§è®­ç»ƒã€‚
* åœ¨çº¿å­¦ä¹ ï¼šé€æ­¥æ›´æ–°æ¨¡å‹ï¼Œé€‚åº”åŠ¨æ€ç¯å¢ƒã€‚
* Banditï¼šæœ‰é™åŠ¨ä½œé€‰æ‹©é—®é¢˜ï¼Œä¼˜åŒ–å¥–åŠ±ã€‚
* æ§åˆ¶ï¼šç¯å¢ƒä¼šè®°ä½æ¨¡å‹çš„è¡Œä¸ºï¼ŒçŠ¶æ€ä¾èµ–å†å²ã€‚
* å¼ºåŒ–å­¦ä¹ ï¼šå¤æ‚åŠ¨æ€ç¯å¢ƒä¸­é€šè¿‡å¥–åŠ±å­¦ä¹ ç­–ç•¥ã€‚


5. Multilayer Perceptrons
-------------------------

5.1. Multilayer Perceptrons
^^^^^^^^^^^^^^^^^^^^^^^^^^^

5.1.1. Hidden Layers
""""""""""""""""""""

5.1.1.1. Limitations of Linear Models
+++++++++++++++++++++++++++++++++++++

* çº¿æ€§æ¨¡å‹çš„å±€é™æ€§ï¼šçº¿æ€§æ¨¡å‹å‡è¾“å‡ºæ˜¯è¾“å…¥çš„çº¿æ€§å˜æ¢ã€‚è¿™ç§å‡è®¾å¤ªå¼ºï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ‹Ÿåˆå¤æ‚å…³ç³»ã€‚
* å¦‚ï¼šé¢„æµ‹è´·æ¬¾å¿è¿˜æ¦‚ç‡æ—¶ï¼Œæ”¶å…¥å¢é•¿å¯¹å¿è¿˜æ¦‚ç‡çš„å½±å“å¹¶éçº¿æ€§ï¼ˆæ”¶å…¥ä» 0 ç¾å…ƒå¢åŠ åˆ° 5 ä¸‡ç¾å…ƒå¯èƒ½æ¯”ä» 100 ä¸‡ç¾å…ƒå¢åŠ åˆ° 105 ä¸‡ç¾å…ƒå¯¹åº”çš„è¿˜æ¬¾å¯èƒ½æ€§æ›´å¤§ï¼‰ã€‚
* å¦‚ï¼šä½“æ¸©ä¸å¥åº·å…³ç³»ï¼šå¯¹äºæ­£å¸¸ä½“æ¸©é«˜äº 37Â°C (98.6Â°F) çš„äººæ¥è¯´ï¼Œæ¸©åº¦è¶Šé«˜è¡¨æ˜é£é™©è¶Šå¤§ã€‚ç„¶è€Œï¼Œå¦‚æœä½“æ¸©ä½äº37Â°Cï¼Œè¾ƒä½çš„æ¸©åº¦è¡¨æ˜é£é™©æ›´å¤§ï¼
* å¦‚ï¼šå›¾åƒåˆ†ç±»é—®é¢˜ï¼šç®€å•çº¿æ€§æ¨¡å‹å‡è®¾åƒç´ å¼ºåº¦ä¸è¾“å‡ºç±»åˆ«ç›´æ¥ç›¸å…³ï¼Œè¿™å¯¹äºå¤æ‚ä»»åŠ¡ï¼ˆå¦‚çŒ«ç‹—åˆ†ç±»ï¼‰æ•ˆæœå¾ˆå·®ã€‚

5.1.1.2. Incorporating Hidden Layers
++++++++++++++++++++++++++++++++++++

* å¤šå±‚æ„ŸçŸ¥æœºå¼•å…¥éšè—å±‚
* é€šè¿‡å åŠ éšè—å±‚ï¼Œä½¿æ¨¡å‹å­¦ä¹ æ•°æ®å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚
* MLPç”±å¤šä¸ªå…¨è¿æ¥å±‚ç»„æˆï¼Œå‰é¢å‡ å±‚è´Ÿè´£å­¦ä¹ ç‰¹å¾è¡¨ç¤ºï¼Œæœ€åä¸€å±‚æ‰§è¡Œçº¿æ€§é¢„æµ‹ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/5DOnFP.png

    Fig. 5.1.1 An MLP(multilayer perceptron) with a hidden layer of five hidden units.

5.1.1.3. From Linear to Nonlinear
+++++++++++++++++++++++++++++++++

* æ¿€æ´»å‡½æ•°çš„å¿…è¦æ€§
* å¦‚æœéšè—å±‚ä»…ä»…æ˜¯çº¿æ€§å˜æ¢ï¼Œæ¨¡å‹ä»ç„¶ç­‰æ•ˆäºå•å±‚çº¿æ€§æ¨¡å‹ï¼Œæ— æ³•çœŸæ­£æå‡è¡¨è¾¾èƒ½åŠ›ã€‚

.. math::

    \mathbf{O} = (\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)})\mathbf{W}^{(2)} + \mathbf{b}^{(2)} \\
               = \mathbf{X} \mathbf{W}^{(1)}\mathbf{W}^{(2)} + \mathbf{b}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)} \\
               = \mathbf{X} \mathbf{W} + \mathbf{b}.


* è§£å†³æ–¹æ³•ï¼šåœ¨æ¯ä¸ªéšè—å•å…ƒåå¼•å…¥éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰ï¼Œç ´é™¤çº¿æ€§é™åˆ¶ï¼Œä½¿æ¨¡å‹å…·å¤‡æ‹Ÿåˆå¤æ‚å‡½æ•°çš„èƒ½åŠ›ã€‚

.. math::

    \begin{aligned}
        \mathbf{H} & = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
        \mathbf{O} & = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\
    \end{aligned}

* ä¸ºäº†æ„å»ºæ›´é€šç”¨çš„ MLPï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­å †å è¿™æ ·çš„éšè—å±‚

.. math::

    \mathbf{H}^{(1)} = \sigma_1(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}) \\
    \mathbf{H}^{(2)} = \sigma_2(\mathbf{H}^{(1)} \mathbf{W}^{(2)} + \mathbf{b}^{(2)})

5.1.1.4. Universal Approximators
++++++++++++++++++++++++++++++++

* é€šç”¨é€¼è¿‘å®šç†ï¼šå³ä½¿åªæœ‰ä¸€ä¸ªéšè—å±‚ï¼Œåªè¦ç¥ç»å…ƒè¶³å¤Ÿå¤šï¼ŒMLPå¯ä»¥é€¼è¿‘ä»»æ„å¤æ‚å‡½æ•°ã€‚
* æ·±åº¦ç½‘ç»œçš„ä¼˜åŠ¿ï¼šå°½ç®¡å•éšè—å±‚èƒ½æ‹Ÿåˆä»»æ„å‡½æ•°ï¼Œä½†æ›´æ·±çš„ç½‘ç»œå¯ä»¥ç”¨æ›´å°‘çš„å‚æ•°è¡¨è¾¾å¤æ‚å‡½æ•°ï¼Œæ›´é«˜æ•ˆã€æ›´æ˜“è®­ç»ƒã€‚

* MLPæ˜¯è§£å†³å¤æ‚ä»»åŠ¡çš„åŸºçŸ³ï¼Œä½†è®¾è®¡æ·±åº¦å’Œå®½åº¦éœ€æƒè¡¡ã€‚
* å®é™…ä¸­ï¼ŒMLPä¸å·ç§¯ç½‘ç»œã€é€’å½’ç½‘ç»œç­‰ç»“åˆï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹è¡¨ç°ã€‚


5.1.2. Activation Functions
"""""""""""""""""""""""""""

* æ¿€æ´»å‡½æ•°é€šè¿‡è®¡ç®—åŠ æƒå’Œå¹¶è¿›ä¸€æ­¥æ·»åŠ åå·®æ¥å†³å®šæ˜¯å¦åº”è¯¥æ¿€æ´»ç¥ç»å…ƒã€‚
* å®ƒä»¬æ˜¯å¯å¾®åˆ†ç®—å­ï¼Œç”¨äºå°†è¾“å…¥ä¿¡å·è½¬æ¢ä¸ºè¾“å‡ºï¼Œä½†å¤§å¤šæ•°éƒ½å¢åŠ äº†éçº¿æ€§ã€‚


* 5.1.2.1. ReLU Function
* 5.1.2.2. Sigmoid Function
* 5.1.2.3. Tanh Function


5.2. Implementation of Multilayer Perceptrons
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

5.2.1. Implementation from Scratch
""""""""""""""""""""""""""""""""""

5.2.1.1. Initializing Model Parameters::

    class MLPScratch(d2l.Classifier):
        def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)
            self.b1 = nn.Parameter(torch.zeros(num_hiddens))
            self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)
            self.b2 = nn.Parameter(torch.zeros(num_outputs))

5.2.1.2. Model::

    def relu(X):
        a = torch.zeros_like(X)
        return torch.max(X, a)

    @d2l.add_to_class(MLPScratch)
    def forward(self, X):
        X = X.reshape((-1, self.num_inputs))
        H = relu(torch.matmul(X, self.W1) + self.b1)
        return torch.matmul(H, self.W2) + self.b2


5.2.1.3. Training::

    # è®­ç»ƒå¾ªç¯ä¸ softmax å›å½’å®Œå…¨ç›¸åŒ
    model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)
    data = d2l.FashionMNIST(batch_size=256)
    trainer = d2l.Trainer(max_epochs=10)
    trainer.fit(model, data)

5.2.2. Concise Implementation
"""""""""""""""""""""""""""""

5.2.2.1. Model::

    # å’Œä¹‹å‰çš„åŒºåˆ«æ˜¯è¿™å„¿æœ‰ä¸¤ä¸ªå…¨è¿æ¥å±‚(ç¬¬ä¸€ä¸ªéšè—å±‚ï¼Œç¬¬äºŒä¸ªè¾“å‡ºå±‚)
    class MLP(d2l.Classifier):
        def __init__(self, num_outputs, num_hiddens, lr):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.Flatten(), 
                nn.LazyLinear(num_hiddens),
                nn.ReLU(), 
                nn.LazyLinear(num_outputs)
            )


5.2.2.2. Training::

    # ä¸å®ç° softmax å›å½’æ—¶å®Œå…¨ç›¸åŒã€‚è¿™ç§æ¨¡å—åŒ–ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†æœ‰å…³æ¨¡å‹æ¶æ„çš„é—®é¢˜ä¸å…¶ä»–æ— å…³çš„å› ç´ åˆ†ç¦»å¼€æ¥
    model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)
    trainer.fit(model, data)


5.3. Forward Propagation, Backward Propagation, and Computational Graphs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

5.3.1. Forward Propagation
""""""""""""""""""""""""""

* å‰å‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒè®¡ç®—æ­¥éª¤ï¼Œå®ƒæŒ‡çš„æ˜¯æŒ‰ç…§ä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚çš„é¡ºåºï¼Œè®¡ç®—å’Œå­˜å‚¨ä¸­é—´å˜é‡ï¼ˆåŒ…æ‹¬è¾“å‡ºï¼‰ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯æŠŠè¾“å…¥æ•°æ®ä¸€å±‚å±‚åœ°ä¼ é€’ï¼Œæœ€ç»ˆå¾—åˆ°è¾“å‡ºç»“æœã€‚

1. è¾“å…¥ä¸æƒé‡çŸ©é˜µ
+++++++++++++++++

* å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥æ ·æœ¬ :math:`\mathbf{x} \in \mathbb{R}^d` ï¼Œè¡¨ç¤º d ç»´ç‰¹å¾çš„æ•°æ®ã€‚
* éšè—å±‚çš„æƒé‡çŸ©é˜µ :math:`\mathbf{W}^{(1)} \in \mathbb{R}^{h \times d}` å°†è¾“å…¥ :math:`\mathbf{x}` æ˜ å°„åˆ°éšè—å±‚ï¼š

.. math::

    \mathbf{z} = \mathbf{W}^{(1)}\mathbf{x}

* **z** æ˜¯éšè—å±‚çš„çº¿æ€§å˜æ¢ç»“æœï¼Œé•¿åº¦ä¸º hï¼Œè¡¨ç¤ºéšè—å±‚æœ‰ h ä¸ªç¥ç»å…ƒ

2. æ¿€æ´»å‡½æ•°ä¸éšè—å±‚è¾“å‡º
+++++++++++++++++++++++

* åº”ç”¨ä¸€ä¸ªæ¿€æ´»å‡½æ•° :math:`\phi` ï¼š

.. math::

    \mathbf{h} = \phi(\mathbf{z})


* **h** æ˜¯éšè—å±‚çš„æ¿€æ´»è¾“å‡ºï¼Œé•¿åº¦ä¸º h
* æ¿€æ´»å‡½æ•° :math:`\phi` å¼•å…¥äº†éçº¿æ€§ï¼Œç¡®ä¿æ¨¡å‹èƒ½å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»

3. è¾“å‡ºå±‚è®¡ç®—
+++++++++++++

* éšè—å±‚è¾“å‡º :math:`\mathbf{h}` å†æ¬¡ç»è¿‡è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µ :math:`\mathbf{W}^{(2)} \in \mathbb{R}^{q \times h}` å˜æ¢ï¼Œç”Ÿæˆæœ€ç»ˆè¾“å‡ºï¼š

.. math::

    \mathbf{o} = \mathbf{W}^{(2)}\mathbf{h}

* :math:`\mathbf{o}` æ˜¯è¾“å‡ºå±‚çš„ç»“æœï¼Œé•¿åº¦ä¸º qï¼Œè¡¨ç¤ºæœ‰ q ä¸ªè¾“å‡ºå•å…ƒã€‚

4. è®¡ç®—æŸå¤±
+++++++++++

è¾“å‡º :math:`\mathbf{o}` é€šè¿‡æŸå¤±å‡½æ•° l ä¸çœŸå®æ ‡ç­¾ y è®¡ç®—æŸå¤±ï¼š

.. math::

    L = l(\mathbf{o}, y)

* L æ˜¯å•ä¸ªæ ·æœ¬çš„æŸå¤±å€¼ï¼Œåæ˜ äº†æ¨¡å‹è¾“å‡ºä¸çœŸå®å€¼ä¹‹é—´çš„å·®è·ã€‚

5. æ­£åˆ™åŒ–é¡¹
+++++++++++

* ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ :math:`\ell_2` æ­£åˆ™åŒ–é¡¹ï¼š

.. math::

    s = \frac{\lambda}{2} \left( \|\mathbf{W}^{(1)}\|_F^2 + \|\mathbf{W}^{(2)}\|_F^2 \right)

* å…¶ä¸­ :math:`\lambda` æ˜¯æ­£åˆ™åŒ–å¼ºåº¦çš„è¶…å‚æ•°ã€‚
* :math:`\|\mathbf{W}^{(1)}\|_F^2` å’Œ :math:`\|\mathbf{W}^{(2)}\|_F^2` åˆ†åˆ«æ˜¯æƒé‡çŸ©é˜µçš„ Frobenius èŒƒæ•°ï¼Œç›¸å½“äºå°†çŸ©é˜µå±•å¹³åè®¡ç®— :math:`\ell_2` èŒƒæ•°ã€‚

6. ç›®æ ‡å‡½æ•°
+++++++++++

æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°æ˜¯ï¼š

.. math::

    ğ½ = ğ¿ + ğ‘ 

* J è¡¨ç¤ºå¸¦æ­£åˆ™åŒ–çš„æŸå¤±å‡½æ•°ï¼Œæ˜¯æ¨¡å‹éœ€è¦æœ€å°åŒ–çš„ç›®æ ‡
* åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¼˜åŒ–çš„æ˜¯ Jï¼Œä»¥ä¾¿åœ¨æŸå¤±ä¸æ¨¡å‹å¤æ‚åº¦ä¹‹é—´å–å¾—å¹³è¡¡


5.3.2. Computational Graph of Forward Propagation
"""""""""""""""""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/BvIxOt.png

    Fig. 5.3.1 Computational graph of forward propagation.


* å…¶ä¸­æ­£æ–¹å½¢è¡¨ç¤ºå˜é‡ï¼Œåœ†åœˆè¡¨ç¤ºè¿ç®—ç¬¦ã€‚
* å·¦ä¸‹è§’è¡¨ç¤ºè¾“å…¥ï¼Œå³ä¸Šè§’è¡¨ç¤º è¾“å‡ºã€‚



5.3.3. Backpropagation
""""""""""""""""""""""

* åå‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œä¸­ç”¨äºè®¡ç®—å‚æ•°æ¢¯åº¦çš„æ–¹æ³•ã€‚
* ç®€å•æ¥è¯´ï¼Œå®ƒæ˜¯é€šè¿‡ä»è¾“å‡ºå±‚å‘è¾“å…¥å±‚çš„åå‘éå†ï¼Œæ ¹æ®å¾®ç§¯åˆ†ä¸­çš„é“¾å¼æ³•åˆ™é€æ­¥è®¡ç®—æ¢¯åº¦çš„è¿‡ç¨‹ã€‚

1. é“¾å¼æ³•åˆ™
+++++++++++

.. math::

    \frac{\partial \mathsf{Z}}{\partial \mathsf{X}} = \textrm{prod}\left(\frac{\partial \mathsf{Z}}{\partial \mathsf{Y}}, \frac{\partial \mathsf{Y}}{\partial \mathsf{X}}\right)

2. åå‘ä¼ æ’­ç›®æ ‡
+++++++++++++++

* å‡è®¾ä¸€ä¸ªç®€å•çš„å•éšè—å±‚ç¥ç»ç½‘ç»œï¼Œå‚æ•°åŒ…æ‹¬ï¼š

.. math::

    éšè—å±‚æƒé‡çŸ©é˜µï¼š \mathbf{W}^{(1)} \\
    è¾“å‡ºå±‚æƒé‡çŸ©é˜µï¼š \mathbf{W}^{(2)} \\

    ç›®æ ‡æ˜¯è®¡ç®—æ¢¯åº¦ï¼š \\
    \partial J / \partial \mathbf{W}^{(1)} \\
    \partial J / \partial \mathbf{W}^{(2)}

3. é€æ­¥è®¡ç®—è¿‡ç¨‹
+++++++++++++++


1. è®¡ç®—ç›®æ ‡å‡½æ•°å¯¹æŸå¤±é¡¹å’Œæ­£åˆ™åŒ–é¡¹çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial L} = 1 \quad \text{å’Œ} \quad \frac{\partial J}{\partial s} = 1


2. è®¡ç®—å¯¹è¾“å‡ºå±‚å˜é‡ :math:`\mathbf{o}` çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial \mathbf{o}} = \frac{\partial L}{\partial \mathbf{o}} \in \mathbb{R}^q

3. è®¡ç®—æ­£åˆ™åŒ–é¡¹å¯¹å‚æ•°çš„æ¢¯åº¦

.. math::

    \frac{\partial s}{\partial \mathbf{W}^{(1)}} = \lambda \mathbf{W}^{(1)} \quad \text{å’Œ} \quad \frac{\partial s}{\partial \mathbf{W}^{(2)}} = \lambda \mathbf{W}^{(2)}


4. è®¡ç®—è¾“å‡ºå±‚å‚æ•° :math:`\mathbf{W}^{(2)}` çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial \mathbf{W}^{(2)}} = \frac{\partial J}{\partial \mathbf{o}} \mathbf{h}^\top + \lambda \mathbf{W}^{(2)}

- è¾“å‡ºå±‚æƒé‡æ¢¯åº¦ç­‰äº **æŸå¤±å¯¹è¾“å‡ºçš„æ¢¯åº¦** ä¹˜ä»¥ **éšè—å±‚æ¿€æ´»è¾“å‡º** ï¼Œå†åŠ ä¸Š **æ­£åˆ™åŒ–é¡¹**

5. è®¡ç®—éšè—å±‚è¾“å‡º :math:`\mathbf{h}` çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial \mathbf{h}} = {\mathbf{W}^{(2)}}^\top \frac{\partial J}{\partial \mathbf{o}}


6. è®¡ç®—éšè—å±‚æ¿€æ´»å‰å˜é‡ :math:`\mathbf{z}` çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial \mathbf{h}} \odot \phi'(\mathbf{z})

- è¿™é‡Œä½¿ç”¨é€å…ƒç´ ä¹˜æ³• :math:`\odot` ï¼Œè¡¨ç¤ºå¯¹æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚

7. è®¡ç®—è¾“å…¥å±‚å‚æ•° :math:`\mathbf{W}^{(1)}` çš„æ¢¯åº¦

.. math::

    \frac{\partial J}{\partial \mathbf{W}^{(1)}} = \frac{\partial J}{\partial \mathbf{z}} \mathbf{x}^\top + \lambda \mathbf{W}^{(1)}

- æœ€ç»ˆï¼Œè¾“å…¥å±‚æƒé‡çš„æ¢¯åº¦æ˜¯æŸå¤±å¯¹ :math:`\mathbf{z}`  çš„æ¢¯åº¦ä¹˜ä»¥è¾“å…¥ :math:`\mathbf{x}` ï¼Œå†åŠ ä¸Šæ­£åˆ™åŒ–é¡¹ã€‚

å°ç»“
++++

- åå‘ä¼ æ’­é€å±‚è®¡ç®—æ¢¯åº¦ï¼Œä»è¾“å‡ºå±‚åå‘å›æº¯åˆ°è¾“å…¥å±‚ã€‚  
- æ¯ä¸€æ­¥éƒ½åº”ç”¨é“¾å¼æ³•åˆ™ï¼Œå°†æŸå¤±é¡¹å’Œæ­£åˆ™åŒ–é¡¹å¯¹å‚æ•°çš„æ¢¯åº¦è¿›è¡Œç´¯ç§¯ã€‚  
- æœ€ç»ˆè®¡ç®—å‡ºçš„æ¢¯åº¦ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œé€æ­¥é™ä½ç›®æ ‡å‡½æ•° Jï¼Œå®ç°æ¨¡å‹ä¼˜åŒ–ã€‚


5.3.4. Training Neural Networks
"""""""""""""""""""""""""""""""

* è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ç›¸äº’ä¾èµ–ã€‚
* æ­£å‘ä¼ æ’­ï¼šæŒ‰è®¡ç®—å›¾çš„ä¾èµ–å…³ç³»ï¼Œä»è¾“å…¥å±‚å¼€å§‹ï¼Œä¸€ç›´åˆ°è¾“å‡ºå±‚ï¼Œè®¡ç®—å¹¶å­˜å‚¨ä¸­é—´å˜é‡ã€‚
* åå‘ä¼ æ’­ï¼šåœ¨åæ–¹å‘ä¸Šï¼Œä»è¾“å‡ºå±‚å›åˆ°è¾“å…¥å±‚ï¼Œåˆ©ç”¨æ­£å‘ä¼ æ’­ä¸­å­˜å‚¨çš„ä¸­é—´å˜é‡è®¡ç®—æ¢¯åº¦ã€‚


è®­ç»ƒè¿‡ç¨‹çš„äº¤æ›¿è¿›è¡Œ
++++++++++++++++++

* æ¨¡å‹åˆå§‹åŒ–ï¼šå…ˆåˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚
* äº¤æ›¿è¿›è¡Œï¼š
    * æ­£å‘ä¼ æ’­ï¼šè®¡ç®—è¾“å‡ºå’ŒæŸå¤±å‡½æ•°ï¼Œå­˜å‚¨ä¸­é—´å˜é‡ã€‚
    * åå‘ä¼ æ’­ï¼šåˆ©ç”¨å­˜å‚¨çš„ä¸­é—´å˜é‡ï¼Œè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡å‹å‚æ•°ã€‚
* å…³é”®ç‚¹ï¼šåå‘ä¼ æ’­é‡ç”¨æ­£å‘ä¼ æ’­å­˜å‚¨çš„ä¸­é—´å˜é‡ï¼Œé¿å…é‡å¤è®¡ç®—ã€‚


å†…å­˜å ç”¨
++++++++

* ä¸­é—´å˜é‡çš„ä¿ç•™ï¼šæ­£å‘ä¼ æ’­äº§ç”Ÿçš„ä¸­é—´å˜é‡éœ€è¦ä¿ç•™ï¼Œç›´åˆ°åå‘ä¼ æ’­å®Œæˆã€‚è¿™æ˜¯è®­ç»ƒé˜¶æ®µå†…å­˜å ç”¨è¾ƒé«˜çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚
* å†…å­˜æ¶ˆè€—çš„å½±å“å› ç´ ï¼š
    * ç½‘ç»œå±‚æ•°ï¼šå±‚æ•°è¶Šå¤šï¼Œä¸­é—´å˜é‡è¶Šå¤šï¼Œå ç”¨çš„å†…å­˜è¶Šå¤§ã€‚
    * æ‰¹å¤§å° (batch size)ï¼šæ‰¹é‡å¤§å°è¶Šå¤§ï¼Œä¸­é—´å˜é‡çš„æ•°é‡å’Œå¤§å°ä¹Ÿä¼šå¢åŠ ã€‚

.. note:: åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œä¸€æ—¦æ¨¡å‹å‚æ•°åˆå§‹åŒ–ï¼Œæˆ‘ä»¬å°±äº¤æ›¿å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œä½¿ç”¨åå‘ä¼ æ’­ç»™å‡ºçš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹å‚æ•°ã€‚è¯·æ³¨æ„ï¼Œåå‘ä¼ æ’­é‡ç”¨å‰å‘ä¼ æ’­ä¸­å­˜å‚¨çš„ä¸­é—´å€¼ä»¥é¿å…é‡å¤è®¡ç®—ã€‚ç»“æœä¹‹ä¸€æ˜¯æˆ‘ä»¬éœ€è¦ä¿ç•™ä¸­é—´å€¼ï¼Œç›´åˆ°åå‘ä¼ æ’­å®Œæˆã€‚è¿™ä¹Ÿæ˜¯è®­ç»ƒæ¯”æ™®é€šé¢„æµ‹éœ€è¦æ›´å¤šå†…å­˜çš„åŸå› ä¹‹ä¸€ã€‚

5.3.5. Summary
""""""""""""""

* å‰å‘ä¼ æ’­é¡ºåºè®¡ç®—å¹¶å­˜å‚¨ç¥ç»ç½‘ç»œå®šä¹‰çš„è®¡ç®—å›¾ä¸­çš„ä¸­é—´å˜é‡ã€‚å®ƒä»è¾“å…¥å±‚è¿›è¡Œåˆ°è¾“å‡ºå±‚ã€‚
* åå‘ä¼ æ’­ä»¥ç›¸åçš„é¡ºåºé¡ºåºè®¡ç®—å¹¶å­˜å‚¨ç¥ç»ç½‘ç»œå†…ä¸­é—´å˜é‡å’Œå‚æ•°çš„æ¢¯åº¦ã€‚
* åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­æ˜¯ç›¸äº’ä¾èµ–çš„ï¼Œå¹¶ä¸”è®­ç»ƒéœ€è¦çš„å†…å­˜æ˜æ˜¾å¤šäºé¢„æµ‹ã€‚


5.4. Numerical Stability and Initialization
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

5.4.1. Vanishing and Exploding Gradients
""""""""""""""""""""""""""""""""""""""""

* æ ¸å¿ƒé—®é¢˜ï¼šåœ¨æ·±åº¦ç¥ç»ç½‘ç»œä¸­ï¼Œéšç€ç½‘ç»œå±‚æ•°å¢åŠ ï¼Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ¢¯åº¦å¯èƒ½å˜å¾—éå¸¸å°ï¼ˆæ¶ˆå¤±ï¼‰æˆ–éå¸¸å¤§ï¼ˆçˆ†ç‚¸ï¼‰ã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹éš¾ä»¥æ”¶æ•›æˆ–å­¦ä¹ é€Ÿåº¦ææ…¢ã€‚

1. æ•°å­¦èƒŒæ™¯ä¸ç›´è§‚ç†è§£
+++++++++++++++++++++

* è€ƒè™‘ä¸€ä¸ªæ·±åº¦ç½‘ç»œ ``L layers`` ï¼Œè¾“å…¥ **x** å’Œè¾“å‡º **o** ã€‚
* With each layer l defined by a transformation :math:`f_l` parametrized by weights :math:`\mathbf{W}^{(l)}`
* å…¶éšè—å±‚è¾“å‡ºä¸º :math:`\mathbf{h}^{(l)}` ï¼ˆè®© :math:`\mathbf{h}^{(0)} = \mathbf{x}` ï¼‰ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå¯ä»¥è¡¨ç¤ºä¸ºï¼š

.. math::

    \mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \\
    å› æ­¤: \mathbf{o} = f_L \circ \cdots \circ f_1(\mathbf{x}) \\

    \mathbf{h}^{(l)} è¡¨ç¤ºç¬¬ $l$ å±‚çš„è¾“å‡ºï¼Œ$\mathbf{W}^{(l)}$ æ˜¯ç¬¬ $l$ å±‚çš„æƒé‡

* å¦‚æœæ‰€æœ‰éšè—å±‚çš„è¾“å‡ºå’Œè¾“å…¥éƒ½æ˜¯å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥å†™æˆ :math:`\mathbf{o}` ç›¸å¯¹äºä»»ä½•ä¸€ç»„å‚æ•° :math:`\mathbf{W}^{(l)}` çš„æ¢¯åº¦

.. math::

    \partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\textrm{def}}{=}} \cdots \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\textrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\textrm{def}}{=}}


ç›´è§‚ç†è§£::

    æ¢¯åº¦çˆ†ç‚¸ï¼šæ¢¯åº¦éå¸¸å¤§ï¼Œå‚æ•°æ›´æ–°å¤ªå‰§çƒˆï¼Œæ¨¡å‹æ— æ³•æ”¶æ•›ï¼Œç”šè‡³æŸåã€‚
    æ¢¯åº¦æ¶ˆå¤±ï¼šæ¢¯åº¦æ¥è¿‘é›¶ï¼Œå‚æ•°æ›´æ–°ç¼“æ…¢æˆ–ä¸æ›´æ–°ï¼Œå­¦ä¹ åœæ»ã€‚


2. æ¢¯åº¦æ¶ˆå¤±(Vanishing Gradient)
+++++++++++++++++++++++++++++++

* ä¸»è¦åŸå› ï¼šæ¿€æ´»å‡½æ•°çš„é€‰æ‹©ï¼Œå°¤å…¶æ˜¯ sigmoid å‡½æ•°ã€‚
    * sigmoid å‡½æ•°åœ¨è¾“å…¥å€¼éå¸¸å¤§æˆ–éå¸¸å°æ—¶ï¼Œå¯¼æ•°æ¥è¿‘é›¶ã€‚
    * å¯¼è‡´åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦å±‚å±‚ç›¸ä¹˜ï¼Œæœ€ç»ˆè¶‹è¿‘äºé›¶ã€‚

* è§£å†³æ–¹æ¡ˆï¼š
    * ä½¿ç”¨ ReLUï¼ˆRectified Linear Unitï¼‰æ¿€æ´»å‡½æ•°ï¼š
    * ReLU å‡½æ•°åœ¨æ­£åŒºé—´æ¢¯åº¦ä¸º 1ï¼Œåœ¨è´ŸåŒºé—´æ¢¯åº¦ä¸º 0ï¼Œé¿å…äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚


3. æ¢¯åº¦çˆ†ç‚¸(Exploding Gradient)
+++++++++++++++++++++++++++++++

* ä¸»è¦åŸå› ï¼šæƒé‡çŸ©é˜µ :math:`\mathbf{W}^{(l)}` çš„åˆå§‹å€¼ä¸åˆé€‚ï¼Œæˆ–è€…ç½‘ç»œå¤ªæ·±ã€‚
    * å±‚ä¸å±‚ä¹‹é—´æƒé‡çŸ©é˜µä¹˜ç§¯å¯èƒ½å¯¼è‡´æ¢¯åº¦æŒ‡æ•°çº§å¢é•¿ã€‚
    * æƒé‡çŸ©é˜µçš„ç‰¹å¾å€¼è¾ƒå¤§ï¼Œå¯¼è‡´æ•´ä½“æ¢¯åº¦çˆ†ç‚¸ã€‚

ç¤ºä¾‹-å¤šæ¬¡ç›¸ä¹˜åï¼ŒçŸ©é˜µå€¼çˆ†ç‚¸::

    M = torch.normal(0, 1, size=(4, 4))
    for i in range(100):
        M = M @ torch.normal(0, 1, size=(4, 4))
    print(M)
    # è¾“å‡º
    tensor([[-7.9222e+22, -1.1940e+23,  1.0915e+23,  1.0751e+23],
            [ 3.8837e+22,  5.8528e+22, -5.3505e+22, -5.2693e+22],
            [-1.9618e+22, -2.9577e+22,  2.7037e+22,  2.6641e+22],
            [ 3.0163e+22,  4.5455e+22, -4.1554e+22, -4.0923e+22]])

* è§£å†³æ–¹æ¡ˆï¼š
    * æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ï¼šè®¾ç½®æ¢¯åº¦é˜ˆå€¼ï¼Œè¶…è¿‡é˜ˆå€¼çš„æ¢¯åº¦ä¼šè¢«ç¼©æ”¾ã€‚
    * æƒé‡åˆå§‹åŒ–ï¼šä½¿ç”¨ Xavier åˆå§‹åŒ–æˆ– He åˆå§‹åŒ–ï¼Œä¿æŒåˆå§‹æ¢¯åº¦ç¨³å®šã€‚


4. å¯¹ç§°æ€§ç ´åé—®é¢˜(Breaking Symmetry)
++++++++++++++++++++++++++++++++++++

* é—®é¢˜æè¿°ï¼š
    * ç½‘ç»œåˆå§‹åŒ–æ—¶ï¼Œå¦‚æœæ‰€æœ‰æƒé‡ç›¸ç­‰ï¼Œéšè—å±‚ç¥ç»å…ƒå°†è¾“å‡ºç›¸åŒçš„å€¼ã€‚
    * è¿™ç§å¯¹ç§°æ€§ä¼šå¯¼è‡´ç½‘ç»œåªèƒ½å­¦ä¹ åˆ°æœ‰é™çš„ç‰¹å¾ï¼Œæµªè´¹ç½‘ç»œå®¹é‡ã€‚
* è§£å†³æ–¹æ¡ˆï¼š
    * åœ¨åˆå§‹åŒ–æ—¶ï¼Œå¼•å…¥éšæœºæ€§ï¼Œç¡®ä¿æƒé‡ä¸åŒã€‚
    * Dropout æ­£åˆ™åŒ–æ–¹æ³•å¯ä»¥å¸®åŠ©æ‰“ç ´å¯¹ç§°æ€§ï¼Œä½¿ä¸åŒç¥ç»å…ƒå­¦ä¹ ä¸åŒçš„ç‰¹å¾ã€‚


5.4.2. Parameter Initialization
"""""""""""""""""""""""""""""""

* è§£å†³ï¼ˆæˆ–è‡³å°‘å‡è½»ï¼‰ä¸Šè¿°é—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯å¯¹ç¥ç»ç½‘ç»œä¸­çš„å‚æ•°åˆå§‹åŒ–æ–¹æ³•çš„å¤„ç†ã€‚

5.4.2.1. Default Initialization
+++++++++++++++++++++++++++++++

* å¦‚æœä¸ç‰¹åˆ«æŒ‡å®šï¼Œæ·±åº¦å­¦ä¹ æ¡†æ¶ä¼šä½¿ç”¨é»˜è®¤çš„éšæœºåˆå§‹åŒ–ï¼Œé€šå¸¸æ˜¯ä»æ­£æ€åˆ†å¸ƒä¸­éšæœºæŠ½å–æƒé‡ã€‚è¿™ä¸ªæ–¹æ³•åœ¨ä¸­ç­‰è§„æ¨¡é—®é¢˜ä¸­æ•ˆæœä¸é”™ï¼Œä½†å¯¹äºéå¸¸æ·±çš„ç½‘ç»œå¯èƒ½ä¼šå¯¼è‡´æ¢¯åº¦é—®é¢˜ã€‚


5.4.2.2. Xavier Initialization
+++++++++++++++++++++++++++++++

* Xavieråˆå§‹åŒ–ï¼ˆæˆ–Glorotåˆå§‹åŒ–ï¼‰æ˜¯ä¸€ç§ä¸“é—¨è®¾è®¡æ¥ä¿æŒå‰å‘å’Œåå‘ä¼ æ’­ä¸­è¾“å‡ºçš„æ–¹å·®ç›¸å¯¹ç¨³å®šçš„æ–¹æ³•ã€‚
* ç›®æ ‡ï¼šé˜²æ­¢ä¿¡å·åœ¨å±‚ä¸å±‚ä¹‹é—´ä¼ é€’æ—¶é€æ¸æ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚
* æ–¹æ³•æ¨å¯¼ï¼š
    * å‡è®¾ä¸€ä¸ªæ— æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥å±‚ï¼Œè¾“å‡º :math:`o_i = \sum_{j=1}^{n_\text{in}} w_{ij} x_j`
    * å‡è®¾æƒé‡ :math:`w_{ij}` æœä»å‡å€¼ä¸º0ã€æ–¹å·®ä¸º :math:`\sigma^2` çš„åˆ†å¸ƒ
    * è¾“å…¥ :math:`x_j` ä¹Ÿå…·æœ‰å‡å€¼0ã€æ–¹å·® :math:`\gamma^2` ï¼Œä¸” :math:`x_j` å’Œ :math:`w_{ij}` ç›¸äº’ç‹¬ç«‹

è®¡ç®—è¾“å‡ºæ–¹å·®ï¼š

.. math::

    \begin{aligned}
        E[o_i] & = \sum_{j=1}^{n_\textrm{in}} E[w_{ij} x_j] \\
        &= \sum_{j=1}^{n_\textrm{in}} E[w_{ij}] E[x_j] \\
        &= 0
    \end{aligned}

and the variance:

.. math::

    \begin{aligned}
        \textrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
            & = \sum_{j=1}^{n_\textrm{in}} E[w^2_{ij} x^2_j] - 0 \\
            & = \sum_{j=1}^{n_\textrm{in}} E[w^2_{ij}] E[x^2_j] \\
            & = n_\textrm{in} \sigma^2 \gamma^2.
    \end{aligned}


* ä¸ºäº†ä¿æŒè¾“å‡ºçš„æ–¹å·®å›ºå®šï¼Œéœ€è¦æ»¡è¶³æ¡ä»¶ï¼š :math:`n_\textrm{in} \sigma^2 = 1`
* åœ¨åå‘ä¼ æ’­æ—¶ï¼Œç±»ä¼¼çš„æ–¹å·®æ¡ä»¶éœ€è¦æ»¡è¶³ï¼š :math:`n_\textrm{out} \sigma^2 = 1`
* ç”±äºæ— æ³•åŒæ—¶æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ï¼ŒXavieråˆå§‹åŒ–å–æŠ˜ä¸­ï¼š

.. math::

    \begin{aligned}
        \frac{1}{2} (n_\textrm{in} + n_\textrm{out}) \sigma^2 = 1 \\
        \textrm{ äºæ˜¯å¾—åˆ°ï¼š } \\
        \sigma = \sqrt{\frac{2}{n_\textrm{in} + n_\textrm{out}}}
    \end{aligned}

å®æ“
~~~~

å¦‚æœä»æ­£æ€åˆ†å¸ƒä¸­æŠ½å–æƒé‡ï¼š

.. math::

    \mathbb{N}\left(0, \frac{2}{n_\textrm{in}+n_\textrm{out}}\right)


å¦‚æœä½¿ç”¨å‡åŒ€åˆ†å¸ƒ :math:`U(-a, a)` åˆ™è¿™å„¿çš„ :math:`a` ä¸º

.. math::

    a = \sqrt{\frac{6}{n_\textrm{in}+n_\textrm{out}}}


* å®è·µæ•ˆæœ
    * å°½ç®¡æ¨å¯¼å‡è®¾æ²¡æœ‰ä½¿ç”¨æ¿€æ´»å‡½æ•°ï¼Œä½†åœ¨å®é™…ç½‘ç»œä¸­ï¼Œå³ä½¿æœ‰éçº¿æ€§æ¿€æ´»ï¼ŒXavieråˆå§‹åŒ–ä¾ç„¶è¡¨ç°è‰¯å¥½ã€‚
    * å®ƒè§£å†³äº†ä¸€éƒ¨åˆ†æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ï¼Œä½¿å¾—ç½‘ç»œæ›´å®¹æ˜“æ”¶æ•›ã€‚


5.4.3. Summary
""""""""""""""

* æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸æ˜¯æ·±åº¦ç½‘ç»œä¸­çš„å¸¸è§é—®é¢˜ã€‚
* åœ¨å‚æ•°åˆå§‹åŒ–æ—¶éœ€è¦éå¸¸å°å¿ƒï¼Œä»¥ç¡®ä¿æ¢¯åº¦å’Œå‚æ•°ä¿æŒè‰¯å¥½çš„æ§åˆ¶ã€‚éœ€è¦åˆå§‹åŒ–å¯å‘æ³•æ¥ç¡®ä¿åˆå§‹æ¢¯åº¦æ—¢ä¸å¤ªå¤§ä¹Ÿä¸å¤ªå°ã€‚éšæœºåˆå§‹åŒ–æ˜¯ç¡®ä¿ä¼˜åŒ–ä¹‹å‰æ‰“ç ´å¯¹ç§°æ€§çš„å…³é”®ã€‚ 
* Xavier åˆå§‹åŒ–è¡¨æ˜ï¼Œå¯¹äºæ¯ä¸€å±‚ï¼Œä»»ä½•è¾“å‡ºçš„æ–¹å·®ä¸å—è¾“å…¥æ•°é‡çš„å½±å“ï¼Œå¹¶ä¸”ä»»ä½•æ¢¯åº¦çš„æ–¹å·®ä¸å—è¾“å‡ºæ•°é‡çš„å½±å“ã€‚ ReLU æ¿€æ´»å‡½æ•°ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚è¿™å¯ä»¥åŠ é€Ÿæ”¶æ•›ã€‚


5.5. Generalization in Deep Learning
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

5.5.1. Revisiting Overfitting and Regularization
""""""""""""""""""""""""""""""""""""""""""""""""

å¼•å…¥èƒŒæ™¯ä¸ä¼ ç»Ÿè®¤çŸ¥
++++++++++++++++++

* â€œæ— å…è´¹åˆé¤å®šç†â€ï¼ˆno free lunch theoremï¼‰
    * å¼ºè°ƒæ‰€æœ‰å­¦ä¹ ç®—æ³•åœ¨æŸäº›åˆ†å¸ƒä¸Šè¡¨ç°æ›´å¥½ï¼Œè€Œåœ¨å…¶ä»–åˆ†å¸ƒä¸Šè¡¨ç°æ›´å·®ã€‚
    * æ ¹æ®Wolpertï¼ˆ1995ï¼‰æå‡ºçš„è¿™ä¸€ç†è®ºï¼Œä»»ä½•å­¦ä¹ ç®—æ³•åœ¨æŸäº›æ•°æ®åˆ†å¸ƒä¸Šä¼šè¡¨ç°å¾—æ›´å¥½ï¼Œåœ¨å…¶ä»–åˆ†å¸ƒä¸Šåˆ™å¯èƒ½æ›´å·®ã€‚
    * è¿™æ„å‘³ç€å¯¹äºæœ‰é™çš„è®­ç»ƒé›†ï¼Œæ¨¡å‹éœ€è¦ä¾èµ–äºä¸€äº›å‡è®¾æ¥è¾¾åˆ°äººç±»çº§åˆ«çš„æ€§èƒ½ï¼Œè€Œè¿™äº›å‡è®¾è¢«ç§°ä¸ºå½’çº³åå¥½ã€‚

* å½’çº³åç½®ï¼ˆinductive biasesï¼‰
    * å®ƒæŒ‡çš„æ˜¯æ¨¡å‹å¯¹å…·æœ‰æŸäº›ç‰¹æ€§çš„è§£å†³æ–¹æ¡ˆçš„åå¥½ã€‚ä¾‹å¦‚ï¼Œæ·±å±‚å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰å€¾å‘äºé€šè¿‡ç»„åˆç®€å•å‡½æ•°æ¥æ„å»ºå¤æ‚å‡½æ•°ã€‚
    * ä¸ºäº†å¼¥è¡¥æœ‰é™è®­ç»ƒæ•°æ®çš„å±€é™æ€§ï¼‰ï¼Œå³ç±»ä¼¼äººç±»å¯¹ä¸–ç•Œçš„æ€è€ƒæ–¹å¼ï¼Œä»è€Œåå‘å…·æœ‰ç‰¹å®šæ€§è´¨çš„è§£å†³æ–¹æ¡ˆã€‚

* ä¸¤é˜¶æ®µè®­ç»ƒ
    * é¦–å…ˆæ˜¯ä½¿æ¨¡å‹å°½å¯èƒ½å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå…¶æ¬¡æ˜¯åœ¨ä¿ç•™çš„æµ‹è¯•æ•°æ®é›†ä¸Šä¼°è®¡æ³›åŒ–è¯¯å·®ã€‚
    * æ³›åŒ–å·®è·æ˜¯æŒ‡è®­ç»ƒè¯¯å·®ä¸æµ‹è¯•è¯¯å·®ä¹‹é—´çš„å·®å¼‚ï¼›å½“è¿™ä¸ªå·®è·è¾ƒå¤§æ—¶ï¼Œè¡¨ç¤ºæ¨¡å‹è¿‡æ‹Ÿåˆäº†è®­ç»ƒæ•°æ®ã€‚

* è¿‡æ‹Ÿåˆ
    * ç»å…¸è§‚ç‚¹ï¼šå¦‚æœæ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆã€‚æ­¤æ—¶å¯ä»¥é€šè¿‡å‡å°‘ç‰¹å¾æ•°é‡ã€éé›¶å‚æ•°çš„æ•°é‡æˆ–å‚æ•°å¤§å°æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚
    * è¿‡æ‹Ÿåˆæ˜¯è®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®ä¹‹é—´çš„å·®è·ï¼Œå½“æ¨¡å‹å¤æ‚åº¦è¿‡é«˜æ—¶ï¼Œå®¹æ˜“å‘ç”Ÿè¿‡æ‹Ÿåˆã€‚


æ·±åº¦å­¦ä¹ çš„åå¸¸ç°è±¡
++++++++++++++++++

* æ·±åº¦å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆï¼š
    * ä¸åŒäºç»å…¸çš„è§‚ç‚¹ï¼Œæ·±åº¦å­¦ä¹ æ‰“ç ´äº†ç»å…¸çš„â€œæ¨¡å‹å¤æ‚åº¦ vs. è¯¯å·®â€çš„ç®€å•å…³ç³»ã€‚æ·±åº¦å­¦ä¹ æ¨¡å‹å¾€å¾€è¶³å¤Ÿè¡¨è¾¾åŠ›å¼ºï¼Œä»¥è‡³äºå¯ä»¥å®Œç¾æ‹Ÿåˆæ¯ä¸ªè®­ç»ƒæ ·æœ¬ã€‚
    * å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬ä»å¯ä»¥é€šè¿‡å¢åŠ æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼ˆå¦‚æ·»åŠ å±‚æ•°ã€èŠ‚ç‚¹æ•°æˆ–å»¶é•¿è®­ç»ƒå‘¨æœŸï¼‰æ¥å‡å°‘æ³›åŒ–è¯¯å·®ï¼Œè¿™ä¸ä¼ ç»Ÿè®¤çŸ¥ç›¸æ‚–ã€‚

* åç›´è§‰ç°è±¡
    * ä¼ ç»Ÿè§‚ç‚¹è®¤ä¸ºï¼Œæ¨¡å‹åœ¨å¤æ‚åº¦è½´çš„æç«¯ä½ç½®æ—¶ï¼Œæ³›åŒ–è¯¯å·®ä¼šå¢åŠ ï¼Œå› æ­¤éœ€è¦é€šè¿‡æ­£åˆ™åŒ–æˆ–é™ä½æ¨¡å‹å¤æ‚åº¦æ¥å‡å°‘è¿‡æ‹Ÿåˆã€‚
    * åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå³ä½¿æ¨¡å‹å®Œå…¨æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¢åŠ æ¨¡å‹å¤æ‚åº¦ï¼ˆå¦‚å¢åŠ å±‚æ•°æˆ–èŠ‚ç‚¹ï¼‰åè€Œå¯èƒ½å‡å°‘æ³›åŒ–è¯¯å·®ã€‚
    * è¿™ç§ç°è±¡è¢«ç§°ä¸ºåŒé‡ä¸‹é™ï¼ˆdouble descentï¼‰ã€‚
    * åŒé‡ä¸‹é™ç°è±¡ï¼šéšç€æ¨¡å‹å¤æ‚åº¦çš„å¢åŠ ï¼Œæ³›åŒ–å·®è·èµ·åˆä¼šå¢å¤§ï¼Œä½†ä¹‹ååˆä¼šå‡å°ã€‚è¿™ç§ç°è±¡è¡¨æ˜ï¼Œæ¨¡å‹å¤æ‚åº¦ä¸æ³›åŒ–æ€§èƒ½ä¹‹é—´çš„å…³ç³»å¹¶éå•è°ƒã€‚
    * æ·±åº¦å­¦ä¹ å®è·µè€…çš„å·¥å…·åŒ…ï¼šåŒ…æ‹¬ä¸€äº›çœ‹ä¼¼é™åˆ¶æ¨¡å‹çš„æ–¹æ³•ï¼ˆå¦‚æ­£åˆ™åŒ–ï¼‰ï¼Œä»¥åŠä¸€äº›çœ‹ä¼¼å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›çš„æ–¹æ³•ï¼Œæ‰€æœ‰è¿™äº›éƒ½æ˜¯ä¸ºäº†å‡è½»è¿‡æ‹Ÿåˆçš„é—®é¢˜ã€‚


æŒ‘æˆ˜ä¼ ç»Ÿç†è®º
++++++++++++

* æ·±åº¦å­¦ä¹ çš„æˆåŠŸä½¿å¾—ä¼ ç»Ÿå­¦ä¹ ç†è®ºéš¾ä»¥è§£é‡Šå…¶æ³›åŒ–èƒ½åŠ›ã€‚å°½ç®¡å¯ä»¥ä½¿ç”¨ :math:`\ell_2` æ­£åˆ™åŒ–ç­‰æ–¹æ³•ä¼˜åŒ–ä½¿ç”¨ï¼Œä½†ä¼ ç»Ÿå¤æ‚åº¦åº¦é‡ï¼ˆå¦‚VCç»´æˆ–Rademacherå¤æ‚åº¦ï¼‰ä»æ— æ³•æœ‰æ•ˆè§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œä¸ºä½•å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§èƒ½ã€‚
* å…³é”®çŸ›ç›¾åœ¨äºï¼Œç¥ç»ç½‘ç»œå³ä½¿èƒ½å¤Ÿæ‹Ÿåˆä»»æ„æ ‡ç­¾æ•°æ®ï¼Œå®é™…æµ‹è¯•è¯¯å·®ä¾ç„¶å¯èƒ½è¾ƒä½ï¼Œè¯´æ˜ç°æœ‰ç†è®ºå­˜åœ¨å±€é™æ€§ã€‚
* è¿™è¡¨æ˜ï¼Œå¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦æ–°çš„ç†è®ºæ¡†æ¶æ¥ç†è§£å…¶æ³›åŒ–èƒ½åŠ›ã€‚


å…³é”®è¯è§£æ
++++++++++

* Inductive Biasï¼šæ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­åå‘äºç‰¹å®šç±»å‹çš„è§£å†³æ–¹æ¡ˆæˆ–å‡è®¾ï¼Œæœ‰åŠ©äºæé«˜æ³›åŒ–èƒ½åŠ›ã€‚
* Generalization Gapï¼šè®­ç»ƒè¯¯å·®ä¸æµ‹è¯•è¯¯å·®ä¹‹é—´çš„å·®è·ã€‚
* Double Descentï¼šæ¨¡å‹å¤æ‚åº¦å¢åŠ æ—¶ï¼Œè¯¯å·®å…ˆä¸‹é™å†ä¸Šå‡ï¼Œéšåå†æ¬¡ä¸‹é™çš„ç°è±¡ï¼Œæ‰“ç ´äº†ç»å…¸â€œUå‹â€è¯¯å·®æ›²çº¿çš„æ¦‚å¿µã€‚
* VC Dimension/Rademacher Complexityï¼šè¡¡é‡æ¨¡å‹å¤æ‚åº¦çš„ç»å…¸ç†è®ºå·¥å…·ï¼Œåœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸé¢ä¸´è§£é‡Šèƒ½åŠ›ä¸è¶³çš„é—®é¢˜ã€‚


5.5.2. Inspiration from Nonparametrics
""""""""""""""""""""""""""""""""""""""

* æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¸éå‚æ•°æ¨¡å‹ï¼ˆnonparametric modelsï¼‰ä¹‹é—´çš„å…³ç³»ï¼ŒæŒ‘æˆ˜äº†å°†æ·±åº¦ç¥ç»ç½‘ç»œä»…è§†ä¸ºå‚æ•°åŒ–æ¨¡å‹çš„ç›´è§‰ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•ä»éå‚æ•°è§†è§’æ¥çœ‹å¾…ç¥ç»ç½‘ç»œçš„è¡Œä¸ºã€‚

æ·±åº¦å­¦ä¹ çš„å‚æ•°åŒ–ä¸éå‚æ•°åŒ–å¯¹æ¯”
++++++++++++++++++++++++++++++

* æ·±åº¦å­¦ä¹ æ¨¡å‹æ‹¥æœ‰å¤§é‡çš„å‚æ•°ï¼Œå› æ­¤ç›´è§‚ä¸Šçœ‹ï¼Œå®ƒä»¬æ˜¯å‚æ•°åŒ–æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°ä¸æ–­æ›´æ–°ï¼Œä¿å­˜æ—¶ä¹Ÿå†™å…¥å‚æ•°ã€‚
* ç„¶è€Œï¼Œæ–‡æœ¬æŒ‡å‡ºï¼Œå°½ç®¡ç¥ç»ç½‘ç»œæœ‰å¤§é‡å‚æ•°ï¼Œä»æŸäº›è§’åº¦æ¥çœ‹ï¼Œå®ƒä»¬çš„è¡Œä¸ºæ›´åƒæ˜¯éå‚æ•°æ¨¡å‹ã€‚è¿™ç§æ€ç»´æ–¹å¼å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£ç¥ç»ç½‘ç»œçš„æ³›åŒ–èƒ½åŠ›å’Œè®­ç»ƒæœºåˆ¶ã€‚


éå‚æ•°æ¨¡å‹çš„å®šä¹‰
++++++++++++++++

* éå‚æ•°æ¨¡å‹çš„å¤æ‚åº¦é€šå¸¸éšç€æ•°æ®é‡çš„å¢åŠ è€Œå¢åŠ ã€‚ç»å…¸çš„éå‚æ•°æ¨¡å‹ä¾‹å­æ˜¯ **k-æœ€è¿‘é‚»ç®—æ³•** ï¼ˆk-nearest neighborï¼ŒKNNï¼‰ã€‚
* KNNæ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µåªæ˜¯è®°ä½æ•°æ®é›†ï¼Œè€Œåœ¨é¢„æµ‹æ—¶ï¼Œé€šè¿‡å¯»æ‰¾æœ€æ¥è¿‘çš„è®­ç»ƒç‚¹æ¥è¿›è¡Œåˆ†ç±»æˆ–å›å½’ã€‚
* å½“ ``k=1`` æ—¶ï¼ŒKNNæ¨¡å‹å¯ä»¥å®ç°é›¶è®­ç»ƒè¯¯å·®ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€å®ƒæ²¡æœ‰æ³›åŒ–èƒ½åŠ›ã€‚äº‹å®ä¸Šï¼Œåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼Œ ``1-æœ€è¿‘é‚»ç®—æ³•`` ä¼šéšç€æ•°æ®é‡çš„å¢åŠ è€Œæ”¶æ•›åˆ°æœ€ä¼˜é¢„æµ‹å™¨ã€‚


åº¦é‡å‡½æ•°å’Œå½’çº³åç½®
++++++++++++++++++

* 1-æœ€è¿‘é‚»ç®—æ³•çš„å…³é”®æ˜¯è·ç¦»å‡½æ•°ï¼ˆdistance functionï¼‰ï¼Œä¹Ÿå³å¦‚ä½•å°†æ•°æ®è½¬æ¢ä¸ºç‰¹å¾å‘é‡ï¼ˆfeaturizing dataï¼‰ã€‚
* ä¸åŒçš„è·ç¦»åº¦é‡ä»£è¡¨ä¸åŒçš„å½’çº³åç½®ï¼Œå³å¯¹æ•°æ®åº•å±‚ç»“æ„çš„å‡è®¾ã€‚é€‰æ‹©ä¸åŒçš„åº¦é‡æ–¹å¼å°†å½±å“æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚


ç¥ç»ç½‘ç»œçš„â€œéå‚æ•°æ€§â€
++++++++++++++++++++

* ç¥ç»ç½‘ç»œçš„ç‰¹ç‚¹æ˜¯è¿‡åº¦å‚æ•°åŒ–ï¼ˆover-parameterizedï¼‰ï¼Œå³æ‹¥æœ‰è¿œå¤šäºè®­ç»ƒæ•°æ®æ‰€éœ€çš„å‚æ•°ã€‚ç”±äºè¿‡åº¦å‚æ•°åŒ–ï¼Œç¥ç»ç½‘ç»œåœ¨è®­ç»ƒæ•°æ®ä¸Šå¸¸å¸¸èƒ½å¤Ÿå®Œç¾æ‹Ÿåˆï¼ˆinterpolateï¼‰ï¼Œè¿™ç§è¡Œä¸ºä¸éå‚æ•°æ¨¡å‹ç›¸ä¼¼ã€‚
* æ·±åº¦å­¦ä¹ çš„æœ€æ–°ç†è®ºç ”ç©¶è¡¨æ˜ï¼Œå¤§å‹ç¥ç»ç½‘ç»œä¸éå‚æ•°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯æ ¸æ–¹æ³•ï¼ˆkernel methodsï¼‰ï¼Œä¹‹é—´æœ‰æ·±åˆ»çš„è”ç³»ã€‚
* å…·ä½“æ¥è¯´ï¼Œç¥ç»åˆ‡çº¿æ ¸ï¼ˆneural tangent kernelï¼‰ç†è®ºè¡¨æ˜ï¼Œå½“å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰çš„å®½åº¦è¶‹äºæ— ç©·å¤§æ—¶ï¼Œå®ƒä»¬çš„è¡Œä¸ºè¶‹è¿‘äºéå‚æ•°çš„æ ¸æ–¹æ³•ã€‚

ç¥ç»åˆ‡çº¿æ ¸ç†è®º
++++++++++++++

* ç¥ç»åˆ‡çº¿æ ¸ï¼ˆneural tangent kernelï¼ŒNTKï¼‰æ˜¯ä¸€ç§ç‰¹æ®Šçš„æ ¸å‡½æ•°ï¼Œç”¨æ¥æè¿°æ·±åº¦ç¥ç»ç½‘ç»œçš„è¡Œä¸ºã€‚
* å°½ç®¡å½“å‰çš„NTKæ¨¡å‹å¯èƒ½ä¸èƒ½å®Œå…¨è§£é‡Šç°ä»£æ·±åº¦ç½‘ç»œçš„è¡Œä¸ºï¼Œä½†å®ƒä¸ºåˆ†æè¿‡åº¦å‚æ•°åŒ–çš„æ·±åº¦ç¥ç»ç½‘ç»œæä¾›äº†æœ‰åŠ›çš„å·¥å…·ï¼Œå¹¶å¼ºè°ƒäº†éå‚æ•°å»ºæ¨¡åœ¨ç†è§£æ·±åº¦ç½‘ç»œè¡Œä¸ºä¸­çš„é‡è¦æ€§ã€‚

ç»“è®º
++++

* é€šè¿‡å¯¹æ¯”ä¼ ç»Ÿçš„å‚æ•°åŒ–å’Œéå‚æ•°åŒ–æ¨¡å‹ï¼Œå¼ºè°ƒäº†ç¥ç»ç½‘ç»œåœ¨æŸäº›æ–¹é¢è¡¨ç°å¾—åƒéå‚æ•°æ¨¡å‹ã€‚
* å°½ç®¡ç¥ç»ç½‘ç»œæœ‰å¤§é‡å‚æ•°ï¼Œä½†å…¶è¿‡åº¦å‚æ•°åŒ–çš„ç‰¹æ€§ä½¿å…¶åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„æ‹Ÿåˆæ–¹å¼ä¸éå‚æ•°æ–¹æ³•ç±»ä¼¼ï¼Œå°¤å…¶æ˜¯åœ¨è®­ç»ƒæ•°æ®é‡å¢å¤§æ—¶ã€‚é€šè¿‡ç¥ç»åˆ‡çº¿æ ¸çš„ç†è®ºï¼Œç ”ç©¶è¡¨æ˜ç¥ç»ç½‘ç»œä¸æ ¸æ–¹æ³•ä¹‹é—´æœ‰ç€æ·±åˆ»çš„è”ç³»ï¼Œè¿™ä¸€ç†è®ºä¸ºç†è§£ç°ä»£æ·±åº¦å­¦ä¹ æ¨¡å‹æä¾›äº†æ–°çš„è§†è§’ã€‚

5.5.3. Early Stopping
"""""""""""""""""""""

* æœ¬æ–‡æ¢è®¨äº† **æ—©åœæ³•ï¼ˆEarly Stoppingï¼‰** åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„ä½œç”¨åŠå…¶åœ¨å¤„ç†æ ‡ç­¾å™ªå£°ï¼ˆlabel noiseï¼‰é—®é¢˜ä¸Šçš„é‡è¦æ€§ã€‚


æ—©åœæ³•çš„åŠ¨æœºä¸èƒŒæ™¯
++++++++++++++++++

* æ·±åº¦ç¥ç»ç½‘ç»œå…·å¤‡æ‹Ÿåˆä»»æ„æ ‡ç­¾çš„èƒ½åŠ›ï¼Œå³ä½¿æ ‡ç­¾æ˜¯éšæœºåˆ†é…æˆ–é”™è¯¯çš„ã€‚ç„¶è€Œï¼Œè¿™ç§æ‹Ÿåˆèƒ½åŠ›é€šå¸¸éœ€è¦å¤šæ¬¡è®­ç»ƒè¿­ä»£åæ‰ä¼šæ˜¾ç°ã€‚
* ç ”ç©¶å‘ç°ï¼Œç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆæ‹Ÿåˆå¹²å‡€çš„æ ‡ç­¾æ•°æ®ï¼Œéšåé€æ­¥æ‹Ÿåˆå¸¦å™ªå£°çš„æ ‡ç­¾æ•°æ®ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœè®­ç»ƒåœ¨æ¨¡å‹æ‹Ÿåˆå¹²å‡€æ•°æ®ååœæ­¢ï¼Œæ¨¡å‹ä»èƒ½ä¿æŒè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚
* å½“æ¨¡å‹åªæ‹Ÿåˆå¹²å‡€æ•°æ®ï¼Œè€Œæœªå®Œå…¨æ‹Ÿåˆéšæœºæ ‡ç­¾æ—¶ï¼Œå®é™…ä¸Šå¯ä»¥ç¡®ä¿æ¨¡å‹å…·å¤‡è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚


æ—©åœæ³•çš„æœºåˆ¶
++++++++++++

* æ—©åœæ³•æ˜¯ä¸€ç§ç»å…¸çš„æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œä¸ç›´æ¥çº¦æŸæƒé‡å€¼ä¸åŒï¼Œå®ƒé€šè¿‡ **é™åˆ¶è®­ç»ƒçš„è¿­ä»£æ¬¡æ•°ï¼ˆepochï¼‰** æ¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚
* æ—©åœçš„å…¸å‹æ–¹æ³•æ˜¯ç›‘æ§éªŒè¯é›†è¯¯å·®ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªepochç»“æŸåè¯„ä¼°ä¸€æ¬¡éªŒè¯é›†è¯¯å·®ã€‚å½“éªŒè¯è¯¯å·®åœ¨è¿ç»­å¤šä¸ªepochå†…æœªæ˜¾è‘—å‡å°‘ï¼ˆå‡å°‘å¹…åº¦å°äº :math:`epsilon` ï¼‰ï¼Œè®­ç»ƒå°±ä¼šæå‰ç»ˆæ­¢ã€‚è¿™ç§ç­–ç•¥è¢«ç§°ä¸ºè€å¿ƒå‡†åˆ™ï¼ˆpatience criterionï¼‰ã€‚


æ—©åœæ³•çš„ä¼˜ç‚¹
++++++++++++

* æé«˜æ³›åŒ–èƒ½åŠ›ï¼šç‰¹åˆ«æ˜¯åœ¨æ ‡ç­¾å­˜åœ¨å™ªå£°æˆ–æ ‡ç­¾å›ºæœ‰ä¸ç¡®å®šæ€§çš„åœºæ™¯ä¸‹ï¼Œæ—©åœèƒ½é˜²æ­¢æ¨¡å‹è¿‡åº¦æ‹Ÿåˆå¸¦å™ªå£°æ•°æ®ï¼Œè¿›è€Œæå‡æ³›åŒ–èƒ½åŠ›ã€‚
* èŠ‚çº¦è®¡ç®—èµ„æºï¼šæ—©åœå¯ä»¥æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´ã€‚å¯¹äºå¤§å‹æ¨¡å‹ï¼ˆå¦‚GPTç­‰ï¼‰ï¼Œè®­ç»ƒå¯èƒ½éœ€è¦æ•°å¤©ä¸”æ¶ˆè€—å¤§é‡GPUèµ„æºï¼Œæ—©åœèƒ½å¤ŸèŠ‚çœå¤§é‡è®¡ç®—æˆæœ¬ã€‚

é€‚ç”¨åœºæ™¯
++++++++

* æ ‡ç­¾å­˜åœ¨å™ªå£°æˆ–ä¸ç¡®å®šæ€§ï¼šä¾‹å¦‚åŒ»ç–—é¢†åŸŸçš„æ­»äº¡ç‡é¢„æµ‹ï¼Œæ‚£è€…æ•°æ®é€šå¸¸å¸¦æœ‰ä¸ç¡®å®šæ€§å’Œå™ªå£°ï¼Œæ—©åœå°¤ä¸ºå…³é”®ã€‚
* çœŸå®å¯åˆ†æ•°æ®é›†ï¼ˆrealizable datasetsï¼‰ï¼šä¾‹å¦‚åŒºåˆ†çŒ«å’Œç‹—çš„ä»»åŠ¡ï¼Œå¦‚æœæ•°æ®é›†æ— æ ‡ç­¾å™ªå£°ä¸”ç±»åˆ«å®Œå…¨å¯åˆ†ï¼Œæ—©åœå¯¹æ³›åŒ–èƒ½åŠ›çš„æå‡ **ä¸æ˜¾è‘—** ã€‚
* é”™è¯¯åšæ³•ï¼šå¦‚æœç»§ç»­è®­ç»ƒç›´åˆ°æ¨¡å‹å®Œå…¨æ‹Ÿåˆå¸¦å™ªå£°çš„æ•°æ®ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼Œè¡¨ç°å‡ºè¾ƒé«˜çš„æµ‹è¯•è¯¯å·®ã€‚


å…³é”®è¯è§£æ
++++++++++

* Label Noiseï¼ˆæ ‡ç­¾å™ªå£°ï¼‰ï¼šè®­ç»ƒæ•°æ®ä¸­å­˜åœ¨é”™è¯¯æˆ–éšæœºåˆ†é…çš„æ ‡ç­¾ï¼Œå¸¸è§äºçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸­ã€‚
* Generalizationï¼ˆæ³›åŒ–ï¼‰ï¼šæ¨¡å‹åœ¨æœªè§è¿‡çš„æ–°æ•°æ®ä¸Šçš„è¡¨ç°èƒ½åŠ›ã€‚
* Patience Criterionï¼ˆè€å¿ƒå‡†åˆ™ï¼‰ï¼šåœ¨éªŒè¯è¯¯å·®å¤šæ¬¡ä¸é™ä½ååœæ­¢è®­ç»ƒçš„ç­–ç•¥ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹åœ¨å¸¦å™ªå£°æ•°æ®ä¸Šç»§ç»­æ‹Ÿåˆã€‚

ç»“è®º
++++

* æ—©åœæ³•æ˜¯åº”å¯¹æ·±åº¦å­¦ä¹ ä¸­è¿‡æ‹Ÿåˆé—®é¢˜çš„é‡è¦æ‰‹æ®µï¼Œå°¤å…¶åœ¨æ ‡ç­¾å™ªå£°å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆæå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼ŒåŒæ—¶é™ä½è®­ç»ƒæ—¶é—´å’Œæˆæœ¬ã€‚
* è™½ç„¶åœ¨ç†æƒ³ã€æ— å™ªå£°çš„æ•°æ®é›†ä¸Šæ•ˆæœæœ‰é™ï¼Œä½†åœ¨ç°å®ä¸­ï¼Œæ ‡ç­¾å™ªå£°å’Œæ•°æ®çš„ä¸ç¡®å®šæ€§ä½¿å¾—æ—©åœæ³•æˆä¸ºæ·±åº¦å­¦ä¹ è®­ç»ƒçš„å¸¸è§æŠ€å·§ä¹‹ä¸€ã€‚

5.5.4. Classical Regularization Methods for Deep Networks
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""

* æœ¬èŠ‚æ¢è®¨äº†æ·±åº¦å­¦ä¹ ä¸­çš„ç»å…¸æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä¸»è¦å›´ç»• **æƒé‡è¡°å‡ï¼ˆweight decayï¼‰** å’Œæ­£åˆ™åŒ–æŠ€æœ¯åœ¨é˜²æ­¢è¿‡æ‹Ÿåˆä¸­çš„ä½œç”¨å’Œå±€é™æ€§ã€‚


ç»å…¸æ­£åˆ™åŒ–æ–¹æ³•çš„å›é¡¾
++++++++++++++++++++

* åœ¨ä¼ ç»Ÿæœºå™¨å­¦ä¹ ä¸­ï¼Œæ­£åˆ™åŒ–é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒ©ç½šé¡¹æ¥é™åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œä»è€Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚
* ä¸»è¦æ–¹æ³•åŒ…æ‹¬ï¼š
    * å²­å›å½’ï¼ˆridge regularizationï¼‰ï¼šæƒ©ç½š :math:`\ell_2` èŒƒæ•°ï¼Œé™åˆ¶æƒé‡çš„å¹³æ–¹å’Œã€‚
    * å¥—ç´¢å›å½’ï¼ˆlasso regularizationï¼‰ï¼šæƒ©ç½š :math:`\ell_1` èŒƒæ•°ï¼Œä½¿éƒ¨åˆ†æƒé‡è¶‹äºé›¶ï¼Œä¿ƒè¿›ç¨€ç–æ€§ã€‚
* è¿™äº›æ–¹æ³•é€šå¸¸è¶³å¤Ÿå¼ºå¤§ï¼Œå¯ä»¥é˜²æ­¢æ¨¡å‹æ‹Ÿåˆéšæœºæ ‡ç­¾ï¼ˆå³é˜²æ­¢æ¨¡å‹å­¦åˆ°å™ªå£°ï¼‰ã€‚


æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨ä¸æŒ‘æˆ˜
++++++++++++++++++++++

* åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæƒé‡è¡°å‡ä¾ç„¶æ˜¯æµè¡Œçš„æ­£åˆ™åŒ–å·¥å…·ã€‚ç„¶è€Œï¼Œç ”ç©¶è¡¨æ˜ï¼š
    * å…¸å‹çš„ :math:`\ell_2` æ­£åˆ™åŒ–å¼ºåº¦ä¸è¶³ï¼Œæ— æ³•å®Œå…¨é˜²æ­¢ç¥ç»ç½‘ç»œå¯¹è®­ç»ƒæ•°æ®çš„æ’å€¼ï¼ˆå³å®Œå…¨æ‹Ÿåˆè®­ç»ƒé›†ï¼Œç”šè‡³æ‹Ÿåˆå™ªå£°æ ‡ç­¾ï¼‰ã€‚
    * æ¢å¥è¯è¯´ï¼Œå•ç‹¬ä¾é æƒé‡è¡°å‡å¯èƒ½æ— æ³•æœ‰æ•ˆæŠ‘åˆ¶è¿‡æ‹Ÿåˆã€‚
    * å…¶çœŸæ­£çš„ä»·å€¼å¯èƒ½ä½“ç°åœ¨ä¸ **æ—©åœæ³•ï¼ˆearly stoppingï¼‰** çš„ç»„åˆä½¿ç”¨ä¸­ï¼Œå½¢æˆåŒé‡æ­£åˆ™åŒ–ç­–ç•¥ã€‚

å¯¹æ­£åˆ™åŒ–æ–¹æ³•çš„æ–°è§£é‡Š
++++++++++++++++++++

* åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ­£åˆ™åŒ–æ–¹æ³•çš„ä½œç”¨å¯èƒ½ä¸ä¼ ç»Ÿç†è®ºä¸åŒã€‚
* å³ä½¿æ­£åˆ™åŒ–ä¸èƒ½ç›´æ¥é™åˆ¶æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›ï¼Œå®ƒå¯èƒ½é€šè¿‡å¼•å…¥**å½’çº³åç½®ï¼ˆinductive biasesï¼‰**æ¥ä¿ƒè¿›æ¨¡å‹å¯¹æ•°æ®å†…åœ¨æ¨¡å¼çš„å­¦ä¹ ã€‚
* ç±»æ¯”äºkè¿‘é‚»ç®—æ³•ä¸­è·ç¦»åº¦é‡çš„é€‰æ‹©ï¼Œä¸åŒçš„æ­£åˆ™åŒ–æ–¹æ³•å¯èƒ½æ›´å¤šåœ°æ˜¯é€šè¿‡æ”¹å˜æ¨¡å‹çš„å­¦ä¹ æ–¹å¼ï¼Œè€Œéæ˜¾è‘—é™åˆ¶æ¨¡å‹å¤æ‚åº¦æ¥æå‡æ³›åŒ–èƒ½åŠ›ã€‚


æ­£åˆ™åŒ–æ–¹æ³•çš„æ‰©å±•ä¸åˆ›æ–°
++++++++++++++++++++++

* æ·±åº¦å­¦ä¹ ç ”ç©¶è€…ä¸ä»…ç»§ç»­ä½¿ç”¨ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•ï¼Œè¿˜åŸºäºè¿™äº›æ–¹æ³•å‘å±•äº†æ–°çš„æŠ€æœ¯ï¼Œå¦‚ï¼š
    * åœ¨æ¨¡å‹è¾“å…¥ä¸Šæ·»åŠ å™ªå£°ï¼šè¿™å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¢åŠ æ¨¡å‹çš„é²æ£’æ€§ï¼Œå‡å°‘å¯¹è®­ç»ƒæ•°æ®çš„è¿‡åº¦ä¾èµ–ã€‚
    * Dropoutï¼šé€šè¿‡éšæœºä¸¢å¼ƒä¸€éƒ¨åˆ†ç¥ç»å…ƒçš„è¾“å‡ºï¼Œé˜²æ­¢ç¥ç»ç½‘ç»œå¯¹ç‰¹å®šè·¯å¾„è¿‡åº¦ä¾èµ–ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„æ­£åˆ™åŒ–æ–¹æ³•ä¹‹ä¸€ã€‚
    * å³ä½¿dropoutçš„ç†è®ºåŸºç¡€å°šä¸å®Œå…¨æ¸…æ™°ï¼Œå®ƒåœ¨å®è·µä¸­çš„æœ‰æ•ˆæ€§å·²è¢«å¹¿æ³›éªŒè¯ã€‚


å…³é”®è¯è§£æ
++++++++++

* Weight Decayï¼ˆæƒé‡è¡°å‡ï¼‰ï¼šé€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ  :math:`\ell_2` æˆ– :math:`\ell_1` æƒ©ç½šé¡¹ï¼Œé˜²æ­¢æ¨¡å‹å‚æ•°æ— é™å¢å¤§ï¼Œä»è€Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©ã€‚
* Inductive Biasï¼ˆå½’çº³åç½®ï¼‰ï¼šæ¨¡å‹åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è¡¨ç°å‡ºçš„å€¾å‘æ€§æˆ–å‡è®¾ï¼Œæœ‰åŠ©äºæ¨¡å‹æ›´å¥½åœ°ç†è§£æ•°æ®åˆ†å¸ƒã€‚
* Dropoutï¼šè®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒç¥ç»å…ƒè¾“å‡ºï¼Œå‡å°‘ç¥ç»å…ƒä¹‹é—´çš„å…±é€‚åº”æ€§ï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚

ç»“è®º
++++

* è¿™æ®µæ–‡å­—å¼ºè°ƒäº†æ·±åº¦å­¦ä¹ å¯¹ç»å…¸æ­£åˆ™åŒ–æ–¹æ³•çš„å€Ÿé‰´å’Œåˆ›æ–°ã€‚
* å°½ç®¡ä¼ ç»Ÿæ­£åˆ™åŒ–æ–¹æ³•å¦‚æƒé‡è¡°å‡ä»ç„¶å¹¿æ³›ä½¿ç”¨ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå®ƒä»¬çš„ä½œç”¨æœºåˆ¶å¯èƒ½æ›´å¤æ‚ï¼Œä¸”å¾€å¾€éœ€è¦ä¸å…¶ä»–ç­–ç•¥ï¼ˆå¦‚æ—©åœæ³•ï¼‰ç»“åˆä½¿ç”¨ã€‚
* æœªæ¥ï¼Œæ¢ç´¢è¿™äº›æ­£åˆ™åŒ–æ–¹æ³•èƒŒåçš„ç†è®ºåŸºç¡€ï¼Œå°†æ˜¯ç†è§£æ·±åº¦å­¦ä¹ æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ–¹å‘ã€‚

5.6. Dropout
^^^^^^^^^^^^

* è¿™éƒ¨åˆ†ä»‹ç»äº†Dropoutï¼Œä¸€ç§åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–æŠ€æœ¯ã€‚

1. ä¸ºä»€ä¹ˆéœ€è¦Dropout::

    ç®€åŒ–æ¨¡å‹æ˜¯æé«˜æ³›åŒ–èƒ½åŠ›çš„æ ¸å¿ƒæ€è·¯ä¹‹ä¸€ï¼š
        å‡å°‘æ¨¡å‹å‚æ•°çš„ç»´åº¦ã€‚
        ä½¿ç”¨æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰ï¼Œé™åˆ¶å‚æ•°çš„å¤§å°ã€‚
        æé«˜æ¨¡å‹çš„å¹³æ»‘æ€§ï¼Œä½¿æ¨¡å‹å¯¹è¾“å…¥çš„å°æ‰°åŠ¨ä¸æ•æ„Ÿã€‚
    ç›´è§‚è§£é‡Šï¼šåœ¨å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¦‚æœè¾“å…¥å›¾åƒçš„åƒç´ å¢åŠ äº†ä¸€äº›éšæœºå™ªå£°ï¼Œæ¨¡å‹ä»åº”ç»™å‡ºç›¸åŒçš„åˆ†ç±»ç»“æœã€‚

2. Dropoutçš„æå‡º::

    Dropout æºè‡ªBishopæå‡ºçš„ä¸€ä¸ªç†è®ºï¼š
        åœ¨è¾“å…¥ä¸­åŠ å…¥éšæœºå™ªå£°ï¼Œç›¸å½“äºTikhonovæ­£åˆ™åŒ–ï¼Œå¯ä»¥æé«˜æ¨¡å‹å¯¹è¾“å…¥æ‰°åŠ¨çš„é²æ£’æ€§ã€‚

    Srivastavaç­‰äººå°†è¿™ä¸€æ€æƒ³æ¨å¹¿åˆ°ç½‘ç»œçš„å†…éƒ¨å±‚ï¼Œæå‡ºäº†Dropoutæ–¹æ³•ã€‚
        Dropoutçš„æ ¸å¿ƒæ€æƒ³ï¼š
            åœ¨æ¯æ¬¡å‰å‘ä¼ æ’­æ—¶ï¼Œéšæœºâ€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œå³å°†å®ƒä»¬çš„è¾“å‡ºç½®é›¶ã€‚
            è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡è¿­ä»£éƒ½ä¼šå¯¹ä¸åŒçš„ç¥ç»å…ƒè¿›è¡Œéšæœºå±è”½ã€‚

    è¿™ç§æ–¹æ³•æ‰“ç ´äº†ç¥ç»ç½‘ç»œä¸­è¿‡åº¦ä¾èµ–ç‰¹å®šæ¿€æ´»æ¨¡å¼çš„é—®é¢˜ï¼Œé˜²æ­¢ç¥ç»å…ƒä¹‹é—´å½¢æˆå…±é€‚åº”ï¼ˆco-adaptationï¼‰ã€‚
        ä½œè€…å°†å…¶ç±»æ¯”äºç”Ÿç‰©çš„æœ‰æ€§ç¹æ®–æ‰“ç ´åŸºå› å…±é€‚åº”çš„è¿‡ç¨‹ã€‚

3. Dropoutçš„å®ç°::

    åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œç¥ç»å…ƒä»¥æ¦‚ç‡$p$è¢«ä¸¢å¼ƒï¼ˆç½®é›¶ï¼‰ï¼Œå…¶ä½™ç¥ç»å…ƒä»¥æ¦‚ç‡$1-p$ä¿ç•™ã€‚
    ä¸ºäº†ä¿æŒè¾“å‡ºæœŸæœ›ä¸å˜ï¼Œä¿ç•™çš„ç¥ç»å…ƒçš„è¾“å‡ºè¢«é™¤ä»¥$(1-p)$è¿›è¡Œç¼©æ”¾ã€‚

å…¬å¼ï¼š

.. math::

    \begin{aligned}
    h' =
    \begin{cases}
        0 & \textrm{ with probability } p \\
        \frac{h}{1-p} & \textrm{ otherwise}
    \end{cases}
    \end{aligned}

* å…¶ä¸­
* ``h`` æ˜¯åŸå§‹æ¿€æ´»å€¼
* ``h'`` æ˜¯Dropoutåçš„æ¿€æ´»å€¼
* è¿™æ ·è®¾è®¡çš„ç›®çš„æ˜¯ç¡®ä¿æœŸæœ›ä¸å˜



å°ç»“::

    Dropoutæ˜¯ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œåœ¨å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸­å·²æˆä¸ºæ ‡å‡†åšæ³•ã€‚
    å®ƒå¯ä»¥æœ‰æ•ˆåœ°å‡å°‘è¿‡æ‹Ÿåˆï¼Œä½¿æ¨¡å‹å¯¹è¾“å…¥æ•°æ®çš„å¾®å°æ‰°åŠ¨æ›´åŠ é²æ£’ã€‚
    Dropoutæ‰“ç ´äº†ç¥ç»å…ƒä¹‹é—´çš„å…±é€‚åº”å…³ç³»ï¼Œæé«˜äº†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚



5.6.1. Dropout in Practice
""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/Q2aH1f.png

    Fig. 5.6.1 MLP before and after dropout. :math:`h_2` å’Œ :math:`h_5` è¢«åˆ é™¤ã€‚å› æ­¤ï¼Œè¾“å‡ºçš„è®¡ç®—ä¸å†ä¾èµ–äº :math:`h_2` æˆ– :math:`h_5` ï¼Œå¹¶ä¸”åœ¨æ‰§è¡Œåå‘ä¼ æ’­æ—¶å®ƒä»¬å„è‡ªçš„æ¢¯åº¦ä¹Ÿä¼šæ¶ˆå¤±ã€‚è¿™æ ·ï¼Œè¾“å‡ºå±‚çš„è®¡ç®—å°±ä¸èƒ½è¿‡åº¦ä¾èµ–äº :math:`h_1, ..., h_5` çš„ä»»ä½•ä¸€ä¸ªå…ƒç´ ã€‚


5.6.2. Implementation from Scratch
""""""""""""""""""""""""""""""""""

* å®ç°äº†ä¸€ä¸ª dropout_layer å‡½æ•°ï¼Œè¯¥å‡½æ•°ä»¥ dropout æ¦‚ç‡ä¸¢å¼ƒå¼ é‡è¾“å…¥ X ä¸­çš„å…ƒç´ ï¼Œå¹¶æŒ‰ä¸Šè¿°æ–¹å¼é‡æ–°è°ƒæ•´ä½™æ•°ï¼šé™¤ä»¥ 1.0-dropout çš„å¹¸å­˜è€…::

    def dropout_layer(X, dropout):
        assert 0 <= dropout <= 1
        if dropout == 1: return torch.zeros_like(X)
        mask = (torch.rand(X.shape) > dropout).float()
        return mask * X / (1.0 - dropout)

å°†è¾“å…¥ X é€šè¿‡ dropout æ“ä½œä¼ é€’ï¼Œæ¦‚ç‡åˆ†åˆ«ä¸º 0ã€0.5 å’Œ 1::

    X = torch.arange(16, dtype = torch.float32).reshape((2, 8))
    print('dropout_p = 0:', dropout_layer(X, 0))
    print('dropout_p = 0.5:', dropout_layer(X, 0.5))
    print('dropout_p = 1:', dropout_layer(X, 1))
    # è¾“å‡º
    # dropout_p = 0: tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],
    #         [ 8.,  9., 10., 11., 12., 13., 14., 15.]])
    # dropout_p = 0.5: tensor([[ 0.,  2.,  0.,  6.,  8.,  0.,  0.,  0.],
    #         [16., 18., 20., 22., 24., 26., 28., 30.]])
    # dropout_p = 1: tensor([[0., 0., 0., 0., 0., 0., 0., 0.],
    #         [0., 0., 0., 0., 0., 0., 0., 0.]])


5.6.2.1. Defining the Model
+++++++++++++++++++++++++++

ä¸‹é¢çš„æ¨¡å‹å°† dropout åº”ç”¨äºæ¯ä¸ªéšè—å±‚çš„è¾“å‡ºï¼ˆåœ¨æ¿€æ´»å‡½æ•°ä¹‹åï¼‰::

    class DropoutMLPScratch(d2l.Classifier):
        def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                     dropout_1, dropout_2, lr):
            super().__init__()
            self.save_hyperparameters()
            self.lin1 = nn.LazyLinear(num_hiddens_1)
            self.lin2 = nn.LazyLinear(num_hiddens_2)
            self.lin3 = nn.LazyLinear(num_outputs)
            self.relu = nn.ReLU()

        def forward(self, X):
            H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))
            if self.training:
                H1 = dropout_layer(H1, self.dropout_1)
            H2 = self.relu(self.lin2(H1))
            if self.training:
                H2 = dropout_layer(H2, self.dropout_2)
            return self.lin3(H2)

5.6.2.2. Training
+++++++++++++++++

.. code-block:: python

    hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,
               'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}
    model = DropoutMLPScratch(**hparams)
    data = d2l.FashionMNIST(batch_size=256)
    trainer = d2l.Trainer(max_epochs=10)
    trainer.fit(model, data)


.. figure:: https://img.zhaoweiguo.com/uPic/2024/12/aeAT6f.png


5.6.3. Concise Implementation
"""""""""""""""""""""""""""""

* åœ¨æ¯ä¸ªå…¨è¿æ¥å±‚ä¹‹åæ·»åŠ ä¸€ä¸ª Dropout å±‚ï¼Œå°† dropout æ¦‚ç‡ä½œä¸ºå”¯ä¸€å‚æ•°ä¼ é€’ç»™å…¶æ„é€ å‡½æ•°
* åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œ Dropout å±‚å°†æ ¹æ®æŒ‡å®šçš„ä¸¢å¼ƒæ¦‚ç‡éšæœºä¸¢å¼ƒå‰ä¸€å±‚çš„è¾“å‡ºï¼ˆæˆ–ç­‰æ•ˆåœ°ï¼Œåç»­å±‚çš„è¾“å…¥ï¼‰ã€‚
* å½“ä¸å¤„äºè®­ç»ƒæ¨¡å¼æ—¶ï¼Œ Dropout å±‚ä»…åœ¨æµ‹è¯•æœŸé—´ä¼ é€’æ•°æ®ã€‚

.. code-block:: python

    class DropoutMLP(d2l.Classifier):
        def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,
                     dropout_1, dropout_2, lr):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.Flatten(), nn.LazyLinear(num_hiddens_1), nn.ReLU(),
                nn.Dropout(dropout_1), nn.LazyLinear(num_hiddens_2), nn.ReLU(),
                nn.Dropout(dropout_2), nn.LazyLinear(num_outputs))

è®­ç»ƒæ¨¡å‹::

    model = DropoutMLP(**hparams)
    trainer.fit(model, data)

5.6.4. Summary
""""""""""""""

* é™¤äº†æ§åˆ¶ç»´æ•°å’Œæƒé‡å‘é‡çš„å¤§å°ä¹‹å¤–ï¼Œdropout æ˜¯é¿å…è¿‡åº¦æ‹Ÿåˆçš„å¦ä¸€ç§å·¥å…·ã€‚
* é€šå¸¸ï¼Œå·¥å…·æ˜¯è”åˆä½¿ç”¨çš„ã€‚
* è¯·æ³¨æ„ï¼Œdropout ä»…åœ¨è®­ç»ƒæœŸé—´ä½¿ç”¨ï¼šå®ƒå°†æ¿€æ´» h æ›¿æ¢ä¸ºå…·æœ‰é¢„æœŸå€¼ h çš„éšæœºå˜é‡


5.7. Predicting House Prices on Kaggle
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ä»¥ä¸€ä¸ª Kaggle ä¸Šçš„ç¤ºä¾‹è¿›è¡Œè®²è§£




Part 2: Modern Deep Learning Techniques
=======================================

6. Buildersâ€™ Guide
------------------

* é™¤äº†åºå¤§çš„æ•°æ®é›†å’Œå¼ºå¤§çš„ç¡¬ä»¶ä¹‹å¤–ï¼Œè¿˜æœ‰å‡ºè‰²çš„è½¯ä»¶å·¥å…· åœ¨æ·±åº¦å­¦ä¹ çš„å¿«é€Ÿè¿›å±•ä¸­å‘æŒ¥äº†ä¸å¯æˆ–ç¼ºçš„ä½œç”¨ å­¦ä¹ ã€‚
* åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æŒ–æ˜æ·±åº¦å­¦ä¹ è®¡ç®—çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå³æ¨¡å‹æ„å»ºã€å‚æ•°è®¿é—®å’Œåˆå§‹åŒ–ã€è®¾è®¡è‡ªå®šä¹‰å±‚å’Œå—ã€å°†æ¨¡å‹è¯»å†™åˆ°ç£ç›˜ï¼Œä»¥åŠåˆ©ç”¨ GPU å®ç°æ¢¦å¹»èˆ¬çš„åŠ é€Ÿã€‚
* è™½ç„¶æœ¬ç« æ²¡æœ‰ä»‹ç»ä»»ä½•æ–°æ¨¡å‹æˆ–æ•°æ®é›†ï¼Œä½†æ¥ä¸‹æ¥çš„é«˜çº§å»ºæ¨¡ç« èŠ‚åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºè¿™äº›æŠ€æœ¯ã€‚


6.1. Layers and Modules
^^^^^^^^^^^^^^^^^^^^^^^

* Individual layers can be modules. 
* Many layers can comprise a module. 
* Many modules can comprise a module.


6.2. Parameter Management
^^^^^^^^^^^^^^^^^^^^^^^^^

ç¤ºä¾‹::

    net = nn.Sequential(nn.LazyLinear(8),
                        nn.ReLU(),
                        nn.LazyLinear(1))

    X = torch.rand(size=(2, 4))
    net(X).shape
    # torch.Size([2, 1])


6.2.1. Parameter Access
"""""""""""""""""""""""

æ£€æŸ¥ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„å‚æ•°::

    # æ‰§è¡Œ net(X) å‰
    $ net[2].state_dict()
    # OrderedDict([('weight', <UninitializedParameter>),
             ('bias', <UninitializedParameter>)])

    # æ‰§è¡Œ net(X) å
    $ net[2].state_dict()
    # OrderedDict([('weight',
              tensor([[-0.1649,  0.0605,  0.1694, -0.2524,  0.3526, -0.3414, -0.2322,  0.0822]])),
             ('bias', tensor([0.0709]))])



6.2.1.1. Targeted Parameters
++++++++++++++++++++++++++++

ä»ç¬¬äºŒä¸ªç¥ç»ç½‘ç»œå±‚æå–åå·®ï¼Œè¯¥å±‚è¿”å›å‚æ•°ç±»å®ä¾‹ï¼Œå¹¶è¿›ä¸€æ­¥è®¿é—®è¯¥å‚æ•°çš„å€¼::

    $ type(net[2].bias), net[2].bias.data
    # (torch.nn.parameter.Parameter, tensor([0.0709]))

    $ net[2].weight
    Parameter containing:
    tensor([[ 0.0205, -0.1554, -0.2950,  0.1296, -0.2784,  0.1173, -0.0230, -0.1530]],
           requires_grad=True)

.. note:: å‚æ•°æ˜¯å¤æ‚çš„å¯¹è±¡ï¼ŒåŒ…å«å€¼ã€æ¢¯åº¦å’Œé™„åŠ ä¿¡æ¯ã€‚


6.2.1.2. All Parameters at Once
+++++++++++++++++++++++++++++++

::

    $ [(name, param.shape) for name, param in net.named_parameters()]
    # [('0.weight', torch.Size([8, 4])),
       ('0.bias', torch.Size([8])),
       ('2.weight', torch.Size([1, 8])),
       ('2.bias', torch.Size([1]))]

6.2.2. Tied Parameters
""""""""""""""""""""""

è·¨å¤šä¸ªå±‚å…±äº«å‚æ•°::

    # We need to give the shared layer a name so that we can refer to its parameters
    shared = nn.LazyLinear(8)
    net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(),
                        shared, nn.ReLU(),
                        shared, nn.ReLU(),
                        nn.LazyLinear(1))

    net(X)
    # Check whether the parameters are the same
    print(net[2].weight.data[0] == net[4].weight.data[0])
    net[2].weight.data[0, 0] = 100
    # Make sure that they are actually the same object rather than just having the
    # same value
    print(net[2].weight.data[0] == net[4].weight.data[0])
    # è¾“å‡º
    # tensor([True, True, True, True, True, True, True, True])
    # tensor([True, True, True, True, True, True, True, True])


.. note:: ç”±äºæ¨¡å‹å‚æ•°åŒ…å«æ¢¯åº¦ï¼Œå› æ­¤åœ¨åå‘ä¼ æ’­æ—¶å°†ç¬¬äºŒéšè—å±‚å’Œç¬¬ä¸‰éšè—å±‚çš„æ¢¯åº¦ç›¸åŠ ã€‚


6.3. Parameter Initialization
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ä½¿ç”¨å†…ç½®å’Œè‡ªå®šä¹‰åˆå§‹åŒ–ç¨‹åºæ¥åˆå§‹åŒ–å‚æ•°ã€‚

ç¤ºä¾‹::

    net = nn.Sequential(nn.LazyLinear(8), nn.ReLU(), nn.LazyLinear(1))
    X = torch.rand(size=(2, 4))
    net(X).shape


6.3.1. Built-in Initialization
""""""""""""""""""""""""""""""

åˆä½¿çŠ¶æ€::

    net[0].weight.data[0], net[0].bias.data[0]
    # Out[27]: (tensor([ 0.4112, -0.0801,  0.4687,  0.3344]), tensor(-0.1189))

ç¤ºä¾‹1-å°†æ‰€æœ‰æƒé‡å‚æ•°åˆå§‹åŒ–ä¸ºæ ‡å‡†å·®ä¸º 0.01 çš„é«˜æ–¯éšæœºå˜é‡ï¼Œè€Œåå·®å‚æ•°åˆ™æ¸…é™¤ä¸ºé›¶::

    def init_normal(module):
        if type(module) == nn.Linear:
            nn.init.normal_(module.weight, mean=0, std=0.01)
            nn.init.zeros_(module.bias)

    net.apply(init_normal)
    net[0].weight.data[0], net[0].bias.data[0]
    # è¾“å‡º
    # (tensor([-0.0082,  0.0074,  0.0116, -0.0061]), tensor(0.))

ç¤ºä¾‹2-å°†æ‰€æœ‰å‚æ•°åˆå§‹åŒ–ä¸ºç»™å®šçš„å¸¸é‡å€¼ï¼ˆä¾‹å¦‚ 1ï¼‰::

    def init_constant(module):
        if type(module) == nn.Linear:
            nn.init.constant_(module.weight, 1)
            nn.init.zeros_(module.bias)

    net.apply(init_constant)
    net[0].weight.data[0], net[0].bias.data[0]
    # è¾“å‡º
    (tensor([1., 1., 1., 1.]), tensor(0.))

ç¤ºä¾‹3-ä¸ºæŸäº›å—åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–å™¨ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æˆ‘ä»¬ä½¿ç”¨ Xavier åˆå§‹åŒ–å™¨åˆå§‹åŒ–ç¬¬ä¸€å±‚ï¼Œå¹¶å°†ç¬¬äºŒå±‚åˆå§‹åŒ–ä¸ºå¸¸é‡å€¼ 42::

    def init_xavier(module):
        if type(module) == nn.Linear:
            nn.init.xavier_uniform_(module.weight)

    def init_42(module):
        if type(module) == nn.Linear:
            nn.init.constant_(module.weight, 42)

    net[0].apply(init_xavier)
    net[2].apply(init_42)
    print(net[0].weight.data[0])
    print(net[2].weight.data)
    # è¾“å‡º
    tensor([-0.0974,  0.1707,  0.5840, -0.5032])
    tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])

6.4. Lazy Initialization
^^^^^^^^^^^^^^^^^^^^^^^^

* å»¶è¿Ÿåˆå§‹åŒ–å¾ˆæ–¹ä¾¿ï¼Œå…è®¸æ¡†æ¶è‡ªåŠ¨æ¨æ–­å‚æ•°å½¢çŠ¶ï¼Œä»è€Œå¯ä»¥è½»æ¾ä¿®æ”¹æ¶æ„å¹¶æ¶ˆé™¤ä¸€ç§å¸¸è§çš„é”™è¯¯æºã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹ä¼ é€’æ•°æ®æ¥è®©æ¡†æ¶æœ€ç»ˆåˆå§‹åŒ–å‚æ•°ã€‚


å®ä¾‹åŒ–ä¸€ä¸ª MLP::

    net = nn.Sequential(nn.LazyLinear(256), nn.ReLU(), nn.LazyLinear(10))

å°è¯•è®¿é—®ä»¥ä¸‹å‚æ•°è¿›è¡Œç¡®è®¤::

    net[0].weight
    # è¾“å‡º
    <UninitializedParameter>

é€šè¿‡ç½‘ç»œä¼ é€’æ•°æ®ï¼Œè®©æ¡†æ¶æœ€ç»ˆåˆå§‹åŒ–å‚æ•°::

    X = torch.rand(2, 20)
    net(X)

ä¸€æ—¦çŸ¥é“æ‰€æœ‰å‚æ•°å½¢çŠ¶ï¼Œæ¡†æ¶å°±å¯ä»¥æœ€ç»ˆåˆå§‹åŒ–å‚æ•°::

    net[0].weight.shape
    # è¾“å‡º
    torch.Size([256, 20])


6.5. Custom Layers
^^^^^^^^^^^^^^^^^^

* é€šè¿‡åŸºæœ¬å›¾å±‚ç±»æ¥è®¾è®¡è‡ªå®šä¹‰å›¾å±‚ã€‚
* è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰çµæ´»çš„æ–°å±‚ï¼Œå…¶è¡Œä¸ºä¸åº“ä¸­ä»»ä½•ç°æœ‰å±‚ä¸åŒã€‚ä¸€æ—¦å®šä¹‰ï¼Œè‡ªå®šä¹‰å±‚å°±å¯ä»¥åœ¨ä»»æ„ä¸Šä¸‹æ–‡å’Œæ¶æ„ä¸­è°ƒç”¨ã€‚å±‚å¯ä»¥å…·æœ‰æœ¬åœ°å‚æ•°ï¼Œå¯ä»¥é€šè¿‡å†…ç½®å‡½æ•°åˆ›å»ºè¿™äº›å‚æ•°ã€‚

6.5.1. Layers without Parameters
""""""""""""""""""""""""""""""""


::

    class CenteredLayer(nn.Module):
        def __init__(self):
            super().__init__()

        def forward(self, X):
            return X - X.mean()

ä½¿ç”¨::

    layer = CenteredLayer()
    layer(torch.tensor([1.0, 2, 3, 4, 5]))
    # è¾“å‡º
    tensor([-2., -1.,  0.,  1.,  2.])

incorporate our layer as a component in constructing more complex models::

    net = nn.Sequential(nn.LazyLinear(128), CenteredLayer())

ä½¿ç”¨::

    Y = net(torch.rand(4, 8))
    Y.mean()

6.5.2. Layers with Parameters
"""""""""""""""""""""""""""""

å…·æœ‰å¯é€šè¿‡è®­ç»ƒè°ƒæ•´çš„å‚æ•°çš„å±‚::

    class MyLinear(nn.Module):
        def __init__(self, in_units, units):
            super().__init__()
            self.weight = nn.Parameter(torch.randn(in_units, units))
            self.bias = nn.Parameter(torch.randn(units,))

        def forward(self, X):
            linear = torch.matmul(X, self.weight.data) + self.bias.data
            return F.relu(linear)


6.6. File I/O
^^^^^^^^^^^^^

6.6.1. Loading and Saving Tensors
"""""""""""""""""""""""""""""""""

å¯¹äºå•ä¸ªå¼ é‡ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è°ƒç”¨loadå’Œsave åˆ†åˆ«è¯»å–å’Œå†™å…¥å®ƒä»¬çš„å‡½æ•°::

    x = torch.arange(4)
    torch.save(x, 'x-file')

    x2 = torch.load('x-file')
    x2
    # è¾“å‡º
    tensor([0, 1, 2, 3])


å­˜å‚¨å¼ é‡åˆ—è¡¨å¹¶å°†å®ƒä»¬è¯»å›å†…å­˜::

    y = torch.zeros(4)
    torch.save([x, y],'x-files')
    x2, y2 = torch.load('x-files')
    (x2, y2)

ç¼–å†™å’Œè¯»å–ä»å­—ç¬¦ä¸²æ˜ å°„åˆ°å¼ é‡çš„å­—å…¸::

    mydict = {'x': x, 'y': y}
    torch.save(mydict, 'mydict')
    mydict2 = torch.load('mydict')
    mydict2

6.6.2. Loading and Saving Model Parameters
""""""""""""""""""""""""""""""""""""""""""

æ¨¡å‹ç¤ºä¾‹::

    class MLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.hidden = nn.LazyLinear(256)
            self.output = nn.LazyLinear(10)

        def forward(self, x):
            return self.output(F.relu(self.hidden(x)))

    net = MLP()
    X = torch.randn(size=(2, 20))
    Y = net(X)

å°†æ¨¡å‹çš„å‚æ•°å­˜å‚¨ä¸ºåä¸ºâ€œmlp.paramsâ€çš„æ–‡ä»¶::

    torch.save(net.state_dict(), 'mlp.params')

æ¢å¤æ¨¡å‹::

    clone = MLP()
    clone.load_state_dict(torch.load('mlp.params'))
    clone.eval()
    # è¾“å‡º
    MLP(
      (hidden): LazyLinear(in_features=0, out_features=256, bias=True)
      (output): LazyLinear(in_features=0, out_features=10, bias=True)
    )

.. note:: å¯ä»¥é€šè¿‡å‚æ•°å­—å…¸ä¿å­˜å’ŒåŠ è½½ç½‘ç»œçš„æ•´å¥—å‚æ•°ã€‚ä¿å­˜æ¶æ„å¿…é¡»é€šè¿‡ä»£ç è€Œä¸æ˜¯å‚æ•°æ¥å®Œæˆã€‚




6.7. GPUs
^^^^^^^^^

.. note:: è¦è¿è¡Œæœ¬èŠ‚ä¸­çš„ç¨‹åºï¼Œæ‚¨è‡³å°‘éœ€è¦ä¸¤ä¸ª GPUã€‚

6.7.1. Computing Devices
""""""""""""""""""""""""

æŒ‡å®šç”¨äºå­˜å‚¨å’Œè®¡ç®—çš„è®¾å¤‡::

    def cpu():  #@save
        """Get the CPU device."""
        return torch.device('cpu')

    def gpu(i=0):  #@save
        """Get a GPU device."""
        return torch.device(f'cuda:{i}')

    cpu(), gpu(), gpu(1)
    # è¾“å‡º
    (device(type='cpu'),
     device(type='cuda', index=0),
     device(type='cuda', index=1))


æŸ¥è¯¢å¯ç”¨GPUçš„æ•°é‡::

    def num_gpus():  #@save
        """Get the number of available GPUs."""
        return torch.cuda.device_count()

    num_gpus()

å·¥å…·(GPUä¸å­˜åœ¨åˆ™ä½¿ç”¨CPU)::

    def try_gpu(i=0):  #@save
        """Return gpu(i) if exists, otherwise return cpu()."""
        if num_gpus() >= i + 1:
            return gpu(i)
        return cpu()

    def try_all_gpus():  #@save
        """Return all available GPUs, or [cpu(),] if no GPU exists."""
        return [gpu(i) for i in range(num_gpus())]

    try_gpu(), try_gpu(10), try_all_gpus()


6.7.2. Tensors and GPUs
"""""""""""""""""""""""

6.7.2.1. Storage on the GPU
+++++++++++++++++++++++++++

ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU::

    X = torch.ones(2, 3, device=try_gpu())
    X
    # è¾“å‡º
    tensor([[1., 1., 1.],
            [1., 1., 1.]], device='cuda:0')


å¦‚æœæœ‰ä¸¤ä¸ªGPU::

    Y = torch.rand(2, 3, device=try_gpu(1))
    Y
    # è¾“å‡º
    tensor([[0.0022, 0.5723, 0.2890],
            [0.1456, 0.3537, 0.7359]], device='cuda:1')


6.7.2.2. Copying
++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/OCgG4P.png

    Fig. 6.7.1 Copy data to perform an operation on the same device.


ç”±äº Y ä½äºç¬¬äºŒä¸ª GPU ä¸Šï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦å°† X ç§»åŠ¨åˆ°é‚£é‡Œï¼Œç„¶åæ‰èƒ½å°†ä¸¤è€…ç›¸åŠ ::

    Z = X.cuda(1)
    print(X)
    print(Z)
    # è¾“å‡º
    tensor([[1., 1., 1.],
            [1., 1., 1.]], device='cuda:0')
    tensor([[1., 1., 1.],
            [1., 1., 1.]], device='cuda:1')


ç°åœ¨æ•°æ®ï¼ˆ Z å’Œ Y ï¼‰éƒ½åœ¨åŒä¸€ä¸ª GPU ä¸Š::

    Y + Z
    # è¾“å‡º
    tensor([[1.0022, 1.5723, 1.2890],
            [1.1456, 1.3537, 1.7359]], device='cuda:1')

6.7.3. Neural Networks and GPUs
"""""""""""""""""""""""""""""""

ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥æŒ‡å®šè®¾å¤‡ã€‚ä»¥ä¸‹ä»£ç å°†æ¨¡å‹å‚æ•°æ”¾åœ¨ GPU ä¸Š::

    net = nn.Sequential(nn.LazyLinear(1))
    net = net.to(device=try_gpu())

Let the trainer support GPU::

    @d2l.add_to_class(d2l.Trainer)  #@save
    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):
        self.save_hyperparameters()
        self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]

    @d2l.add_to_class(d2l.Trainer)  #@save
    def prepare_batch(self, batch):
        if self.gpus:
            batch = [a.to(self.gpus[0]) for a in batch]
        return batch

    @d2l.add_to_class(d2l.Trainer)  #@save
    def prepare_model(self, model):
        model.trainer = self
        model.board.xlim = [0, self.max_epochs]
        if self.gpus:
            model.to(self.gpus[0])
        self.model = model

6.7.4. Summary
""""""""""""""

* é»˜è®¤æƒ…å†µä¸‹ï¼Œæ•°æ®åœ¨ä¸»å­˜ä¸­åˆ›å»ºï¼Œç„¶åä½¿ç”¨CPUè¿›è¡Œè®¡ç®—ã€‚
* æ·±åº¦å­¦ä¹ æ¡†æ¶è¦æ±‚è®¡ç®—çš„æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½ä½äºåŒä¸€è®¾å¤‡ä¸Šï¼Œæ— è®ºæ˜¯CPUè¿˜æ˜¯åŒä¸€ä¸ªGPUã€‚
* å¦‚æœä¸å°å¿ƒç§»åŠ¨æ•°æ®ï¼Œæ‚¨å¯èƒ½ä¼šæŸå¤±æ˜¾ç€çš„æ€§èƒ½ã€‚

.. note:: ä¸€ä¸ªå…¸å‹çš„é”™è¯¯å¦‚ä¸‹ï¼šè®¡ç®— GPU ä¸Šæ¯ä¸ªå°æ‰¹é‡çš„æŸå¤±å¹¶åœ¨å‘½ä»¤è¡Œä¸Šå°†å…¶æŠ¥å‘Šç»™ç”¨æˆ·ï¼ˆæˆ–å°†å…¶è®°å½•åœ¨ NumPy ndarray ä¸­ï¼‰å°†è§¦å‘å…¨å±€è§£é‡Šå™¨é”å®šï¼Œè¯¥é”å®šä¼šåœæ­¢æ‰€æœ‰ GPUã€‚æœ€å¥½åœ¨ GPU å†…éƒ¨åˆ†é…å†…å­˜ç”¨äºæ—¥å¿—è®°å½•ï¼Œå¹¶ä¸”åªç§»åŠ¨è¾ƒå¤§çš„æ—¥å¿—ã€‚

* è·¨è®¾å¤‡ä¼ è¾“æ•°æ®ä¼šå¯¼è‡´æ€§èƒ½æŸå¤±ã€‚æ¯”å¦‚ï¼Œæ•°æ®ä» CPU ä¼ åˆ° GPUï¼Œæˆ–è€…åœ¨å¤šä¸ª GPU ä¹‹é—´é¢‘ç¹ä¼ è¾“ï¼Œéƒ½ä¼šå¢åŠ å¼€é”€ã€‚å…³é”®ç‚¹æ˜¯ï¼š **å°½é‡é¿å…ä¸å¿…è¦çš„æ•°æ®ä¼ è¾“ï¼Œå°¤å…¶æ˜¯å°æ‰¹é‡æ•°æ®çš„é¢‘ç¹ç§»åŠ¨ã€‚**
* é”™è¯¯ç¤ºä¾‹ï¼šåœ¨ GPU ä¸Šè®¡ç®—æ¯ä¸ªå°æ‰¹é‡ï¼ˆminibatchï¼‰çš„æŸå¤±ï¼ˆlossï¼‰ï¼Œç„¶åå°†å…¶ä¼ å› CPU å¹¶è½¬æ¢ä¸º NumPy æ•°ç»„è¿›è¡Œè®°å½•æˆ–æ˜¾ç¤ºã€‚ é—®é¢˜ï¼šè¿™ç§æ–¹å¼ä¼šè§¦å‘å…¨å±€è§£é‡Šå™¨é”ï¼ˆGlobal Interpreter Lockï¼ŒGILï¼‰ï¼Œå¯¼è‡´ GPU æš‚åœï¼Œç­‰å¾… CPU å®Œæˆæ“ä½œï¼Œä»è€Œé™ä½è®¡ç®—æ•ˆç‡ã€‚
* ä¼˜åŒ–å»ºè®®ï¼šæœ€ä½³åšæ³•æ˜¯ç›´æ¥åœ¨ GPU ä¸Šåˆ†é…å†…å­˜è®°å½•æ—¥å¿—ï¼Œå‡å°‘æ•°æ®åœ¨ CPU å’Œ GPU ä¹‹é—´é¢‘ç¹ç§»åŠ¨ã€‚ç­‰æ—¥å¿—ç´¯ç§¯åˆ°è¶³å¤Ÿå¤§çš„æ‰¹æ¬¡æ—¶ï¼Œå†å°†å…¶ç§»åŠ¨åˆ° CPU è¿›è¡Œåç»­å¤„ç†æˆ–æ˜¾ç¤ºã€‚


7. Convolutional Neural Networks
--------------------------------

7.1. From Fully Connected Layers to Convolutions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

7.1.1. Invariance(ä¸å˜æ€§)
"""""""""""""""""""""""""

* è§£é‡Š CNN å¦‚ä½•é€šè¿‡å¹³ç§»ä¸å˜æ€§å’Œå±€éƒ¨æ€§æ¥å‡å°‘æ¨¡å‹å¤æ‚åº¦ï¼Œæé«˜å¯¹å›¾åƒä¸­ç‰©ä½“ä½ç½®çš„é²æ£’æ€§ï¼ˆå³ä½¿ä½ç½®å˜åŒ–ä¹Ÿèƒ½è¯†åˆ«ï¼‰ã€‚
* CNN é€šè¿‡æ¨¡æ‹Ÿäººç±»è§†è§‰çš„é€å±‚å¤„ç†æ–¹å¼ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ å›¾åƒç‰¹å¾ï¼Œå¹¶åœ¨ä¸åŒä½ç½®è¿›è¡Œä¸€è‡´è¯†åˆ«ã€‚

å®šä¹‰
++++

* ç›®æ ‡ï¼šè¯†åˆ«å›¾åƒä¸­çš„å¯¹è±¡æ—¶ï¼Œå¸Œæœ›æ¨¡å‹å¯¹å¯¹è±¡åœ¨å›¾åƒä¸­çš„å…·ä½“ä½ç½®ä¸æ•æ„Ÿã€‚
* æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šè¯†åˆ«ä¸€ä¸ªå¯¹è±¡æ—¶ï¼Œå…³æ³¨å¯¹è±¡çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯å…¶ä½ç½®ã€‚
* æ¯”å¦‚ï¼Œæ— è®ºä¸€åªçŒªå‡ºç°åœ¨å›¾åƒé¡¶éƒ¨è¿˜æ˜¯åº•éƒ¨ï¼Œæˆ‘ä»¬éƒ½åº”è¯¥èƒ½å¤Ÿè¯†åˆ«å‡ºå®ƒæ˜¯ä¸€åªçŒªã€‚è¿™ç§ä½ç½®ä¸æ•æ„Ÿæ€§å°±æ˜¯ä¸å˜æ€§ï¼ˆinvarianceï¼‰ã€‚


CNN å¦‚ä½•å®ç°è¿™ç§ä¸å˜æ€§
++++++++++++++++++++++

* (1)å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Invarianceï¼‰ï¼š
    * æ— è®ºå›¾åƒçš„æŸä¸ªå±€éƒ¨åŒºåŸŸï¼ˆpatchï¼‰å‡ºç°åœ¨ä»€ä¹ˆä½ç½®ï¼Œç½‘ç»œéƒ½åº”è¯¥å¯¹å®ƒæœ‰ç›¸ä¼¼çš„å“åº”ã€‚
    * å·ç§¯å±‚é€šè¿‡åœ¨å›¾åƒä¸Šæ»‘åŠ¨ä¸€ä¸ªæ»¤æ³¢å™¨ï¼ˆkernelï¼‰æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå³ä½¿ç‰©ä½“çš„ä½ç½®å‘ç”Ÿå˜åŒ–ï¼Œæ»¤æ³¢å™¨ä¾ç„¶å¯ä»¥æ£€æµ‹åˆ°å®ƒã€‚
    * ä¾‹å­ï¼šåœ¨ä¸€å¼ çŒ«çš„å›¾ç‰‡ä¸­ï¼Œæ»¤æ³¢å™¨åœ¨çŒ«çš„è€³æœµä¸Šæˆ–çŒ«çš„çˆªå­ä¸Šæ»‘åŠ¨æ—¶ï¼Œéƒ½ä¼šäº§ç”Ÿç›¸åº”çš„æ¿€æ´»ï¼Œä»è€Œæ£€æµ‹åˆ°çŒ«çš„å­˜åœ¨ã€‚

* (2)å±€éƒ¨æ€§åŸåˆ™ï¼ˆLocalityï¼‰ï¼š
    * åœ¨ç½‘ç»œçš„å‰å‡ å±‚ï¼Œå·ç§¯æ“ä½œä¸»è¦å…³æ³¨å±€éƒ¨åŒºåŸŸï¼ˆlocal regionï¼‰ï¼Œè€Œä¸è€ƒè™‘å›¾åƒä¸­è¿œå¤„çš„å†…å®¹ã€‚
    * è¿™ç§æ–¹æ³•æ¨¡æ‹Ÿäº†äººç±»è§†è§‰ç³»ç»Ÿä¸­å¯¹å±€éƒ¨ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜ã€è§’ç­‰ï¼‰çš„å…³æ³¨ã€‚
    * æ„ä¹‰ï¼šæ¯æ¬¡åªå¤„ç†ä¸€å°éƒ¨åˆ†å›¾åƒï¼Œæœ‰åŠ©äºå‡å°‘è®¡ç®—é‡ï¼Œå¹¶æ•æ‰åŸºæœ¬ç‰¹å¾ã€‚æœ€ç»ˆï¼Œé€šè¿‡ä¸æ–­å †å å·ç§¯å±‚å’Œæ± åŒ–å±‚ï¼Œæ¨¡å‹å¯ä»¥èšåˆè¿™äº›å±€éƒ¨ç‰¹å¾ï¼Œå½¢æˆå¯¹æ•´ä¸ªå›¾åƒçš„ç†è§£ã€‚

* (3)æ·±å±‚ç‰¹å¾æå–ï¼š
    * éšç€ç½‘ç»œé€æ¸åŠ æ·±ï¼Œæ„Ÿå—é‡ï¼ˆreceptive fieldï¼‰å˜å¤§ï¼Œç½‘ç»œå¯ä»¥å­¦ä¹ åˆ°æ›´é•¿è·ç¦»çš„ç‰¹å¾å…³ç³»ï¼Œç±»ä¼¼äºæ›´é«˜çº§åˆ«çš„è§†è§‰æ„ŸçŸ¥ã€‚
    * è¿™ç§æ–¹å¼ä½¿å¾— CNN èƒ½å¤Ÿæ•æ‰æ›´å¤æ‚ã€æ›´æŠ½è±¡çš„å›¾åƒç‰¹å¾ã€‚
    * ä¾‹å­ï¼šåœ¨æµ…å±‚ï¼Œç½‘ç»œå¯èƒ½å­¦ä¹ åˆ°è¾¹ç¼˜å’Œçº¹ç†ç­‰ç®€å•ç‰¹å¾ï¼Œè€Œåœ¨æ›´æ·±çš„å±‚ä¸­ï¼Œç½‘ç»œå¯èƒ½å­¦ä¹ åˆ°çœ¼ç›ã€é¼»å­ç­‰å¤æ‚ç‰¹å¾ï¼Œæœ€ç»ˆèƒ½å¤Ÿè¯†åˆ«å‡ºæ•´ä¸ªè„¸éƒ¨ã€‚


7.1.2. Constraining the MLP
"""""""""""""""""""""""""""

æ•´ä½“
++++

* æœ¬èŠ‚æ¢è®¨çš„æ˜¯å¦‚ä½•å°†å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰åº”ç”¨åˆ°äºŒç»´å›¾åƒä¸Šï¼Œå¹¶åˆ†æäº†è¿™æ ·åšå¸¦æ¥çš„æŒ‘æˆ˜å’Œå‚æ•°çˆ†ç‚¸é—®é¢˜ã€‚
* åœ¨ä¼ ç»Ÿçš„ MLP ä¸­ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªæ‰å¹³çš„å‘é‡ï¼Œä½†å¯¹äºå›¾åƒæ¥è¯´ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿ç•™ç©ºé—´ç»“æ„ï¼ˆspatial structureï¼‰ï¼Œå³å›¾åƒçš„äºŒç»´å½¢çŠ¶ã€‚

ç†è§£ MLP å¯¹äºŒç»´å›¾åƒçš„å»ºæ¨¡
~~~~~~~~~~~~~~~~~~~~~~~~~

* å‡è®¾è¾“å…¥å›¾åƒ :math:`\mathbf{X}` å’Œéšè—å±‚è¡¨ç¤º :math:`\mathbf{H}` éƒ½æ˜¯äºŒç»´çŸ©é˜µï¼Œä¸”äºŒè€…å½¢çŠ¶ç›¸åŒï¼ˆä¾‹å¦‚ :math:`1000 \times 1000` åƒç´ ï¼‰ã€‚
* :math:`[\mathbf{X}]{i,j}` ä»£è¡¨è¾“å…¥å›¾åƒåœ¨ (i,j) ä½ç½®çš„åƒç´ å€¼ï¼Œè€Œ :math:`\mathbf{H}]{i,j}` ä»£è¡¨éšè—å±‚åœ¨ (i,j) ä½ç½®çš„æ¿€æ´»å€¼ã€‚


æƒé‡çŸ©é˜µåˆ°æƒé‡å¼ é‡çš„åˆ‡æ¢
~~~~~~~~~~~~~~~~~~~~~~~~

* ä¼ ç»Ÿ MLPï¼šåœ¨ä¼ ç»Ÿ MLP ä¸­ï¼Œéšè—å±‚çš„æ¯ä¸ªç¥ç»å…ƒä¸è¾“å…¥å±‚çš„æ‰€æœ‰åƒç´ ç›¸è¿ï¼Œè¿æ¥æƒé‡ç”¨ä¸€ä¸ªäºŒç»´çŸ©é˜µ :math:`\mathsf{W}` è¡¨ç¤ºã€‚
* äºŒç»´å›¾åƒ MLPï¼š
    * å¦‚æœå°†æ¯ä¸ªéšè—å•å…ƒéƒ½è¿æ¥åˆ°è¾“å…¥å›¾åƒçš„æ‰€æœ‰åƒç´ ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå››é˜¶æƒé‡å¼ é‡ :math:`\mathsf{W}_{i,j,k,l}`
    * è¿™ä¸ªå››é˜¶å¼ é‡è¡¨ç¤ºä»è¾“å…¥å›¾åƒä½ç½® (k,l) åˆ°éšè—å±‚ä½ç½® (i,j) çš„æƒé‡ã€‚

å…¬å¼è§£é‡Šï¼š

.. math::

    \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}

* ä½ç½® (i,j) å¤„çš„éšè—å•å…ƒ :math:`\mathbf{H}{i,j}` æ˜¯è¾“å…¥å›¾åƒæ‰€æœ‰åƒç´ çš„åŠ æƒå’Œï¼Œå†åŠ ä¸Šåç½® :math:\mathbf{U}{i,j}`

å·ç§¯çš„å¼•å…¥ä¸å‚æ•°é‡ç´¢å¼•
~~~~~~~~~~~~~~~~~~~~~~

* ä¸ºäº†å‡å°‘å¤æ‚åº¦ï¼Œæˆ‘ä»¬å¼•å…¥å·ç§¯çš„æ€æƒ³ï¼Œå°†æƒé‡çŸ©é˜µ :math:`\mathsf{W}` é‡æ–°è¡¨ç¤ºä¸º :math:`\mathsf{V}` ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

.. math::

    \left[\mathbf{H}\right]_{i, j} &= [\mathbf{U}]_{i, j} + \sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}

* è¿™é‡Œçš„ :math:`\mathsf{V}` ä»£è¡¨ä¸€ä¸ªå±€éƒ¨æƒé‡çª—å£ï¼Œè¡¨ç¤ºå·ç§¯æ ¸ï¼Œåªå…³æ³¨å›¾åƒä¸­ä¸ä½ç½® (i,j) ç›¸é‚»çš„å±€éƒ¨åŒºåŸŸã€‚
* æƒé‡ç´¢å¼•å˜åŒ–ï¼š

.. math::

    [\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+k}

* è¿™ä¸ªç´¢å¼•å˜æ¢æ„å‘³ç€ï¼šå·ç§¯æ ¸ä»¥ (i,j) ä¸ºä¸­å¿ƒï¼Œé‡‡æ ·ç›¸é‚»åç§»é‡ (a,b) å¤„çš„åƒç´ ã€‚

å‚æ•°é‡çˆ†ç‚¸é—®é¢˜
~~~~~~~~~~~~~~

* å‡è®¾è¾“å…¥å›¾åƒå¤§å°ä¸º :math:`1000 \times 1000` ï¼Œéšè—å±‚è¡¨ç¤ºåŒæ ·ä¸º :math:`1000 \times 1000`
* å¦‚æœæ¯ä¸ªåƒç´ éƒ½ä¸æ‰€æœ‰åƒç´ ç›¸è¿ï¼Œæƒé‡å¼ é‡ :math:`\mathsf{W}` éœ€è¦ :math:`1000 \times 1000 \times 1000 \times 1000 = 10^{12}` ä¸ªå‚æ•°ï¼
* è¿™ç§å‚æ•°é‡è¿œè¿œè¶…å‡ºäº†è®¡ç®—æœºçš„å¤„ç†èƒ½åŠ›ã€‚

é—®é¢˜çš„æ ¹æºä¸è§£å†³æ€è·¯
~~~~~~~~~~~~~~~~~~~~

* é—®é¢˜æ ¹æºï¼š MLP çš„å…¨è¿æ¥ç»“æ„ä½¿æ¯ä¸ªéšè—å•å…ƒéƒ½ä¸æ‰€æœ‰è¾“å…¥åƒç´ ç›¸è¿ï¼Œå¯¼è‡´å‚æ•°é‡çˆ†ç‚¸ã€‚
* è§£å†³æ–¹æ³•ï¼š
    * å±€éƒ¨æ„Ÿå—é‡ï¼ˆlocal receptive fieldï¼‰ï¼š ä»…è®©éšè—å•å…ƒä¸è¾“å…¥å›¾åƒçš„å±€éƒ¨åŒºåŸŸç›¸è¿ï¼Œè€Œéæ•´ä¸ªå›¾åƒã€‚
    * å…±äº«æƒé‡ï¼š é€šè¿‡å·ç§¯å±‚çš„æ–¹å¼ï¼Œå‡å°‘å‚æ•°æ•°é‡å¹¶å¢å¼ºå¹³ç§»ä¸å˜æ€§ã€‚
    * æ± åŒ–å±‚ï¼ˆpoolingï¼‰ï¼š è¿›ä¸€æ­¥é™ä½åˆ†è¾¨ç‡ï¼Œå‡å°‘è®¡ç®—é‡ã€‚

å°ç»“
~~~~

* æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šç›´æ¥å°† MLP åº”ç”¨äºé«˜ç»´å›¾åƒä¼šå¯¼è‡´å‚æ•°é‡å·¨å¤§ï¼Œä¸å…·å¤‡å®é™…å¯è¡Œæ€§ã€‚
* é€šè¿‡å¼•å…¥å·ç§¯çš„æ¦‚å¿µï¼Œé™åˆ¶æ„Ÿå—é‡èŒƒå›´ï¼Œå¯ä»¥æœ‰æ•ˆå‡å°‘å‚æ•°é‡ï¼Œå¹¶ä¿æŒå¯¹å›¾åƒç©ºé—´ç»“æ„çš„æ•æ„Ÿæ€§ã€‚
* è¿™ç§æ–¹æ³•æ„æˆäº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„åŸºç¡€ï¼Œæå¤§æå‡äº†æ¨¡å‹çš„è®¡ç®—æ•ˆç‡å’Œè¯†åˆ«èƒ½åŠ›ã€‚


7.1.2.1. Translation Invariance
+++++++++++++++++++++++++++++++

* å¹³ç§»ä¸å˜æ€§ï¼ˆTranslation Invarianceï¼‰ï¼š
* å®šä¹‰ï¼š å¹³ç§»ä¸å˜æ€§æ„å‘³ç€ï¼šå¦‚æœè¾“å…¥å›¾åƒå‘ç”Ÿå¹³ç§»ï¼Œéšè—å±‚çš„è¾“å‡ºä¹Ÿåº”å‘ç”ŸåŒæ ·çš„å¹³ç§»ï¼Œè€Œä¸ä¼šæ”¹å˜ç‰¹å¾æœ¬èº«çš„æ€§è´¨ã€‚
* ç›´è§‚ç†è§£ï¼š å¦‚æœåœ¨å›¾åƒçš„ä¸åŒä½ç½®çœ‹åˆ°ç›¸åŒçš„ç‰¹å¾ï¼ˆå¦‚è¾¹ç¼˜æˆ–è§’ï¼‰ï¼Œç½‘ç»œåº”å½“èƒ½è¯†åˆ«å®ƒï¼Œè€Œä¸åœ¨æ„ç‰¹å¾çš„å…·ä½“ä½ç½®ã€‚

æ•°å­¦è§£é‡Š
~~~~~~~~

* åŸå§‹è¡¨ç¤ºæ–¹å¼ä¸­ï¼Œæƒé‡ :math:`\mathsf{V}` ä¾èµ–äºåƒç´ ä½ç½® (i,j) ï¼Œå³ :math:`[\mathsf{V}]_{i,j,a,b}` è¡¨ç¤ºåœ¨ä½ç½® (i,j) çš„æƒé‡å¯èƒ½ä¸å…¶ä»–ä½ç½®ä¸åŒã€‚
* å¼•å…¥å¹³ç§»ä¸å˜æ€§åï¼š æƒé‡åªä¸ç›¸å¯¹åç§»é‡ (a,b) æœ‰å…³ï¼Œä¸å†ä¾èµ–å…·ä½“ä½ç½®ï¼š

.. math::

    [\mathsf{V}]_{i,j,a,b} = [\mathsf{V}]_{a,b}

* åç½® :math:`\mathbf{U}` ç®€åŒ–ä¸ºå¸¸æ•° uã€‚

    \left[\mathbf{H}\right]_{i, j} &= u + \sum_a \sum_b [\mathsf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}



7.1.2.2. Locality
+++++++++++++++++

* å±€éƒ¨æ„Ÿå—é‡ï¼ˆLocalityï¼‰
* å®šä¹‰ï¼š å±€éƒ¨æ„Ÿå—é‡æ„å‘³ç€ï¼šéšè—å±‚çš„æ¯ä¸ªç¥ç»å…ƒåªå…³æ³¨è¾“å…¥å›¾åƒçš„å±€éƒ¨åŒºåŸŸï¼Œè€Œéæ•´ä¸ªå›¾åƒã€‚
* åŠ¨æœºï¼š åœ¨å›¾åƒä¸­ï¼Œè¿œç¦»å½“å‰åƒç´ çš„åŒºåŸŸå¯¹ç†è§£è¯¥ä½ç½®åƒç´ çš„å½±å“è¾ƒå°ã€‚å› æ­¤ï¼Œæ„Ÿå—é‡å¯ä»¥é™åˆ¶åœ¨è¾ƒå°èŒƒå›´å†…ã€‚
* å®ç°æ–¹å¼ï¼š åœ¨è·ç¦»è¶…è¿‡ :math:`\Delta` çš„åœ°æ–¹ï¼Œå°†æƒé‡è®¾ä¸º 0ï¼š

.. math::

    [\mathbf{V}]_{a, b} =0   \text{if} |a| > \Delta \text{or} |b| > \Delta

* å…¬å¼è¡¨ç¤ºï¼š

.. math::

    [\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}

* å¼•å…¥å±€éƒ¨æ„Ÿå—é‡åï¼Œå·ç§¯æ ¸å°ºå¯¸ä» :math:`2000 \times 2000`  å‡å°‘åˆ°ä¸€ä¸ªè¾ƒå°å€¼ :math:`\Delta \times \Delta`ï¼ˆé€šå¸¸ :math:`\Delta < 10` ï¼‰ã€‚
* è¿™å°†å‚æ•°é‡è¿›ä¸€æ­¥å‡å°‘åˆ°ï¼š

.. math::

    4\Delta^2

* ç»“æœï¼š å‚æ•°é‡å‡å°‘äº†å››ä¸ªæ•°é‡çº§ï¼Œä½¿å¾—å·ç§¯å±‚æ›´åŠ é«˜æ•ˆã€‚



æ€»ç»“ä¸å¯ç¤º
++++++++++

* å·ç§¯æ ¸çš„ä½œç”¨ï¼š æå–å›¾åƒå±€éƒ¨ç‰¹å¾ï¼Œå‡å°‘å‚æ•°é‡ï¼Œæå‡ç½‘ç»œå¯¹å›¾åƒçš„ç©ºé—´æ•æ„Ÿæ€§ã€‚
* å¹³ç§»ä¸å˜æ€§çš„ä»£ä»·ï¼š å·ç§¯æ ¸åªèƒ½æ„ŸçŸ¥å±€éƒ¨ç‰¹å¾ï¼Œåœ¨å¤„ç†å…¨å±€ä¿¡æ¯æ—¶éœ€è¦æ›´æ·±çš„ç½‘ç»œç»“æ„ï¼ˆæ›´å¤šå±‚çš„å·ç§¯+æ± åŒ–ï¼‰ã€‚
* æ·±å±‚ç½‘ç»œï¼š é€šè¿‡å †å å¤šå±‚å·ç§¯å±‚ï¼Œå¯ä»¥é€å±‚æ•è·æ›´å¤æ‚ã€æ›´æŠ½è±¡çš„ç‰¹å¾ï¼Œå®ç°å¯¹æ•´å¹…å›¾åƒçš„ç†è§£ã€‚

* ç›´è§‚ç±»æ¯”ï¼šå¯ä»¥æŠŠ CNN çš„å·ç§¯æ ¸æƒ³è±¡æˆæ”¾å¤§é•œæˆ–æ¢æµ‹å™¨ï¼Œå®ƒåœ¨å›¾åƒä¸­ç§»åŠ¨ï¼Œå¯»æ‰¾æ„Ÿå…´è¶£çš„ç‰¹å¾ã€‚é€šè¿‡é™åˆ¶æ¢æµ‹å™¨çš„å¤§å°å’Œç§»åŠ¨èŒƒå›´ï¼Œæˆ‘ä»¬æ—¢å‡å°‘äº†å¤æ‚åº¦ï¼Œåˆæé«˜äº†æ¨¡å‹çš„æ•ˆç‡å’Œæ³›åŒ–èƒ½åŠ›ã€‚

CNNçš„ä¼˜åŠ¿
++++++++++

* å‚æ•°é«˜æ•ˆï¼š CNN åªéœ€å°‘é‡å‚æ•°å³å¯å¤„ç†é«˜åˆ†è¾¨ç‡å›¾åƒã€‚
* é²æ£’æ€§å¼ºï¼š CNN å¯¹å›¾åƒçš„å¹³ç§»å’Œå±€éƒ¨å˜åŒ–å…·æœ‰è¾ƒå¼ºçš„é²æ£’æ€§ï¼Œèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–åˆ°æ–°æ•°æ®ã€‚
* ç‰¹å¾å±‚æ¬¡åŒ–ï¼š é€šè¿‡å¤šå±‚å·ç§¯ï¼Œå¯ä»¥é€æ­¥å­¦ä¹ ä½çº§ï¼ˆå¦‚è¾¹ç¼˜ï¼‰åˆ°é«˜çº§ï¼ˆå¦‚ç‰©ä½“ï¼‰ç‰¹å¾ã€‚
* è®¡ç®—é«˜æ•ˆï¼š å·ç§¯æ“ä½œæ˜“äºå¹¶è¡Œè®¡ç®—ï¼Œé€‚åˆ GPU åŠ é€Ÿã€‚


7.1.3. Convolutions
"""""""""""""""""""

å®šä¹‰
++++

* å·ç§¯æ˜¯ä¿¡å·å¤„ç†å’Œå›¾åƒå¤„ç†ä¸­å¸¸ç”¨çš„æ•°å­¦æ“ä½œã€‚
* æœ¬è´¨ä¸Šï¼Œå®ƒæ˜¯ä¸€ç§æ»‘åŠ¨çª—å£æ“ä½œï¼Œè®¡ç®—ä¸¤ä¸ªå‡½æ•°æˆ–çŸ©é˜µçš„é‡å ç¨‹åº¦ã€‚
* å·ç§¯çš„å…³é”®åœ¨äºä¸€ä¸ªå‡½æ•°æˆ–çŸ©é˜µåœ¨å¦ä¸€ä¸ªå‡½æ•°æˆ–çŸ©é˜µä¸Šè¿›è¡Œæ»‘åŠ¨ï¼Œå¹¶è®¡ç®—åœ¨æ¯ä¸ªä½ç½®ä¸Šçš„é‡å é‡ã€‚

æ•°å­¦å®šä¹‰
++++++++

è¿ç»­å·ç§¯å…¬å¼
~~~~~~~~~~~~

.. math::

    (f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}

* è§£é‡Š
    * f(z)ï¼šåŸå§‹ä¿¡å·æˆ–å›¾åƒã€‚
    * ğ‘”(ğ‘¥âˆ’ğ‘§)ï¼šæ»¤æ³¢å™¨ (æˆ–æ ¸å‡½æ•°) åœ¨ä½ç½® ğ‘¥ å¤„ç¿»è½¬åå¹³ç§»çš„ç»“æœã€‚
    * ç§¯åˆ†è¡¨ç¤ºå¯¹æ‰€æœ‰å¯èƒ½çš„é‡å è¿›è¡Œç´¯åŠ ã€‚
* ç›´è§‚ç†è§£ï¼š
    * ç±»ä¼¼å°†ä¸€ä¸ªå›¾å½¢ ğ‘” ç¿»è½¬åï¼Œåœ¨å›¾å½¢ ğ‘“ ä¸Šé€ä¸ªä½ç½®æ»‘åŠ¨ï¼Œè®¡ç®—é‡å åŒºåŸŸçš„é¢ç§¯ã€‚
    * æ¯ä¸ªæ»‘åŠ¨ä½ç½®éƒ½ç”Ÿæˆä¸€ä¸ªæ•°å€¼ï¼Œè¡¨ç¤ºè¯¥ä½ç½®çš„åŒ¹é…ç¨‹åº¦ã€‚



ç¦»æ•£å·ç§¯å…¬å¼(ä¸€ç»´)
~~~~~~~~~~~~~~~~~~

.. math::

    (f * g)(i) = \sum_a f(a) g(i-a)

* åœ¨ç¦»æ•£åœºæ™¯ä¸‹ï¼Œç§¯åˆ†å˜ä¸ºæ±‚å’Œã€‚æ»‘åŠ¨çª—å£åœ¨æ¯ä¸ªä½ç½® ğ‘– ä¸Šè®¡ç®— ğ‘“ å’Œ ğ‘” çš„å†…ç§¯ã€‚

äºŒç»´å·ç§¯å…¬å¼
~~~~~~~~~~~~

.. math::

    (f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b)

* äºŒç»´å·ç§¯æ‰©å±•äº†æ¦‚å¿µï¼Œåœ¨ ğ‘,ğ‘ æ–¹å‘æ»‘åŠ¨æ»¤æ³¢å™¨ï¼Œè®¡ç®—äºŒç»´å›¾åƒä¸æ»¤æ³¢å™¨çš„é‡å ç¨‹åº¦ã€‚


å·ç§¯å’Œäº¤å‰ç›¸å…³çš„åŒºåˆ«
++++++++++++++++++++

* äº¤å‰ç›¸å…³ (Cross-correlation)ï¼š
    * æ»¤æ³¢å™¨ ğ‘” åœ¨æ»‘åŠ¨æ—¶ä¸è¿›è¡Œç¿»è½¬ï¼Œç›´æ¥å¹³ç§»å’Œç´¯åŠ ã€‚
    * å·ç§¯æ ¸å’ŒåŸå§‹å›¾åƒåœ¨ç›¸åŒæ–¹å‘æ»‘åŠ¨ã€‚

.. math::

    (f * g)(i, j) = \sum_a\sum_b f(a, b) g(i+a, j+b)

* å·ç§¯ (Convolution)ï¼š
    * å·ç§¯çš„æ»¤æ³¢å™¨ ğ‘”(ğ‘–âˆ’ğ‘,ğ‘—âˆ’ğ‘) æ˜¯ç¿»è½¬åçš„ç‰ˆæœ¬ã€‚
    * å®é™…æ·±åº¦å­¦ä¹ ä¸­ï¼Œé€šå¸¸ä½¿ç”¨äº¤å‰ç›¸å…³çš„å½¢å¼ï¼Œç§°ä¹‹ä¸ºâ€œå·ç§¯â€ã€‚

* ä¸ºä»€ä¹ˆä½¿ç”¨äº¤å‰ç›¸å…³ï¼Ÿ
    * è®¡ç®—æ•ˆç‡é«˜ï¼Œç»“æœç›¸ä¼¼ã€‚
    * ç¿»è½¬æ“ä½œåœ¨å®ç°ä¸­å¯å¿½ç•¥ï¼Œå› æ­¤æ·±åº¦å­¦ä¹ æ¡†æ¶é»˜è®¤ä½¿ç”¨äº¤å‰ç›¸å…³ã€‚

å·ç§¯çš„å®é™…æ„ä¹‰
++++++++++++++

* å·ç§¯å¯åœ¨å›¾åƒä¸­æå–å±€éƒ¨ç‰¹å¾ï¼Œä¾‹å¦‚ï¼š
    * æ£€æµ‹è¾¹ç¼˜ã€è§’ç‚¹ã€çº¹ç†ç­‰ã€‚
    * åœ¨ä¸åŒå±‚ä¸­é€æ¸æ•æ‰æ›´å¤æ‚çš„æ¨¡å¼ï¼Œä»ä½çº§ç‰¹å¾ (è¾¹ç¼˜) åˆ°é«˜çº§ç‰¹å¾ (ç‰©ä½“è½®å»“æˆ–ç±»åˆ«)ã€‚
* å·ç§¯æ˜¯æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œ (CNN) çš„æ ¸å¿ƒæ“ä½œ


7.1.4. Channels
"""""""""""""""

1. å›¾åƒçš„å¤šé€šé“ç‰¹æ€§
+++++++++++++++++++

* å®é™…å›¾åƒé€šå¸¸æœ‰ ä¸‰é€šé“ï¼šçº¢ (R)ã€ç»¿ (G)ã€è“ (B)ã€‚
* æ¯ä¸ªåƒç´ ç‚¹ä¸ä»…åŒ…å«ä¸€ä¸ªç°åº¦å€¼ï¼Œè€Œæ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¡¨ç¤ºä¸åŒé¢œè‰²çš„ç»„åˆã€‚
* å› æ­¤ï¼Œå›¾åƒæ˜¯ä¸‰é˜¶å¼ é‡ 1024Ã—1024Ã—3ï¼Œè¡¨ç¤ºé«˜åº¦ã€å®½åº¦å’Œé€šé“æ•°ã€‚

2. ä¸ºä»€ä¹ˆéœ€è¦å¤šé€šé“å·ç§¯
+++++++++++++++++++++++

* ç®€å•çš„äºŒç»´å·ç§¯åªèƒ½å¤„ç†ç°åº¦å›¾åƒã€‚
* å¤šé€šé“å·ç§¯å¯ä»¥ï¼š
    * å¤„ç†å¤æ‚çš„å½©è‰²å›¾åƒã€‚
    * åœ¨æ¯ä¸ªé€šé“ä¸Šåˆ†åˆ«æå–ç‰¹å¾ï¼Œæœ€ç»ˆç»“åˆç”Ÿæˆæ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨è¾¾ã€‚
    * å…è®¸ä¸åŒæ»¤æ³¢å™¨å­¦ä¹ æ•æ‰å›¾åƒçš„ä¸åŒæ–¹é¢ï¼Œä¾‹å¦‚è¾¹ç¼˜ã€çº¹ç†å’Œé¢œè‰²åˆ†å¸ƒã€‚

3. æ•°å­¦å®šä¹‰
+++++++++++

å¤šé€šé“å·ç§¯å…¬å¼ï¼š

.. math::

    [\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c}

* i,jï¼šè¡¨ç¤ºå·ç§¯åœ¨å›¾åƒä¸Šçš„ç©ºé—´ä½ç½®ã€‚
* ğ‘ï¼šè¾“å…¥é€šé“ç´¢å¼•ã€‚
* ğ‘‘ï¼šè¾“å‡ºé€šé“ç´¢å¼• (ä¸åŒæ»¤æ³¢å™¨äº§ç”Ÿä¸åŒçš„è¾“å‡ºé€šé“)ã€‚
* æ ¸ ğ‘‰ æ˜¯ä¸€ä¸ªå››é˜¶å¼ é‡ï¼Œç»´åº¦ä¸º ğ‘,ğ‘,ğ‘,ğ‘‘ï¼Œå³æ»¤æ³¢å™¨å¤§å°å’Œè¾“å…¥è¾“å‡ºé€šé“æ•°ã€‚

4. ç›´è§‚ç†è§£
+++++++++++

* å°†æ»¤æ³¢å™¨åœ¨æ¯ä¸ªé€šé“ä¸Šåˆ†åˆ«æ»‘åŠ¨ï¼Œç„¶ååœ¨æ‰€æœ‰é€šé“ä¸Šçš„ç»“æœç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºç‰¹å¾ã€‚
* è¿™ç§æ–¹å¼ç±»ä¼¼äºå¯¹å›¾åƒçš„ä¸åŒéƒ¨åˆ†è¿›è¡Œç‰¹å¾ç»„åˆï¼Œç”Ÿæˆæ›´åŠ å¤æ‚çš„è¡¨è¾¾ã€‚

5. éšè—è¡¨ç¤ºå’Œç‰¹å¾å›¾ (Feature Maps)
++++++++++++++++++++++++++++++++++

* æ¯ä¸ªå·ç§¯å±‚çš„è¾“å‡ºä¹Ÿæ˜¯å¤šé€šé“çš„ï¼Œå³éšè—è¡¨ç¤º ğ» ä¹Ÿæ˜¯ä¸‰é˜¶å¼ é‡ã€‚
* æ¯ä¸ªéšè—é€šé“æ•æ‰ä¸åŒç±»å‹çš„ç‰¹å¾ã€‚ä¾‹å¦‚ï¼š
    * ç¬¬ä¸€ä¸ªé€šé“æ£€æµ‹è¾¹ç¼˜ã€‚
    * ç¬¬äºŒä¸ªé€šé“æ£€æµ‹è§’ç‚¹ã€‚
    * ç¬¬ä¸‰ä¸ªé€šé“æ£€æµ‹çº¹ç†æˆ–å¤æ‚å½¢çŠ¶ã€‚
* è¿™äº›è¾“å‡ºé€šé“å¸¸ç§°ä¸ºâ€œç‰¹å¾å›¾â€æˆ–â€œç‰¹å¾é€šé“â€ã€‚

6. å¤šå±‚å·ç§¯çš„ç‰¹å¾æå–æœºåˆ¶
+++++++++++++++++++++++++

* ä½å±‚å·ç§¯å±‚ï¼š æå–ç®€å•ç‰¹å¾ (å¦‚è¾¹ç¼˜)ã€‚
* ä¸­å±‚å·ç§¯å±‚ï¼š ç»„åˆä½å±‚ç‰¹å¾ï¼Œæå–æ›´å¤æ‚çš„å½¢çŠ¶æˆ–ç»“æ„ã€‚
* é«˜å±‚å·ç§¯å±‚ï¼š è¯†åˆ«ç‰©ä½“æˆ–åœºæ™¯ä¸­çš„é«˜çº§ç‰¹å¾ï¼Œå¦‚äººè„¸æˆ–ç‰¹å®šç‰©ä½“ã€‚


7. é€šé“çš„ç°å®æ„ä¹‰
+++++++++++++++++

* å¤šé€šé“å·ç§¯å¸®åŠ©æ¨¡å‹æ•æ‰æ›´å¤æ‚çš„è§†è§‰ç‰¹å¾ï¼Œæé«˜æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ã€‚
* æ·±åº¦å·ç§¯ç½‘ç»œ (CNN) åœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œåˆ†å‰²ç­‰ä»»åŠ¡ä¸­è¡¨ç°å“è¶Šï¼Œéƒ¨åˆ†åŸå› åœ¨äºå¤šé€šé“å·ç§¯çš„å¼ºå¤§èƒ½åŠ›ã€‚

8. å…³é”®ç‚¹æ€»ç»“
+++++++++++++

* é€šé“å…è®¸å·ç§¯å±‚å¤„ç†å½©è‰²å›¾åƒæˆ–å¤šç»´ç‰¹å¾ï¼Œä½¿CNNåœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°æ›´å¼ºå¤§ã€‚
* å¤šé€šé“è¾“å‡ºè¡¨ç¤ºä¸åŒå±‚æ¬¡çš„ç‰¹å¾ç»„åˆï¼Œä¸ºç½‘ç»œæä¾›æ›´å…¨é¢çš„å›¾åƒè¡¨ç¤ºèƒ½åŠ›ã€‚


7.1.5. Summary and Discussion
"""""""""""""""""""""""""""""

å…³é”®ç‚¹
++++++

* å¹³ç§»ä¸å˜æ€§ (Translation Invariance)ï¼šå›¾åƒä¸­çš„ç‰©ä½“æ— è®ºå‡ºç°åœ¨å›¾åƒçš„å“ªä¸ªä½ç½®ï¼Œå·ç§¯æ“ä½œéƒ½èƒ½ä»¥ç›¸åŒçš„æ–¹å¼å¤„ç†å®ƒã€‚è¿™ç§ä¸å˜æ€§æ˜¯ CNN è®¾è®¡çš„é‡è¦åŸåˆ™ä¹‹ä¸€ã€‚
* å±€éƒ¨æ€§ (Locality)ï¼šæ¯ä¸ªå·ç§¯æ ¸ä»…å…³æ³¨å›¾åƒçš„å°åŒºåŸŸ (å±€éƒ¨æ„Ÿå—é‡)ï¼Œå¹¶é€šè¿‡æ»‘åŠ¨çª—å£æœºåˆ¶é€ä¸ªä½ç½®è®¡ç®—ç‰¹å¾ï¼Œé€æ­¥æ„å»ºå…¨å±€ç†è§£ã€‚

ç†è§£
++++

* è§£é‡Šï¼šCNN çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºèƒ½å¤Ÿé«˜æ•ˆåœ°æå–å±€éƒ¨ç‰¹å¾ï¼Œå¹¶ä¿æŒå¯¹ä½ç½®ä¿¡æ¯çš„ä¸æ•æ„Ÿæ€§ï¼Œè¿™ç§è®¾è®¡ä½¿ CNN é€‚ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ç­‰ä»»åŠ¡ã€‚
* ç±»æ¯”ï¼šæƒ³è±¡ä¸€ä¸ªäººè§‚å¯Ÿä¸€å¼ å›¾ç‰‡æ—¶ï¼Œé€šå¸¸ä¸ä¼šä¸€æ¬¡æ€§å…³æ³¨æ•´å¼ å›¾åƒï¼Œè€Œæ˜¯é€ä¸ªåŒºåŸŸè§‚å¯Ÿå¹¶ç†è§£ç»†èŠ‚ã€‚è¿™ç§å±€éƒ¨å…³æ³¨å’Œé€æ­¥æ±‡æ€»çš„æ–¹å¼ä¸ CNN çš„å·¥ä½œæ–¹å¼ç›¸ä¼¼ã€‚


é™ä½å¤æ‚åº¦ä¸å‚æ•°æ•°é‡
++++++++++++++++++++

* é—®é¢˜ï¼šå¤§è§„æ¨¡å›¾åƒæˆ–é«˜ç»´æ•°æ®é€šå¸¸å…·æœ‰æé«˜çš„ç»´åº¦ï¼Œç›´æ¥å»ºæ¨¡è®¡ç®—é‡å·¨å¤§ä¸”å‚æ•°è¿‡å¤šï¼Œå®¹æ˜“å¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆæˆ–è®¡ç®—ä¸å¯è¡Œã€‚
* è§£å†³æ–¹æ¡ˆï¼šCNN é€šè¿‡å·ç§¯æ“ä½œï¼Œå¼ºåˆ¶æ¨¡å‹åªå…³æ³¨å±€éƒ¨åŒºåŸŸï¼Œå‡å°‘å‚æ•°æ•°é‡ï¼ŒåŒæ—¶ä¿ç•™è¶³å¤Ÿçš„è¡¨è¾¾èƒ½åŠ›ã€‚è¿™ç§ç­–ç•¥å°†å¤æ‚çš„è®¡ç®—é—®é¢˜è½¬åŒ–ä¸ºå¯è¡Œçš„æ¨¡å‹ï¼Œé¿å…äº†è®¡ç®—å’Œç»Ÿè®¡ä¸Šçš„ä¸å¯è¡Œæ€§ã€‚
* ç¤ºä¾‹ï¼šå‡è®¾åŸå§‹å›¾åƒå¤§å°ä¸º 1024Ã—1024Ã—3ï¼Œå¦‚æœç›´æ¥ä½¿ç”¨å…¨è¿æ¥å±‚ï¼Œå‚æ•°é‡éå¸¸åºå¤§ã€‚è€Œå·ç§¯æ ¸é€šå¸¸å¤§å°ä¸º 3Ã—3 æˆ– 5Ã—5ï¼Œå‚æ•°æ•°é‡æ˜¾è‘—å‡å°‘ã€‚
* ç†è§£è¦ç‚¹ï¼šé™ç»´å’Œç‰¹å¾æå–çš„åŒæ—¶ä¸ä¸¢å¤±å…³é”®ä¿¡æ¯ï¼Œæ˜¯ CNN åœ¨é«˜æ•ˆå¤„ç†å¤æ‚æ•°æ®æ—¶çš„é‡è¦ç‰¹æ€§ã€‚

å¼•å…¥é€šé“ (Channels) å¢å¼ºæ¨¡å‹èƒ½åŠ›
++++++++++++++++++++++++++++++++

* èƒŒæ™¯ï¼šå·ç§¯æ ¸çš„å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§åœ¨é™ä½å¤æ‚åº¦çš„åŒæ—¶ï¼Œä¹Ÿé™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
* è§£å†³æ–¹æ³•ï¼šå¼•å…¥å¤šä¸ªé€šé“ (å¦‚ RGB)ï¼Œå…è®¸æ¨¡å‹å­¦ä¹ æ›´å¤æ‚å’Œå¤šæ ·çš„ç‰¹å¾ï¼Œå¼¥è¡¥äº†å±€éƒ¨å·ç§¯å¸¦æ¥çš„è¡¨è¾¾èƒ½åŠ›æŸå¤±ã€‚
* è¿›ä¸€æ­¥æ‰©å±•ï¼šå›¾åƒä¸­å¸¸è§çš„ä¸‰ä¸ªé€šé“ (çº¢ã€ç»¿ã€è“)ï¼Œæ˜¯åŸºæœ¬çš„é¢œè‰²ä¿¡æ¯ã€‚ç„¶è€Œï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½å­˜åœ¨æ›´å¤šé€šé“ã€‚ä¾‹å¦‚ï¼š
    * å«æ˜Ÿå›¾åƒï¼š å¯èƒ½åŒ…å«å‡ åç”šè‡³ä¸Šç™¾ä¸ªé€šé“ (å¤šå…‰è°±æˆ–é«˜å…‰è°±å›¾åƒ)ï¼Œè®°å½•ä¸åŒæ³¢é•¿çš„åå°„æ•°æ®ã€‚
    * åŒ»å­¦æˆåƒï¼š MRI æˆ– CT æ‰«æä¸­ä¸åŒé€šé“å¯èƒ½ä»£è¡¨ä¸åŒçš„å±‚é¢æˆ–ç»„ç»‡ç‰¹æ€§ã€‚
* ç†è§£ï¼šé€šè¿‡å¼•å…¥é¢å¤–é€šé“ï¼ŒCNN å¯ä»¥å¤„ç†æ›´å¤æ‚çš„æ•°æ®ï¼Œå­¦ä¹ å¤šç»´åº¦çš„ç‰¹å¾è¡¨ç¤ºï¼Œä»è€Œæå‡æ¨¡å‹çš„è¡¨ç°åŠ›å’Œæ³›åŒ–èƒ½åŠ›ã€‚



7.2. Convolutions for Images
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

7.2.1. The Cross-Correlation Operation
""""""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/vuP5GV.png

    è¾“å…¥æ˜¯ä¸€ä¸ªé«˜åº¦ä¸º3ã€å®½åº¦ä¸º3çš„äºŒç»´å¼ é‡ã€‚æˆ‘ä»¬å°†å¼ é‡çš„å½¢çŠ¶æ ‡è®°ä¸º ``3x3 æˆ– (3ï¼Œ3ï¼‰``ã€‚kernelçš„é«˜åº¦å’Œå®½åº¦éƒ½æ˜¯2ã€‚kernel window (or convolution window)çš„å½¢çŠ¶ç”±å†…æ ¸çš„é«˜åº¦å’Œå®½åº¦ç»™å‡ºï¼ˆè¿™é‡Œæ˜¯ ``2x2`` ï¼‰ã€‚

* åœ¨äºŒç»´äº’ç›¸å…³è¿ç®—(two-dimensional cross-correlation operation)ä¸­ï¼Œæˆ‘ä»¬ä»ä½äºè¾“å…¥å¼ é‡å·¦ä¸Šè§’çš„å·ç§¯çª—å£å¼€å§‹ï¼Œå¹¶å°†å…¶ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹æ»‘åŠ¨ç©¿è¿‡è¾“å…¥å¼ é‡ã€‚
* å½“å·ç§¯çª—å£æ»‘åŠ¨åˆ°æŸä¸ªä½ç½®æ—¶ï¼Œè¯¥çª—å£ä¸­åŒ…å«çš„è¾“å…¥å­å¼ é‡ä¸å†…æ ¸å¼ é‡æŒ‰å…ƒç´ ç›¸ä¹˜ï¼Œå¹¶å°†æ‰€å¾—å¼ é‡ç›¸åŠ ï¼Œç”Ÿæˆå•ä¸ªæ ‡é‡å€¼ã€‚
* è¯¥ç»“æœç»™å‡ºäº†ç›¸åº”ä½ç½®å¤„çš„è¾“å‡ºå¼ é‡çš„å€¼ã€‚

.. math::

    0\times0+1\times1+3\times2+4\times3=19,\\
    1\times0+2\times1+4\times2+5\times3=25,\\
    3\times0+4\times1+6\times2+7\times3=37,\\
    4\times0+5\times1+7\times2+8\times3=43.

* è¾“å‡ºå¤§å°ç”±è¾“å…¥å¤§å° :math:`n_h \times n_w` å‡å» kernel å¤§å° :math:`k_h \times k_w`

.. math::

    (n_h - k_h +1) \times (n_w - k_w +1)

ä»£ç ::

    def corr2d(X, K):  #@save
        """Compute 2D cross-correlation."""
        h, w = K.shape
        Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))
        for i in range(Y.shape[0]):
            for j in range(Y.shape[1]):
                Y[i, j] = (X[i:i + h, j:j + w] * K).sum()
        return Y

    X = torch.tensor([[0.0, 1.0, 2.0], 
                      [3.0, 4.0, 5.0], 
                      [6.0, 7.0, 8.0]])
    K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
    corr2d(X, K)
    # è¾“å‡º
    tensor([[19., 25.],
            [37., 43.]])


7.2.2. Convolutional Layers
"""""""""""""""""""""""""""

å‰å‘ä¼ æ’­æ–¹æ³•::

    class Conv2D(nn.Module):
        def __init__(self, kernel_size):
            super().__init__()
            self.weight = nn.Parameter(torch.rand(kernel_size))
            self.bias = nn.Parameter(torch.zeros(1))

        def forward(self, x):
            return corr2d(x, self.weight) + self.bias

7.2.3. Object Edge Detection in Images
""""""""""""""""""""""""""""""""""""""

æ„å»ºä¸€ä¸ª 6x8 åƒç´ çš„â€œå›¾åƒâ€ã€‚ä¸­é—´å››åˆ—æ˜¯é»‘è‰²ï¼ˆ 0 ï¼‰ï¼Œå…¶ä½™æ˜¯ç™½è‰²ï¼ˆ 1 ï¼‰::

    X = torch.ones((6, 8))
    X[:, 2:6] = 0
    X
    # è¾“å‡º
    tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.],
            [1., 1., 0., 0., 0., 0., 1., 1.]])


* æ„é€ ä¸€ä¸ªé«˜åº¦ä¸º1ã€å®½åº¦ä¸º2çš„ kernel K
* æ­¤å†…æ ¸æ˜¯æœ‰é™å·®åˆ†è¿ç®—ç¬¦çš„ç‰¹ä¾‹: At location :math:`(i,j)` it computes :math:`x_{i,j} - x_{(i+1),j}` ï¼Œå³è®¡ç®—æ°´å¹³ç›¸é‚»åƒç´ çš„å€¼ä¹‹é—´çš„å·®å¼‚
* ä»£ç å®ç°::

    K = torch.tensor([[1.0, -1.0]])

å‚æ•° X ï¼ˆæˆ‘ä»¬çš„ inputï¼‰ å’Œ K ï¼ˆæˆ‘ä»¬çš„ kernelï¼‰ æ‰§è¡Œ cross-correlation operation::

    Y = corr2d(X, K)
    Y
    # è¾“å‡º
    tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
            [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])

å°†å†…æ ¸åº”ç”¨äºè½¬ç½®å›¾åƒã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œå®ƒæ¶ˆå¤±äº†ã€‚å†…æ ¸ K ä»…æ£€æµ‹å‚ç›´è¾¹ç¼˜::

    corr2d(X.t(), K)
    # è¾“å‡º
    tensor([[0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.],
            [0., 0., 0., 0., 0.]])

7.2.4. Learning a Kernel
""""""""""""""""""""""""

.. code-block:: python

    # Construct a two-dimensional convolutional layer with 1 output channel and a kernel of shape (1, 2). 
    # For the sake of simplicity, we ignore the bias here
    conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)

    # The two-dimensional convolutional layer uses four-dimensional input and output in the format of (example, channel, height, width), 
    # where the batch size (number of examples in the batch) and the number of channels are both 1
    X = X.reshape((1, 1, 6, 8))
    Y = Y.reshape((1, 1, 6, 7))
    lr = 3e-2  # Learning rate

    for i in range(10):
        Y_hat = conv2d(X)
        l = (Y_hat - Y) ** 2
        conv2d.zero_grad()
        l.sum().backward()
        # Update the kernel
        conv2d.weight.data[:] -= lr * conv2d.weight.grad
        if (i + 1) % 2 == 0:
            print(f'epoch {i + 1}, loss {l.sum():.3f}')
    # è¾“å‡º
    epoch 2, loss 16.481
    epoch 4, loss 5.069
    epoch 6, loss 1.794
    epoch 8, loss 0.688
    epoch 10, loss 0.274


å­¦ä¹ çš„ kernel tensor::

    # ä¸æˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„æ ¸å¼ é‡ K éå¸¸æ¥è¿‘
    conv2d.weight.data.reshape((1, 2))
    # è¾“å‡º
    tensor([[ 1.0398, -0.9328]])



7.2.5. Cross-Correlation and Convolution
""""""""""""""""""""""""""""""""""""""""

äº¤å‰ç›¸å…³ (Cross-Correlation) ä¸å·ç§¯ (Convolution) çš„å…³ç³»::

    æ ¸å¿ƒå·®å¼‚ï¼š
        äº¤å‰ç›¸å…³ (Cross-Correlation)ï¼š ä¸ç¿»è½¬å·ç§¯æ ¸ï¼Œç›´æ¥åœ¨è¾“å…¥ä¸Šæ»‘åŠ¨å¹¶è®¡ç®—åŠ æƒå’Œã€‚
        å·ç§¯ (Convolution)ï¼š åœ¨æ‰§è¡Œäº¤å‰ç›¸å…³ä¹‹å‰ï¼Œå…ˆå°†å·ç§¯æ ¸æ°´å¹³å’Œå‚ç›´ç¿»è½¬ï¼Œç„¶åå†è¿›è¡Œæ»‘åŠ¨å’Œè®¡ç®—ã€‚

* äº¤å‰ç›¸å…³å…¬å¼

.. math::

    (f * g)(t) = \sum_x{f(x)g(x+t)}

* å·ç§¯å…¬å¼

.. math::

    (f * g)(t) = \sum_x{f(x)g(t-x)}

* æ ¸å¿ƒåŒºåˆ«ï¼š
    * äº¤å‰ç›¸å…³ä¸å¯¹ä¿¡å·è¿›è¡Œç¿»è½¬ï¼Œåªæ˜¯ç®€å•åœ°æ»‘åŠ¨å¹¶è®¡ç®—é‡å éƒ¨åˆ†çš„å†…ç§¯ã€‚
    * å·ç§¯å¯¹ä¿¡å·è¿›è¡Œç¿»è½¬ï¼ˆå³ ``ğ‘”(ğ‘¡)â†’ğ‘”(âˆ’ğ‘¡)`` ï¼‰ï¼Œç„¶åæ»‘åŠ¨å¹¶è®¡ç®—å†…ç§¯ã€‚


* ä¸ºä»€ä¹ˆæ·±åº¦å­¦ä¹ ä¸­äº¤å‰ç›¸å…³å’Œå·ç§¯æ— å®è´¨åŒºåˆ«::
    * åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå·ç§¯æ ¸ (filter) æ˜¯ä»æ•°æ®ä¸­å­¦ä¹ çš„ã€‚
    * ä¸è®ºå·ç§¯å±‚æ‰§è¡Œçš„æ˜¯ä¸¥æ ¼å·ç§¯è¿˜æ˜¯äº¤å‰ç›¸å…³ï¼Œå·ç§¯æ ¸çš„å­¦ä¹ è¿‡ç¨‹éƒ½ä¼šè‡ªåŠ¨è°ƒæ•´ï¼Œä½¿å¾—è¾“å‡ºç»“æœä¸€è‡´ã€‚
    * ç›´è§‚ç†è§£ï¼šå·ç§¯æ ¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯åŠ¨æ€è°ƒæ•´çš„ï¼Œå³ä¾¿åœ¨æ•°å­¦ä¸Šäº¤å‰ç›¸å…³å’Œå·ç§¯ç¨æœ‰ä¸åŒï¼Œè®­ç»ƒåæœ€ç»ˆå¾—åˆ°çš„å·ç§¯æ ¸å·²ç»éšå«è¿™ç§å·®å¼‚ï¼Œä½¿å¾—ä¸¤ç§æ“ä½œçš„è¾“å‡ºä¸€è‡´ã€‚

* æ·±åº¦å­¦ä¹ æœ¯è¯­çš„çº¦å®šä¿—æˆ
    * å°½ç®¡åœ¨ä¸¥æ ¼æ„ä¹‰ä¸Šï¼Œäº¤å‰ç›¸å…³å’Œå·ç§¯å­˜åœ¨å·®å¼‚ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ æ–‡çŒ®ä¸­ï¼Œäº¤å‰ç›¸å…³é€šå¸¸ä¹Ÿç›´æ¥è¢«ç§°ä¸ºâ€œå·ç§¯â€ã€‚
    * è¿™ç§çº¦å®šä½¿å¾—æœ¯è¯­æ›´åŠ ç®€æ´ï¼Œé¿å…åœ¨æè¿°æ¨¡å‹æ¶æ„æ—¶åå¤å¼ºè°ƒäºŒè€…çš„åŒºåˆ«ã€‚

* æ€»ç»“è¦ç‚¹ï¼š
    * äº¤å‰ç›¸å…³ä¸å·ç§¯ï¼š æ•°å­¦ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå·ç§¯æ¶‰åŠå·ç§¯æ ¸çš„ç¿»è½¬ï¼Œä½†åœ¨æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ä¸åŠ ä»¥åŒºåˆ†ã€‚
    * å®è´¨å½±å“ï¼š å·ç§¯æ ¸åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è°ƒæ•´ï¼Œå› æ­¤ä¸è®ºæ‰§è¡Œäº¤å‰ç›¸å…³è¿˜æ˜¯ä¸¥æ ¼å·ç§¯ï¼Œè¾“å‡ºç»“æœä¿æŒä¸€è‡´ã€‚
    * æœ¯è¯­çº¦å®šï¼š æ·±åº¦å­¦ä¹ æ–‡çŒ®ä¸­ï¼Œäº¤å‰ç›¸å…³æ“ä½œé€šå¸¸ç›´æ¥ç§°ä¸ºå·ç§¯ã€‚
    * ç®€åŒ–ç†è§£ï¼š æ·±åº¦å­¦ä¹ æ¨¡å‹å®ç°å·ç§¯å±‚æ—¶ï¼Œå…³æ³¨ç‚¹åœ¨äºå·ç§¯æ ¸çš„å­¦ä¹ å’Œç‰¹å¾æå–æ•ˆæœï¼Œè€Œéä¸¥æ ¼çš„æ•°å­¦å®šä¹‰å·®å¼‚ã€‚


7.2.6. Feature Map and Receptive Field
""""""""""""""""""""""""""""""""""""""

* ç‰¹å¾å›¾ (Feature Map) ä¸æ„Ÿå—é‡ (Receptive Field) çš„å®šä¹‰
    * ç‰¹å¾å›¾ (Feature Map)ï¼šç‰¹å¾å›¾æ˜¯å·ç§¯å±‚çš„è¾“å‡ºï¼Œå®ƒå¯ä»¥è¢«çœ‹ä½œæ˜¯å¯¹è¾“å…¥ç©ºé—´ç»´åº¦ï¼ˆå¦‚å®½åº¦å’Œé«˜åº¦ï¼‰çš„å­¦ä¹ è¡¨ç¤ºã€‚ç‰¹å¾å›¾åœ¨æ¯ä¸€å±‚æå–è¾“å…¥æ•°æ®çš„ä¸åŒç‰¹å¾ï¼Œä¾›åç»­å±‚è¿›ä¸€æ­¥å¤„ç†ã€‚
    * æ„Ÿå—é‡ (Receptive Field)ï¼šæ„Ÿå—é‡æ˜¯æŒ‡å·ç§¯ç½‘ç»œä¸­æŸä¸ªå…ƒç´  (å¦‚ç‰¹å¾å›¾ä¸­çš„ä¸€ä¸ªåƒç´ ) å—åˆ°å‰é¢å“ªäº›è¾“å…¥å…ƒç´ å½±å“çš„èŒƒå›´ã€‚æ¢å¥è¯è¯´ï¼Œæ„Ÿå—é‡è¡¨ç¤ºäº†è¾“å‡ºä¸­æŸä¸ªå…ƒç´ åœ¨è¾“å…¥ä¸­â€œçœ‹åˆ°â€çš„åŒºåŸŸå¤§å°ã€‚
    * å…³é”®ç‚¹ï¼š
        * æ„Ÿå—é‡ä¸ä»…å–å†³äºå½“å‰å±‚ï¼Œè¿˜å–å†³äºæ‰€æœ‰å‰é¢å±‚çš„ç´¯ç§¯å½±å“ã€‚
        * æ„Ÿå—é‡å¯ä»¥æ¯”è¾“å…¥æœ¬èº«æ›´å¤§ï¼Œè¡¨ç¤ºè¯¥ä½ç½®çš„ç‰¹å¾æ±‡é›†äº†æ›´å¤§åŒºåŸŸçš„ä¿¡æ¯ã€‚
    * ç›´è§‚ç†è§£ï¼š
        * æ¯å †å ä¸€å±‚å·ç§¯ï¼Œè¾“å‡ºçš„æ„Ÿå—é‡éƒ½ä¼šæ‰©å¤§ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿæ•æ‰æ›´å¹¿æ³›çš„ç©ºé—´ä¿¡æ¯ã€‚

* æ€»ç»“è¦ç‚¹ï¼š
    * ç‰¹å¾å›¾ï¼š æ˜¯å·ç§¯å±‚è¾“å‡ºçš„ç©ºé—´è¡¨ç¤ºï¼Œç”¨äºæå–è¾“å…¥çš„å±€éƒ¨ç‰¹å¾ã€‚
    * æ„Ÿå—é‡ï¼š æŒ‡å·ç§¯å±‚è¾“å‡ºä¸­æŸä¸ªå…ƒç´ åœ¨è¾“å…¥ä¸­â€œçœ‹åˆ°â€çš„åŒºåŸŸå¤§å°ã€‚æ„Ÿå—é‡å¯ä»¥éšç€ç½‘ç»œæ·±åº¦å¢åŠ è€Œæ‰©å¤§ï¼Œå¸®åŠ©æ¨¡å‹æ•æ‰æ›´å¤§èŒƒå›´çš„ä¿¡æ¯ã€‚
    * å±‚æ¬¡ç‰¹å¾æå–ï¼š è¾ƒä½å±‚æå–è¾¹ç¼˜å’Œç®€å•å½¢çŠ¶ï¼Œé«˜å±‚æå–å¤æ‚æ¨¡å¼å’Œè¯­ä¹‰ç‰¹å¾ã€‚
    * ç”Ÿç‰©å¯å‘ï¼š å·ç§¯çš„è®¾è®¡çµæ„Ÿæºäºè§†è§‰çš®å±‚çš„ç ”ç©¶ï¼Œè¯æ˜äº†å·ç§¯åœ¨ç”Ÿç‰©å’Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æœ‰æ•ˆæ€§ã€‚


7.2.7. Summary
""""""""""""""

* å·ç§¯å±‚æ ¸å¿ƒè®¡ç®—ï¼š
    * å·ç§¯å±‚çš„æ ¸å¿ƒè®¡ç®—æ˜¯äº’ç›¸å…³ (cross-correlation) æ“ä½œã€‚
    * è®¡ç®—äº’ç›¸å…³éå¸¸ç®€å•ï¼Œä½¿ç”¨ä¸€ä¸ªåµŒå¥—çš„ for å¾ªç¯å³å¯å®Œæˆï¼Œè¿™è¡¨æ˜å·ç§¯æ“ä½œçš„è®¡ç®—æ˜¯ç›´æ¥ä¸”å±€éƒ¨çš„ (local)ã€‚
    * å…³é”®ç‚¹ï¼š
        * å±€éƒ¨æ€§ï¼š äº’ç›¸å…³åªæ¶‰åŠè¾“å…¥æ•°æ®çš„å±€éƒ¨åŒºåŸŸï¼Œè¿™ç§å±€éƒ¨æ“ä½œåœ¨ç¡¬ä»¶ä¼˜åŒ–æ–¹é¢éå¸¸é‡è¦ã€‚
        * çŸ©é˜µè¿ç®—ï¼š å½“æœ‰å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºé€šé“æ—¶ï¼Œå·ç§¯å±‚çš„è®¡ç®—ç›¸å½“äºåœ¨é€šé“ä¹‹é—´è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ“ä½œï¼Œè¿›ä¸€æ­¥å¼ºè°ƒäº†è®¡ç®—çš„ç®€æ´æ€§å’Œå±€éƒ¨æ€§ã€‚

* å·ç§¯çš„åº”ç”¨åœºæ™¯ï¼š
    * è¾¹ç¼˜æ£€æµ‹ (Edge Detection)ï¼š è¯†åˆ«å›¾åƒä¸­çš„è¾¹ç¼˜å’Œè½®å»“ã€‚
    * çº¿æ¡æ£€æµ‹ (Line Detection)ï¼š æå–å›¾åƒä¸­çš„çº¿æ¡å’Œå½¢çŠ¶ã€‚
    * å›¾åƒæ¨¡ç³Š (Blurring)ï¼š å¹³æ»‘å›¾åƒï¼Œå‡å°‘å™ªå£°ã€‚
    * å›¾åƒé”åŒ– (Sharpening)ï¼š å¢å¼ºå›¾åƒçš„ç»†èŠ‚å’Œå¯¹æ¯”åº¦ã€‚

* å·ç§¯æ ¸å­¦ä¹ ï¼š
    *  ä¼ ç»Ÿæ–¹æ³•éœ€è¦äººå·¥è®¾è®¡æ»¤æ³¢å™¨ï¼ˆå¦‚ Sobel æ»¤æ³¢å™¨æˆ– Canny è¾¹ç¼˜æ£€æµ‹ï¼‰ï¼Œä½†å·ç§¯ç¥ç»ç½‘ç»œ (CNN) èƒ½å¤Ÿç›´æ¥ä»æ•°æ®ä¸­å­¦ä¹ æœ€ä¼˜çš„æ»¤æ³¢å™¨ã€‚
    *  è¿™ç§æ–¹å¼é¿å…äº†å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ï¼Œé€šè¿‡æ•°æ®é©±åŠ¨çš„æ–¹æ³•è‡ªåŠ¨å­¦ä¹ ç‰¹å¾ï¼Œæå¤§æé«˜äº†æ¨¡å‹çš„è¡¨ç°èƒ½åŠ›ã€‚
    *  æ ¸å¿ƒä¼˜åŠ¿ï¼šå–ä»£äº†æ‰‹å·¥è®¾è®¡ç‰¹å¾çš„å¯å‘å¼æ–¹æ³•ï¼Œè½¬è€Œä½¿ç”¨åŸºäºæ•°æ®çš„ç»Ÿè®¡æ–¹æ³•æ¥å­¦ä¹ æœ‰æ•ˆç‰¹å¾ã€‚


7.3. Padding and Stride
^^^^^^^^^^^^^^^^^^^^^^^

7.3.1. Padding
""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/MRcs2k.png

    Fig. 7.3.1 Pixel utilization for convolutions of size 1x1, 2x2, and 3x3 respectively.

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/DfsuXi.png

    Fig. 7.3.2 Two-dimensional cross-correlation with padding.


* In general, if we add a total of :math:`p_h` rows of padding (roughly half on top and half on bottom) and a total of :math:`p_w` columns of padding (roughly half on the left and half on the right), the output shape will be:

.. math::

    (n_h - k_h + p_h +1) \times (n_w - k_w + p_w +1)

* In many cases, we will want to set :math:`p_h = k_h -1` and :math:`p_w = k_w -1` to give the input and output the same height and width. 
* This will make it easier to predict the output shape of each layer when constructing the network. 
* æ‰€ä»¥ï¼ŒCNN é€šå¸¸ä½¿ç”¨é«˜åº¦å’Œå®½åº¦å€¼ä¸ºå¥‡æ•°çš„å·ç§¯æ ¸( :math:`k_h \time k_w` )ï¼Œä¾‹å¦‚ 1ã€3ã€5 æˆ– 7ã€‚é€‰æ‹©å¥‡æ•°çš„å†…æ ¸å¤§å°çš„å¥½å¤„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨é¡¶éƒ¨å’Œåº•éƒ¨ä½¿ç”¨ç›¸åŒçš„è¡Œæ•°ï¼Œå·¦ä¾§å’Œå³ä¾§å…·æœ‰ç›¸åŒæ•°é‡çš„åˆ—æ•°è¿›è¡Œå¡«å……æ—¶ä¿æŒç»´åº¦ï¼ˆå› ä¸ºKernelæ˜¯å¥‡æ•°æ—¶ï¼ŒPaddingæ˜¯å¶æ•°ï¼‰ã€‚

åˆ›å»ºä¸€ä¸ªé«˜åº¦å’Œå®½åº¦å‡ä¸º 3 çš„äºŒç»´å·ç§¯å±‚ï¼Œå¹¶åœ¨æ‰€æœ‰ä¾§é¢åº”ç”¨ 1 ä¸ªåƒç´ çš„å¡«å……ã€‚ç»™å®šä¸€ä¸ª height å’Œ width ä¸º 8 çš„è¾“å…¥ï¼Œæˆ‘ä»¬å‘ç°è¾“å‡ºçš„ height å’Œ width ä¹Ÿæ˜¯ 8

.. code-block:: python

    # We define a helper function to calculate convolutions. 
    # It initializes the convolutional layer weights and performs corresponding dimensionality elevations and reductions on the input and output
    def comp_conv2d(conv2d, X):
        # (1, 1) indicates that batch size and the number of channels are both 1
        X = X.reshape((1, 1) + X.shape)
        Y = conv2d(X)
        # Strip the first two dimensions: examples and channels
        return Y.reshape(Y.shape[2:])

    # 1 row and column is padded on either side, so a total of 2 rows or columns are added
    conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)
    X = torch.rand(size=(8, 8))
    comp_conv2d(conv2d, X).shape

å½“å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦ä¸åŒæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸º height å’Œ width è®¾ç½®ä¸åŒçš„å¡«å……æ•°æ¥ä½¿ output å’Œ input å…·æœ‰ç›¸åŒçš„ height å’Œ width::

    # We use a convolution kernel with height 5 and width 3. 
    # The padding on either side of the height and width are 2 and 1, respectively
    conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))
    comp_conv2d(conv2d, X).shape

* å¡«å……çš„ä½œç”¨å’Œæ„ä¹‰
    * æ ¸å¿ƒä½œç”¨ï¼šå¡«å……é€šè¿‡åœ¨è¾“å…¥å›¾åƒçš„è¾¹ç¼˜æ·»åŠ é¢å¤–åƒç´ ï¼ˆé€šå¸¸æ˜¯é›¶ï¼‰æ¥å¢åŠ è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ã€‚
    * ç›®çš„ï¼š
        * é˜²æ­¢è¾“å‡ºå°ºå¯¸ç¼©å°ï¼š åœ¨å·ç§¯è¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡å·ç§¯éƒ½ä¼šå¯¼è‡´è¾“å‡ºå°ºå¯¸ç¼©å°ã€‚å¦‚æœä¸å¸Œæœ›è¾“å‡ºå˜å°ï¼Œå¯ä»¥ä½¿ç”¨å¡«å……ä¿æŒè¾“å…¥å’Œè¾“å‡ºçš„å°ºå¯¸ä¸€è‡´ã€‚
        * ä¿æŒæ‰€æœ‰åƒç´ çš„å¹³ç­‰ä½¿ç”¨ï¼š åœ¨æ— å¡«å……çš„æƒ…å†µä¸‹ï¼Œè¾¹ç¼˜åƒç´ çš„ä½¿ç”¨é¢‘ç‡è¾ƒä½ã€‚å¡«å……ç¡®ä¿è¾¹ç¼˜åƒç´ ä¸ä¸­å¿ƒåƒç´ ä¸€æ ·é¢‘ç¹åœ°å‚ä¸è®¡ç®—ï¼Œä»è€Œæå‡æ¨¡å‹å¯¹è¾¹ç¼˜ç‰¹å¾çš„å­¦ä¹ èƒ½åŠ›ã€‚
    * å¸¸è§æ–¹å¼ï¼š
        * å¯¹ç§°å¡«å……ï¼š åœ¨è¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦ä¸¤ä¾§å‡åŒ€æ·»åŠ ç›¸åŒæ•°é‡çš„åƒç´ ï¼Œè®°ä½œ :math:`(ğ‘_â„,ğ‘_ğ‘¤)`
        * ç®€åŒ–è®°æ³•ï¼š å¦‚æœå‚ç›´å’Œæ°´å¹³å¡«å……ç›¸ç­‰ï¼Œç›´æ¥è®°ä½œ p

* é›¶å¡«å……çš„ä¼˜ç‚¹
    * è®¡ç®—ç®€å•ï¼š ç›´æ¥åœ¨å›¾åƒè¾¹ç¼˜è¡¥é›¶ï¼Œè®¡ç®—å’Œå®ç°éƒ½å¾ˆç®€å•ï¼Œç¡¬ä»¶ä¼˜åŒ–æ›´å®¹æ˜“ã€‚
    * éšå«ä½ç½®ä¿¡æ¯ï¼š CNNå¯ä»¥å­¦ä¹ åˆ°å¡«å……å€¼çš„åˆ†å¸ƒï¼Œä»è€Œç†è§£å›¾åƒçš„è¾¹ç¼˜å’Œä¸­å¿ƒåŒºåŸŸçš„å·®å¼‚ã€‚
    * å†…å­˜å‹å¥½ï¼š ä¸éœ€è¦é¢å¤–åˆ†é…å¤§é‡å†…å­˜ï¼Œå¯ä»¥åœ¨å·ç§¯æ“ä½œä¸­éšå¼å®Œæˆã€‚
* éé›¶å¡«å……çš„æƒ…å†µ
    * å­˜åœ¨å¤šç§éé›¶å¡«å……æ–¹æ³•ï¼š è™½ç„¶é›¶å¡«å……æ˜¯æœ€å¸¸ç”¨çš„ï¼Œä½†ä¹Ÿæœ‰å…¶ä»–æ–¹å¼ï¼ˆå¦‚é•œåƒå¡«å……ã€é‡å¤è¾¹ç¼˜å€¼ç­‰ï¼‰ã€‚
    * ä½¿ç”¨åœºæ™¯ï¼š é€šå¸¸åªæœ‰åœ¨å‘ç°é›¶å¡«å……å¸¦æ¥ä¸è‰¯çš„è§†è§‰æˆ–ç‰¹å¾æå–æ•ˆæœæ—¶ï¼Œæ‰ä¼šè€ƒè™‘ä½¿ç”¨å…¶ä»–ç±»å‹çš„å¡«å……ã€‚


7.3.2. Stride
"""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/XbNI1B.png

    Fig. 7.3.3 Cross-correlation with strides of 3 and 2 for height and width, respectively.

æ”¹å˜æ­¥é•¿çš„ä½œç”¨::

    è®¡ç®—æ•ˆç‡ï¼š è¾ƒå¤§çš„æ­¥é•¿å¯ä»¥å‡å°‘è®¡ç®—é‡ï¼Œå› ä¸ºæ»‘åŠ¨çª—å£è¦†ç›–çš„ä½ç½®æ›´å°‘ï¼Œè¾“å‡ºå¼ é‡çš„å°ºå¯¸ä¹Ÿæ›´å°ã€‚
    ä¸‹é‡‡æ · (Downsampling)ï¼š è¾ƒå¤§çš„æ­¥é•¿å¯ä»¥å¯¹è¾“å…¥è¿›è¡Œä¸‹é‡‡æ ·ï¼Œå‡å°‘åˆ†è¾¨ç‡çš„åŒæ—¶ä¿ç•™å…³é”®ä¿¡æ¯ã€‚
    å¤§å·ç§¯æ ¸ï¼š å¦‚æœå·ç§¯æ ¸è¾ƒå¤§ï¼Œå®ƒæœ¬èº«å·²èƒ½æ•è·å¤§èŒƒå›´çš„ç‰¹å¾ï¼Œæ­¤æ—¶ä½¿ç”¨è¾ƒå¤§æ­¥é•¿å¯ä»¥å‡å°‘å†—ä½™è®¡ç®—ã€‚




å½“ height çš„ stride ä¸º :math:`s_h` ä¸” width çš„ stride ä¸º :math:`s_w` æ—¶ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º

.. math::

    \lfloor\frac{(n_\textrm{h}-k_\textrm{h}+p_\textrm{h}+s_\textrm{h})}{s_\textrm{h}}\rfloor \times \lfloor\frac{(n_\textrm{w}-k_\textrm{w}+p_\textrm{w}+s_\textrm{w})}{s_\textrm{w}}\rfloor

* If we set :math:`p_\textrm{h}=k_\textrm{h}-1` and :math:`p_\textrm{w}=k_\textrm{w}-1` , then the output shape can be simplified to :math:`\lfloor\frac{(n_\textrm{h}+s_\textrm{h}-1)}{s_\textrm{h}}\rfloor \times \lfloor\frac{(n_\textrm{w}+s_\textrm{w}-1)}{s_\textrm{w}}\rfloor` .
* è¿›ä¸€æ­¥ç®€åŒ–ï¼Œè¾“å‡ºå°ºå¯¸ **çº¦** ç­‰äº :math:`(n_\textrm{h}/s_\textrm{h}) \times (n_\textrm{w}/s_\textrm{w})` .

å°† height å’Œ width çš„æ­¥å¹…éƒ½è®¾ç½®ä¸º 2ï¼Œä»è€Œå°†è¾“å…¥çš„ height å’Œ width å‡åŠ::

    X = torch.rand(size=(8, 8))

    conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)
    comp_conv2d(conv2d, X).shape
    # è¾“å‡º
    torch.Size([4, 4])
    # shapeè®¡ç®—è¿‡ç¨‹
    (n-k+s+1)/s = (8-3+2+1)/2 = 4

ç¨å¾®å¤æ‚ä¸€ç‚¹çš„ä¾‹å­::

    conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
    comp_conv2d(conv2d, X).shape
    # è¾“å‡º
    torch.Size([2, 2])
    # è¯´æ˜
    ç¬¬ä¸€ç»´: (8-3+0+3)/3 = 2.67   -> 2
    ç¬¬äºŒç»´: (8-5+2+4)/4 = 2.25   -> 2


* æ­¥å¹…çš„ä½œç”¨å’Œæ„ä¹‰
    * æ ¸å¿ƒä½œç”¨ï¼šæ­¥å¹…æ§åˆ¶å·ç§¯çª—å£åœ¨è¾“å…¥å›¾åƒä¸Šæ»‘åŠ¨çš„æ­¥é•¿ï¼ˆç§»åŠ¨è·ç¦»ï¼‰ã€‚
    * ç›®çš„ï¼š
        * é™ä½è¾“å‡ºåˆ†è¾¨ç‡ï¼š æ­¥å¹…å¤§äº1æ—¶ï¼Œæ¯æ¬¡å·ç§¯çª—å£æ»‘åŠ¨ä¼šè·³è¿‡éƒ¨åˆ†ä½ç½®ï¼Œä»è€Œå‡å°‘è¾“å‡ºå°ºå¯¸ã€‚ä¾‹å¦‚ï¼Œæ­¥å¹…ä¸º2æ—¶ï¼Œè¾“å‡ºçš„å°ºå¯¸æ˜¯è¾“å…¥çš„ 1/2ã€‚
        * åŠ é€Ÿè®¡ç®—ï¼š å¤§æ­¥å¹…å‡å°‘äº†å·ç§¯è®¡ç®—æ¬¡æ•°ï¼Œæé«˜äº†è®¡ç®—æ•ˆç‡ã€‚
    * å¸¸è§æ–¹å¼ï¼š
        * å¯¹ç§°æ­¥å¹…ï¼š å¦‚æœæ°´å¹³å’Œå‚ç›´æ­¥å¹…ç›¸åŒï¼Œç›´æ¥è®°ä½œ ğ‘ 
        * é»˜è®¤è®¾ç½®ï¼š å¡«å……é»˜è®¤ä¸º0ï¼Œæ­¥å¹…é»˜è®¤ä¸º1ï¼Œå³ä¸å¡«å……ä¸”æ»‘åŠ¨ä¸€ä¸ªåƒç´ ã€‚

è®¡ç®—è¾“å‡ºå°ºå¯¸
""""""""""""

* GPTçš„è¾“å‡ºï¼ˆå¥½åƒä¸å¯¹ï¼Œå¾…ç¡®å®šï¼‰

.. math::

    \begin{array}{l}
    H_{\text {out }}=\left\lfloor\frac{H_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel_size }-1)-1}{\text { stride }}+1\right\rfloor \\
    W_{\text {out }}=\left\lfloor\frac{W_{\text {in }}+2 \times \text { padding }- \text { dilation } \times(\text { kernel_size }-1)-1}{\text { stride }}+1\right\rfloor
    \end{array}

* è¾“å…¥é«˜åº¦å’Œå®½åº¦ï¼šH_in = 32, W_in = 32
* å·ç§¯æ ¸å¤§å°ï¼škernel_size = 3
* æ­¥å¹…ï¼šstride = 1
* å¡«å……ï¼špadding = 1
* æ‰©å¼ ç‡ï¼šdilation = 1



æ€»ç»“
""""

* å¡«å……å’Œæ­¥å¹…æ˜¯å·ç§¯å±‚çš„é‡è¦è¶…å‚æ•°ï¼Œå®ƒä»¬å½±å“è¾“å‡ºå°ºå¯¸ã€è®¡ç®—æ•ˆç‡å’Œç‰¹å¾æå–æ•ˆæœã€‚
    * å¡«å……ï¼š ä¿æŒå°ºå¯¸ä¸€è‡´ï¼Œé˜²æ­¢è¾¹ç¼˜ä¿¡æ¯ä¸¢å¤±ã€‚
    * æ­¥å¹…ï¼š æ§åˆ¶åˆ†è¾¨ç‡ï¼Œå‡å°‘è®¡ç®—é‡ã€‚




7.4. Multiple Input and Multiple Output Channels
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

7.4.1. Multiple Input Channels
""""""""""""""""""""""""""""""

æ ¸å¿ƒè¦ç‚¹
++++++++

* å¤šé€šé“è¾“å…¥çš„å·ç§¯æ ¸å¿ƒè¦æ±‚ï¼š
    * å½“è¾“å…¥æ•°æ®æœ‰å¤šä¸ªé€šé“ï¼ˆchannelsï¼‰æ—¶ï¼Œå·ç§¯æ ¸çš„é€šé“æ•°éœ€è¦å’Œè¾“å…¥æ•°æ®çš„é€šé“æ•°ç›¸åŒï¼Œæ‰èƒ½è¿›è¡Œé€é€šé“çš„äº¤å‰ç›¸å…³è¿ç®—ã€‚
    * å¦‚æœè¾“å…¥æ•°æ®æœ‰ :math:`ğ‘_{in}`  ä¸ªé€šé“ï¼Œé‚£ä¹ˆå·ç§¯æ ¸ä¹Ÿéœ€è¦æœ‰ :math:`ğ‘_{in}` ä¸ªé€šé“ã€‚

* å·ç§¯æ ¸çš„ç»“æ„ï¼š
    * å½“è¾“å…¥æ•°æ®åªæœ‰ä¸€ä¸ªé€šé“ï¼ˆ :math:`ğ‘_{in}=1` ï¼‰ï¼Œå·ç§¯æ ¸æ˜¯ä¸€ä¸ªç®€å•çš„äºŒç»´å¼ é‡ï¼Œå½¢çŠ¶æ˜¯ :math:`ğ‘˜_h \times ğ‘˜_w`
    * å½“è¾“å…¥æ•°æ®æœ‰å¤šä¸ªé€šé“ï¼ˆ :math:`ğ‘_{in}>1` ï¼‰ï¼Œå·ç§¯æ ¸å¯¹æ¯ä¸ªè¾“å…¥é€šé“éƒ½æœ‰ä¸€ä¸ªäºŒç»´å¼ é‡ï¼Œæœ€ç»ˆçš„å·ç§¯æ ¸å½¢çŠ¶ä¸º :math:`ğ‘_{in} \times ğ‘˜_h \times ğ‘˜_w`

* å¤šé€šé“å·ç§¯è®¡ç®—æ–¹å¼ï¼š
    * å¯¹æ¯ä¸ªé€šé“åˆ†åˆ«è¿›è¡ŒäºŒç»´äº¤å‰ç›¸å…³æ“ä½œï¼Œç„¶åå°†æ¯ä¸ªé€šé“çš„ç»“æœç›¸åŠ ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/QkorY1.png

    Fig. 7.4.1 Cross-correlation computation with two input channels.

å®ç°å…·æœ‰å¤šä¸ª input é€šé“çš„äº¤å‰ç›¸å…³æ“ä½œ::

    def corr2d_multi_in(X, K):
        # Iterate through the 0th dimension (channel) of K first, then add them up
        return sum(d2l.corr2d(x, k) for x, k in zip(X, K))

æ„é€ ä¸å›¾ 7.4.1 ä¸­çš„å€¼ K ç›¸å¯¹åº”çš„è¾“å…¥å¼ é‡ X å’Œæ ¸å¼ é‡ï¼Œä»¥éªŒè¯äº’ç›¸å…³è¿ç®—çš„è¾“å‡º::

    X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],
                      [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])
    K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], 
                      [[1.0, 2.0], [3.0, 4.0]]])
    corr2d_multi_in(X, K)
    # è¾“å‡º
    tensor([[ 56.,  72.],
            [104., 120.]])
    # è®¡ç®—: n-k+1 = 3-2+1 = 2
    # shape: torch.Size([2, 2])


7.4.2. Multiple Output Channels
"""""""""""""""""""""""""""""""

* è¦äº§ç”Ÿå¤šä¸ªè¾“å‡ºé€šé“ï¼Œéœ€è¦ä¸ºæ¯ä¸ªè¾“å‡ºé€šé“åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„å·ç§¯æ ¸ã€‚
* æ­¤æ—¶ï¼Œå·ç§¯æ ¸æˆä¸ºä¸€ä¸ª 4 ç»´å¼ é‡ï¼Œå½¢çŠ¶ä¸º ( :math:`c_o \times c_i \times k_h \times k_w` )
* å…¶ä¸­ ( :math:`c_o, c_i` ) æ˜¯è¾“å‡ºå’Œè¾“å…¥é€šé“çš„æ•°é‡ï¼› ( :math:`k_h, k_w` ) æ˜¯å†…æ ¸çš„é«˜åº¦å’Œå®½åº¦
* æ¯ä¸ªè¾“å‡ºé€šé“çš„ç»“æœéƒ½ç”±å…¶å¯¹åº”çš„å·ç§¯æ ¸è®¡ç®—å¾—å‡ºï¼Œå¹¶ä»æ‰€æœ‰è¾“å…¥é€šé“è·å–è¾“å…¥ã€‚


å®ç°ä¸€ä¸ªäº’ç›¸å…³å‡½æ•°æ¥è®¡ç®—å¤šä¸ªé€šé“çš„è¾“å‡º::

    def corr2d_multi_in_out(X, K):
        # Iterate through the 0th dimension of K, and each time, perform cross-correlation operations with input X. 
        # All of the results are stacked together
        return torch.stack([corr2d_multi_in(X, k) for k in K], 0)

We construct a trivial convolution kernel with three output channels by concatenating the kernel tensor for K with K+1 and K+2::

    K = torch.stack((K, K + 1, K + 2), 0)
    K.shape
    # è¾“å‡º
    torch.Size([3, 2, 2, 2])

å¯¹ X å…·æœ‰æ ¸å¼ é‡çš„è¾“å…¥å¼ é‡æ‰§è¡Œäº’ç›¸å…³è¿ç®— K::

    corr2d_multi_in_out(X, K)
    # è¾“å‡º
    tensor([[[ 56.,  72.],
             [104., 120.]],

            [[ 76., 100.],
             [148., 172.]],

            [[ 96., 128.],
             [192., 224.]]])
    # shapeè®¡ç®—è¿‡ç¨‹

    # shape: torch.Size([3, 2, 2])



7.4.3. 1x1 Convolutional Layer
""""""""""""""""""""""""""""""

* 1x1 å·ç§¯æ ¸ä¸è€ƒè™‘é«˜åº¦å’Œå®½åº¦ç»´åº¦ä¸Šçš„ç›¸é‚»åƒç´ ï¼Œä»…åœ¨é€šé“ç»´åº¦ä¸Šè¿›è¡Œæ“ä½œã€‚
* å®ƒå¯¹è¾“å…¥å›¾åƒä¸­ç›¸åŒä½ç½®çš„å…ƒç´ è¿›è¡Œçº¿æ€§ç»„åˆã€‚
* å¯ä»¥å°†å…¶è§†ä¸ºåœ¨æ¯ä¸ªåƒç´ ä½ç½®åº”ç”¨çš„å…¨è¿æ¥å±‚ï¼Œå°† ( :math:`c_i` ) ä¸ªè¾“å…¥å€¼è½¬æ¢ä¸º ( :math:`c_o` ) ä¸ªè¾“å‡ºå€¼ï¼Œä½†æƒé‡åœ¨åƒç´ ä½ç½®ä¹‹é—´å…±äº«ã€‚
* å®ƒéœ€è¦ ( :math:`c_o \times c_i` ) ä¸ªæƒé‡ï¼ˆåŠ ä¸Šåç½®ï¼‰

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/m0G1zI.png

    Fig. 7.4.2 The cross-correlation computation uses the convolution kernel with three input channels and two output channels. The input and output have the same height and width.

ä½¿ç”¨å…¨è¿æ¥å±‚å®ç° 1x1 å·ç§¯::

    def corr2d_multi_in_out_1x1(X, K):
        c_i, h, w = X.shape
        c_o = K.shape[0]
        X = X.reshape((c_i, h * w))
        K = K.reshape((c_o, c_i))
        # Matrix multiplication in the fully connected layer
        Y = torch.matmul(K, X)
        return Y.reshape((c_o, h, w))

ç”¨ä¸€äº›ç¤ºä¾‹æ•°æ®æ¥æ£€æŸ¥::

    X = torch.normal(0, 1, (3, 3, 3))
    K = torch.normal(0, 1, (2, 3, 1, 1))
    Y1 = corr2d_multi_in_out_1x1(X, K)
    Y2 = corr2d_multi_in_out(X, K)
    assert float(torch.abs(Y1 - Y2).sum()) < 1e-6

7.4.4. Discussion
"""""""""""""""""

* è®¡ç®—æˆæœ¬
    * å·ç§¯æ“ä½œå¤æ‚åº¦ï¼š
        * ç»™å®šä¸€ä¸ªå¤§å°ä¸º ``â„Ã—ğ‘¤`` çš„å›¾åƒï¼Œä½¿ç”¨ä¸€ä¸ª ``ğ‘˜Ã—ğ‘˜`` çš„å·ç§¯æ ¸
            * è®¡ç®—å¤æ‚åº¦æ˜¯ï¼š :math:`ğ‘‚(â„â‹…ğ‘¤â‹…ğ‘˜^2)`
        * å¦‚æœè¾“å…¥é€šé“æ•°æ˜¯ :math:`ğ‘_i` ï¼Œè¾“å‡ºé€šé“æ•°æ˜¯ :math:`ğ‘_o` 
            * å¤æ‚åº¦å¢åŠ ä¸ºï¼š :math:`ğ‘‚(â„â‹…ğ‘¤â‹…ğ‘˜^2â‹…ğ‘_iâ‹…ğ‘_o)`
    * è®¡ç®—ç¤ºä¾‹ï¼š
        * å¯¹ä¸€ä¸ª 256Ã—256 çš„å›¾åƒï¼Œä½¿ç”¨ 5Ã—5 çš„å·ç§¯æ ¸ï¼Œè¾“å…¥å’Œè¾“å‡ºé€šé“æ•°å‡ä¸º 128ï¼Œè®¡ç®—é‡è¶…è¿‡ 530 äº¿æ¬¡æ“ä½œï¼ˆä¹˜æ³•å’ŒåŠ æ³•åˆ†å¼€è®¡ç®—ï¼‰ã€‚
        * è¿™æ˜¯å› ä¸ºå·ç§¯æ“ä½œä¸ä»…éœ€è¦éå†æ•´ä¸ªå›¾åƒï¼Œè¿˜è¦åœ¨æ‰€æœ‰é€šé“ä¸Šè¿›è¡Œè¿ç®—ï¼Œè®¡ç®—é‡ä¼šè¿…é€Ÿå¢åŠ ã€‚

* å…³é”®ç†è§£ï¼š
    * å¤šé€šé“å¢åŠ äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†ä¹Ÿå¸¦æ¥äº†è®¡ç®—æˆæœ¬çš„ä¸Šå‡ã€‚
    * è®¾è®¡ CNN ç»“æ„æ—¶ï¼Œéœ€è¦åœ¨è®¡ç®—å¤æ‚åº¦å’Œæ¨¡å‹è¡¨è¾¾èƒ½åŠ›ä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚
    * æœªæ¥çš„æ¨¡å‹è®¾è®¡ä¸­ï¼Œé™ä½è®¡ç®—æˆæœ¬æ˜¯é‡è¦çš„ç ”ç©¶æ–¹å‘ä¹‹ä¸€ã€‚

7.5. Pooling
^^^^^^^^^^^^

* æ± åŒ–çš„ç›®çš„:
    * å‡å°‘å·ç§¯å±‚å¯¹è¾“å…¥ç‰¹å¾ç²¾ç¡®ä½ç½®çš„æ•æ„Ÿæ€§ï¼Œå³æé«˜å¹³ç§»ä¸å˜æ€§ã€‚è¿™æ„å‘³ç€å³ä½¿ç‰¹å¾ç¨å¾®ç§»åŠ¨ï¼Œæ¨¡å‹ä»ç„¶å¯ä»¥æ£€æµ‹åˆ°å®ƒã€‚
    * åœ¨ç©ºé—´ä¸Šå¯¹ç‰¹å¾è¡¨ç¤ºè¿›è¡Œä¸‹é‡‡æ ·ã€‚è¿™ä¼šé™ä½ç‰¹å¾å›¾çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œä»è€Œå‡å°‘è¡¨ç¤ºçš„å¤§å°ï¼Œå¹¶åŠ å¿«è®¡ç®—é€Ÿåº¦


7.5.1. Maximum Pooling and Average Pooling
""""""""""""""""""""""""""""""""""""""""""

* æ± åŒ–æ“ä½œä½¿ç”¨ **å›ºå®šå½¢çŠ¶çš„çª—å£ï¼ˆæ± åŒ–çª—å£ï¼‰** åœ¨è¾“å…¥ä¸Šæ»‘åŠ¨
* ä¸å·ç§¯å±‚ä¸­ inputs å’Œ kernel çš„äº’ç›¸å…³è®¡ç®—ä¸åŒï¼Œæ± åŒ–å±‚ä¸åŒ…å«ä»»ä½•å‚æ•°ï¼ˆæ²¡æœ‰ kernel ï¼‰
* æœ€å¤§æ± åŒ– (max-pooling) è®¡ç®—æ¯ä¸ªæ± åŒ–çª—å£å†…çš„æœ€å¤§å€¼
* å¹³å‡æ± åŒ– (average pooling) è®¡ç®—æ¯ä¸ªæ± åŒ–çª—å£å†…çš„å¹³å‡å€¼
* åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæœ€å¤§æ± åŒ–ä¼˜äºå¹³å‡æ± åŒ–ã€‚æœ€å¤§æ± åŒ–çš„æ¦‚å¿µæºäºè®¤çŸ¥ç¥ç»ç§‘å­¦ï¼Œç”¨äºæè¿°å¯¹è±¡è¯†åˆ«ä¸­ä¿¡æ¯çš„å±‚çº§èšåˆ
* å¹³å‡æ± åŒ–å¯ä»¥çœ‹ä½œæ˜¯å¯¹å›¾åƒè¿›è¡Œä¸‹é‡‡æ ·çš„ä¸€ç§æ–¹å¼ï¼Œé€šè¿‡ç»„åˆå¤šä¸ªç›¸é‚»åƒç´ çš„ä¿¡æ¯æ¥æé«˜ä¿¡å™ªæ¯”

* æ± åŒ–å¦‚ä½•å·¥ä½œ:
    * æ± åŒ–çª—å£ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹æ»‘åŠ¨è¾“å…¥å¼ é‡ã€‚
    * åœ¨æ¯ä¸ªä½ç½®ï¼Œå®ƒè®¡ç®—çª—å£å†…è¾“å…¥å­å¼ é‡çš„æœ€å¤§å€¼æˆ–å¹³å‡å€¼.
    * ä¸€ä¸ª ( :math:`p \times q` ) çš„æ± åŒ–å±‚ä¼šåœ¨è¯¥å°ºå¯¸çš„åŒºåŸŸä¸Šè¿›è¡Œèšåˆã€‚ä¾‹å¦‚ï¼Œå¯¹è¾“å…¥å¼ é‡ [,,] è¿›è¡Œ (2\times 2) æœ€å¤§æ± åŒ–ï¼Œä¼šäº§ç”Ÿè¾“å‡º


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/5x5Tif.png

    Fig. 7.5.1 Max-pooling with a pooling window shape of 2x2. The shaded portions are the first output element as well as the input tensor elements used for the output computation: ``max(0,1,3,4)=4`` .è¯´æ˜ï¼š2x2æ»‘åŠ¨å®¹å™¨çš„æœ€å¤§æ± åŒ–å°±æ˜¯ï¼Œè¿™è·å–4ä¸ªå…ƒç´ é‡Œé¢çš„æœ€å¤§å€¼


æ± åŒ–å±‚çš„æ­£å‘ä¼ æ’­(ä¸éœ€è¦å†…æ ¸ï¼Œå°†è¾“å‡ºè®¡ç®—ä¸ºè¾“å…¥ä¸­æ¯ä¸ªåŒºåŸŸçš„æœ€å¤§å€¼æˆ–å¹³å‡å€¼)::

    def pool2d(X, pool_size, mode='max'):
        p_h, p_w = pool_size
        Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))
        for i in range(Y.shape[0]):
            for j in range(Y.shape[1]):
                if mode == 'max':
                    Y[i, j] = X[i: i + p_h, j: j + p_w].max()
                elif mode == 'avg':
                    Y[i, j] = X[i: i + p_h, j: j + p_w].mean()
        return Y

éªŒè¯äºŒç»´æœ€å¤§æ± åŒ–å±‚çš„è¾“å‡º::

    X = torch.tensor([[0.0, 1.0, 2.0], 
                      [3.0, 4.0, 5.0], 
                      [6.0, 7.0, 8.0]])
    pool2d(X, (2, 2))
    # è¾“å‡º
    tensor([[4., 5.],
            [7., 8.]])

å¹³å‡æ± åŒ–å±‚::

    pool2d(X, (2, 2), 'avg')
    # è¾“å‡º
    tensor([[2., 3.],
            [5., 6.]])


7.5.2. Padding and Stride
"""""""""""""""""""""""""

ç”¨å†…ç½®çš„äºŒç»´ max-pooling å±‚æ¥æ¼”ç¤º padding å’Œ strides åœ¨æ± åŒ–å±‚ä¸­çš„ä½¿ç”¨::

    X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))
    X
    # è¾“å‡º
    tensor([[[[ 0.,  1.,  2.,  3.],
              [ 4.,  5.,  6.,  7.],
              [ 8.,  9., 10., 11.],
              [12., 13., 14., 15.]]]])

æ·±åº¦å­¦ä¹ æ¡†æ¶é»˜è®¤åŒ¹é…æ± åŒ–çª—å£å¤§å°å’Œæ­¥å¹…(if we use a pooling window of shape (3, 3) we get a stride shape of (3, 3) by default)::

    pool2d = nn.MaxPool2d(3)
    # Pooling has no model parameters, hence it needs no initialization
    pool2d(X)
    # è¾“å‡º
    tensor([[[[10.]]]])

æ‰‹åŠ¨æŒ‡å®šæ­¥å¹…å’Œå¡«å……æ¥è¦†ç›–æ¡†æ¶é»˜è®¤å€¼::

    pool2d = nn.MaxPool2d(3, padding=1, stride=2)
    pool2d(X)
    # è¾“å‡º
    tensor([[[[ 5.,  7.],
              [13., 15.]]]])

æŒ‡å®šä¸€ä¸ªä»»æ„é«˜åº¦å’Œå®½åº¦çš„ä»»æ„çŸ©å½¢æ± åŒ–çª—å£::

    pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))
    pool2d(X)
    # è¾“å‡º
    tensor([[[[ 5.,  7.],
              [13., 15.]]]])


7.5.3. Multiple Channels
""""""""""""""""""""""""

* åœ¨å¤„ç†å¤šé€šé“ input æ•°æ®æ—¶ï¼Œpooling layer å•ç‹¬æ± åŒ–æ¯ä¸ª input é€šé“ï¼Œè€Œä¸æ˜¯åƒå·ç§¯å±‚é‚£æ ·åœ¨ channels ä¸Šå¯¹ inputs æ±‚å’Œã€‚è¿™æ„å‘³ç€æ± åŒ–å±‚çš„è¾“å‡ºé€šé“æ•°ä¸è¾“å…¥é€šé“æ•°ç›¸åŒ
* è¿æ¥å¼  X é‡ å’Œ X + 1 é€šé“ç»´åº¦ï¼Œä»¥æ„é€ å…·æœ‰ä¸¤ä¸ªé€šé“çš„è¾“å…¥::

    X = torch.cat((X, X + 1), 1)
    X
    # è¾“å‡º
    tensor([[[[ 0.,  1.,  2.,  3.],
              [ 4.,  5.,  6.,  7.],
              [ 8.,  9., 10., 11.],
              [12., 13., 14., 15.]],

             [[ 1.,  2.,  3.,  4.],
              [ 5.,  6.,  7.,  8.],
              [ 9., 10., 11., 12.],
              [13., 14., 15., 16.]]]])

pooling å output channels çš„æ•°é‡ä»ç„¶æ˜¯ 2::

    pool2d = nn.MaxPool2d(3, padding=1, stride=2)
    pool2d(X)
    # tensor([[[[ 5.,  7.],
                [13., 15.]],

               [[ 6.,  8.],
                [14., 16.]]]])


7.5.4. Summary
""""""""""""""

* æ± åŒ–æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æ“ä½œã€‚
* pooling ä¸ channels æ— å…³ï¼Œå³ï¼Œå®ƒä¿æŒ channels æ•°é‡ä¸å˜ï¼Œå¹¶ä¸”åˆ†åˆ«åº”ç”¨äºæ¯ä¸ª channelã€‚
* åœ¨ä¸¤ç§æµè¡Œçš„æ± åŒ–é€‰é¡¹ä¸­ï¼Œmax-pooling æ¯” average pooling æ›´å¯å–ï¼Œå› ä¸ºå®ƒä¸º output èµ‹äºˆäº†ä¸€å®šç¨‹åº¦çš„ä¸å˜æ€§ã€‚
* ä¸€ç§å¸¸è§çš„é€‰æ‹©æ˜¯é€‰æ‹©æ± åŒ–çª—å£å¤§å° 2x2 ï¼Œä»¥å°†è¾“å‡ºçš„åŸç©ºé—´åˆ†è¾¨ç‡çš„å››åˆ†ä¹‹ä¸€ã€‚
* æ‰©å±•
    * stochastic pooling (Zeiler and Fergus, 2013)
    * fractional max-pooling (Graham, 2014)


7.6. Convolutional Neural Networks (LeNet)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

7.6.1. LeNet
""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/jCTEhQ.png

    Fig. 7.6.1 Data flow in LeNet. The input is a handwritten digit, the output is a probability over 10 possible outcomes.

* The basic units in each **convolutional** block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation.
* Each convolutional layer uses a 5x5 kernel and a sigmoid activation function. 
* è¿™äº›å±‚å°†ç©ºé—´æ’åˆ—çš„è¾“å…¥æ˜ å°„åˆ°è®¸å¤šäºŒç»´ç‰¹å¾å›¾ï¼Œé€šå¸¸ä¼šå¢åŠ é€šé“çš„æ•°é‡ã€‚ç¬¬ä¸€ä¸ªå·ç§¯å±‚æœ‰ 6 ä¸ªè¾“å‡ºé€šé“ï¼Œè€Œç¬¬äºŒä¸ªå·ç§¯å±‚æœ‰ 16 ä¸ªè¾“å‡ºé€šé“ã€‚

ä»£ç å®ç°æ­¤ç±»æ¨¡å‹::

    def init_cnn(module):  #@save
        """Initialize weights for CNNs."""
        if type(module) == nn.Linear or type(module) == nn.Conv2d:
            nn.init.xavier_uniform_(module.weight)

    class LeNet(d2l.Classifier):  #@save
        """The LeNet-5 model."""
        def __init__(self, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),
                nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),
                nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(),
                nn.LazyLinear(120), nn.Sigmoid(),
                nn.LazyLinear(84), nn.Sigmoid(),
                nn.LazyLinear(num_classes))

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/fsDzSL.png

    Fig. 7.6.2 Compressed notation for LeNet-5.


.. code-block:: python

    @d2l.add_to_class(d2l.Classifier)  #@save
    def layer_summary(self, X_shape):
        X = torch.randn(*X_shape)
        for layer in self.net:
            X = layer(X)
            print(layer.__class__.__name__, 'output shape:\t', X.shape)

    model = LeNet()
    model.layer_summary((1, 1, 28, 28))
    # è¾“å‡º
    Conv2d output shape:         torch.Size([1, 6, 28, 28])
    Sigmoid output shape:        torch.Size([1, 6, 28, 28])
    AvgPool2d output shape:      torch.Size([1, 6, 14, 14])
    Conv2d output shape:         torch.Size([1, 16, 10, 10])
    Sigmoid output shape:        torch.Size([1, 16, 10, 10])
    AvgPool2d output shape:      torch.Size([1, 16, 5, 5])
    Flatten output shape:        torch.Size([1, 400])
    Linear output shape:         torch.Size([1, 120])
    Sigmoid output shape:        torch.Size([1, 120])
    Linear output shape:         torch.Size([1, 84])
    Sigmoid output shape:        torch.Size([1, 84])
    Linear output shape:         torch.Size([1, 10])


7.6.2. Training
"""""""""""""""

è®¡ç®—æˆæœ¬æ¯”ç±»ä¼¼æ·±åº¦çš„ MLP æ›´é«˜::

    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128)
    model = LeNet(lr=0.1)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)
    trainer.fit(model, data)



8. Modern Convolutional Neural Networks
---------------------------------------

* ç°ä»£ CNN ä¸ä»…å¯ä»¥è¢«ä½¿ç”¨ ç›´æ¥ç”¨äºè§†è§‰ä»»åŠ¡ï¼Œä½†å®ƒä»¬ä¹Ÿä½œä¸ºåŸºæœ¬ç‰¹å¾ ç”¨äºæ›´é«˜çº§ä»»åŠ¡ï¼ˆä¾‹å¦‚è·Ÿè¸ªï¼‰çš„ç”Ÿæˆå™¨ ï¼ˆ Zhang et al. ï¼Œ2021 ï¼‰ ï¼Œåˆ†å‰² ï¼ˆ Long et al. ï¼Œ2015 ï¼‰ ï¼Œç‰©ä½“æ£€æµ‹ ï¼ˆ Redmon å’Œ Farhadiï¼Œ2018 ï¼‰ ï¼Œæˆ–é£æ ¼è½¬å˜ ï¼ˆç›–è’‚æ–¯ç­‰äººï¼Œ2016 ï¼‰ ã€‚
* ä» AlexNet å¼€å§‹ç°ä»£ CNN ä¹‹æ—… ï¼ˆ Krizhevsky et al. ï¼Œ2012 ï¼‰ ï¼Œç¬¬ä¸€ä¸ªå¤§è§„æ¨¡éƒ¨ç½²ç½‘ç»œä»¥å‡»è´¥ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æ–¹æ³• å¤§è§„æ¨¡çš„è§†åŠ›æŒ‘æˆ˜ï¼› 
* VGGç½‘ç»œ ï¼ˆ Simonyan å’Œ Zissermanï¼Œ2014 ï¼‰ ï¼Œå®ƒåˆ©ç”¨äº†ä¸€äº› é‡å¤çš„å…ƒç´ å—ï¼›
* ç½‘ç»œä¸­çš„ç½‘ç»œ (NiN) åœ¨è¾“å…¥ä¸Šé€å—å·ç§¯æ•´ä¸ªç¥ç»ç½‘ç»œ ï¼ˆæ—ç­‰äººï¼Œ2013 ï¼‰ ï¼› 
* GoogLeNet ä½¿ç”¨å¤šåˆ†æ”¯å·ç§¯ç½‘ç»œï¼ˆ Szegedyç­‰äººï¼Œ2015 ï¼‰ ï¼›
* æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰ ï¼ˆ He et al. ï¼Œ2016 ï¼‰ ï¼Œå®ƒä»ç„¶æ˜¯è®¡ç®—æœºè§†è§‰ä¸­æœ€æµè¡Œçš„ç°æˆæ¶æ„ä¹‹ä¸€ï¼› ResNeXt å—ï¼ˆ Xie et al. , 2017 ï¼‰ç”¨äºç¨€ç–è¿æ¥ï¼›
* DenseNet ï¼ˆ Huang et al. ï¼Œ2017 ï¼‰ç”¨äºæ®‹å·®æ¶æ„çš„æ³›åŒ–ã€‚

8.1. Deep Convolutional Neural Networks (AlexNet)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


8.1.1. Representation Learning
""""""""""""""""""""""""""""""

* åœ¨ AlexNet å‡ºç°ä¹‹å‰ï¼Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå¹¶æœªå æ®ä¸»å¯¼åœ°ä½ã€‚å°½ç®¡ LeNet åœ¨æ—©æœŸçš„å°å‹æ•°æ®é›†ä¸Šå–å¾—äº†ä¸é”™çš„æ•ˆæœï¼Œä½†åœ¨æ›´å¤§çš„ã€æ›´çœŸå®çš„æ•°æ®é›†ä¸Šè®­ç»ƒ CNN çš„æ€§èƒ½å’Œå¯è¡Œæ€§å°šæœªå¾—åˆ°è¯å®ã€‚
* ç¬¬ä¸€ä¸ªç°ä»£ CNNï¼ˆKrizhevsky ç­‰äººï¼Œ2012ï¼‰ä»¥å…¶å‘æ˜è€…ä¹‹ä¸€ Alex Krizhevsky çš„åå­—å‘½åä¸º AlexNetï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å¯¹ LeNet çš„è¿›åŒ–æ”¹è¿›ã€‚å®ƒåœ¨2012å¹´ImageNetæŒ‘æˆ˜èµ›ä¸­å–å¾—äº†ä¼˜å¼‚çš„è¡¨ç°ã€‚
* AlexNet (2012) åŠå…¶å‰èº« LeNet (1995) å…±äº«è®¸å¤šæ¶æ„å…ƒç´ ã€‚è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šä¸ºä»€ä¹ˆèŠ±äº†è¿™ä¹ˆé•¿æ—¶é—´ï¼Ÿä¸€ä¸ªå…³é”®çš„åŒºåˆ«æ˜¯ï¼Œåœ¨è¿‡å»çš„äºŒåå¹´é‡Œï¼Œå¯ç”¨çš„æ•°æ®é‡å’Œè®¡ç®—èƒ½åŠ›æ˜¾ç€å¢åŠ ã€‚å› æ­¤ï¼ŒAlexNet è§„æ¨¡è¦å¤§å¾—å¤šï¼šä¸ 1995 å¹´å¯ç”¨çš„ CPU ç›¸æ¯”ï¼Œå®ƒä½¿ç”¨æ›´å¤šçš„æ•°æ®å’Œæ›´å¿«çš„ GPU è¿›è¡Œè®­ç»ƒã€‚


8.1.2. AlexNet
""""""""""""""

* AlexNet é‡‡ç”¨ 8 å±‚ CNNï¼Œä»¥å¤§å¹…ä¼˜åŠ¿èµ¢å¾—äº† 2012 å¹´ ImageNet å¤§è§„æ¨¡è§†è§‰è¯†åˆ«æŒ‘æˆ˜èµ› (Russakovsky et al., 2013)ã€‚è¯¥ç½‘ç»œé¦–æ¬¡è¡¨æ˜ï¼Œé€šè¿‡å­¦ä¹ è·å¾—çš„ç‰¹å¾å¯ä»¥è¶…è¶Šäººå·¥è®¾è®¡çš„ç‰¹å¾ï¼Œæ‰“ç ´äº†è®¡ç®—æœºè§†è§‰ä»¥å¾€çš„èŒƒå¼ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/tGioru.png

    Fig. 8.1.2 From LeNet (left) to AlexNet (right).


.. code-block:: python

    class AlexNet(d2l.Classifier):
        def __init__(self, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.LazyConv2d(96, kernel_size=11, stride=4, padding=1),
                nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2),
                nn.LazyConv2d(256, kernel_size=5, padding=2), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2),
                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
                nn.LazyConv2d(384, kernel_size=3, padding=1), nn.ReLU(),
                nn.LazyConv2d(256, kernel_size=3, padding=1), nn.ReLU(),
                nn.MaxPool2d(kernel_size=3, stride=2), nn.Flatten(),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(p=0.5),
                nn.LazyLinear(4096), nn.ReLU(),nn.Dropout(p=0.5),
                nn.LazyLinear(num_classes))
            self.net.apply(d2l.init_cnn)
        def layer_summary(self, X_shape):
            X = d2l.randn(*X_shape)
            for layer in self.net:
                X = layer(X)
                print(layer.__class__.__name__, 'output shape:\t', X.shape)


æ„é€ ä¸€ä¸ªé«˜å’Œå®½å‡ä¸º224çš„å•é€šé“æ•°æ®ç¤ºä¾‹æ¥è§‚å¯Ÿæ¯ä¸€å±‚çš„è¾“å‡ºå½¢çŠ¶::

    AlexNet().layer_summary((1, 1, 224, 224))
    # è¾“å‡º
    Conv2d output shape:         torch.Size([1, 96, 54, 54])
    ReLU output shape:   torch.Size([1, 96, 54, 54])
    MaxPool2d output shape:      torch.Size([1, 96, 26, 26])
    Conv2d output shape:         torch.Size([1, 256, 26, 26])
    ReLU output shape:   torch.Size([1, 256, 26, 26])
    MaxPool2d output shape:      torch.Size([1, 256, 12, 12])
    Conv2d output shape:         torch.Size([1, 384, 12, 12])
    ReLU output shape:   torch.Size([1, 384, 12, 12])
    Conv2d output shape:         torch.Size([1, 384, 12, 12])
    ReLU output shape:   torch.Size([1, 384, 12, 12])
    Conv2d output shape:         torch.Size([1, 256, 12, 12])
    ReLU output shape:   torch.Size([1, 256, 12, 12])
    MaxPool2d output shape:      torch.Size([1, 256, 5, 5])
    Flatten output shape:        torch.Size([1, 6400])
    Linear output shape:         torch.Size([1, 4096])
    ReLU output shape:   torch.Size([1, 4096])
    Dropout output shape:        torch.Size([1, 4096])
    Linear output shape:         torch.Size([1, 4096])
    ReLU output shape:   torch.Size([1, 4096])
    Dropout output shape:        torch.Size([1, 4096])
    Linear output shape:         torch.Size([1, 10])


8.1.3. Training
"""""""""""""""

::

    model = AlexNet(lr=0.01)
    data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))  # ä½¿ç”¨ resize å‚æ•°æ‰§è¡Œæ­¤å¤§å°è°ƒæ•´ï¼Œä¸ºäº†é€‚é…æ¨¡å‹æ”¹é€ çš„æ•°æ®
    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    trainer.fit(model, data)

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/yeTMvt.png


8.2. Networks Using Blocks (VGG)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* VGG (Visual Geometry Group)

èƒŒæ™¯å’Œæ¦‚å¿µæ¼”è¿›
""""""""""""""

* æ·±åº¦ç½‘ç»œè®¾è®¡çš„æ¼”å˜
    * AlexNet è¯æ˜äº†æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„æœ‰æ•ˆæ€§ï¼Œä½†æ²¡æœ‰æä¾›é€šç”¨çš„è®¾è®¡æ¨¡æ¿ã€‚
    * éšç€ç ”ç©¶çš„å‘å±•ï¼Œç½‘ç»œè®¾è®¡é€æ¸ä»å•ä¸ªç¥ç»å…ƒæ‰©å±•åˆ°æ•´å±‚ï¼Œå†åˆ°å¦‚ä»Šçš„â€œæ¨¡å—åŒ–è®¾è®¡â€ï¼Œå³åŸºäºé‡å¤æ¨¡å¼çš„å—ï¼ˆblocksï¼‰ã€‚è¿™ç§æ¨¡å—åŒ–ç†å¿µæºè‡ªèŠ¯ç‰‡è®¾è®¡é¢†åŸŸä¸­é€»è¾‘å•å…ƒåˆ°é€»è¾‘å—çš„æŠ½è±¡è¿‡ç¨‹ã€‚
* VGG çš„æå‡º
    * VGG æ˜¯ç”±ç‰›æ´¥å¤§å­¦çš„è§†è§‰å‡ ä½•ç»„ï¼ˆVisual Geometry Groupï¼‰æå‡ºçš„ç½‘ç»œï¼Œå…¶æ ¸å¿ƒåˆ›æ–°æ˜¯é€šè¿‡é‡å¤çš„å·ç§¯å—è®¾è®¡æ·±åº¦ç½‘ç»œã€‚
    * VGG çš„è®¾è®¡æ—¨åœ¨æ¢ç´¢æ·±å±‚ç½‘ç»œä¸å®½ç½‘ç»œçš„æ€§èƒ½å·®å¼‚ï¼Œæœ€ç»ˆéªŒè¯äº†æ·±è€Œçª„çš„ç½‘ç»œåœ¨æ€§èƒ½ä¸Šä¼˜äºæµ…è€Œå®½çš„ç½‘ç»œã€‚


VGG çš„æ ¸å¿ƒè®¾è®¡
""""""""""""""

8.2.1. VGG Blocks
+++++++++++++++++

* ä¸€ä¸ª VGG å—ç”±å¤šä¸ª 3x3 çš„å·ç§¯å±‚ç»„æˆï¼Œæ¯ä¸ªå·ç§¯å±‚åæ¥ä¸€ä¸ªéçº¿æ€§æ¿€æ´»ï¼ˆå¦‚ ReLUï¼‰ï¼Œå†æ¥ä¸€ä¸ª 2x2 çš„æœ€å¤§æ± åŒ–å±‚ï¼ˆstride ä¸º 2ï¼‰ã€‚
* ä½¿ç”¨å¤šä¸ª 3x3 å·ç§¯å±‚æ›¿ä»£å•ä¸ªè¾ƒå¤§å·ç§¯ï¼ˆå¦‚ 5x5 æˆ– 7x7ï¼‰ï¼Œæ—¢èƒ½å‡å°‘å‚æ•°æ•°é‡ï¼Œåˆèƒ½æé«˜æ€§èƒ½ã€‚

.. code-block:: python

    def vgg_block(num_convs, out_channels):
        layers = []
        for _ in range(num_convs):
            layers.append(nn.LazyConv2d(out_channels, kernel_size=3, padding=1))
            layers.append(nn.ReLU())
        layers.append(nn.MaxPool2d(kernel_size=2,stride=2))
        return nn.Sequential(*layers)




8.2.2. VGG Network
++++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/WFgQj9.png

    Fig. 8.2.1 From AlexNet to VGG. The key difference is that VGG consists of blocks of layers, whereas AlexNetâ€™s layers are all designed individually.


* VGG ç½‘ç»œå¯ä»¥åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š
    * å·ç§¯éƒ¨åˆ†ï¼šç”±å¤šä¸ª VGG å—ç»„æˆï¼Œé€æ¸å‡å°‘ç©ºé—´åˆ†è¾¨ç‡ã€‚
    * å…¨è¿æ¥éƒ¨åˆ†ï¼šä¸ AlexNet ç±»ä¼¼ï¼ŒåŒ…å«å¤šä¸ªå…¨è¿æ¥å±‚ã€‚
* åŸå§‹ VGG-11 ç½‘ç»œåŒ…æ‹¬ 5 ä¸ªå·ç§¯å—ï¼Œæœ€åˆçš„ä¸¤ä¸ªå—å„æœ‰ 1 å±‚å·ç§¯ï¼Œåä¸‰ä¸ªå—å„æœ‰ 2 å±‚å·ç§¯ã€‚æ¯ä¸ªå—çš„è¾“å‡ºé€šé“æ•°é€æ­¥ç¿»å€ï¼Œä» 64 å¢åŠ åˆ° 512ã€‚

.. code-block:: python

    class VGG(d2l.Classifier):
        def __init__(self, arch, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            conv_blks = []
            for (num_convs, out_channels) in arch:
                conv_blks.append(vgg_block(num_convs, out_channels))
            self.net = nn.Sequential(
                *conv_blks, 
                nn.Flatten(),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
                nn.LazyLinear(4096), nn.ReLU(), nn.Dropout(0.5),
                nn.LazyLinear(num_classes))
            self.net.apply(d2l.init_cnn)


8.2.3. Training
+++++++++++++++

.. code-block:: python

    model = VGG(arch=((1, 16), (1, 32), (2, 64), (2, 128), (2, 128)), lr=0.01)
    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model, data)


æ„ä¹‰å’Œæ‰©å±•
""""""""""

* VGG çš„è´¡çŒ®
    * VGG æ˜¯é¦–ä¸ªçœŸæ­£ç°ä»£åŒ–çš„ CNN ç½‘ç»œï¼Œå¼•å…¥äº†â€œæ¨¡å—åŒ–è®¾è®¡â€çš„ç†å¿µï¼Œé€šè¿‡æ·±è€Œçª„çš„æ¶æ„æ˜¾è‘—æå‡æ€§èƒ½ã€‚
    * å®ƒå¼€åˆ›äº†è®¾è®¡â€œç½‘ç»œå®¶æ—â€çš„è¶‹åŠ¿ï¼Œå³é€šè¿‡è°ƒæ•´å—çš„æ•°é‡å’Œå‚æ•°ï¼Œå½¢æˆå…·æœ‰ä¸åŒæ€§èƒ½å’Œå¤æ‚åº¦çš„ç½‘ç»œå˜ä½“ã€‚
* æ€§èƒ½æƒè¡¡
    * åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè®¾è®¡è€…å¯ä»¥æ ¹æ®éœ€è¦åœ¨é€Ÿåº¦å’Œç²¾åº¦ä¹‹é—´æ‰¾åˆ°é€‚åˆçš„å¹³è¡¡ç‚¹ã€‚
* æ–°æ–¹å‘
    * æœ€è¿‘çš„ç ”ç©¶ï¼ˆå¦‚ ParNetï¼‰è¡¨æ˜ï¼Œé€šè¿‡æ›´å¤šå¹¶è¡Œè®¡ç®—ï¼Œæµ…å±‚ç½‘ç»œä¹Ÿå¯ä»¥å®ç°ç«äº‰æ€§æ€§èƒ½ï¼Œè¿™å¯èƒ½ä¸ºæœªæ¥çš„ç½‘ç»œæ¶æ„è®¾è®¡æä¾›æ–°æ€è·¯ã€‚

æ€»ç»“
""""

* VGG çš„è®¾è®¡ä¸ä»…å¥ å®šäº†ç°ä»£æ·±åº¦å­¦ä¹ ç½‘ç»œçš„åŸºç¡€ï¼Œä¹Ÿä¸ºæ·±åº¦å­¦ä¹ çš„æ™®åŠå’Œå¿«é€Ÿå®ç°æä¾›äº†ä¾¿åˆ©ã€‚å…¶æ¨¡å—åŒ–è®¾è®¡æ€æƒ³è‡³ä»Šä»è¢«åç»­ç ”ç©¶å¹¿æ³›å€Ÿé‰´ã€‚




8.3. Network in Network (NiN)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* NiN ç½‘ç»œåœ¨ LeNetã€AlexNet å’Œ VGG çš„åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ”¹è¿›
* VGG ç­‰è¿™äº›ç½‘ç»œé€šè¿‡å·ç§¯å±‚å’Œæ± åŒ–å±‚æå–ç©ºé—´ç‰¹å¾ï¼Œå¹¶é€šè¿‡å…¨è¿æ¥å±‚è¿›è¡Œåç»­å¤„ç†ã€‚
* è¿™ç§è®¾è®¡å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
    * å…¨è¿æ¥å±‚å‚æ•°é‡å¤§ï¼šä¼ ç»Ÿçš„å…¨è¿æ¥å±‚éœ€è¦å¤§é‡çš„å‚æ•°å’Œå†…å­˜ï¼Œä¾‹å¦‚ VGG-11 çš„å…¨è¿æ¥å±‚å°±å æ®äº†è¿‘ 400MB å†…å­˜ã€‚è¿™åœ¨ç§»åŠ¨è®¾å¤‡å’ŒåµŒå…¥å¼è®¾å¤‡ä¸Šéš¾ä»¥å®ç°ã€‚
    * æ— æ³•åœ¨ç½‘ç»œæ—©æœŸå¼•å…¥å…¨è¿æ¥å±‚ï¼šåœ¨ç½‘ç»œæ—©æœŸå¼•å…¥å…¨è¿æ¥å±‚ä¼šç ´åç©ºé—´ç»“æ„ï¼ŒåŒæ—¶è¿˜ä¼šè¿›ä¸€æ­¥å¢åŠ å†…å­˜éœ€æ±‚ã€‚

* NiN çš„åˆ›æ–°ç‚¹
    * 1x1 å·ç§¯ï¼šä½œä¸ºå±€éƒ¨å…¨è¿æ¥å±‚ï¼Œåœ¨ä¸æ”¹å˜ç©ºé—´ç»´åº¦çš„æƒ…å†µä¸‹ä¸ºæ¯ä¸ªåƒç´ ä½ç½®æ·»åŠ éçº¿æ€§ã€‚
    * å…¨å±€å¹³å‡æ± åŒ–ï¼šåœ¨ç½‘ç»œæœ€åä¸€å±‚é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–æ•´åˆç©ºé—´ä¿¡æ¯ï¼Œå®Œå…¨æ›¿ä»£å…¨è¿æ¥å±‚ï¼Œå¤§å¹…å‡å°‘å‚æ•°é‡ã€‚




8.3.1. NiN Blocks
"""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/8X2MQC.png

    Fig. 8.3.1 Comparing the architectures of VGG and NiN, and of their blocks.

* æ³¨æ„ NiN å—ä¸­çš„å·®å¼‚ï¼ˆåˆå§‹å·ç§¯åé¢æ˜¯ 1x1 å·ç§¯ï¼Œè€Œ VGG ä¿ç•™ 3x3 å·ç§¯ï¼‰ä»¥åŠæœ€åæˆ‘ä»¬ä¸å†éœ€è¦å·¨å¤§çš„å…¨è¿æ¥å±‚ã€‚
* ç»“æ„ç‰¹ç‚¹
    * å·ç§¯è®¾è®¡ï¼šåˆå§‹å·ç§¯å±‚ä¸ AlexNet ç±»ä¼¼ï¼Œä½¿ç”¨ 11x11ã€5x5 å’Œ 3x3 çš„å·ç§¯æ ¸ã€‚
    * NiN å—ï¼šæ¯ä¸ªå—åŒ…å«ä¸€ä¸ªæ ‡å‡†å·ç§¯å±‚ï¼Œåæ¥ä¸¤ä¸ª 1x1 å·ç§¯å±‚ï¼Œæå‡ç‰¹å¾æå–æ•ˆç‡ã€‚
    * å…¨å±€å¹³å‡æ± åŒ–ï¼šåœ¨æœ€åçš„ç‰¹å¾è¡¨ç¤ºå±‚é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–ä»£æ›¿å…¨è¿æ¥å±‚ï¼Œç”¨äºç”Ÿæˆåˆ†ç±»çš„ logitsã€‚

* ä¼˜åŠ¿
    * å‚æ•°é‡æ˜¾è‘—å‡å°‘ï¼šä¸éœ€è¦å¤§å‹çš„å…¨è¿æ¥å±‚ï¼Œé€‚åˆå†…å­˜å—é™çš„è®¾å¤‡ã€‚
    * æå‡å¹³ç§»ä¸å˜æ€§ï¼šå…¨å±€å¹³å‡æ± åŒ–å¢å¼ºäº†æ¨¡å‹å¯¹å¹³ç§»çš„é²æ£’æ€§ã€‚
    * æé«˜éçº¿æ€§å»ºæ¨¡èƒ½åŠ›ï¼šé€šè¿‡ 1x1 å·ç§¯åœ¨æ¯ä¸ªä½ç½®æ•è·é€šé“é—´çš„äº¤äº’ï¼Œæé«˜ç½‘ç»œè¡¨è¾¾èƒ½åŠ›ã€‚


.. code-block:: python

    def nin_block(out_channels, kernel_size, strides, padding):
        return nn.Sequential(
            nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),
            nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),
            nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())


8.3.2. NiN Model
""""""""""""""""

* NiN ä¸ AlexNet å’Œ VGG ä¹‹é—´çš„ç¬¬äºŒä¸ªæ˜¾ç€åŒºåˆ«æ˜¯ NiN å®Œå…¨é¿å…äº†å…¨è¿æ¥å±‚ã€‚
* NiN ä½¿ç”¨ NiN å—ï¼Œå…¶è¾“å‡ºé€šé“æ•°ç­‰äºæ ‡ç­¾ç±»çš„æ•°é‡ï¼Œåè·Ÿå…¨å±€å¹³å‡æ± åŒ–å±‚ï¼Œäº§ç”Ÿ logits å‘é‡ã€‚è¿™ç§è®¾è®¡æ˜¾ç€å‡å°‘äº†æ‰€éœ€æ¨¡å‹å‚æ•°çš„æ•°é‡ï¼Œä½†ä»£ä»·æ˜¯å¯èƒ½ä¼šå¢åŠ è®­ç»ƒæ—¶é—´ã€‚

.. code-block:: python

    class NiN(d2l.Classifier):
        def __init__(self, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nin_block(96, kernel_size=11, strides=4, padding=0),
                nn.MaxPool2d(3, stride=2),
                nin_block(256, kernel_size=5, strides=1, padding=2),
                nn.MaxPool2d(3, stride=2),
                nin_block(384, kernel_size=3, strides=1, padding=1),
                nn.MaxPool2d(3, stride=2),
                nn.Dropout(0.5),
                nin_block(num_classes, kernel_size=3, strides=1, padding=1),
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten())
            self.net.apply(d2l.init_cnn)


åˆ›å»ºä¸€ä¸ªæ•°æ®ç¤ºä¾‹æ¥æŸ¥çœ‹æ¯ä¸ªå—çš„è¾“å‡ºå½¢çŠ¶::

    NiN().layer_summary((1, 1, 224, 224))
    # è¾“å‡º
    Sequential output shape:     torch.Size([1, 96, 54, 54])
    MaxPool2d output shape:      torch.Size([1, 96, 26, 26])
    Sequential output shape:     torch.Size([1, 256, 26, 26])
    MaxPool2d output shape:      torch.Size([1, 256, 12, 12])
    Sequential output shape:     torch.Size([1, 384, 12, 12])
    MaxPool2d output shape:      torch.Size([1, 384, 5, 5])
    Dropout output shape:        torch.Size([1, 384, 5, 5])
    Sequential output shape:     torch.Size([1, 10, 5, 5])
    AdaptiveAvgPool2d output shape:      torch.Size([1, 10, 1, 1])
    Flatten output shape:        torch.Size([1, 10])





8.3.3. Training
"""""""""""""""

.. code-block:: python

    model = NiN(lr=0.05)
    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128, resize=(224, 224))
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model, data)

8.3.4. Summary
""""""""""""""

* NiN çš„å‚æ•°æ¯” AlexNet å’Œ VGG å°‘å¾—å¤šã€‚è¿™ä¸»è¦æºäºè¿™æ ·ä¸€ä¸ªäº‹å®ï¼šå®ƒä¸éœ€è¦å·¨å¤§çš„å…¨è¿æ¥å±‚ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨å…¨å±€å¹³å‡æ± æ¥èšåˆç½‘ç»œä¸»ä½“æœ€åé˜¶æ®µä¹‹åçš„æ‰€æœ‰å›¾åƒä½ç½®ã€‚è¿™æ¶ˆé™¤äº†å¯¹æ˜‚è´µçš„ï¼ˆå­¦ä¹ çš„ï¼‰å½’çº¦æ“ä½œçš„éœ€è¦ï¼Œå¹¶ç”¨ç®€å•çš„å¹³å‡å€¼ä»£æ›¿å®ƒä»¬ã€‚
* é€‰æ‹©è¾ƒå°‘çš„å®½æ ¸å·ç§¯å¹¶å°†å…¶æ›¿æ¢ä¸º 1x1 å·ç§¯æœ‰åŠ©äºè¿›ä¸€æ­¥å‡å°‘å‚æ•°ã€‚å®ƒå¯ä»¥æ»¡è¶³ä»»ä½•ç»™å®šä½ç½®å†…è·¨é€šé“çš„å¤§é‡éçº¿æ€§ã€‚ 1x1 å·ç§¯å’Œå…¨å±€å¹³å‡æ± åŒ–éƒ½æ˜¾ç€å½±å“äº†åç»­çš„ CNN è®¾è®¡ã€‚

* ã€å½±å“ã€‘NiN çš„ 1x1 å·ç§¯å’Œå…¨å±€å¹³å‡æ± åŒ–è®¾è®¡å¯¹åç»­çš„ CNN ç½‘ç»œæ¶æ„äº§ç”Ÿäº†æ·±è¿œå½±å“ï¼Œä½¿å¾—æ¨¡å‹åœ¨ä¿æŒé«˜å‡†ç¡®ç‡çš„åŒæ—¶å˜å¾—æ›´åŠ é«˜æ•ˆã€‚


8.4. Multi-Branch Networks (GoogLeNet)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 2014 å¹´ï¼ŒGoogLeNet èµ¢å¾—äº† ImageNet æŒ‘æˆ˜èµ›ï¼ˆSzegedy ç­‰äººï¼Œ2015 å¹´ï¼‰ï¼Œå…¶ç»“æ„ç»“åˆäº† NiNï¼ˆLin ç­‰äººï¼Œ2013 å¹´ï¼‰ã€é‡å¤å—ï¼ˆSimonyan å’Œ Zissermanï¼Œ2014 å¹´ï¼‰ä»¥åŠæ··åˆå·ç§¯çš„ä¼˜ç‚¹å†…æ ¸ã€‚
* GoogLeNet çš„å…³é”®è´¡çŒ®æ˜¯ç½‘ç»œä¸»ä½“çš„è®¾è®¡ã€‚å®ƒå·§å¦™åœ°è§£å†³äº†å·ç§¯æ ¸é€‰æ‹©çš„é—®é¢˜ã€‚

è®¾è®¡æ¦‚è¿°
""""""""

* åˆ›æ–°ç‚¹
    * **NiNï¼ˆNetwork in Networkï¼‰** çš„æ€æƒ³ï¼Œå³åˆ©ç”¨ 1x1 å·ç§¯æ¥æå–ç‰¹å¾å¹¶å‡å°‘è®¡ç®—é‡ã€‚
    * é‡å¤æ¨¡å—çš„è®¾è®¡ï¼Œç±»ä¼¼äºVGGç½‘ç»œçš„æ¨¡å—åŒ–æ€æƒ³ã€‚
    * å¤šç§å·ç§¯æ ¸çš„ç»“åˆï¼Œé€šè¿‡å¤šåˆ†æ”¯ç»“æ„åŒæ—¶ä½¿ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸ï¼Œè€Œä¸æ˜¯å•ç‹¬é€‰æ‹©æŸä¸€ç§ã€‚

* GoogLeNeté¦–æ¬¡æ˜ç¡®äº†å·ç§¯ç¥ç»ç½‘ç»œçš„ä¸‰éƒ¨åˆ†ç»“æ„ï¼š
    * Stem(è¾“å…¥éƒ¨åˆ†): 
        * å‰ä¸¤ä¸‰ä¸ªå·ç§¯å±‚ï¼Œç”¨äºæå–ä½çº§ç‰¹å¾ã€‚
        * ä»¥AlexNetå’ŒLeNetä¸ºåŸºç¡€çš„ 7x7 å·ç§¯å’Œæœ€å¤§æ± åŒ–ã€‚
    * Body(ä¸»ä½“éƒ¨åˆ†): 
        * å¤šä¸ªå·ç§¯å—ï¼Œç”¨äºæ·±å±‚æ¬¡ç‰¹å¾æå–ã€‚
        * æ¯ç»„åŒ…å«å¤šä¸ªInceptionå—ï¼Œä¾æ¬¡å¢åŠ é€šé“æ•°ã€‚
    * Head(è¾“å‡ºéƒ¨åˆ†): 
        * å°†æå–çš„ç‰¹å¾æ˜ å°„åˆ°ç‰¹å®šçš„ä»»åŠ¡ï¼ˆåˆ†ç±»ã€æ£€æµ‹ç­‰ï¼‰ã€‚
        * é€šè¿‡å…¨å±€å¹³å‡æ± åŒ–ï¼Œç›´æ¥è¾“å‡ºåˆ†ç±»ç»“æœã€‚




8.4.1. Inception Blocks
"""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/hLbjyp.png

    Fig. 8.4.1 Structure of the Inception block.(GoogLeNet ä¸­çš„åŸºæœ¬å·ç§¯å—ç§°ä¸º Inception å—)

* èµ·å§‹å—ç”±å››ä¸ªå¹¶è¡Œåˆ†æ”¯ç»„æˆã€‚
    * ä½¿ç”¨ 1x1 å·ç§¯æå–ä½çº§ç‰¹å¾ã€‚
    * ä½¿ç”¨ 1x1 å·ç§¯é™ç»´åï¼Œå†ç”¨ 3x3 å·ç§¯æå–ä¸­ç­‰å°ºåº¦ç‰¹å¾ã€‚
    * ä½¿ç”¨ 1x1 å·ç§¯é™ç»´åï¼Œå†ç”¨ 5x5 å·ç§¯æå–æ›´å¤§å°ºåº¦ç‰¹å¾ã€‚
    * ä½¿ç”¨ 3x3 æœ€å¤§æ± åŒ–åï¼Œå†ç”¨ 1x1 å·ç§¯æ”¹å˜é€šé“æ•°ã€‚
* æœ€åï¼Œæ¯ä¸ªåˆ†æ”¯çš„è¾“å‡ºæ²¿ç€é€šé“ç»´åº¦è¿æ¥å¹¶æ„æˆå—çš„è¾“å‡ºã€‚
* å…³é”®è¶…å‚æ•°ï¼šæ¯å±‚çš„è¾“å‡ºé€šé“æ•°ï¼Œå†³å®šäº†ä¸åŒåˆ†æ”¯åˆ†é…çš„å®¹é‡å¤§å°ã€‚

.. code-block:: python

    class Inception(nn.Module):
        # c1--c4 are the number of output channels for each branch
        def __init__(self, c1, c2, c3, c4, **kwargs):
            super(Inception, self).__init__(**kwargs)
            # Branch 1
            self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)
            # Branch 2
            self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)
            self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)
            # Branch 3
            self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)
            self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)
            # Branch 4
            self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
            self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)

        def forward(self, x):
            b1 = F.relu(self.b1_1(x))
            b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))
            b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))
            b4 = F.relu(self.b4_2(self.b4_1(x)))
            return torch.cat((b1, b2, b3, b4), dim=1)


8.4.2. GoogLeNet Model
""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/bXEC5A.png

    Fig. 8.4.2 The GoogLeNet architecture.

* GoogLeNet ä½¿ç”¨æ€»å…± 9 ä¸ªåˆå§‹å—çš„å †æ ˆï¼Œåˆ†ä¸ºä¸‰ç»„ï¼Œä¸­é—´æœ‰æœ€å¤§æ± åŒ–ï¼Œå¤´éƒ¨æœ‰å…¨å±€å¹³å‡æ± åŒ–æ¥ç”Ÿæˆä¼°è®¡ã€‚
* åˆå§‹å—ä¹‹é—´çš„æœ€å¤§æ± åŒ–é™ä½äº†ç»´åº¦ã€‚

ç¬¬ä¸€ä¸ªæ¨¡å—ä½¿ç”¨ 64 é€šé“ 7x7 å·ç§¯å±‚::

    class GoogleNet(d2l.Classifier):
        def b1(self):
            return nn.Sequential(
                nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),
                nn.ReLU(), 
                nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

ç¬¬äºŒä¸ªæ¨¡å—ä½¿ç”¨ä¸¤ä¸ªå·ç§¯å±‚ï¼šé¦–å…ˆæ˜¯ 64 é€šé“ 1x1 å·ç§¯å±‚ï¼Œç„¶åæ˜¯é€šé“æ•°é‡å¢åŠ ä¸‰å€çš„ 3x3 å·ç§¯å±‚::

    @d2l.add_to_class(GoogleNet)
    def b2(self):
        return nn.Sequential(
            nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),
            nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

ç¬¬ä¸‰ä¸ªæ¨¡å—ä¸²è”ä¸¤ä¸ªå®Œæ•´çš„Inceptionå—::

    # ç¬¬ä¸€ä¸ª Inception å—çš„è¾“å‡ºé€šé“æ•°ä¸º:  64+128+32+32 = 256
    # ç¬¬äºŒä¸ª Inception å—çš„è¾“å‡ºé€šé“æ•°ä¸º: 128+192+96+64 = 480
    @d2l.add_to_class(GoogleNet)
    def b3(self):
        return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),
                             Inception(128, (128, 192), (32, 96), 64),
                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

ç¬¬å››ä¸ªæ¨¡å—æ¯”è¾ƒå¤æ‚ã€‚å®ƒä¸²è”è¿æ¥äº†äº”ä¸ª Inception å—::

    @d2l.add_to_class(GoogleNet)
    def b4(self):
        return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),
                             Inception(160, (112, 224), (24, 64), 64),
                             Inception(128, (128, 256), (24, 64), 64),
                             Inception(112, (144, 288), (32, 64), 64),
                             Inception(256, (160, 320), (32, 128), 128),
                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))

ç¬¬äº”ä¸ªæ¨¡å—å¸¦æœ‰ä¸¤ä¸ª Inception å—::

    @d2l.add_to_class(GoogleNet)
    def b5(self):
        return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),
                             Inception(384, (192, 384), (48, 128), 128),
                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())

å°†å®ƒä»¬å…¨éƒ¨ç»„è£…æˆä¸€ä¸ªå®Œæ•´çš„ç½‘ç»œ::

    @d2l.add_to_class(GoogleNet)
    def __init__(self, lr=0.1, num_classes=10):
        super(GoogleNet, self).__init__()
        self.save_hyperparameters()
        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),
                                 self.b5(), nn.LazyLinear(num_classes))
        self.net.apply(d2l.init_cnn)

çœ‹çœ‹å„ä¸ªæ¨¡å—ä¹‹é—´è¾“å‡ºå½¢çŠ¶çš„å˜åŒ–::

    model = GoogleNet().layer_summary((1, 1, 96, 96))
    # è¾“å‡º
    Sequential output shape:     torch.Size([1, 64, 24, 24])
    Sequential output shape:     torch.Size([1, 192, 12, 12])
    Sequential output shape:     torch.Size([1, 480, 6, 6])
    Sequential output shape:     torch.Size([1, 832, 3, 3])
    Sequential output shape:     torch.Size([1, 1024])
    Linear output shape:         torch.Size([1, 10])


8.4.3. Training
"""""""""""""""

.. code-block:: python

    model = GoogleNet(lr=0.01)
    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model, data)


8.4.4. Discussion
"""""""""""""""""

* GoogLeNet çš„ä¸€ä¸ªå…³é”®ç‰¹å¾æ˜¯ï¼Œå®ƒçš„è®¡ç®—æˆæœ¬å®é™…ä¸Šæ¯”å…¶å‰èº«æ›´ä¾¿å®œï¼ŒåŒæ—¶æä¾›æ›´é«˜çš„å‡†ç¡®æ€§ã€‚
* ç°åœ¨ï¼Œæ‚¨å¯ä»¥è‡ªè±ªåœ°å®ç°äº†å¯ä»¥è¯´æ˜¯ç¬¬ä¸€ä¸ªçœŸæ­£ç°ä»£çš„ CNNã€‚

* ä¼˜åŠ¿
    * å¤šå°ºåº¦ç‰¹å¾æå–ï¼šé€šè¿‡å¹¶è¡Œä½¿ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸ï¼Œèƒ½å¤Ÿæœ‰æ•ˆæ•è·å›¾åƒçš„ç»†èŠ‚å’Œå…¨å±€ä¿¡æ¯ã€‚
    * å‚æ•°é«˜æ•ˆï¼šåˆ©ç”¨ 1x1 å·ç§¯é™ç»´å’Œå…¨å±€å¹³å‡æ± åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°é‡ã€‚
    * æ¨¡å—åŒ–è®¾è®¡ï¼šæ–¹ä¾¿æ‰©å±•å’Œè°ƒæ•´ï¼Œé€‚åˆæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¿«é€Ÿè¿­ä»£ã€‚





8.5. Batch Normalization
^^^^^^^^^^^^^^^^^^^^^^^^

* è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå¾ˆéš¾ï¼Œç‰¹åˆ«æ˜¯åœ¨æ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§ä¸Šã€‚
* æ‰¹é‡å½’ä¸€åŒ–é€šè¿‡æ ‡å‡†åŒ–ä¸­é—´å±‚çš„æ¿€æ´»å€¼ï¼Œè§£å†³äº†æ¿€æ´»åˆ†å¸ƒæ¼‚ç§»çš„é—®é¢˜ï¼Œä»è€ŒåŠ é€Ÿè®­ç»ƒå¹¶æé«˜æ¨¡å‹ç¨³å®šæ€§ã€‚
* å®ƒè¿˜æœ‰é¢å¤–çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œå¯ä»¥å‡å°‘è¿‡æ‹Ÿåˆã€‚


8.5.1. Training Deep Networks
"""""""""""""""""""""""""""""

æ‰¹é‡å½’ä¸€åŒ–é€šè¿‡åœ¨æ¯ä¸ªè®­ç»ƒæ­¥éª¤ä¸­å¯¹è¾“å…¥è¿›è¡Œæ ‡å‡†åŒ–ï¼Œå¾—åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼š

.. math::

    \textrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}

* :math:`\hat{\boldsymbol{\mu}}\mathcal{B}` å’Œ :math:`\hat{\boldsymbol{\sigma}}\mathcal{B}` æ˜¯åŸºäºå½“å‰å°æ‰¹é‡è®¡ç®—çš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚
* :math:`\boldsymbol{\gamma}` å’Œ :math:`\boldsymbol{\beta}` æ˜¯éœ€è¦è®­ç»ƒçš„ç¼©æ”¾å’Œåç§»å‚æ•°ï¼Œæ¢å¤ä¸¢å¤±çš„è‡ªç”±åº¦ã€‚

* æ‰¹é‡å½’ä¸€åŒ–å…·æœ‰ä»¥ä¸‹ä¼˜ç‚¹ï¼š
    * é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–æ¿€æ´»å€¼ï¼Œå‡å°‘æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±é—®é¢˜ã€‚
    * æ•°å€¼ç¨³å®šæ€§ï¼šä½¿ä¼˜åŒ–å™¨èƒ½ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡ã€‚
    * æ­£åˆ™åŒ–ï¼šå¼•å…¥äº†å™ªå£°ï¼Œæœ‰æ•ˆå‡å°‘è¿‡æ‹Ÿåˆã€‚





8.5.2. Batch Normalization Layers
"""""""""""""""""""""""""""""""""

8.5.2.1. Fully Connected Layers
+++++++++++++++++++++++++++++++

* é€šå¸¸åº”ç”¨åœ¨ä»¿å°„å˜æ¢ä¹‹åï¼Œæ¿€æ´»å‡½æ•°ä¹‹å‰ï¼š

.. math::

    \mathbf{h} = \phi(\textrm{BN}(\mathbf{W}\mathbf{x} + \mathbf{b}) )


8.5.2.2. Convolutional Layers
+++++++++++++++++++++++++++++

*  å¯¹äºå·ç§¯å±‚ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å·ç§¯ä¹‹åä½†éçº¿æ€§æ¿€æ´»å‡½æ•°ä¹‹å‰åº”ç”¨æ‰¹é‡å½’ä¸€åŒ–ã€‚ä¸å…¨è¿æ¥å±‚ä¸­çš„æ‰¹é‡å½’ä¸€åŒ–çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰ä½ç½®çš„æ¯ä¸ªé€šé“çš„åŸºç¡€ä¸Šåº”ç”¨è¯¥æ“ä½œã€‚
* å¯¹æ¯ä¸ªé€šé“ç‹¬ç«‹æ ‡å‡†åŒ–ï¼š
    * æ±‡æ€»æ‰€æœ‰ç©ºé—´ä½ç½®çš„å€¼ï¼ˆä¾‹å¦‚å·ç§¯è¾“å‡ºçš„é«˜åº¦å’Œå®½åº¦ï¼‰æ¥è®¡ç®—å‡å€¼å’Œæ–¹å·®ã€‚
    * æ¯ä¸ªé€šé“æœ‰è‡ªå·±çš„ç¼©æ”¾å’Œåç§»å‚æ•°ã€‚

8.5.2.3. Layer Normalization
++++++++++++++++++++++++++++

* é’ˆå¯¹å°æ‰¹é‡ï¼ˆç”šè‡³æ‰¹é‡å¤§å°ä¸º1ï¼‰æˆ–åºåˆ—ä»»åŠ¡ï¼Œå±‚å½’ä¸€åŒ–å¯¹æ¯ä¸ªæ ·æœ¬çš„æ‰€æœ‰ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼š

.. math::

    \mathbf{x} \rightarrow \textrm{LN}(\mathbf{x}) =  \frac{\mathbf{x} - \hat{\mu}}{\hat\sigma}


* ç‹¬ç«‹äºæ‰¹é‡å¤§å°ï¼Œé€‚ç”¨äºå•æ ·æœ¬åœºæ™¯ã€‚
* é¿å…äº†å› å°æ‰¹é‡å¼•å…¥çš„ä¸ç¨³å®šæ€§ã€‚

8.5.2.4. Batch Normalization During Prediction
++++++++++++++++++++++++++++++++++++++++++++++




8.5.3. Implementation from Scratch
""""""""""""""""""""""""""""""""""

.. code-block:: python

    def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
        # Use is_grad_enabled to determine whether we are in training mode
        if not torch.is_grad_enabled():
            # In prediction mode, use mean and variance obtained by moving average
            X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
        else:
            assert len(X.shape) in (2, 4)
            if len(X.shape) == 2:
                # When using a fully connected layer, calculate the mean and
                # variance on the feature dimension
                mean = X.mean(dim=0)
                var = ((X - mean) ** 2).mean(dim=0)
            else:
                # When using a two-dimensional convolutional layer, calculate the
                # mean and variance on the channel dimension (axis=1). Here we
                # need to maintain the shape of X, so that the broadcasting
                # operation can be carried out later
                mean = X.mean(dim=(0, 2, 3), keepdim=True)
                var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
            # In training mode, the current mean and variance are used
            X_hat = (X - mean) / torch.sqrt(var + eps)
            # Update the mean and variance using moving average
            moving_mean = (1.0 - momentum) * moving_mean + momentum * mean
            moving_var = (1.0 - momentum) * moving_var + momentum * var
        Y = gamma * X_hat + beta  # Scale and shift
        return Y, moving_mean.data, moving_var.data

.. code-block:: python

    class BatchNorm(nn.Module):
        # num_features: the number of outputs for a fully connected layer 
        #       or the number of output channels for a convolutional layer. 
        # num_dims: 2 for a fully connected layer and 4 for a convolutional layer
        def __init__(self, num_features, num_dims):
            super().__init__()
            if num_dims == 2:
                shape = (1, num_features)
            else:
                shape = (1, num_features, 1, 1)
            # The scale parameter and the shift parameter (model parameters) are initialized to 1 and 0
            self.gamma = nn.Parameter(torch.ones(shape))
            self.beta = nn.Parameter(torch.zeros(shape))
            # The variables that are not model parameters are initialized to 0 and 1
            self.moving_mean = torch.zeros(shape)
            self.moving_var = torch.ones(shape)

        def forward(self, X):
            # If X is not on the main memory, copy moving_mean and moving_var to the device where X is located
            if self.moving_mean.device != X.device:
                self.moving_mean = self.moving_mean.to(X.device)
                self.moving_var = self.moving_var.to(X.device)
            # Save the updated moving_mean and moving_var
            Y, self.moving_mean, self.moving_var = batch_norm(
                X, self.gamma, self.beta, self.moving_mean,
                self.moving_var, eps=1e-5, momentum=0.1)
            return Y




8.5.4. LeNet with Batch Normalization
"""""""""""""""""""""""""""""""""""""

.. code-block:: python

    class BNLeNetScratch(d2l.Classifier):
        def __init__(self, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(), nn.LazyLinear(120),
                BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),
                BatchNorm(84, num_dims=2), nn.Sigmoid(),
                nn.LazyLinear(num_classes))

    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128)
    model = BNLeNetScratch(lr=0.1)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model, data)


::

    model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))


8.5.5. Concise Implementation
"""""""""""""""""""""""""""""

.. code-block:: python

    class BNLeNet(d2l.Classifier):
        def __init__(self, lr=0.1, num_classes=10):
            super().__init__()
            self.save_hyperparameters()
            self.net = nn.Sequential(
                nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),
                nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),
                nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),
                nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),
                nn.Sigmoid(), nn.LazyLinear(num_classes))

::

    trainer = d2l.Trainer(max_epochs=10, num_gpus=1)
    data = d2l.FashionMNIST(batch_size=128)
    model = BNLeNet(lr=0.1)
    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)
    trainer.fit(model, data)




8.6. Residual Networks (ResNet) and ResNeXt
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* æ®‹å·®ç½‘ç»œ (ResNet)
* ä½•å‡¯æ˜ç­‰äºº2016å¹´æå‡ºçš„æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰ï¼Œè¯¥ç½‘ç»œå¼•å…¥äº†æ®‹å·®å—ï¼ˆresidual blockï¼‰ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯è®©æ¯ä¸€å±‚é™¤äº†å­¦ä¹ åˆ°æœŸæœ›çš„åº•å±‚æ˜ å°„å¤–ï¼Œè¿˜èƒ½æ›´è½»æ¾åœ°åŒ…å«æ’ç­‰æ˜ å°„ä½œä¸ºä¸€ä¸ªå…ƒç´ ã€‚è¿™è§£å†³äº†éå¸¸æ·±çš„ç½‘ç»œéš¾ä»¥è®­ç»ƒçš„é—®é¢˜ï¼Œå¹¶ä¸”ResNetåœ¨2015å¹´çš„ImageNetå¤§è§„æ¨¡è§†è§‰è¯†åˆ«æŒ‘æˆ˜èµ›ä¸­å–å¾—äº†èƒœåˆ©ã€‚æ­¤è®¾è®¡å¯¹åç»­æ·±åº¦ç¥ç»ç½‘ç»œçš„å‘å±•æœ‰ç€æ·±è¿œçš„å½±å“ã€‚
* æ®‹å·®å—çš„å·¥ä½œåŸç†ï¼šè¾“å…¥å¯ä»¥ç›´æ¥è·³è¿‡æŸäº›å±‚ä¼ é€’åˆ°åé¢çš„å±‚ï¼Œå½¢æˆæ‰€è°“çš„â€œæ®‹å·®è¿æ¥â€æˆ–â€œæ·å¾„è¿æ¥â€ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯å¯ä»¥ä½¿ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„ï¼Œå› ä¸ºå½“éœ€è¦å­¦ä¹ çš„æ˜ å°„ä¸ºæ’ç­‰æ˜ å°„æ—¶ï¼Œç½‘ç»œåªéœ€å°†æƒé‡è°ƒæ•´ä¸ºé›¶å³å¯ã€‚


8.7. Densely Connected Networks (DenseNet)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ä¸»è¦è®²äº†DenseNetï¼ˆå¯†é›†è¿æ¥ç½‘ç»œï¼‰ç›¸è¾ƒäº ResNet çš„ç‰¹ç‚¹ã€æ•°å­¦åŸç†ä»¥åŠå®ç°æ–¹å¼ã€‚

æ ¸å¿ƒæ¦‚å¿µ
""""""""

* å¯†é›†è¿æ¥æ¨¡å¼ï¼šDenseNet çš„æ¯ä¸€å±‚ä¸ä¹‹å‰æ‰€æœ‰å±‚ç›¸è¿æ¥ã€‚è¿™ç§è¿æ¥æ–¹å¼é€šè¿‡ **ç‰¹å¾çº§æ‹¼æ¥(concatenation)** æ¥ä¿ç•™å’Œå¤ç”¨ç‰¹å¾ï¼Œè€Œä¸æ˜¯åƒ ResNet ä¸­é€šè¿‡ç›¸åŠ ï¼ˆadditionï¼‰è¿æ¥ã€‚
* ç‰¹å¾å¤ç”¨ï¼šæ¯ä¸€å±‚çš„è¾“å‡ºä½œä¸ºåç»­å±‚çš„è¾“å…¥ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿æ¯å±‚éƒ½èƒ½ç›´æ¥è®¿é—®æœ€åˆè¾“å…¥å’Œæ¯å±‚ä¸­é—´ç»“æœï¼Œä»è€Œå‡å°‘ä¿¡æ¯ä¸¢å¤±ã€‚

å…³é”®ç»„æˆéƒ¨åˆ†
""""""""""""

* Dense Blockï¼ˆå¯†é›†å—ï¼‰ï¼š
    * æ¯ä¸ª Dense Block åŒ…æ‹¬å¤šä¸ªå·ç§¯å—ï¼Œè¾“å…¥å’Œæ¯ä¸ªå·ç§¯å—çš„è¾“å‡ºåœ¨é€šé“ç»´åº¦ä¸Šæ‹¼æ¥ã€‚
    * ç”Ÿé•¿ç‡ï¼šæ¯ä¸ª Dense Block å¢åŠ è¾“å‡ºé€šé“æ•°çš„é€Ÿç‡ã€‚
* Transition Layerï¼ˆè¿‡æ¸¡å±‚ï¼‰ï¼š
    * æ§åˆ¶æ¨¡å‹å¤æ‚åº¦ï¼Œå‡å°‘é€šé“æ•°å¹¶é€šè¿‡å¹³å‡æ± åŒ–é™ä½åˆ†è¾¨ç‡ã€‚
    * ä½¿ç”¨ :math:`1 \times 1` å·ç§¯å‡å°‘é€šé“æ•°ï¼Œç¡®ä¿é€šé“å¢é•¿ä¸ä¼šè¿‡å¿«ã€‚

DenseNet çš„ä¼˜ç‚¹
"""""""""""""""

* é«˜æ•ˆç‰¹å¾å¤ç”¨ï¼šæ¯å±‚çš„ç‰¹å¾éƒ½ç›´æ¥æä¾›ç»™åç»­å±‚ï¼Œå‡å°‘å†—ä½™è®¡ç®—ã€‚
* æ›´è½»çš„æ¨¡å‹ï¼šç›¸æ¯” ResNetï¼ŒDenseNet åœ¨å‚æ•°é‡ä¸Šæ›´é«˜æ•ˆã€‚
* ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼šå¯†é›†è¿æ¥æ¨¡å¼æä¾›äº†æ›´çŸ­çš„æ¢¯åº¦ä¼ æ’­è·¯å¾„ã€‚



8.8. Designing Convolution Network Architectures
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/mcdubD.png

    Fig. 8.8.1 The AnyNet design space.

* The numbers :math:`(\mathit{c}, \mathit{r})` along each arrow indicate the number of channels ``c`` and the resolution :math:`\mathit{r} \times \mathit{r}` of the images at that point. 
* From left to right: generic network structure composed of ``stem`` , ``body`` , and ``head`` ; 
    * body composed of four stages; 
    * detailed structure of a stage; 
    * two alternative structures for blocks, 
        * one without downsampling and one that halves the resolution in each dimension. 
    * Design choices include 
        * depth :math:`\mathit{d_i}` , 
        * the number of output channels :math:`\mathit{c_i}` , 
        * the number of groups $\mathit{g_i}$, 
        * and bottleneck ratio $\mathit{k_i}$ for any stage $\mathit{i}$.](../img/anynet.svg)


ä¼ ç»Ÿæ¶æ„è®¾è®¡çš„ç›´è§‰æ€§
""""""""""""""""""""

* æ—©æœŸçš„ CNN æ¶æ„ï¼ˆå¦‚ AlexNet å’Œ VGGï¼‰ä¾èµ–ç§‘å­¦å®¶çš„ç›´è§‰è®¾è®¡ã€‚
* å¸¸è§æ–¹æ³•åŒ…æ‹¬å †å å·ç§¯å±‚ï¼ˆå¦‚ 3x3 å·ç§¯ï¼‰æ¥å¢åŠ æ·±åº¦ï¼Œä»¥æå‡ç½‘ç»œæ€§èƒ½ã€‚
* NiN å¼•å…¥äº† 1x1 å·ç§¯ï¼Œè§£å†³äº†å±€éƒ¨éçº¿æ€§é—®é¢˜ï¼Œå¹¶ä¼˜åŒ–äº†ä¿¡æ¯èšåˆã€‚
* GoogLeNet çš„å¤šåˆ†æ”¯è®¾è®¡ï¼ˆInception æ¨¡å—ï¼‰ç»“åˆäº† VGG å’Œ NiN çš„ä¼˜ç‚¹ã€‚
* ResNet æ”¹å˜äº†å½’çº³åç½®ï¼Œé€šè¿‡å¼•å…¥æ®‹å·®è¿æ¥ä½¿å¾—è®­ç»ƒæ›´æ·±çš„ç½‘ç»œæˆä¸ºå¯èƒ½ã€‚
* SENet å’Œ ResNeXt ç­‰åç»­æ¶æ„è¿›ä¸€æ­¥ä¼˜åŒ–äº†ç½‘ç»œè®¡ç®—æ•ˆç‡å’Œå‚æ•°æƒè¡¡ã€‚

ç¥ç»æ¶æ„æœç´¢(NAS)
"""""""""""""""""

* NAS æ˜¯é€šè¿‡è‡ªåŠ¨åŒ–æ–¹æ³•ï¼ˆå¦‚é—ä¼ ç®—æ³•ã€å¼ºåŒ–å­¦ä¹ ï¼‰æ¢ç´¢æœ€ä½³ç½‘ç»œæ¶æ„ã€‚
* è™½ç„¶ NAS èƒ½ç”Ÿæˆé«˜æ€§èƒ½ç½‘ç»œï¼ˆå¦‚ EfficientNetsï¼‰ï¼Œä½†å…¶è®¡ç®—æˆæœ¬æé«˜ã€‚

RegNet ä¸è®¾è®¡ç©ºé—´ä¼˜åŒ–
"""""""""""""""""""""

* ç›¸æ¯”å¯»æ‰¾â€œå•ä¸€æœ€ä½³ç½‘ç»œâ€ï¼Œæ¢ç´¢æ•´ä¸ªç½‘ç»œè®¾è®¡ç©ºé—´æ›´æœ‰ä»·å€¼ã€‚
* Radosavovic ç­‰äººæå‡ºäº†ä¸€ç§ç»“åˆæ‰‹åŠ¨è®¾è®¡å’Œ NAS ä¼˜åŠ¿çš„æ–¹æ³•ï¼Œé€šè¿‡ä¼˜åŒ–ç½‘ç»œåˆ†å¸ƒï¼Œè€Œéå•ä¸€å®ä¾‹ï¼Œå¾—åˆ°äº† RegNet ç³»åˆ—ã€‚
* RegNet æä¾›äº†æ€§èƒ½è‰¯å¥½çš„ CNN è®¾è®¡æŒ‡å¯¼åŸåˆ™ï¼Œå¼ºè°ƒåœ¨è®¾è®¡è¿‡ç¨‹ä¸­æ—¢è¦ç§‘å­¦æ¢ç´¢ï¼Œåˆè¦ä¿è¯è®¡ç®—æˆæœ¬ä½å»‰ã€‚

AnyNet è®¾è®¡ç©ºé—´
"""""""""""""""

* AnyNet æ˜¯ä¸€ä¸ªé€šç”¨çš„è®¾è®¡æ¨¡æ¿ï¼Œç”±â€œstemâ€ï¼ˆåˆå§‹å¤„ç†ï¼‰ã€â€œbodyâ€ï¼ˆæ ¸å¿ƒè®¡ç®—ï¼‰å’Œâ€œheadâ€ï¼ˆè¾“å‡ºå±‚ï¼‰ç»„æˆã€‚
* è®¾è®¡çš„æ ¸å¿ƒåœ¨äºâ€œbodyâ€ï¼Œå®ƒé€šè¿‡å¤šä¸ªé˜¶æ®µå’Œæ¨¡å—ï¼ˆå¦‚ ResNeXt å—ï¼‰é€æ­¥æå–ç‰¹å¾ã€‚
* ä¸ºäº†æœ‰æ•ˆæ¢ç´¢è®¾è®¡ç©ºé—´ï¼Œéœ€è¦è°ƒæ•´è¯¸å¤šå‚æ•°ï¼ˆå¦‚é€šé“æ•°ã€æ·±åº¦ã€åˆ†ç»„æ•°ç­‰ï¼‰å¹¶ä¼˜åŒ–å®ƒä»¬çš„ç»„åˆã€‚

åˆ†å¸ƒä¼˜åŒ–ä¸å‡è®¾
""""""""""""""

* ç½‘ç»œæ€§èƒ½ä¼˜åŒ–çš„ç›®æ ‡ä»å¯»æ‰¾å•ä¸€æœ€ä½³å‚æ•°ï¼Œè½¬ä¸ºå¯»æ‰¾â€œå¥½çš„å‚æ•°åˆ†å¸ƒâ€ã€‚
* å‡è®¾åŒ…æ‹¬ï¼š
    * å¥½çš„è®¾è®¡åŸåˆ™å­˜åœ¨ï¼Œå¹¶é€‚ç”¨äºå¤šä¸ªç½‘ç»œã€‚
    * ä¸éœ€è¦å®Œå…¨è®­ç»ƒç½‘ç»œï¼Œæ—©æœŸæ€§èƒ½å¯ä»¥æä¾›æŒ‡å¯¼ã€‚
    * å°è§„æ¨¡å®éªŒç»“æœå¯ä»¥æ¨å¹¿åˆ°å¤§è§„æ¨¡ç½‘ç»œã€‚
    * è®¾è®¡é—®é¢˜å¯ä»¥åˆ†è§£æˆç›¸å¯¹ç‹¬ç«‹çš„æ¨¡å—ã€‚



9. Recurrent Neural Networks
----------------------------

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/2Rj2To.png

    Fig. 9.1 On the left recurrent connections are depicted via cyclic edges. On the right, we unfold the RNN over time steps. Here, recurrent edges span adjacent time steps, while conventional connections are computed synchronously.

* RNN åœ¨ 2010 å¹´ä»£å¼€å§‹æµè¡Œï¼ˆGraves ç­‰äººï¼Œ2008 å¹´ï¼‰ ï¼‰ã€æœºå™¨ç¿»è¯‘ï¼ˆSutskever ç­‰äººï¼Œ2014ï¼‰ä»¥åŠè¯†åˆ«åŒ»ç–—è¯Šæ–­ï¼ˆLipton ç­‰äººï¼Œ2016ï¼‰ã€‚



9.1. Working with Sequences
^^^^^^^^^^^^^^^^^^^^^^^^^^^

1. åºåˆ—æ•°æ®çš„ç‰¹ç‚¹
"""""""""""""""""

* ä¼ ç»Ÿå•è¾“å…¥æ¨¡å‹ï¼šä¹‹å‰æˆ‘ä»¬è®¨è®ºçš„æ¨¡å‹è¾“å…¥æ˜¯å•ä¸€ç‰¹å¾å‘é‡ :math:`\mathbf{x} \in \mathbb{R}^d` ã€‚
* åºåˆ—è¾“å…¥çš„åŒºåˆ«ï¼šåºåˆ—è¾“å…¥ç”±ä¸€ç»„æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„ç‰¹å¾å‘é‡ :math:`\mathbf{x}_1, \dots, \mathbf{x}_T` ç»„æˆï¼Œæ¯ä¸ªç‰¹å¾å‘é‡ :math:`\mathbf{x}_t` æœ‰æ—¶é—´æ­¥(time step) t çš„ç´¢å¼•ã€‚
* åºåˆ—æ•°æ®çš„ä¾‹å­åŒ…æ‹¬ï¼š

    * æ–‡æ¡£åºåˆ—ï¼šæ¯ä¸ªæ–‡æ¡£æ˜¯ä¸€ç»„å•è¯åºåˆ—ã€‚
    * åŒ»é™¢æ‚£è€…æ•°æ®ï¼šä¸€ä¸ªä½é™¢è¿‡ç¨‹ç”±ä¸€ç³»åˆ—äº‹ä»¶ç»„æˆã€‚
    * è‚¡ç¥¨ä»·æ ¼åºåˆ—ï¼šæŒ‰æ—¶é—´è®°å½•çš„è‚¡ä»·æ•°æ®ã€‚

* åœ¨åºåˆ—æ•°æ®ä¸­ï¼Œç›¸é‚»æ—¶é—´æ­¥çš„æ•°æ®é€šå¸¸å­˜åœ¨ä¾èµ–å…³ç³»ï¼ˆå¦‚å•è¯ä¹‹é—´çš„ä¸Šä¸‹æ–‡å…³ç³»æˆ–æ‚£è€…æ²»ç–—æ–¹æ¡ˆçš„æ—¶é—´é¡ºåºï¼‰ï¼Œè€Œä¸æ˜¯ç‹¬ç«‹é‡‡æ ·çš„ã€‚

2. åºåˆ—å»ºæ¨¡çš„ç›®æ ‡
"""""""""""""""""

* é¢„æµ‹å›ºå®šç›®æ ‡ï¼šå¦‚æ ¹æ®å½±è¯„æ–‡æœ¬åˆ¤æ–­æƒ…æ„Ÿï¼ˆæ­£å‘æˆ–è´Ÿå‘ï¼‰ã€‚
* é¢„æµ‹åºåˆ—ç›®æ ‡ï¼šå¦‚æ ¹æ®å›¾ç‰‡ç”Ÿæˆæè¿°æ–‡æœ¬ã€‚
* åºåˆ—åˆ°åºåˆ—å»ºæ¨¡ï¼šå¦‚æœºå™¨ç¿»è¯‘ï¼Œå°†ä¸€ä¸ªè¯­è¨€çš„å¥å­è½¬æ¢ä¸ºå¦ä¸€è¯­è¨€ã€‚


3. åºåˆ—å»ºæ¨¡çš„åŸºæœ¬æ–¹æ³•
"""""""""""""""""""""

9.1.1. Autoregressive Models
++++++++++++++++++++++++++++

* ç›®æ ‡ï¼šé€šè¿‡å†å²æ•°æ® :math:`\mathbf{x}_{t-1}, \dots, \mathbf{x}_1` é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ® :math:`\mathbf{x}_t`
* é—®é¢˜ï¼šå†å²æ•°æ®çš„é•¿åº¦éšç€æ—¶é—´å¢é•¿ï¼Œè¾“å…¥ç‰¹å¾çš„æ•°é‡ä¹Ÿéšä¹‹è¶Šæ¥è¶Šé•¿ã€‚
* è§£å†³æ–¹æ³•ï¼š
    * é™åˆ¶çª—å£å¤§å°ï¼šä»…è€ƒè™‘å›ºå®šé•¿åº¦çª—å£ :math:`\tau` çš„å†å²æ•°æ®ï¼Œå¦‚ :math:`\mathbf{x}{t-1}, \dots, \mathbf{x}{t-\tau}` ã€‚
    * éšå˜é‡è¡¨ç¤ºï¼šç”¨ä¸€ä¸ªéšè—çŠ¶æ€ :math:`h_t` æ€»ç»“å†å²ä¿¡æ¯ï¼Œå¹¶é€šè¿‡æ›´æ–°å…¬å¼ :math:`h_t = g(h_{t-1}, x_{t-1})` åŠ¨æ€ç»´æŠ¤ã€‚



9.1.2. Sequence Models(Markov Models)
+++++++++++++++++++++++++++++++++++++

* å¦‚æœå¯ä»¥ä»…ä¾èµ–æœ€è¿‘çš„ :math:`\tau` ä¸ªæ—¶é—´æ­¥çš„ä¿¡æ¯é¢„æµ‹æœªæ¥ï¼Œåˆ™æ»¡è¶³é©¬å°”ç§‘å¤«æ¡ä»¶ã€‚
* ä¸€é˜¶é©¬å°”ç§‘å¤«æ¨¡å‹ï¼šåªè€ƒè™‘æœ€è¿‘çš„ä¸€ä¸ªæ—¶é—´æ­¥ :math:`x_{t-1}` ï¼Œé¢„æµ‹ :math:`P(x_t \mid x_{t-1})`
* è™½ç„¶å®é™…æ•°æ®å¯èƒ½ä¸å®Œå…¨ç¬¦åˆé©¬å°”ç§‘å¤«æ¡ä»¶ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™ç§è¿‘ä¼¼ç®€åŒ–äº†è®¡ç®—ã€‚

4. è¯­è¨€æ¨¡å‹ä¸åºåˆ—æ¨¡å‹
"""""""""""""""""""""

è¯­è¨€æ¨¡å‹ (Language Model)
+++++++++++++++++++++++++

* ç”¨äºä¼°è®¡æ•´ä¸ªåºåˆ—ï¼ˆå¦‚å¥å­ï¼‰çš„è”åˆæ¦‚ç‡ :math:`P(x_1, \dots, x_T)`
* åˆ©ç”¨é“¾å¼æ³•åˆ™ï¼Œå°†è”åˆæ¦‚ç‡åˆ†è§£ä¸ºæ¡ä»¶æ¦‚ç‡çš„ä¹˜ç§¯ï¼š 

.. math::

    P(x_1, \ldots, x_T) = P(x_1) \prod_{t=2}^T P(x_t \mid x_{t-1}, \ldots, x_1)

* è¯­è¨€æ¨¡å‹å¯ç”¨äºè‡ªç„¶è¯­è¨€ç”Ÿæˆã€è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰ä»»åŠ¡ã€‚

è§£ç é¡ºåº
++++++++

* ä¸€èˆ¬ä»å·¦åˆ°å³å»ºæ¨¡ï¼ˆä¸é˜…è¯»ä¹ æƒ¯ä¸€è‡´ï¼‰ï¼Œå› ä¸ºé¢„æµ‹ç›¸é‚»å•è¯æ›´å®¹æ˜“ã€‚
* æœªæ¥äº‹ä»¶å¯èƒ½å½±å“åç»­ç»“æœï¼Œä½†ä¸ä¼šå½±å“è¿‡å»ã€‚


5. å®é™…åº”ç”¨
^^^^^^^^^^^

* è‚¡ç¥¨ä»·æ ¼é¢„æµ‹ï¼šå¦‚é€šè¿‡å†å²è‚¡ä»·æ•°æ®é¢„æµ‹ä¸‹ä¸€æ—¶åˆ»ä»·æ ¼ã€‚
* æ‚£è€…ç—…ç¨‹å»ºæ¨¡ï¼šæ ¹æ®å‰å‡ å¤©çš„æ²»ç–—æ–¹æ¡ˆï¼Œé¢„æµ‹ç¬¬åå¤©çš„ç”¨è¯ã€‚
* æ–‡æœ¬ç”Ÿæˆä¸è¡¥å…¨ï¼šåŸºäºå·²çŸ¥å‰ç¼€ï¼Œé¢„æµ‹å¯èƒ½çš„åç»­æ–‡æœ¬ã€‚





9.2. Converting Raw Text into Sequence Data
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Typical preprocessing pipelines execute the following steps::

    1. Load text as strings into memory.
    2. Split the strings into tokens (e.g., words or characters).
    3. Build a vocabulary dictionary to associate each vocabulary element with a numerical index.
    4. Convert the text into sequences of numerical indices.




9.2.1. Reading the Dataset
""""""""""""""""""""""""""

.. code-block:: python

    class TimeMachine(d2l.DataModule): #@save
        """The Time Machine dataset."""
        def _download(self):
            fname = d2l.download(d2l.DATA_URL + 'timemachine.txt', self.root,
                                 '090b5e7e70c295757f55df93cb0a180b9691891a')
            with open(fname) as f:
                return f.read()

    data = TimeMachine()
    raw_text = data._download()
    raw_text[:60]

    @d2l.add_to_class(TimeMachine)  #@save
    def _preprocess(self, text):
        return re.sub('[^A-Za-z]+', ' ', text).lower()

    text = data._preprocess(raw_text)
    text[:60]
    # è¾“å‡º
    # 'the time machine by h g wells i the time traveller for so it'



9.2.2. Tokenization
"""""""""""""""""""

::

    @d2l.add_to_class(TimeMachine)  #@save
    def _tokenize(self, text):
        return list(text)

    tokens = data._tokenize(text)
    ','.join(tokens[:30])
    # è¾“å‡º
    # 't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '



9.2.3. Vocabulary
"""""""""""""""""

.. code-block:: python

    class Vocab:  #@save
        """Vocabulary for text."""
        def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):
            # Flatten a 2D list if needed
            if tokens and isinstance(tokens[0], list):
                tokens = [token for line in tokens for token in line]
            # Count token frequencies
            counter = collections.Counter(tokens)
            self.token_freqs = sorted(counter.items(), key=lambda x: x[1],
                                      reverse=True)
            # The list of unique tokens
            self.idx_to_token = list(sorted(set(['<unk>'] + reserved_tokens + [
                token for token, freq in self.token_freqs if freq >= min_freq])))
            self.token_to_idx = {token: idx
                                 for idx, token in enumerate(self.idx_to_token)}

        def __len__(self):
            return len(self.idx_to_token)

        def __getitem__(self, tokens):
            if not isinstance(tokens, (list, tuple)):
                return self.token_to_idx.get(tokens, self.unk)
            return [self.__getitem__(token) for token in tokens]

        def to_tokens(self, indices):
            if hasattr(indices, '__len__') and len(indices) > 1:
                return [self.idx_to_token[int(index)] for index in indices]
            return self.idx_to_token[indices]

        @property
        def unk(self):  # Index for the unknown token
            return self.token_to_idx['<unk>']

ä½¿ç”¨::

    vocab = Vocab(tokens)
    indices = vocab[tokens[:10]]
    print('indices:', indices)
    print('words:', vocab.to_tokens(indices))
    # è¾“å‡º
    #    indices: [21, 9, 6, 0, 21, 10, 14, 6, 0, 14]
    #    words: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ' ', 'm']




9.2.4. Putting It All Together
""""""""""""""""""""""""""""""

::

    @d2l.add_to_class(TimeMachine)  #@save
    def build(self, raw_text, vocab=None):
        tokens = self._tokenize(self._preprocess(raw_text))
        if vocab is None: vocab = Vocab(tokens)
        corpus = [vocab[token] for token in tokens]
        return corpus, vocab

    corpus, vocab = data.build(raw_text)
    len(corpus), len(vocab)
    # (173428, 28)



9.2.5. Exploratory Language Statistics
""""""""""""""""""""""""""""""""""""""

ä½¿ç”¨çœŸå®çš„è¯­æ–™åº“å’Œé’ˆå¯¹å•è¯å®šä¹‰çš„ Vocab ç±»::

    words = text.split()
    vocab = Vocab(words)
    vocab.token_freqs[:10]
    # è¾“å‡º
    [('the', 2261),
     ('i', 1267),
     ('and', 1245),
     ('of', 1155),
     ('a', 816),
     ('to', 695),
     ('was', 552),
     ('in', 541),
     ('that', 443),
     ('my', 440)]

::

    bigram_tokens = ['--'.join(pair) for pair in zip(words[:-1], words[1:])]
    bigram_vocab = Vocab(bigram_tokens)
    bigram_vocab.token_freqs[:10]
    # è¾“å‡º
    [('of--the', 309),
     ('in--the', 169),
     ('i--had', 130),
     ('i--was', 112),
     ('and--the', 109),
     ('the--time', 102),
     ('it--was', 99),
     ('to--the', 85),
     ('as--i', 78),
     ('of--a', 73)]



9.2.6. Summary
""""""""""""""

* æ–‡æœ¬æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸è§çš„åºåˆ—æ•°æ®å½¢å¼ä¹‹ä¸€ã€‚
* æ„æˆæ ‡è®°çš„å¸¸è§é€‰æ‹©æ˜¯å­—ç¬¦ã€å•è¯å’Œå•è¯ç‰‡æ®µã€‚
* ä¸ºäº†é¢„å¤„ç†æ–‡æœ¬ï¼Œæˆ‘ä»¬é€šå¸¸ 
    * (i) å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ ‡è®°ï¼› 
    * (ii) æ„å»ºè¯æ±‡è¡¨ä»¥å°†æ ‡è®°å­—ç¬¦ä¸²æ˜ å°„åˆ°æ•°å­—ç´¢å¼•ï¼› 
    * (iii) å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ ‡è®°ç´¢å¼•ä»¥ä¾›æ¨¡å‹æ“ä½œã€‚
* åœ¨å®è·µä¸­ï¼Œå•è¯çš„é¢‘ç‡å¾€å¾€éµå¾ªé½æ™®å¤«å®šå¾‹ã€‚è¿™ä¸ä»…é€‚ç”¨äºå•ä¸ªå•è¯ï¼ˆä¸€å…ƒè¯­æ³•ï¼‰ï¼Œä¹Ÿé€‚ç”¨äº n-gram



9.3. Language Models
^^^^^^^^^^^^^^^^^^^^

9.3.1. Learning Language Models
"""""""""""""""""""""""""""""""

* è¯­è¨€æ¨¡å‹çš„åŸºç¡€å…¬å¼

.. math::

    P(x_1, x_2, \ldots, x_T) = \prod_{t=1}^T P(x_t  \mid  x_1, \ldots, x_{t-1})

ä¾‹å¦‚ï¼ŒåŒ…å«å››ä¸ªå•è¯çš„æ–‡æœ¬åºåˆ—çš„æ¦‚ç‡(è”åˆæ¦‚ç‡)å¦‚ä¸‹ï¼š

.. math::

    P(\textrm{deep}, \textrm{learning}, \textrm{is}, \textrm{fun}) \\
    =&P(\textrm{deep}) P(\textrm{learning}  \mid  \textrm{deep}) P(\textrm{is}  \mid  \textrm{deep}, \textrm{learning}) P(\textrm{fun}  \mid  \textrm{deep}, \textrm{learning}, \textrm{is})


9.3.1.1. Markov Models and n-grams
++++++++++++++++++++++++++++++++++

* ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œè¯­è¨€æ¨¡å‹é€šå¸¸å‡è®¾åºåˆ—æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ï¼Œå³åªè€ƒè™‘æœ‰é™é•¿åº¦çš„ä¸Šä¸‹æ–‡ï¼š
    * Unigramï¼ˆå•è¯ç‹¬ç«‹ï¼‰: :math:`P(x_1, x_2, x_3, x_4) = P(x_1)P(x_2)P(x_3)P(x_4)`
    * Bigramï¼ˆäºŒå…ƒç»„ï¼‰: :math:`P(x_4 \mid x_3)`
    * Trigramï¼ˆä¸‰å…ƒç»„ï¼‰: :math:`P(x_4 \mid x_2, x_3)`

9.3.1.2. Word Frequency
+++++++++++++++++++++++

* åœ¨å¤§è§„æ¨¡è¯­æ–™åº“ï¼ˆå¦‚Wikipediaæˆ–Project Gutenbergï¼‰ä¸­ï¼Œè¯çš„æ¦‚ç‡å¯ä»¥é€šè¿‡é¢‘ç‡è®¡ç®—ï¼š
* :math:`P(\textrm{deep})` ï¼šæ–‡æœ¬ä¸­å•è¯â€œdeepâ€çš„å‡ºç°æ¬¡æ•°å æ¯”ã€‚

.. math::

    \hat{P}(\textrm{learning} \mid \textrm{deep}) = \frac{n(\textrm{deep, learning})}{n(\textrm{deep})}

9.3.1.3. Laplace Smoothing
++++++++++++++++++++++++++

* æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘
* ä¸ºäº†è§£å†³ä½é¢‘æˆ–æœªè§è¯ç»„çš„é—®é¢˜ï¼Œä½¿ç”¨å¹³æ»‘æ–¹æ³•åœ¨è®¡æ•°ä¸­åŠ å…¥å°å¸¸æ•° :math:`\epsilon` ï¼š
    * å•è¯æ¦‚ç‡ï¼š :math:`\hat{P}(x) = \frac{n(x) + \epsilon/m}{n + \epsilon}`
    * äºŒå…ƒç»„æ¦‚ç‡ï¼š :math:`\hat{P}(x' \mid x) = \frac{n(x, x') + \epsilon}{n(x) + \epsilon}`
* å°½ç®¡å¹³æ»‘æ–¹æ³•æœ‰æ•ˆï¼Œå®ƒä¾ç„¶æœ‰å±€é™æ€§ï¼Œä¾‹å¦‚é«˜é˜¶ç»„åˆçš„ç¨€ç–æ€§å’Œå­˜å‚¨æˆæœ¬é—®é¢˜ã€‚


9.3.2. Perplexity
"""""""""""""""""

* å›°æƒ‘åº¦è¡¡é‡è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬é¢„æµ‹çš„å¥½åï¼š
* å®šä¹‰ä¸ºäº¤å‰ç†µçš„æŒ‡æ•°å½¢å¼ï¼š

.. math::

    Perplexity = \exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right)


* ç†è§£ï¼š
    * æœ€ä¼˜æ¨¡å‹ï¼šå›°æƒ‘åº¦=1ï¼ˆå®Œç¾é¢„æµ‹ï¼‰ã€‚
    * éšæœºæ¨¡å‹ï¼šå›°æƒ‘åº¦æ¥è¿‘è¯æ±‡è¡¨å¤§å°ã€‚



9.3.3. Partitioning Sequences
"""""""""""""""""""""""""""""

* è®­ç»ƒè¯­è¨€æ¨¡å‹æ—¶ï¼Œå°†é•¿åºåˆ—åˆ†å‰²æˆå¤šä¸ªå­åºåˆ—ï¼Œå¹¶å°†è¾“å…¥ä¸ç›®æ ‡é”™å¼€ä¸€ä½ã€‚
* ä¾‹å¦‚ï¼š
    * è¾“å…¥ï¼š :math:`[x_1, x_2, x_3, x_4, x_5]`
    * ç›®æ ‡ï¼š :math:`[x_2, x_3, x_4, x_5, x_6]`
* è¿™ç§åˆ†åŒºæ–¹å¼ä½¿æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„èƒ½åŠ›ã€‚



9.4. Recurrent Neural Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

1. è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ä¸æ”¹è¿›
"""""""""""""""""""""""""

* ä¼ ç»Ÿçš„ nå…ƒ(n-grams)æ¨¡å‹å‡è®¾ä¸€ä¸ªæ—¶é—´æ­¥ t çš„è¯ :math:`x_t` åªä¾èµ–äºå‰ n-1 ä¸ªè¯ã€‚
    * ç¼ºç‚¹ï¼šå¦‚æœè¦æ•æ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œéœ€è¦å¢åŠ  n ï¼Œä½†è¿™æ ·å‚æ•°æ•°é‡ä¼šæŒ‡æ•°çº§å¢é•¿ï¼ˆ :math:`|\mathcal{V}|^n` ï¼Œ :math:`\mathcal{V}` æ˜¯è¯æ±‡è¡¨ï¼‰ã€‚
* è§£å†³æ–¹æ¡ˆï¼šå¼•å…¥éšå˜é‡æ¨¡å‹ï¼Œç”¨ä¸€ä¸ªâ€œéšè—çŠ¶æ€â€ h_{t-1} æ¥å­˜å‚¨ä» t-1 ä¹‹å‰çš„åºåˆ—ä¿¡æ¯ï¼š


2. éšè—çŠ¶æ€çš„å®šä¹‰ä¸ä½œç”¨
"""""""""""""""""""""""

* éšè—çŠ¶æ€ï¼ˆhidden stateï¼‰ï¼š :math:`h_t` æ ¹æ®å½“å‰è¾“å…¥ :math:`x_t` å’Œå‰ä¸€ä¸ªéšè—çŠ¶æ€ :math:`h_{t-1}` è®¡ç®—å¾—å‡ºï¼š

.. math::

    H_t = f(X_t, h_{t-1})


* :math:`h_t` å¯ä»¥è§†ä¸ºä¸€ä¸ªâ€œè®°å¿†å•å…ƒâ€ï¼Œå®ƒè®°å½•äº†ä»åºåˆ—å¼€å§‹åˆ°å½“å‰æ—¶é—´æ­¥ t çš„å†å²ä¿¡æ¯ã€‚
* å¥½å¤„ï¼šç›¸æ¯” n-grams æ¨¡å‹ï¼Œéšè—çŠ¶æ€å…è®¸æˆ‘ä»¬åœ¨å‚æ•°ä¸éšæ—¶é—´æ­¥å¢åŠ çš„æƒ…å†µä¸‹æ•è·é•¿ç¨‹ä¾èµ–ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/TGFa1e.png

    Fig. 9.4.1 An RNN with a hidden state.




3. RNNçš„è®¡ç®—é€»è¾‘
""""""""""""""""

* a. RNNçš„éšè—å±‚è¾“å‡ºï¼ˆå³éšè—çŠ¶æ€ï¼‰å…¬å¼ï¼š

.. math::

    \mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hh}}  + \mathbf{b}_\textrm{h})

* å…¶ä¸­
    * :math:`\mathbf{X}_t` ï¼šå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ã€‚
    * :math:`\mathbf{H}_{t-1}` ï¼šå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚
    * :math:`\mathbf{W}{\text{xh}}, \mathbf{W}{\text{hh}}` ï¼šæƒé‡çŸ©é˜µï¼Œåˆ†åˆ«ç”¨äºå¤„ç†è¾“å…¥å’Œéšè—çŠ¶æ€ã€‚
    * :math:`\phi` ï¼šæ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚ReLUæˆ–tanhï¼‰ã€‚

* b. è¾“å‡ºå±‚çš„è®¡ç®—ï¼š

.. math::

    \mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q}


* c.ç‰¹ç‚¹ï¼š
    * éšè—çŠ¶æ€çš„è®¡ç®—æ˜¯é€’å½’çš„ï¼ˆrecurrentï¼‰ã€‚
    * å‚æ•°( :math:`\mathbf{W}` å’Œ :math:`\mathbf{b}` )åœ¨æ‰€æœ‰æ—¶é—´æ­¥ä¹‹é—´å…±äº«ï¼Œå‚æ•°æ•°é‡ä¸æ—¶é—´æ­¥æ•°æ— å…³ã€‚


4. RNNä¸MLPçš„åŒºåˆ«
"""""""""""""""""

* MLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰ï¼š
    *  æ¯ä¸ªè¾“å…¥æ ·æœ¬ç‹¬ç«‹å¤„ç†ï¼Œä¸è€ƒè™‘æ—¶é—´æ­¥ä¹‹é—´çš„å…³è”ã€‚
    *  éšè—å±‚çš„è¾“å‡ºå…¬å¼ï¼š :math:`ğ»=ğœ™(ğ‘‹ğ‘Š_{xh}+ğ‘_h)`
* RNNï¼š
    * åˆ©ç”¨éšè—çŠ¶æ€æ•è·æ—¶é—´æ­¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
    * éšè—çŠ¶æ€é€’å½’è®¡ç®—ï¼Œå…·å¤‡â€œè®°å¿†â€èƒ½åŠ›ã€‚

5. RNNçš„åº”ç”¨
""""""""""""

* å­—ç¬¦çº§è¯­è¨€æ¨¡å‹ï¼ˆcharacter-level language modelï¼‰ï¼š
    * è¾“å…¥åºåˆ—ï¼ˆå¦‚â€œmachinâ€ï¼‰çš„æ¯ä¸ªå­—ç¬¦ä½œä¸ºä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ã€‚
    * è¾“å‡ºåºåˆ—æ˜¯é¢„æµ‹çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆå¦‚â€œachineâ€ï¼‰ã€‚
    * RNNé€šè¿‡éšè—çŠ¶æ€æ•æ‰å†å²ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¹¶é€æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/TJg72T.png

    Fig. 9.4.2 A character-level language model based on the RNN. The input and target sequences are â€œmachinâ€ and â€œachineâ€, respectively.




6. RNNçš„ä¼˜ç‚¹ä¸å±€é™
""""""""""""""""""

ä¼˜ç‚¹::

    æ•è·åºåˆ—æ•°æ®çš„ä¾èµ–å…³ç³»ã€‚
    å‚æ•°å…±äº«ï¼Œé€‚åˆå¤„ç†é•¿åºåˆ—ã€‚

å±€é™::

    å¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯å¤„ç†é•¿åºåˆ—æ—¶ã€‚
    ä¸ºæ­¤ï¼Œé€šå¸¸ä¼šæ”¹è¿›ä¸ºLSTMæˆ–GRUç­‰å˜ç§æ¨¡å‹ã€‚


æ€»ç»“
""""

* RNNæ˜¯ä¸€ç§èƒ½å¤Ÿæ•è·åºåˆ—ä¿¡æ¯çš„ç¥ç»ç½‘ç»œï¼Œé€šè¿‡é€’å½’è®¡ç®—éšè—çŠ¶æ€ï¼Œå®ç°äº†å¯¹æ—¶é—´åºåˆ—æ•°æ®çš„æœ‰æ•ˆå»ºæ¨¡ã€‚
* å®ƒçš„å…³é”®æ€æƒ³åœ¨äºå…±äº«æ¨¡å‹å‚æ•°å¹¶åˆ©ç”¨éšè—çŠ¶æ€è®°å½•åºåˆ—å†å²ä¿¡æ¯ï¼Œåœ¨è¯­è¨€æ¨¡å‹ç­‰ä»»åŠ¡ä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚




9.5. Recurrent Neural Network Implementation from Scratch
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.5.1. RNN Model
""""""""""""""""

.. code-block:: python

    class RNNScratch(d2l.Module):  #@save
        """The RNN model implemented from scratch."""
        def __init__(self, num_inputs, num_hiddens, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.W_xh = nn.Parameter(
                torch.randn(num_inputs, num_hiddens) * sigma)
            self.W_hh = nn.Parameter(
                torch.randn(num_hiddens, num_hiddens) * sigma)
            self.b_h = nn.Parameter(torch.zeros(num_hiddens))

    @d2l.add_to_class(RNNScratch)  #@save
    def forward(self, inputs, state=None):
        if state is None:
            # Initial state with shape: (batch_size, num_hiddens)
            state = torch.zeros((inputs.shape[1], self.num_hiddens),
                              device=inputs.device)
        else:
            state, = state
        outputs = []
        for X in inputs:  # Shape of inputs: (num_steps, batch_size, num_inputs)
            state = torch.tanh(torch.matmul(X, self.W_xh) +
                             torch.matmul(state, self.W_hh) + self.b_h)
            outputs.append(state)
        return outputs, state

åº”ç”¨::

    batch_size, num_inputs, num_hiddens, num_steps = 2, 16, 32, 100
    rnn = RNNScratch(num_inputs, num_hiddens)
    X = torch.ones((num_steps, batch_size, num_inputs))
    outputs, state = rnn(X)


    def check_len(a, n):  #@save
        """Check the length of a list."""
        assert len(a) == n, f'list\'s length {len(a)} != expected length {n}'

    def check_shape(a, shape):  #@save
        """Check the shape of a tensor."""
        assert a.shape == shape, \
                f'tensor\'s shape {a.shape} != expected shape {shape}'

    check_len(outputs, num_steps)
    check_shape(outputs[0], (batch_size, num_hiddens))
    check_shape(state, (batch_size, num_hiddens))



9.5.2. RNN-Based Language Model
"""""""""""""""""""""""""""""""

.. code-block:: python

    class RNNLMScratch(d2l.Classifier):  #@save
        """The RNN-based language model implemented from scratch."""
        def __init__(self, rnn, vocab_size, lr=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.init_params()

        def init_params(self):
            self.W_hq = nn.Parameter(
                torch.randn(self.rnn.num_hiddens, self.vocab_size) * self.rnn.sigma)
            self.b_q = nn.Parameter(torch.zeros(self.vocab_size))

        def training_step(self, batch):
            l = self.loss(self(*batch[:-1]), batch[-1])
            self.plot('ppl', torch.exp(l), train=True)
            return l

        def validation_step(self, batch):
            l = self.loss(self(*batch[:-1]), batch[-1])
            self.plot('ppl', torch.exp(l), train=False)

9.5.2.1. One-Hot Encoding
+++++++++++++++++++++++++

ç¤ºä¾‹::

    F.one_hot(torch.tensor([0, 2]), 5)
    # è¾“å‡º
    tensor([[1, 0, 0, 0, 0],
            [0, 0, 1, 0, 0]])


.. code-block:: python

    @d2l.add_to_class(RNNLMScratch)  #@save
    def one_hot(self, X):
        # Output shape: (num_steps, batch_size, vocab_size)
        return F.one_hot(X.T, self.vocab_size).type(torch.float32)


9.5.2.2. Transforming RNN Outputs
+++++++++++++++++++++++++++++++++

.. code-block:: python

    @d2l.add_to_class(RNNLMScratch)  #@save
    def output_layer(self, rnn_outputs):
        outputs = [torch.matmul(H, self.W_hq) + self.b_q for H in rnn_outputs]
        return torch.stack(outputs, 1)

    @d2l.add_to_class(RNNLMScratch)  #@save
    def forward(self, X, state=None):
        embs = self.one_hot(X)
        rnn_outputs, _ = self.rnn(embs, state)
        return self.output_layer(rnn_outputs)

æ£€æŸ¥å‰å‘è®¡ç®—æ˜¯å¦äº§ç”Ÿå…·æœ‰æ­£ç¡®å½¢çŠ¶çš„è¾“å‡º::

    model = RNNLMScratch(rnn, num_inputs)
    outputs = model(torch.ones((batch_size, num_steps), dtype=torch.int64))
    check_shape(outputs, (batch_size, num_steps, num_inputs))


9.5.3. Gradient Clipping
""""""""""""""""""""""""

* æ¢¯åº¦æˆªæ–­ï¼ˆgradient clippingï¼‰
* ã€åºåˆ—é•¿åº¦ä¸æ·±åº¦ã€‘ç¥ç»ç½‘ç»œä¹‹æ‰€ä»¥è¢«ç§°ä¸ºâ€œæ·±â€ç½‘ç»œï¼Œæ˜¯å› ä¸ºå®ƒä»¬é€šå¸¸æœ‰è®¸å¤šå±‚ä»è¾“å…¥åˆ°è¾“å‡ºçš„ä¼ é€’ã€‚è€Œåœ¨RNNä¸­ï¼Œç”±äºå¤„ç†çš„æ˜¯æ—¶é—´åºåˆ—æ•°æ®ï¼Œåºåˆ—çš„é•¿åº¦å¼•å…¥äº†ä¸€ç§æ–°çš„â€œæ·±åº¦â€æ¦‚å¿µã€‚å…·ä½“æ¥è¯´ï¼ŒRNNä¸ä»…ä»…æ˜¯å¤„ç†è¾“å…¥åˆ°è¾“å‡ºçš„ç½‘ç»œï¼Œå®ƒè¿˜éœ€è¦åœ¨æ—¶é—´ç»´åº¦ä¸Šè·¨è¶Šå¤šä¸ªæ—¶é—´æ­¥é•¿ï¼Œå› æ­¤åœ¨ä¸€ä¸ªè¾“å…¥åºåˆ—çš„å‰æœŸæ—¶é—´æ­¥ä¸Šçš„ä¿¡æ¯å¿…é¡»é€šè¿‡æ¯ä¸€ä¸ªæ—¶é—´æ­¥çš„â€œå±‚â€é€æ­¥ä¼ é€’ï¼Œä»¥å½±å“æœ€ç»ˆçš„è¾“å‡ºã€‚
* ã€åå‘ä¼ æ’­ä¸­çš„æ—¶é—´ç»´åº¦ã€‘åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ¢¯åº¦æ˜¯é€šè¿‡æ—¶é—´åå‘ä¼ æ’­çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ä»…æ˜¯ç½‘ç»œçš„å±‚ä¹‹é—´ä¼ é€’ä¿¡æ¯ï¼Œè¿˜æ¶‰åŠæ¯ä¸ªæ—¶é—´æ­¥ä¹‹é—´çš„ä¼ æ’­ã€‚éšç€æ—¶é—´æ­¥æ•°çš„å¢åŠ ï¼Œåå‘ä¼ æ’­éœ€è¦é€šè¿‡çš„çŸ©é˜µä¹˜æ³•é“¾æ¡é•¿åº¦ä¸º Tï¼ˆæ—¶é—´æ­¥é•¿ï¼‰ï¼Œè¿™ä½¿å¾—æ¢¯åº¦çš„è®¡ç®—å˜å¾—æ›´å¤æ‚ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼ä¸ç¨³å®šã€‚
* ã€æ¢¯åº¦å‰ªåˆ‡ï¼ˆGradient Clippingï¼‰ã€‘ä¸ºäº†è§£å†³æ¢¯åº¦çˆ†ç‚¸é—®é¢˜ï¼Œä¸€ç§å¸¸ç”¨çš„â€œhackâ€æ˜¯æ¢¯åº¦å‰ªåˆ‡ã€‚æ¢¯åº¦å‰ªåˆ‡é€šè¿‡å¯¹æ¢¯åº¦è¿›è¡Œé™åˆ¶ï¼Œç¡®ä¿å®ƒä»¬çš„èŒƒæ•°ä¸ä¼šè¶…è¿‡ä¸€ä¸ªè®¾å®šçš„æœ€å¤§å€¼ :math:`\theta` ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯å³ä½¿æ¢¯åº¦åœ¨æŸäº›æ—¶é—´æ­¥é•¿ä¸Šçªç„¶å˜å¤§ï¼Œä¹Ÿä¸ä¼šå¯¹è®­ç»ƒé€ æˆä¸¥é‡å½±å“ã€‚



.. code-block:: python

    @d2l.add_to_class(d2l.Trainer)  #@save
    def clip_gradients(self, grad_clip_val, model):
        params = [p for p in model.parameters() if p.requires_grad]
        norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
        if norm > grad_clip_val:
            for param in params:
                param.grad[:] *= grad_clip_val / norm





9.5.4. Training
"""""""""""""""

.. code-block:: python

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    rnn = RNNScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = RNNLMScratch(rnn, vocab_size=len(data.vocab), lr=1)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)




9.5.5. Decoding
"""""""""""""""

.. code-block:: python

    @d2l.add_to_class(RNNLMScratch)  #@save
    def predict(self, prefix, num_preds, vocab, device=None):
        state, outputs = None, [vocab[prefix[0]]]
        for i in range(len(prefix) + num_preds - 1):
            X = torch.tensor([[outputs[-1]]], device=device)
            embs = self.one_hot(X)
            rnn_outputs, state = self.rnn(embs, state)
            if i < len(prefix) - 1:  # Warm-up period
                outputs.append(vocab[prefix[i + 1]])
            else:  # Predict num_preds steps
                Y = self.output_layer(rnn_outputs)
                outputs.append(int(Y.argmax(axis=2).reshape(1)))
        return ''.join([vocab.idx_to_token[i] for i in outputs])

    model.predict('it has', 20, data.vocab, d2l.try_gpu())
    # è¾“å‡º
    'it has in the the the the '






9.6. Concise Implementation of Recurrent Neural Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

9.6.1. Defining the Model
"""""""""""""""""""""""""

.. code-block:: python

    class RNN(d2l.Module):  #@save
        """The RNN model implemented with high-level APIs."""
        def __init__(self, num_inputs, num_hiddens):
            super().__init__()
            self.save_hyperparameters()
            self.rnn = nn.RNN(num_inputs, num_hiddens)

        def forward(self, inputs, H=None):
            return self.rnn(inputs, H)

    class RNNLM(d2l.RNNLMScratch):  #@save
        """The RNN-based language model implemented with high-level APIs."""
        def init_params(self):
            self.linear = nn.LazyLinear(self.vocab_size)

        def output_layer(self, hiddens):
            return self.linear(hiddens).swapaxes(0, 1)


9.6.2. Training and Predicting
""""""""""""""""""""""""""""""

.. code-block:: python

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    rnn = RNN(num_inputs=len(data.vocab), num_hiddens=32)
    model = RNNLM(rnn, vocab_size=len(data.vocab), lr=1)
    model.predict('it has', 20, data.vocab)
    # è¾“å‡º
    'it hasoadd dd dd dd dd dd '

è®­ç»ƒ::

    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)

è®­ç»ƒåæ¨ç†::

    model.predict('it has', 20, data.vocab, d2l.try_gpu())






9.7. Backpropagation Through Time
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* æ—¶é—´åå‘ä¼ æ’­ï¼ˆBackpropagation Through Time, BPTTï¼‰


åº”å¯¹æ¢¯åº¦é—®é¢˜çš„æ–¹æ³•
""""""""""""""""""

* å…¨è®¡ç®—ï¼ˆFull Computationï¼‰ï¼šå®Œæ•´è®¡ç®—æ‰€æœ‰æ—¶é—´æ­¥çš„æ¢¯åº¦ã€‚
    * ç¼ºç‚¹ï¼š
        * è®¡ç®—ä»£ä»·é«˜ã€‚
        * å®¹æ˜“å—åˆ°æ¢¯åº¦çˆ†ç‚¸å’Œæ•°å€¼ä¸ç¨³å®šçš„å½±å“ã€‚
    * å®é™…ä¸Šå¾ˆå°‘ä½¿ç”¨ã€‚

* æ—¶é—´æ­¥æˆªæ–­ï¼ˆTruncating Time Stepsï¼‰ï¼šåªè®¡ç®—æœ€è¿‘ :math:`\tau` ä¸ªæ—¶é—´æ­¥çš„æ¢¯åº¦ï¼Œå¿½ç•¥æ›´æ—©çš„æ—¶é—´æ­¥ã€‚
    * ä¼˜ç‚¹ï¼š
        * ç®€åŒ–è®¡ç®—ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ã€‚
        * åå‘çŸ­æœŸä¾èµ–ï¼Œæœ‰ä¸€å®šæ­£åˆ™åŒ–æ•ˆæœã€‚
    * ç¼ºç‚¹ï¼š
        * å¯èƒ½ä¸¢å¤±é•¿æœŸä¾èµ–çš„ä¿¡æ¯ã€‚

* éšæœºæˆªæ–­ï¼ˆRandomized Truncationï¼‰ï¼šç”¨éšæœºå˜é‡æ›¿ä»£éƒ¨åˆ†æ¢¯åº¦è®¡ç®—ï¼Œä»¥æˆªæ–­åºåˆ—é•¿åº¦ã€‚
    * ä¼˜ç‚¹ï¼š
        * æˆªæ–­ä½ç½®éšæœºåŒ–ï¼Œå¯èƒ½æå‡è®­ç»ƒçš„æ³›åŒ–èƒ½åŠ›ã€‚
    * ç¼ºç‚¹ï¼š
        * å¢åŠ æ¢¯åº¦ä¼°è®¡çš„æ–¹å·®ï¼Œå®é™…æ•ˆæœä¸æ˜¾è‘—ã€‚

æ•°å­¦åˆ†æ
""""""""

* æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œè¾“å‡ºæ˜¯
    * f æ˜¯éšè—å±‚çš„å˜æ¢
    * g æ˜¯è¾“å‡ºå±‚çš„å˜æ¢

.. math::

    \begin{aligned}
        h_t &= f(x_t, h_{t-1}, w_\textrm{h}),\\
        o_t &= g(h_t, w_\textrm{o}),
    \end{aligned}


* å‰å‘ä¼ æ’­ç›¸å½“ç®€å•
    * é€šè¿‡æ‰€æœ‰ T æ—¶é—´æ­¥é•¿çš„ç›®æ ‡å‡½æ•°æ¥è¯„ä¼°è¾“å‡º :math:`o_t` ä¸æ‰€éœ€ç›®æ ‡ :math:`y_t` ä¹‹é—´çš„å·®å¼‚

.. math::

    L(x_1, \ldots, x_T, y_1, \ldots, y_T, w_\textrm{h}, w_\textrm{o}) = \frac{1}{T}\sum_{t=1}^T l(y_t, o_t)

* åå‘ä¼ æ’­ï¼Œäº‹æƒ…æœ‰ç‚¹æ£˜æ‰‹
    * æ ¹æ®é“¾å¼æ³•åˆ™
    * ä¹˜ç§¯çš„ç¬¬ä¸€å’Œç¬¬äºŒå› å­å¾ˆå®¹æ˜“è®¡ç®—ã€‚ç¬¬ä¸‰ä¸ªå› ç´  :math:`\partial h_t/\partial w_\textrm{h}` æ˜¯äº‹æƒ…å˜å¾—æ£˜æ‰‹çš„åœ°æ–¹ï¼Œå› ä¸ºéœ€è¦é€’å½’åœ°ç´¯ç§¯æ¯ä¸ªæ—¶é—´æ­¥å¯¹ :math:`w_\text{h}` çš„å½±å“ã€‚

.. math::

    \begin{aligned}
        \frac{\partial L}{\partial w_\textrm{h}}  
            & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial w_\textrm{h}}  \\
            & = \frac{1}{T}\sum_{t=1}^T \frac{\partial l(y_t, o_t)}{\partial o_t} \frac{\partial g(h_t, w_\textrm{o})}{\partial h_t}  \frac{\partial h_t}{\partial w_\textrm{h}}
    \end{aligned}


ä¸ºäº†å¯¼å‡ºä¸Šè¿°æ¢¯åº¦ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸‰ä¸ªåºåˆ— :math:`\{a_{t}\},\{b_{t}\},\{c_{t}\}` å¯¹äº :math:`t=1, 2,\ldots` æ»¡è¶³ :math:`a_{0}=0` and :math:`a_{t}=b_{t}+c_{t}a_{t-1}` .é‚£ä¹ˆå¯¹äº :math:`t\geq 1` , å¾ˆå®¹æ˜“å¾—å‡º

.. math::

    a_{t}=b_{t}+\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t}c_{j}\right)b_{i}

ç”±äºï¼š

.. math::

    \begin{aligned}
        a_t &= \frac{\partial h_t}{\partial w_\textrm{h}},\\
        b_t &= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}, \\
        c_t &= \frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial h_{t-1}}
    \end{aligned}$$

æŠŠä¸Šé¢ä¸¤å…¬å¼ä»£å…¥ä¸€èµ·ï¼Œå³å¯åˆ é™¤å¾ªç¯è®¡ç®—å¯å¾—å¦‚ä¸‹å…¬å¼

.. math::

    \frac{\partial h_t}{\partial w_\textrm{h}}
        =\frac{\partial f(x_{t},h_{t-1},w_\textrm{h})}{\partial w_\textrm{h}}
            +\sum_{i=1}^{t-1}\left(\prod_{j=i+1}^{t} \frac{\partial f(x_{j},h_{j-1},w_\textrm{h})}{\partial h_{j-1}} \right) 
                \frac{\partial f(x_{i},h_{i-1},w_\textrm{h})}{\partial w_\textrm{h}}

æ€»ç»“ä¸å®è·µæ„ä¹‰
""""""""""""""

* åœ¨å®é™…ä¸­ï¼Œæ—¶é—´æ­¥æˆªæ–­ï¼ˆTruncated BPTTï¼‰æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ï¼Œå®ƒåœ¨è®¡ç®—æ•ˆç‡å’Œæ¨¡å‹ç¨³å®šæ€§ä¹‹é—´è¾¾æˆäº†å¹³è¡¡ã€‚
* å¯¹äºå¤„ç†é•¿åºåˆ—æ•°æ®çš„ä»»åŠ¡ï¼ˆå¦‚æ–‡æœ¬åºåˆ—ï¼‰ï¼Œè¿™ç§æ–¹æ³•æ—¢èƒ½æ•è·çŸ­æœŸä¾èµ–ï¼Œåˆèƒ½é¿å…æ¢¯åº¦é—®é¢˜å¸¦æ¥çš„è®­ç»ƒå›°éš¾ã€‚


10. Modern Recurrent Neural Networks
------------------------------------


10.1. Long Short-Term Memory (LSTM)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* è™½ç„¶æ¢¯åº¦è£å‰ªæœ‰åŠ©äºæ¢¯åº¦çˆ†ç‚¸ï¼Œä½†å¤„ç†æ¢¯åº¦æ¶ˆå¤±ä¼¼ä¹éœ€è¦æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚ 
* Hochreiter å’Œ Schmidhuber (1997) æå‡ºçš„è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„ç¬¬ä¸€ä¸ªä¹Ÿæ˜¯æœ€æˆåŠŸçš„æŠ€æœ¯ä¹‹ä¸€æ˜¯é•¿çŸ­æœŸè®°å¿† (LSTM) æ¨¡å‹ã€‚ 
* LSTM ç±»ä¼¼äºæ ‡å‡†çš„å¾ªç¯ç¥ç»ç½‘ç»œï¼Œä½†è¿™é‡Œæ¯ä¸ªæ™®é€šçš„å¾ªç¯èŠ‚ç‚¹éƒ½è¢«ä¸€ä¸ªè®°å¿†å•å…ƒå–ä»£ã€‚
* æ¯ä¸ªè®°å¿†å•å…ƒåŒ…å«ä¸€ä¸ªå†…éƒ¨çŠ¶æ€ï¼Œå³å…·æœ‰å›ºå®šæƒé‡1çš„è‡ªè¿æ¥å¾ªç¯è¾¹çš„èŠ‚ç‚¹ï¼Œç¡®ä¿æ¢¯åº¦å¯ä»¥è·¨è¶Šè®¸å¤šæ—¶é—´æ­¥è€Œä¸ä¼šæ¶ˆå¤±æˆ–çˆ†ç‚¸ã€‚

* â€œé•¿çŸ­æœŸè®°å¿†â€ä¸€è¯æ¥è‡ªä»¥ä¸‹ç›´è§‰:
    * ç®€å•çš„å¾ªç¯ç¥ç»ç½‘ç»œå…·æœ‰æƒé‡å½¢å¼çš„é•¿æœŸè®°å¿†ã€‚
        * æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç¼“æ…¢å˜åŒ–ï¼Œç¼–ç æœ‰å…³æ•°æ®çš„ä¸€èˆ¬çŸ¥è¯†ã€‚
    * å®ƒä»¬è¿˜å…·æœ‰çŸ­æš‚æ¿€æ´»å½¢å¼çš„çŸ­æœŸè®°å¿†ï¼Œä»æ¯ä¸ªèŠ‚ç‚¹ä¼ é€’åˆ°è¿ç»­çš„èŠ‚ç‚¹ã€‚ 
    * LSTM æ¨¡å‹é€šè¿‡è®°å¿†å•å…ƒå¼•å…¥äº†ä¸­é—´ç±»å‹çš„å­˜å‚¨ã€‚
    * å­˜å‚¨å•å…ƒæ˜¯ä¸€ä¸ªå¤åˆå•å…ƒï¼Œç”±ç‰¹å®šè¿æ¥æ¨¡å¼ä¸­çš„ç®€å•èŠ‚ç‚¹æ„å»ºè€Œæˆï¼Œå¹¶æ–°é¢–åœ°åŒ…å«ä¹˜æ³•èŠ‚ç‚¹ã€‚



10.1.1. Gated Memory Cell
"""""""""""""""""""""""""

10.1.1.1. Gated Hidden State
++++++++++++++++++++++++++++

* æ™®é€š RNN å’Œ LSTM ä¹‹é—´çš„ä¸»è¦åŒºåˆ«åœ¨äºåè€…æ”¯æŒéšè—çŠ¶æ€çš„é—¨æ§ã€‚
* è¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ä¸“é—¨çš„æœºåˆ¶æ¥ç¡®å®šä½•æ—¶åº”è¯¥æ›´æ–°éšè—çŠ¶æ€ä»¥åŠä½•æ—¶åº”è¯¥é‡ç½®éšè—çŠ¶æ€ã€‚

10.1.1.2. Input Gate, Forget Gate, and Output Gate
++++++++++++++++++++++++++++++++++++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/nDlrQH.png

    Fig. 10.1.1 Computing the input gate, the forget gate, and the output gate in an LSTM model.


* The data feeding into the LSTM gates are the ``input`` at the current time step and the ``hidden state`` of the previous time step
* The  ``input`` gate determines how much of the input nodeâ€™s value should be added to the current memory cell internal state. 
* The ``forget`` gate determines whether to keep the current value of the memory or flush it. 
* The ``output`` gate determines whether the memory cell should influence the output at the current time step.

* ä»æ•°å­¦ä¸Šè®²ï¼Œå‡è®¾æœ‰ h ä¸ªéšè—å•å…ƒï¼Œæ‰¹é‡å¤§å°ä¸º n ï¼Œè¾“å…¥æ•°é‡ä¸º d 
* å› æ­¤ï¼Œè¾“å…¥æ˜¯ :math:`\mathbf{X}_t \in \mathbb{R}^{n \times d}` ï¼Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€æ˜¯ :math:`\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}`
* Correspondingly, the gates at time step ``t`` are defined as follows: 
    * the  input gate is :math:`\mathbf{I}_t \in \mathbb{R}^{n \times h}` ,
    * the forget gate is :math:`\mathbf{F}_t \in \mathbb{R}^{n \times h}` ,
    * the output gate is :math:`\mathbf{O}_t \in \mathbb{R}^{n \times h}` .
* å…¬å¼ï¼š

.. math::

    \begin{aligned}
        \mathbf{I}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xi}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hi}} + \mathbf{b}_\textrm{i}),\\
        \mathbf{F}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xf}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hf}} + \mathbf{b}_\textrm{f}),\\
        \mathbf{O}_t &= \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xo}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{ho}} + \mathbf{b}_\textrm{o}),
    \end{aligned}

* where :math:`\mathbf{W}_{\textrm{xi}}, \mathbf{W}_{\textrm{xf}}, \mathbf{W}_{\textrm{xo}} \in \mathbb{R}^{d \times h}` 
*   and :math:`\mathbf{W}_{\textrm{hi}}, \mathbf{W}_{\textrm{hf}}, \mathbf{W}_{\textrm{ho}} \in \mathbb{R}^{h \times h}` are weight parameters 
*   and :math:`\mathbf{b}_\textrm{i}, \mathbf{b}_\textrm{f}, \mathbf{b}_\textrm{o} \in \mathbb{R}^{1 \times h}` are bias parameters.


10.1.1.3. Input Node
++++++++++++++++++++

* å…¬å¼å’Œä¸Šè¿°ä¸‰ä¸ªé—¨ç±»ä¼¼ï¼Œä½†ä½¿ç”¨å–å€¼èŒƒå›´ä¸º (-1, 1) çš„ ``tanh`` å‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚

.. math::

    \tilde{\mathbf{C}}_t = \textrm{tanh}(\mathbf{X}_t \mathbf{W}_{\textrm{xc}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hc}} + \mathbf{b}_\textrm{c})

* *input node* : :math:`\tilde{\mathbf{C}}_t \in \mathbb{R}^{n \times h}`
* where :math:`\mathbf{W}_{\textrm{xc}} \in \mathbb{R}^{d \times h}` 
* and   :math:`\mathbf{W}_{\textrm{hc}} \in \mathbb{R}^{h \times h}` are weight parameters and $\mathbf{b}_\textrm{c} \in \mathbb{R}^{1 \times h}$ is a bias parameter.



.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/HvpMT7.png

    Fig. 10.1.2 Computing the input node in an LSTM model.


10.1.1.4. Memory Cell Internal State
++++++++++++++++++++++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/G2kAIc.png

    Fig. 10.1.3 Computing the memory cell internal state in an LSTM model.


* In LSTMs, the input gate :math:`\mathbf{I}_t` governs how much we take new data into account via :math:`\tilde{\mathbf{C}}_t` 
    * and the forget gate :math:`\mathbf{F}_t` addresses how much of the old cell internal state :math:`\mathbf{C}_{t-1} \in \mathbb{R}^{n \times h}` we retain. 
* Using the Hadamard (elementwise) product operator :math:`\odot` we arrive at the following update equation:

.. math::

    \mathbf{C}_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{\mathbf{C}}_t


10.1.1.5. Hidden State
++++++++++++++++++++++

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/sMfLde.png

    Fig. 10.1.4 Computing the hidden state in an LSTM model.

Now compute the output of the memory cell(the hidden state :math:`\mathbf{H}_t \in \mathbb{R}^{n \times h}` )

.. math::

    \mathbf{H}_t = \mathbf{O}_t \odot \tanh(\mathbf{C}_t)



10.1.2. Implementation from Scratch
"""""""""""""""""""""""""""""""""""

10.1.2.1. Initializing Model Parameters
+++++++++++++++++++++++++++++++++++++++

.. code-block:: python

    class LSTMScratch(d2l.Module):
        def __init__(self, num_inputs, num_hiddens, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()

            init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              nn.Parameter(torch.zeros(num_hiddens)))
            self.W_xi, self.W_hi, self.b_i = triple()  # Input gate
            self.W_xf, self.W_hf, self.b_f = triple()  # Forget gate
            self.W_xo, self.W_ho, self.b_o = triple()  # Output gate
            self.W_xc, self.W_hc, self.b_c = triple()  # Input node

    @d2l.add_to_class(LSTMScratch)
    def forward(self, inputs, H_C=None):
        if H_C is None:
            # Initial state with shape: (batch_size, num_hiddens)
            H = torch.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
            C = torch.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        else:
            H, C = H_C
        outputs = []
        for X in inputs:
            I = torch.sigmoid(torch.matmul(X, self.W_xi) +
                            torch.matmul(H, self.W_hi) + self.b_i)
            F = torch.sigmoid(torch.matmul(X, self.W_xf) +
                            torch.matmul(H, self.W_hf) + self.b_f)
            O = torch.sigmoid(torch.matmul(X, self.W_xo) +
                            torch.matmul(H, self.W_ho) + self.b_o)
            C_tilde = torch.tanh(torch.matmul(X, self.W_xc) +
                               torch.matmul(H, self.W_hc) + self.b_c)
            C = F * C + I * C_tilde
            H = O * torch.tanh(C)
            outputs.append(H)
        return outputs, (H, C)


10.1.2.2. Training and Prediction
+++++++++++++++++++++++++++++++++

::

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    lstm = LSTMScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLMScratch(lstm, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)



10.1.3. Concise Implementation
""""""""""""""""""""""""""""""

.. code-block:: python

    class LSTM(d2l.RNN):
        def __init__(self, num_inputs, num_hiddens):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.LSTM(num_inputs, num_hiddens)

        def forward(self, inputs, H_C=None):
            return self.rnn(inputs, H_C)

    lstm = LSTM(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLM(lstm, vocab_size=len(data.vocab), lr=4)
    trainer.fit(model, data)

ä½¿ç”¨::

    model.predict('it has', 20, data.vocab, d2l.try_gpu())
    # è¾“å‡º
    'it has a the time travelly'


10.1.4. Summary
"""""""""""""""

* LSTM äº 1997 å¹´å‘å¸ƒï¼Œä½†åœ¨ 2000 å¹´ä»£ä¸­æœŸçš„é¢„æµ‹ç«èµ›ä¸­å–å¾—äº†ä¸€äº›èƒœåˆ©ï¼Œä½¿å…¶å£°åé¹Šèµ·ï¼Œå¹¶ä» 2011 å¹´å¼€å§‹æˆä¸ºåºåˆ—å­¦ä¹ çš„ä¸»å¯¼æ¨¡å‹ï¼Œç›´åˆ° 2017 å¹´å¼€å§‹ Transformer æ¨¡å‹çš„å…´èµ·ã€‚
* LSTM å…·æœ‰ä¸‰ç§ç±»å‹çš„é—¨ï¼šè¾“å…¥é—¨ã€é—å¿˜é—¨å’Œæ§åˆ¶ä¿¡æ¯æµçš„è¾“å‡ºé—¨ã€‚ LSTMçš„éšè—å±‚è¾“å‡ºåŒ…æ‹¬éšè—çŠ¶æ€å’Œè®°å¿†å•å…ƒå†…éƒ¨çŠ¶æ€ã€‚åªæœ‰éšè—çŠ¶æ€è¢«ä¼ é€’åˆ°è¾“å‡ºå±‚ï¼Œè€Œå­˜å‚¨å•å…ƒå†…éƒ¨çŠ¶æ€å®Œå…¨ä¿æŒåœ¨å†…éƒ¨ã€‚ LSTM å¯ä»¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸ã€‚



10.2. Gated Recurrent Units (GRU)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* é—¨æ§å¾ªç¯å•å…ƒ (GRU)ï¼ˆCho ç­‰äººï¼Œ2014ï¼‰æä¾›äº† LSTM å­˜å‚¨å•å…ƒçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œé€šå¸¸å¯ä»¥å®ç°ç›¸å½“çš„æ€§èƒ½ï¼Œä½†å…·æœ‰è®¡ç®—é€Ÿåº¦æ›´å¿«çš„ä¼˜ç‚¹ï¼ˆChung ç­‰äººï¼Œ2014ï¼‰ã€‚

10.2.1. Reset Gate and Update Gate
""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/yvWA3J.png

    Fig. 10.2.1 Computing the reset gate and the update gate in a GRU model. ç»™å®šå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥å’Œå‰ä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚

* LSTM çš„ä¸‰ä¸ªé—¨è¢«æ›¿æ¢ä¸ºä¸¤ä¸ªï¼šé‡ç½®é—¨å’Œæ›´æ–°é—¨ã€‚
* ç›´è§‚ä¸Šï¼Œ
    * é‡ç½®é—¨æ§åˆ¶ç€æˆ‘ä»¬å¯èƒ½ä»æƒ³è®°ä½å¤šå°‘å…ˆå‰çš„çŠ¶æ€ã€‚
    * æ›´æ–°é—¨å°†å…è®¸æˆ‘ä»¬æ§åˆ¶æ–°çŠ¶æ€ä¸­æœ‰å¤šå°‘åªæ˜¯æ—§çŠ¶æ€çš„å‰¯æœ¬ã€‚

* Mathematically, for a given time step ``t`` , suppose that the input is a minibatch :math:`\mathbf{X}_t \in \mathbb{R}^{n \times d}` (``number of examples =n; number of inputs =d``)
* and the hidden state of the previous time step is :math:`\mathbf{H}_{t-1} \in \mathbb{R}^{n \times h}` (``number of hidden units =h``). 
* Then the reset gate :math:`\mathbf{R}_t \in \mathbb{R}^{n \times h}` and update gate :math:`\mathbf{Z}_t \in \mathbb{R}^{n \times h}` are computed as follows:

.. math::

    \begin{aligned}
        \mathbf{R}_t = \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xr}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hr}} + \mathbf{b}_\textrm{r}),\\
        \mathbf{Z}_t = \sigma(\mathbf{X}_t \mathbf{W}_{\textrm{xz}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hz}} + \mathbf{b}_\textrm{z}),
    \end{aligned}



10.2.2. Candidate Hidden State
""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/8A9kVr.png

    Fig. 10.2.2 Computing the candidate hidden state in a GRU model.

å…¬å¼:

.. math::

    \tilde{\mathbf{H}}_t = \tanh(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + \left(\mathbf{R}_t \odot \mathbf{H}_{t-1}\right) \mathbf{W}_{\textrm{hh}} + \mathbf{b}_\textrm{h})


* å½“é‡ç½®é—¨ :math:`\mathbf{R}_t` ä¸­çš„æ¡ç›®æ¥è¿‘ 1 æ—¶ï¼Œæˆ‘ä»¬å°±ä¼šæ¢å¤ä¸€ä¸ªæ™®é€šçš„ RNN
* å½“é‡ç½®é—¨ :math:`\mathbf{R}_t` ä¸­çš„æ¡ç›®æ¥è¿‘ 0 æ—¶ï¼Œå€™é€‰éšè—çŠ¶æ€æ˜¯ä»¥ :math:`\mathbf{X}_t` ä½œä¸ºè¾“å…¥çš„ MLP çš„ç»“æœã€‚


10.2.3. Hidden State
""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/yy2z50.png

    Fig. 10.2.3 Computing the hidden state in a GRU model.


.. math::

    \mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1}  + (1 - \mathbf{Z}_t) \odot \tilde{\mathbf{H}}_t


* æ¯å½“æ›´æ–°é—¨ :math:`\mathbf{Z}_t` æ¥è¿‘1æ—¶ï¼Œè¿”å›å‰ä¸€ä¸ªéšè—çŠ¶æ€ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¥è‡ª :math:`\mathbf{X}_t` çš„ä¿¡æ¯å°†è¢«å¿½ç•¥ï¼Œä»è€Œæœ‰æ•ˆåœ°è·³è¿‡ä¾èµ–é“¾ä¸­çš„æ—¶é—´æ­¥ t ã€‚
* ç›¸åï¼Œæ¯å½“ :math:`\mathbf{Z}_t` æ¥è¿‘ 0 æ—¶ï¼Œæ–°çš„æ½œåœ¨çŠ¶æ€ :math:`\mathbf{H}_t` å°±ä¼šæ¥è¿‘å€™é€‰æ½œåœ¨çŠ¶æ€ :math:`\tilde{\mathbf{H}}_t` ã€‚

* GRUæœ‰ä»¥ä¸‹ä¸¤ä¸ªæ˜¾ç€ç‰¹å¾ï¼š
    * Reset gates help capture short-term dependencies in sequences.
    * Update gates help capture long-term dependencies in sequences.


10.2.4. Implementation from Scratch
"""""""""""""""""""""""""""""""""""

10.2.4.1. Initializing Model Parameters
+++++++++++++++++++++++++++++++++++++++

.. code-block:: python

    class GRUScratch(d2l.Module):
        def __init__(self, num_inputs, num_hiddens, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()

            init_weight = lambda *shape: nn.Parameter(torch.randn(*shape) * sigma)
            triple = lambda: (init_weight(num_inputs, num_hiddens),
                              init_weight(num_hiddens, num_hiddens),
                              nn.Parameter(torch.zeros(num_hiddens)))
            self.W_xz, self.W_hz, self.b_z = triple()  # Update gate
            self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate
            self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state

10.2.4.2. Defining the Model
++++++++++++++++++++++++++++

.. code-block:: python

    @d2l.add_to_class(GRUScratch)
    def forward(self, inputs, H=None):
        if H is None:
            # Initial state with shape: (batch_size, num_hiddens)
            H = torch.zeros((inputs.shape[1], self.num_hiddens),
                          device=inputs.device)
        outputs = []
        for X in inputs:
            Z = torch.sigmoid(torch.matmul(X, self.W_xz) +
                            torch.matmul(H, self.W_hz) + self.b_z)
            R = torch.sigmoid(torch.matmul(X, self.W_xr) +
                            torch.matmul(H, self.W_hr) + self.b_r)
            H_tilde = torch.tanh(torch.matmul(X, self.W_xh) +
                               torch.matmul(R * H, self.W_hh) + self.b_h)
            H = Z * H + (1 - Z) * H_tilde
            outputs.append(H)
        return outputs, H

10.2.4.3. Training
++++++++++++++++++

::

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)
    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)


10.2.5. Concise Implementation
""""""""""""""""""""""""""""""

::

    class GRU(d2l.RNN):
        def __init__(self, num_inputs, num_hiddens):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.GRU(num_inputs, num_hiddens)

    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)
    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)
    trainer.fit(model, data)

ä½¿ç”¨::

    model.predict('it has', 20, data.vocab, d2l.try_gpu())
    # è¾“å‡º
    'it has so it and the time '



10.2.6. Summary
"""""""""""""""

* ä¸ LSTM ç›¸æ¯”ï¼ŒGRU å®ç°äº†ç›¸ä¼¼çš„æ€§èƒ½ï¼Œä½†è®¡ç®—é‡å¾€å¾€æ›´è½»ã€‚
* å½“é‡ç½®é—¨æ‰“å¼€æ—¶ï¼ŒGRU åŒ…å«åŸºæœ¬çš„ RNN ä½œä¸ºå…¶æç«¯æƒ…å†µã€‚
* å½“æ›´æ–°é—¨æ‰“å¼€æ—¶ï¼Œå¯ä»¥ç”¨æ¥è·³è¿‡å­åºåˆ—ã€‚



10.3. Deep Recurrent Neural Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/AmmtES.png

    Fig. 10.3.1 Architecture of a deep RNN.

* the hidden state of the :math:`l^\textrm{th}` hidden layer ( :math:`l=1,\ldots,L` ) be :math:`\mathbf{H}_t^{(l)} \in \mathbb{R}^{n \times h}` (``number of hidden units =h``)
* the output layer variable be :math:`\mathbf{O}_t \in \mathbb{R}^{n \times q}` (``number of outputs: q``).
* Setting :math:`\mathbf{H}_t^{(0)} = \mathbf{X}_t`
* the hidden state of the :math:`l^\textrm{th}` hidden layer that uses the activation function :math:`\phi_l` is calculated as follows:

.. math::

    \mathbf{H}_t^{(l)} = \phi_l(\mathbf{H}_t^{(l-1)} \mathbf{W}_{\textrm{xh}}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{\textrm{hh}}^{(l)}  + \mathbf{b}_\textrm{h}^{(l)})


æœ€åï¼Œè¾“å‡ºå±‚çš„è®¡ç®—ä»…åŸºäºæœ€ç»ˆ :math:`\mathbf{L}^{th}` éšè—å±‚çš„éšè—çŠ¶æ€ï¼š

.. math::

    \mathbf{O}_t = \mathbf{H}_t^{(L)} \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q}




10.3.1. Implementation from Scratch
"""""""""""""""""""""""""""""""""""

.. code-block:: python

    class StackedRNNScratch(d2l.Module):
        def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.rnns = nn.Sequential(*[d2l.RNNScratch(
                num_inputs if i==0 else num_hiddens, num_hiddens, sigma)
                                        for i in range(num_layers)])

    @d2l.add_to_class(StackedRNNScratch)
    def forward(self, inputs, Hs=None):
        outputs = inputs
        if Hs is None: Hs = [None] * self.num_layers
        for i in range(self.num_layers):
            outputs, Hs[i] = self.rnns[i](outputs, Hs[i])
            outputs = torch.stack(outputs, 0)
        return outputs, Hs

ä½¿ç”¨(ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†å±‚æ•°è®¾ç½®ä¸º 2)::

    data = d2l.TimeMachine(batch_size=1024, num_steps=32)
    rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),
                                  num_hiddens=32, num_layers=2)
    model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)
    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)



10.3.2. Concise Implementation
""""""""""""""""""""""""""""""

.. code-block:: python

    class GRU(d2l.RNN):  #@save
        """The multilayer GRU model."""
        def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,
                              dropout=dropout)

    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)
    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)
    trainer.fit(model, data)

ä½¿ç”¨::

    model.predict('it has', 20, data.vocab, d2l.try_gpu())
    # è¾“å‡º
    'it has for and the time th'




10.3.3. Summary
"""""""""""""""

* åœ¨æ·±åº¦ RNN ä¸­ï¼Œéšè—çŠ¶æ€ä¿¡æ¯è¢«ä¼ é€’åˆ°å½“å‰å±‚çš„ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥å’Œä¸‹ä¸€å±‚çš„å½“å‰æ—¶é—´æ­¥ã€‚
* å­˜åœ¨è®¸å¤šä¸åŒé£æ ¼çš„æ·±åº¦ RNNï¼Œä¾‹å¦‚ LSTMã€GRU æˆ–æ™®é€š RNNã€‚


10.4. Bidirectional Recurrent Neural Networks
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* å°†ä»»ä½•å•å‘ RNN è½¬æ¢ä¸ºåŒå‘ RNNï¼ˆSchuster å’Œ Paliwalï¼Œ1997ï¼‰ã€‚
* å®ç°ä¸¤ä¸ªå•å‘ RNN å±‚ï¼Œå®ƒä»¬ä»¥ç›¸åçš„æ–¹å‘é“¾æ¥åœ¨ä¸€èµ·å¹¶ä½œç”¨äºç›¸åŒçš„è¾“å…¥
* å¯¹äºç¬¬ä¸€ä¸ª RNN å±‚ï¼Œç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ :math:`\mathbf{x}_1` ï¼Œæœ€åä¸€ä¸ªè¾“å…¥æ˜¯ :math:`\mathbf{x}_T` ï¼Œä½†å¯¹äºç¬¬äºŒä¸ª RNN å±‚ï¼Œç¬¬ä¸€ä¸ªè¾“å…¥æ˜¯ :math:`\mathbf{x}_T` ï¼Œæœ€åä¸€ä¸ªè¾“å…¥æ˜¯ :math:`\mathbf{x}_1` ï¼Œæœ€ç»ˆçš„è¾“å…¥æ˜¯ :math:`\mathbf{x}_1` ã€‚ä¸ºäº†äº§ç”Ÿè¿™ä¸ªåŒå‘ RNN å±‚çš„è¾“å‡ºï¼Œæˆ‘ä»¬åªéœ€å°†ä¸¤ä¸ªåº•å±‚å•å‘ RNN å±‚çš„ç›¸åº”è¾“å‡ºè¿æ¥åœ¨ä¸€èµ·ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/iQf4dQ.png

    Fig. 10.4.1 Architecture of a bidirectional RNN.

å‰å‘å’Œåå‘éšè—çŠ¶æ€æ›´æ–°ï¼š

.. math::

    \begin{aligned}
        \overrightarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{\textrm{xh}}^{(f)} + \overrightarrow{\mathbf{H}}_{t-1} \mathbf{W}_{\textrm{hh}}^{(f)}  + \mathbf{b}_\textrm{h}^{(f)}),\\
        \overleftarrow{\mathbf{H}}_t &= \phi(\mathbf{X}_t \mathbf{W}_{\textrm{xh}}^{(b)} + \overleftarrow{\mathbf{H}}_{t+1} \mathbf{W}_{\textrm{hh}}^{(b)}  + \mathbf{b}_\textrm{h}^{(b)}),
    \end{aligned}

* :math:`\mathbf{H}_t \in \mathbb{R}^{n \times 2h}` æ˜¯æŠŠ :math:`\overrightarrow{\mathbf{H}}_t` å’Œ :math:`\overleftarrow{\mathbf{H}}_t` åˆå¹¶èµ·æ¥å¾—åˆ°
* æœ€åï¼Œè¾“å‡ºå±‚è®¡ç®—è¾“å‡º :math:`\mathbf{O}_t \in \mathbb{R}^{n \times q}` (``number of outputs =q``):

.. math::

    \mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q}



10.4.1. Implementation from Scratch
"""""""""""""""""""""""""""""""""""

.. code-block:: python

    class BiRNNScratch(d2l.Module):
        def __init__(self, num_inputs, num_hiddens, sigma=0.01):
            super().__init__()
            self.save_hyperparameters()
            self.f_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
            self.b_rnn = d2l.RNNScratch(num_inputs, num_hiddens, sigma)
            self.num_hiddens *= 2  # The output dimension will be doubled

    @d2l.add_to_class(BiRNNScratch)
    def forward(self, inputs, Hs=None):
        f_H, b_H = Hs if Hs is not None else (None, None)
        f_outputs, f_H = self.f_rnn(inputs, f_H)
        b_outputs, b_H = self.b_rnn(reversed(inputs), b_H)
        outputs = [torch.cat((f, b), -1) for f, b in zip(
            f_outputs, reversed(b_outputs))]
        return outputs, (f_H, b_H)



10.4.2. Concise Implementation
""""""""""""""""""""""""""""""

.. code-block:: python

    class BiGRU(d2l.RNN):
        def __init__(self, num_inputs, num_hiddens):
            d2l.Module.__init__(self)
            self.save_hyperparameters()
            self.rnn = nn.GRU(num_inputs, num_hiddens, bidirectional=True)
            self.num_hiddens *= 2




10.4.3. Summary
"""""""""""""""

* åœ¨åŒå‘ RNN ä¸­ï¼Œæ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ç”±å½“å‰æ—¶é—´æ­¥ä¹‹å‰å’Œä¹‹åçš„æ•°æ®åŒæ—¶ç¡®å®šã€‚
* åŒå‘ RNN æœ€é€‚ç”¨äºåºåˆ—ç¼–ç å’Œç»™å®šåŒå‘ä¸Šä¸‹æ–‡çš„è§‚æµ‹å€¼ä¼°è®¡ã€‚ç”±äºæ¢¯åº¦é“¾è¾ƒé•¿ï¼ŒåŒå‘ RNN çš„è®­ç»ƒæˆæœ¬éå¸¸é«˜ã€‚


10.5. Machine Translation and the Dataset
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

10.5.1. Downloading and Preprocessing the Dataset
"""""""""""""""""""""""""""""""""""""""""""""""""

::

    class MTFraEng(d2l.DataModule):  #@save
        """The English-French dataset."""
        def _download(self):
            d2l.extract(d2l.download(
                d2l.DATA_URL+'fra-eng.zip', self.root,
                '94646ad1522d915e7b0f9296181140edcf86a4f5'))
            with open(self.root + '/fra-eng/fra.txt', encoding='utf-8') as f:
                return f.read()

    data = MTFraEng()
    raw_text = data._download()
    print(raw_text[:75])

    # è¾“å‡º
    Downloading ../data/fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.com/fra-eng.zip...
    Go. Va !
    Hi. Salut !
    Run!        Coursâ€¯!
    Run!        Courezâ€¯!
    Who?        Qui ?
    Wow!        Ã‡a alors !


é¢„å¤„ç†(ç”¨ç©ºæ ¼æ›¿æ¢ä¸é—´æ–­ç©ºæ ¼ï¼Œå°†å¤§å†™å­—æ¯è½¬æ¢ä¸ºå°å†™å­—æ¯ï¼Œä»¥åŠåœ¨å•è¯å’Œæ ‡ç‚¹ç¬¦å·ä¹‹é—´æ’å…¥ç©ºæ ¼)::

    @d2l.add_to_class(MTFraEng)  #@save
    def _preprocess(self, text):
        # Replace non-breaking space with space
        text = text.replace('\u202f', ' ').replace('\xa0', ' ')
        # Insert space between words and punctuation marks
        no_space = lambda char, prev_char: char in ',.!?' and prev_char != ' '
        out = [' ' + char if i > 0 and no_space(char, text[i - 1]) else char
               for i, char in enumerate(text.lower())]
        return ''.join(out)

    text = data._preprocess(raw_text)
    print(text[:80])

    # è¾“å‡º
    go .        va !
    hi .        salut !
    run !       cours !
    run !       courez !
    who ?       qui ?
    wow !       Ã§a alors !

10.5.2. Tokenization
""""""""""""""""""""

src[i] æ˜¯æºè¯­è¨€ï¼ˆæ­¤å¤„ä¸ºè‹±è¯­ï¼‰çš„æ–‡æœ¬åºåˆ—ä¸­çš„æ ‡è®°åˆ—è¡¨ï¼Œ tgt[i] æ˜¯ç›®æ ‡è¯­è¨€ï¼ˆæ­¤å¤„ä¸ºæ³•è¯­ï¼‰çš„æ ‡è®°åˆ—è¡¨::

    @d2l.add_to_class(MTFraEng)  #@save
    def _tokenize(self, text, max_examples=None):
        src, tgt = [], []
        for i, line in enumerate(text.split('\n')):
            if max_examples and i > max_examples: break
            parts = line.split('\t')
            if len(parts) == 2:
                # Skip empty tokens
                src.append([t for t in f'{parts[0]} <eos>'.split(' ') if t])
                tgt.append([t for t in f'{parts[1]} <eos>'.split(' ') if t])
        return src, tgt

    src, tgt = data._tokenize(text)
    src[:6], tgt[:6]
    # è¾“å‡º
    ([['go', '.', '<eos>'],
      ['hi', '.', '<eos>'],
      ['run', '!', '<eos>'],
      ['run', '!', '<eos>'],
      ['who', '?', '<eos>'],
      ['wow', '!', '<eos>']],
     [['va', '!', '<eos>'],
      ['salut', '!', '<eos>'],
      ['cours', '!', '<eos>'],
      ['courez', '!', '<eos>'],
      ['qui', '?', '<eos>'],
      ['Ã§a', 'alors', '!', '<eos>']])


10.5.3. Loading Sequences of Fixed Length
"""""""""""""""""""""""""""""""""""""""""

.. code-block:: python

    @d2l.add_to_class(MTFraEng)  #@save
    def __init__(self, batch_size, num_steps=9, num_train=512, num_val=128):
        super(MTFraEng, self).__init__()
        self.save_hyperparameters()
        self.arrays, self.src_vocab, self.tgt_vocab = self._build_arrays(
            self._download())

    @d2l.add_to_class(MTFraEng)  #@save
    def _build_arrays(self, raw_text, src_vocab=None, tgt_vocab=None):
        def _build_array(sentences, vocab, is_tgt=False):
            pad_or_trim = lambda seq, t: (
                seq[:t] if len(seq) > t else seq + ['<pad>'] * (t - len(seq)))
            sentences = [pad_or_trim(s, self.num_steps) for s in sentences]
            if is_tgt:
                sentences = [['<bos>'] + s for s in sentences]
            if vocab is None:
                vocab = d2l.Vocab(sentences, min_freq=2)
            array = torch.tensor([vocab[s] for s in sentences])
            valid_len = (array != vocab['<pad>']).type(torch.int32).sum(1)
            return array, vocab, valid_len
        src, tgt = self._tokenize(self._preprocess(raw_text),
                                  self.num_train + self.num_val)
        src_array, src_vocab, src_valid_len = _build_array(src, src_vocab)
        tgt_array, tgt_vocab, _ = _build_array(tgt, tgt_vocab, True)
        return ((src_array, tgt_array[:,:-1], src_valid_len, tgt_array[:,1:]),
                src_vocab, tgt_vocab)


10.5.4. Reading the Dataset
"""""""""""""""""""""""""""

å®šä¹‰ get_dataloader æ–¹æ³•æ¥è¿”å›æ•°æ®è¿­ä»£å™¨::

    @d2l.add_to_class(MTFraEng)  #@save
    def get_dataloader(self, train):
        idx = slice(0, self.num_train) if train else slice(self.num_train, None)
        return self.get_tensorloader(self.arrays, train, idx)

è¯»å–è‹±è¯­-æ³•è¯­æ•°æ®é›†ä¸­çš„ç¬¬ä¸€ä¸ªå°æ‰¹é‡::

    data = MTFraEng(batch_size=3)
    src, tgt, src_valid_len, label = next(iter(data.train_dataloader()))
    print('source:', src.type(torch.int32))
    print('decoder input:', tgt.type(torch.int32))
    print('source len excluding pad:', src_valid_len.type(torch.int32))
    print('label:', label.type(torch.int32))
    # è¾“å‡º
    source: tensor([
            [117, 182,   0,   3,   4,   4,   4,   4,   4],
            [ 62,  72,   2,   3,   4,   4,   4,   4,   4],
            [ 57, 124,   0,   3,   4,   4,   4,   4,   4]], dtype=torch.int32)
    decoder input: tensor([
            [  3,  37, 100,  58, 160,   0,   4,   5,   5],
            [  3,   6,   2,   4,   5,   5,   5,   5,   5],
            [  3, 180,   0,   4,   5,   5,   5,   5,   5]], dtype=torch.int32)
    source len excluding pad: tensor([4, 4, 4], dtype=torch.int32)
    label: tensor([
            [ 37, 100,  58, 160,   0,   4,   5,   5,   5],
            [  6,   2,   4,   5,   5,   5,   5,   5,   5],
            [180,   0,   4,   5,   5,   5,   5,   5,   5]], dtype=torch.int32)


.. code-block:: python

    @d2l.add_to_class(MTFraEng)  #@save
    def build(self, src_sentences, tgt_sentences):
        raw_text = '\n'.join([src + '\t' + tgt for src, tgt in zip(
            src_sentences, tgt_sentences)])
        arrays, _, _ = self._build_arrays(
            raw_text, self.src_vocab, self.tgt_vocab)
        return arrays

    src, tgt, _,  _ = data.build(['hi .'], ['salut .'])
    print('source:', data.src_vocab.to_tokens(src[0].type(torch.int32)))
    print('target:', data.tgt_vocab.to_tokens(tgt[0].type(torch.int32)))
    # è¾“å‡º
    source: ['hi', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']
    target: ['<bos>', 'salut', '.', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']




10.5.5. Summary
"""""""""""""""

* åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ï¼Œæœºå™¨ç¿»è¯‘æ˜¯æŒ‡å°†æºè¯­è¨€ä¸­è¡¨ç¤ºæ–‡æœ¬å­—ç¬¦ä¸²çš„åºåˆ—è‡ªåŠ¨æ˜ å°„åˆ°ç›®æ ‡è¯­è¨€ä¸­è¡¨ç¤ºåˆç†ç¿»è¯‘çš„å­—ç¬¦ä¸²çš„ä»»åŠ¡ã€‚
* ä½¿ç”¨å•è¯çº§æ ‡è®°åŒ–ï¼Œè¯æ±‡é‡å°†æ˜æ˜¾å¤§äºä½¿ç”¨å­—ç¬¦çº§æ ‡è®°åŒ–ï¼Œä½†åºåˆ—é•¿åº¦ä¼šçŸ­å¾—å¤šã€‚
* ä¸ºäº†å‡è½»è¯æ±‡é‡è¿‡å¤§çš„å½±å“ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸å¸¸è§çš„æ ‡è®°è§†ä¸ºä¸€äº›â€œæœªçŸ¥â€æ ‡è®°ã€‚
* æˆ‘ä»¬å¯ä»¥æˆªæ–­å’Œå¡«å……æ–‡æœ¬åºåˆ—ï¼Œä»¥ä¾¿æ‰€æœ‰æ–‡æœ¬åºåˆ—éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ä»¥å°æ‰¹é‡åŠ è½½ã€‚
* ç°ä»£å®ç°é€šå¸¸å¯¹å…·æœ‰ç›¸ä¼¼é•¿åº¦çš„åºåˆ—è¿›è¡Œå­˜å‚¨ï¼Œä»¥é¿å…åœ¨å¡«å……ä¸Šæµªè´¹è¿‡å¤šçš„è®¡ç®—ã€‚





10.6. The Encoderâ€“Decoder Architecture
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* åœ¨ä¸€èˆ¬çš„åºåˆ—åˆ°åºåˆ—é—®é¢˜ä¸­ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œè¾“å…¥å’Œè¾“å‡ºçš„é•¿åº¦ä¸åŒä¸”æœªå¯¹é½ã€‚
* å¤„ç†æ­¤ç±»æ•°æ®çš„æ ‡å‡†æ–¹æ³•æ˜¯è®¾è®¡ä¸€ä¸ªç¼–ç å™¨-è§£ç å™¨æ¶æ„
* è¯¥æ¶æ„ç”±ä¸¤ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼š å°†å¯å˜é•¿åº¦åºåˆ—ä½œä¸ºè¾“å…¥çš„ç¼–ç å™¨ï¼Œä»¥åŠè§£ç å™¨å……å½“æ¡ä»¶è¯­è¨€æ¨¡å‹ï¼Œæ¥æ”¶ç¼–ç è¾“å…¥å’Œç›®æ ‡åºåˆ—çš„å·¦ä¾§ä¸Šä¸‹æ–‡ï¼Œå¹¶é¢„æµ‹ç›®æ ‡åºåˆ—ä¸­çš„åç»­æ ‡è®°ã€‚
* ç›®æ ‡ï¼šé€šè¿‡å°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºå›ºå®šå½¢çŠ¶çš„çŠ¶æ€ï¼Œå†è§£ç ä¸ºè¾“å‡ºåºåˆ—ï¼Œè§£å†³è¾“å…¥è¾“å‡ºä¸å¯¹é½çš„é—®é¢˜ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/P6Mywo.png

    Fig. 10.6.1 The encoderâ€“decoder architecture.







10.6.1. Encoder
"""""""""""""""

.. code-block:: python

    class Encoder(nn.Module):  #@save
        """The base encoder interface for the encoder--decoder architecture."""
        def __init__(self):
            super().__init__()

        # Later there can be additional arguments (e.g., length excluding padding)
        def forward(self, X, *args):
            raise NotImplementedError

* Encoder æ¥å£
    * åŠŸèƒ½ï¼šæ¥æ”¶è¾“å…¥åºåˆ—ï¼ˆXï¼‰ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå›ºå®šå½¢çŠ¶çš„çŠ¶æ€ã€‚
    * æ–¹æ³•ï¼šforward æˆ– callï¼Œç”±ç»§æ‰¿ç±»å®ç°å…·ä½“é€»è¾‘ã€‚



10.6.2. Decoder
"""""""""""""""

.. code-block:: python

    class Decoder(nn.Module):  #@save
        """The base decoder interface for the encoder--decoder architecture."""
        def __init__(self):
            super().__init__()

        # Later there can be additional arguments (e.g., length excluding padding)
        def init_state(self, enc_all_outputs, *args):
            raise NotImplementedError

        def forward(self, X, state):
            raise NotImplementedError

* Decoder æ¥å£
    * åŠŸèƒ½ï¼š
        * åˆå§‹åŒ–çŠ¶æ€ï¼šå°†ç¼–ç å™¨çš„è¾“å‡ºè½¬æ¢ä¸ºè§£ç å™¨çš„åˆå§‹çŠ¶æ€ã€‚
        * åºåˆ—ç”Ÿæˆï¼šåŸºäºåˆå§‹çŠ¶æ€å’Œå½“å‰è¾“å…¥ï¼ˆå¦‚ä¸Šä¸€æ­¥ç”Ÿæˆçš„è¯ï¼‰ï¼Œé€æ­¥ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚
    * æ–¹æ³•ï¼š
        * init_stateï¼šåˆå§‹åŒ–çŠ¶æ€ã€‚
        * forward æˆ– callï¼šå¤„ç†è¾“å…¥å¹¶ç”Ÿæˆè¾“å‡ºã€‚



10.6.3. Putting the Encoder and Decoder Together
""""""""""""""""""""""""""""""""""""""""""""""""

.. code-block:: python

    class EncoderDecoder(d2l.Classifier):  #@save
        """The base class for the encoder--decoder architecture."""
        def __init__(self, encoder, decoder):
            super().__init__()
            self.encoder = encoder
            self.decoder = decoder

        def forward(self, enc_X, dec_X, *args):
            enc_all_outputs = self.encoder(enc_X, *args)
            dec_state = self.decoder.init_state(enc_all_outputs, *args)
            # Return decoder output only
            return self.decoder(dec_X, dec_state)[0]


10.6.4. Summary
"""""""""""""""

* ç¼–ç å™¨-è§£ç å™¨æ¶æ„å¯ä»¥å¤„ç†ç”±å¯å˜é•¿åº¦åºåˆ—ç»„æˆçš„è¾“å…¥å’Œè¾“å‡ºï¼Œå› æ­¤é€‚ç”¨äºåºåˆ—åˆ°åºåˆ—çš„é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€‚
* ç¼–ç å™¨å°†å¯å˜é•¿åº¦åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå…·æœ‰å›ºå®šå½¢çŠ¶çš„çŠ¶æ€ã€‚è§£ç å™¨å°†å›ºå®šå½¢çŠ¶çš„ç¼–ç çŠ¶æ€æ˜ å°„åˆ°å¯å˜é•¿åº¦åºåˆ—ã€‚


* æ¶æ„çš„ä¼˜ç‚¹
    * é€‚åº”æ€§å¼ºï¼šé€‚ç”¨äºä»»æ„é•¿åº¦çš„è¾“å…¥è¾“å‡ºåºåˆ—ã€‚
    * åŸºç¡€æ€§ï¼šæ˜¯åç»­å¤æ‚åºåˆ—æ¨¡å‹ï¼ˆå¦‚åŸºäº RNN çš„åºåˆ—æ¨¡å‹ï¼‰çš„åŸºç¡€ã€‚


10.7. Sequence-to-Sequence Learning for Machine Translation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºç¼–ç å™¨-è§£ç å™¨æ¶æ„åœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­çš„åº”ç”¨ï¼Œå…¶ä¸­ç¼–ç å™¨å’Œè§£ç å™¨å‡å®ç°ä¸º RNN


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/fkaE89.png

    Fig. 10.7.1 Sequence-to-sequence learning with an RNN encoder and an RNN decoder.


* ç‰¹ç‚¹ï¼šè¾“å…¥å’Œè¾“å‡ºæ˜¯é•¿åº¦å¯å˜çš„éå¯¹é½åºåˆ—ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ã€‚
* æ¨¡å‹ç»“æ„ï¼š
    * ç¼–ç å™¨ï¼ˆEncoderï¼‰ï¼šRNNï¼ˆå¦‚GRUï¼‰å°†å¯å˜é•¿åº¦çš„è¾“å…¥åºåˆ—å‹ç¼©ä¸ºå›ºå®šå½¢çŠ¶çš„éšè—çŠ¶æ€ï¼Œç§°ä¸ºä¸Šä¸‹æ–‡å˜é‡ï¼ˆcontext variableï¼‰ã€‚
    * è§£ç å™¨ï¼ˆDecoderï¼‰ï¼šRNNä»ç¼–ç å™¨ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å˜é‡å’Œå…ˆå‰è¾“å‡ºçš„ç›®æ ‡åºåˆ—ä¸­ï¼Œé€æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªè¾“å‡ºæ ‡è®°ã€‚
    * æ³¨æ„æœºåˆ¶çš„é¢„å‘Šï¼šæœªæ¥ç« èŠ‚ä¼šå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥é¿å…å°†æ•´ä¸ªè¾“å…¥å‹ç¼©ä¸ºå›ºå®šé•¿åº¦ï¼Œå¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚



10.7.1. Teacher Forcing
"""""""""""""""""""""""

* è®­ç»ƒæ–¹æ³•ï¼šTeacher Forcing
    * è§£ç å™¨çš„è¾“å…¥ä¸ºçœŸå®ç›®æ ‡åºåˆ—ï¼ˆå³â€œground truthâ€ï¼‰çš„åç§»ç‰ˆæœ¬ï¼Œä¾‹å¦‚ï¼š<bos>ã€"Ils"ã€"regardent"ã€"." å¯¹åº” "Ils"ã€"regardent"ã€"."ã€<eos>ã€‚
    * é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè§£ç å™¨å§‹ç»ˆä»¥æ­£ç¡®çš„å…ˆå‰æ ‡è®°ä½œä¸ºè¾“å…¥ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¿«æ”¶æ•›ã€‚



10.7.2. Encoder
"""""""""""""""

* æ¥å—è¾“å…¥åºåˆ—ï¼Œå°†æ¯ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾å‘é‡( :math:`\mathbf{x}_t` )å’Œå‰ä¸€éšè—çŠ¶æ€( :math:`\mathbf{h}_{t-1}` ) è½¬æ¢ä¸ºå½“å‰éšè—çŠ¶æ€( :math:`\mathbf{h}_{t}` )ã€‚
    * å³: :math:`\mathbf{h}_t = f(\mathbf{x}_t, \mathbf{h}_{t-1})`
* ä¸Šä¸‹æ–‡å˜é‡å¯ä»¥æ˜¯æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œæˆ–è€…ç»è¿‡è‡ªå®šä¹‰å‡½æ•°å¤„ç†çš„æ‰€æœ‰éšè—çŠ¶æ€ã€‚
    * ç¼–ç å™¨é€šè¿‡è‡ªå®šä¹‰å‡½æ•° q å°†æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€è½¬æ¢ä¸ºä¸Šä¸‹æ–‡å˜é‡
    * :math:`\mathbf{c}=q(\mathbf{h}_1, \cdot, \mathbf{h}_T)`
* å¯ä»¥ä½¿ç”¨å•å‘æˆ–åŒå‘RNNï¼ŒåŒå‘RNNå¯ä»¥æ•è·æ›´å¤šåºåˆ—ä¿¡æ¯ã€‚


.. code-block:: python

    def init_seq2seq(module):  #@save
        """Initialize weights for sequence-to-sequence learning."""
        if type(module) == nn.Linear:
             nn.init.xavier_uniform_(module.weight)
        if type(module) == nn.GRU:
            for param in module._flat_weights_names:
                if "weight" in param:
                    nn.init.xavier_uniform_(module._parameters[param])

    class Seq2SeqEncoder(d2l.Encoder):  #@save
        """The RNN encoder for sequence-to-sequence learning."""
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = d2l.GRU(embed_size, num_hiddens, num_layers, dropout)
            self.apply(init_seq2seq)

        def forward(self, X, *args):
            # X shape: (batch_size, num_steps)
            embs = self.embedding(X.t().type(torch.int64))
            # embs shape: (num_steps, batch_size, embed_size)
            outputs, state = self.rnn(embs)
            # outputs shape: (num_steps, batch_size, num_hiddens)
            # state shape: (num_layers, batch_size, num_hiddens)
            return outputs, state

å®ä¾‹åŒ–ä¸€ä¸ªä¸¤å±‚ GRU ç¼–ç å™¨ï¼Œå…¶éšè—å•å…ƒæ•°ä¸º 16ã€‚ç»™å®šä¸€ä¸ªå°æ‰¹é‡åºåˆ—è¾“å…¥ X ï¼ˆæ‰¹é‡å¤§å° =4 ï¼›æ—¶é—´æ­¥æ•° =9 ï¼‰æ˜¯ä¸€ä¸ªå½¢çŠ¶å¼ é‡ï¼ˆæ—¶é—´æ­¥æ•°ã€æ‰¹é‡å¤§å°ã€éšè—å±‚æ•°ï¼‰å•ä½ï¼‰::

    vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2
    batch_size, num_steps = 4, 9
    encoder = Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
    X = torch.zeros((batch_size, num_steps))
    enc_outputs, enc_state = encoder(X)
    d2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))



10.7.3. Decoder
"""""""""""""""

* è¾“å…¥åŒ…æ‹¬å…ˆå‰æ—¶é—´æ­¥çš„è¾“å‡ºåºåˆ—( :math:`y_1, y_2, \cdot, y_{T'}` )ã€ä¸Šä¸‹æ–‡å˜é‡( :math:`\mathbf{c}` )å’Œå‰ä¸€éšè—çŠ¶æ€( :math:`\mathbf{s}_{t'}` )ã€‚
    * ç”¨ä¸€ä¸ªå‡½æ•° g() æ¥è¡¨è¾¾è§£ç å™¨éšè—å±‚çš„å˜æ¢
    * :math:`\mathbf{s}_{t'} = g(y_{t'-1}, \mathbf{c}, \mathbf{s}_{t'-1})`
* è®¡ç®—å‡ºæ–°çš„éšè—çŠ¶æ€åï¼Œé€šè¿‡softmaxè®¡ç®—è¾“å‡ºæ ‡è®° :math:`t'+1` çš„é¢„æµ‹åˆ†å¸ƒ :math:`p(y_{t^{\prime}+1} \mid y_1, \ldots, y_{t^\prime}, \mathbf{c})` ã€‚
* é€šå¸¸ï¼Œè§£ç å™¨çš„åˆå§‹éšè—çŠ¶æ€ç”±ç¼–ç å™¨çš„æœ€ç»ˆéšè—çŠ¶æ€åˆå§‹åŒ–ã€‚

.. code-block:: python

    class Seq2SeqDecoder(d2l.Decoder):
        """The RNN decoder for sequence to sequence learning."""
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                     dropout=0):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = d2l.GRU(embed_size+num_hiddens, num_hiddens, num_layers, dropout)
            self.dense = nn.LazyLinear(vocab_size)
            self.apply(init_seq2seq)

        def init_state(self, enc_all_outputs, *args):
            return enc_all_outputs

        def forward(self, X, state):
            # X shape: (batch_size, num_steps)
            # embs shape: (num_steps, batch_size, embed_size)
            embs = self.embedding(X.t().type(torch.int32))
            enc_output, hidden_state = state
            # context shape: (batch_size, num_hiddens)
            context = enc_output[-1]
            # Broadcast context to (num_steps, batch_size, num_hiddens)
            context = context.repeat(embs.shape[0], 1, 1)
            # Concat at the feature dimension
            embs_and_context = torch.cat((embs, context), -1)
            outputs, hidden_state = self.rnn(embs_and_context, hidden_state)
            outputs = self.dense(outputs).swapaxes(0, 1)
            # outputs shape: (batch_size, num_steps, vocab_size)
            # hidden_state shape: (num_layers, batch_size, num_hiddens)
            return outputs, [enc_output, hidden_state]

è§£ç å™¨çš„è¾“å‡ºå½¢çŠ¶å˜ä¸ºï¼ˆæ‰¹é‡å¤§å°ã€æ—¶é—´æ­¥æ•°ã€è¯æ±‡å¤§å°ï¼‰ï¼Œå…¶ä¸­å¼ é‡çš„æœ€ç»ˆç»´åº¦å­˜å‚¨é¢„æµ‹çš„æ ‡è®°åˆ†å¸ƒ::

    decoder = Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)
    state = decoder.init_state(encoder(X))
    dec_outputs, state = decoder(X, state)
    d2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))
    d2l.check_shape(state[1], (num_layers, batch_size, num_hiddens))


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/gsE5N1.png

    Fig. 10.7.2 Layers in an RNN encoderâ€“decoder model.



10.7.4. Encoderâ€“Decoder for Sequence-to-Sequence Learning
"""""""""""""""""""""""""""""""""""""""""""""""""""""""""

.. code-block:: python

    class Seq2Seq(d2l.EncoderDecoder):  #@save
        """The RNN encoder--decoder for sequence to sequence learning."""
        def __init__(self, encoder, decoder, tgt_pad, lr):
            super().__init__(encoder, decoder)
            self.save_hyperparameters()

        def validation_step(self, batch):
            Y_hat = self(*batch[:-1])
            self.plot('loss', self.loss(Y_hat, batch[-1]), train=False)

        def configure_optimizers(self):
            # Adam optimizer is used here
            return torch.optim.Adam(self.parameters(), lr=self.lr)



10.7.5. Loss Function with Masking
""""""""""""""""""""""""""""""""""

.. code-block:: python

    @d2l.add_to_class(Seq2Seq)
    def loss(self, Y_hat, Y):
        l = super(Seq2Seq, self).loss(Y_hat, Y, averaged=False)
        mask = (Y.reshape(-1) != self.tgt_pad).type(torch.float32)
        return (l * mask).sum() / mask.sum()

* ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œæ’é™¤å¡«å……æ ‡è®°ï¼ˆpadding tokensï¼‰çš„è®¡ç®—ä»¥é¿å…å¯¹æ¨¡å‹ä¼˜åŒ–çš„å¹²æ‰°ã€‚
* æ©ç æœºåˆ¶é€šè¿‡å°†æ— å…³ä½ç½®è®¾ç½®ä¸ºé›¶å®ç°ã€‚


10.7.6. Training
""""""""""""""""

.. code-block:: python

    data = d2l.MTFraEng(batch_size=128)
    embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2
    encoder = Seq2SeqEncoder(
        len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
    decoder = Seq2SeqDecoder(
        len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
    model = Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'],
                    lr=0.005)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)

10.7.7. Prediction
""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/zNoJ2d.png

    Fig. 10.7.3 Predicting the output sequence token by token using an RNN encoderâ€“decoder.

* é¢„æµ‹æ–¹æ³•
    * è§£ç å™¨åœ¨æµ‹è¯•æ—¶åŸºäºå·²é¢„æµ‹çš„æ ‡è®°ä½œä¸ºè¾“å…¥é€æ­¥ç”Ÿæˆè¾“å‡ºï¼Œç›´åˆ°é¢„æµ‹åˆ°ç»“æŸæ ‡è®°<eos>ã€‚
    * æµç¨‹å¦‚å›¾æ‰€ç¤ºï¼Œé€ä¸ªæ ‡è®°é¢„æµ‹ï¼Œç›´åˆ°åºåˆ—ç»“æŸã€‚

.. code-block:: python

    @d2l.add_to_class(d2l.EncoderDecoder)  #@save
    def predict_step(self, batch, device, num_steps, save_attention_weights=False):
        batch = [a.to(device) for a in batch]
        src, tgt, src_valid_len, _ = batch
        enc_all_outputs = self.encoder(src, src_valid_len)
        dec_state = self.decoder.init_state(enc_all_outputs, src_valid_len)
        outputs, attention_weights = [tgt[:, (0)].unsqueeze(1), ], []
        for _ in range(num_steps):
            Y, dec_state = self.decoder(outputs[-1], dec_state)
            outputs.append(Y.argmax(2))
            # Save attention weights (to be covered later)
            if save_attention_weights:
                attention_weights.append(self.decoder.attention_weights)
        return torch.cat(outputs[1:], 1), attention_weights


10.7.8. Evaluation of Predicted Sequences
"""""""""""""""""""""""""""""""""""""""""

* BLEU: Bilingual Evaluation Understudy
    * æµ‹é‡é¢„æµ‹åºåˆ—ä¸ç›®æ ‡åºåˆ—ä¹‹é—´çš„åŒ¹é…åº¦ï¼ŒåŸºäº `n-gram` çš„ç²¾ç¡®åº¦è®¡ç®—ã€‚
    * æƒé‡æœºåˆ¶ï¼š
        * åŒ¹é…æ›´é•¿çš„ `n-gram` èµ‹äºˆæ›´é«˜æƒé‡ã€‚
        * çŸ­åºåˆ—æƒ©ç½šé¡¹é˜²æ­¢æ¨¡å‹ç”Ÿæˆè¿‡çŸ­çš„ç»“æœã€‚


.. code-block:: python

    def bleu(pred_seq, label_seq, k):  #@save
        """Compute the BLEU."""
        pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')
        len_pred, len_label = len(pred_tokens), len(label_tokens)
        score = math.exp(min(0, 1 - len_label / len_pred))
        for n in range(1, min(k, len_pred) + 1):
            num_matches, label_subs = 0, collections.defaultdict(int)
            for i in range(len_label - n + 1):
                label_subs[' '.join(label_tokens[i: i + n])] += 1
            for i in range(len_pred - n + 1):
                if label_subs[' '.join(pred_tokens[i: i + n])] > 0:
                    num_matches += 1
                    label_subs[' '.join(pred_tokens[i: i + n])] -= 1
            score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))
        return score

ç¤ºä¾‹::

    engs = ['go .', 'i lost .', 'he\'s calm .', 'i\'m home .']
    fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
    preds, _ = model.predict_step(
        data.build(engs, fras), d2l.try_gpu(), data.num_steps)
    for en, fr, p in zip(engs, fras, preds):
        translation = []
        for token in data.tgt_vocab.to_tokens(p):
            if token == '<eos>':
                break
            translation.append(token)
        print(f'{en} => {translation}, bleu,'
              f'{bleu(" ".join(translation), fr, k=2):.3f}')

    # è¾“å‡º
    go . => ['va', '!'], bleu,1.000
    i lost . => ["j'ai", 'perdu', '.'], bleu,1.000
    he's calm . => ['elle', 'court', '.'], bleu,0.000
    i'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000



10.8. Beam Search
^^^^^^^^^^^^^^^^^

* åœ¨åºåˆ—ç”Ÿæˆä»»åŠ¡ä¸­ä½¿ç”¨çš„ä¸‰ç§æœç´¢ç­–ç•¥ï¼šè´ªå©ªæœç´¢ï¼ˆGreedy Searchï¼‰ã€ç©·ä¸¾æœç´¢ï¼ˆExhaustive Searchï¼‰å’ŒæŸæœç´¢ï¼ˆBeam Searchï¼‰ã€‚
* å‰é¢ç« èŠ‚åªæåˆ°äº†è´ªå©ªç­–ç•¥

10.8.1. Greedy Search
"""""""""""""""""""""

.. math::

    y_{t'} = \operatorname*{argmax}_{y \in \mathcal{Y}} P(y \mid y_1, \ldots, y_{t'-1}, \mathbf{c})

* åœ¨ä»»ä½•æ—¶é—´æ­¥éª¤ t' ï¼Œæˆ‘ä»¬åªéœ€ä» y ä¸­é€‰æ‹©æ¡ä»¶æ¦‚ç‡æœ€é«˜çš„æ ‡è®°

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/bOIFny.png

    Fig. 10.8.1 At each time step, greedy search selects the token with the highest conditional probability.

* åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè´ªå©ªæœç´¢éƒ½ä¼šé€‰æ‹©æ¡ä»¶æ¦‚ç‡æœ€é«˜çš„æ ‡è®°ã€‚å› æ­¤ï¼Œå°†é¢„æµ‹è¾“å‡ºåºåˆ—â€œAâ€ã€â€œBâ€ã€â€œCâ€å’Œâ€œâ€ï¼ˆå›¾10.8.1ï¼‰ã€‚è¯¥è¾“å‡ºåºåˆ—çš„æ¡ä»¶æ¦‚ç‡ä¸º :math:`0.5\times0.4\times0.4\times0.6 = 0.048`

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/88RudG.png

    Fig. 10.8.2 The four numbers under each time step represent the conditional probabilities of generating â€œAâ€, â€œBâ€, â€œCâ€, and â€œ<eos>â€ at that time step. At time step 2, the token â€œCâ€, which has the second highest conditional probability, is selected.

* è¾“å‡ºåºåˆ—â€œAâ€ã€â€œCâ€ã€â€œBâ€ã€â€œâ€çš„æ¡ä»¶æ¦‚ç‡ä¸º :math:`0.5\times0.3 \times0.6\times0.6=0.054` ï¼Œå¤§äºå›¾ 10.8.1 ä¸­çš„è´ªå¿ƒæœç´¢ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œè´ªå¿ƒæœç´¢å¾—åˆ°çš„è¾“å‡ºåºåˆ—â€œAâ€ã€â€œBâ€ã€â€œCâ€å’Œâ€œâ€ä¸æ˜¯æœ€ä¼˜çš„ã€‚



10.8.2. Exhaustive Search
"""""""""""""""""""""""""

* æšä¸¾æ‰€æœ‰å¯èƒ½çš„è¾“å‡ºåºåˆ—åŠå…¶æ¡ä»¶æ¦‚ç‡ï¼Œç„¶åè¾“å‡ºé¢„æµ‹æ¦‚ç‡æœ€é«˜çš„åºåˆ—ã€‚
* å®ƒä¼šå¸¦æ¥ä»¤äººæœ›è€Œå´æ­¥çš„è®¡ç®—æˆæœ¬ :math:`\mathcal{O}(\left|\mathcal{Y}\right|^{T'})` ï¼Œåºåˆ—é•¿åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œå¹¶ä¸”è¯æ±‡é‡å¤§å°ç»™å®šäº†å·¨å¤§çš„åŸºç¡€ã€‚
* ä¾‹å¦‚ï¼Œå½“ :math:`|\mathcal{Y}|=10000` å’Œ :math:`T'=10` ä¸å®é™…åº”ç”¨ä¸­çš„æ•°å­—ç›¸æ¯”éƒ½è¾ƒå°æ—¶ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼° :math:`10000^10 = 10^40` åºåˆ—ï¼Œè¿™å·²ç»è¶…å‡ºäº†ä»»ä½•å¯é¢„è§çš„è®¡ç®—æœºçš„èƒ½åŠ›ã€‚
* å¦ä¸€æ–¹é¢ï¼Œè´ªå©ªæœç´¢çš„è®¡ç®—æˆæœ¬æ˜¯ :math:`\mathcal{O}(\left|\mathcal{Y}\right|{T'})` ï¼šéå¸¸ä¾¿å®œï¼Œä½†è¿œéæœ€ä¼˜ã€‚
* ä¾‹å¦‚ï¼Œå½“  :math:`|\mathcal{Y}|=10000` å’Œ :math:`T'=10` æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦è¯„ä¼° :math:`10000 \times 10 = 10^5` åºåˆ—ã€‚




10.8.3. Beam Search
"""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/0Juc3f.png

    Fig. 10.8.3 The process of beam search (beam size =2; maximum length of an output sequence =3). The candidate output sequences are A, C, AB, CE, ABD, and CED.

* é›†æŸæœç´¢çš„è®¡ç®—æˆæœ¬ä¸º :math:`\mathcal{O}(k\left|\mathcal{Y}\right|T')` ã€‚è¿™ä¸ªç»“æœä»‹äºè´ªå©ªæœç´¢å’Œç©·ä¸¾æœç´¢ä¹‹é—´ã€‚è´ªå©ªæœç´¢å¯ä»¥è¢«è§†ä¸ºå½“æ³¢æŸå¤§å°è®¾ç½®ä¸º 1 æ—¶å‡ºç°çš„æ³¢æŸæœç´¢çš„ç‰¹æ®Šæƒ…å†µã€‚




11. Attention Mechanisms and Transformers
-----------------------------------------


11.1. Queries, Keys, and Values
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* ã€1. ä¼ ç»Ÿç½‘ç»œçš„å±€é™æ€§ï¼ˆFixed Input Sizeï¼‰ã€‘æœ€åˆçš„ç¥ç»ç½‘ç»œï¼ˆå¦‚CNNã€RNNç­‰ï¼‰ä¾èµ–äºè¾“å…¥çš„å¤§å°æ˜¯å›ºå®šçš„ï¼Œæ¯”å¦‚ImageNetä¸­çš„å›¾åƒå¤§å°æ˜¯ :math:`224 \times 224` ã€‚å³ä½¿åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼ŒRNNçš„è¾“å…¥å¤§å°ä¹Ÿæ˜¯å›ºå®šçš„ã€‚è¿™ç§æ–¹æ³•åœ¨é¢å¯¹é•¿åº¦å¯å˜çš„è¾“å…¥æ—¶ï¼ˆå¦‚æ–‡æœ¬ç¿»è¯‘æˆ–å˜é•¿çš„åºåˆ—ï¼‰ä¼šé‡åˆ°å›°éš¾ã€‚ç‰¹åˆ«æ˜¯å¯¹äºé•¿åºåˆ—ï¼Œç½‘ç»œéœ€è¦â€œè®°ä½â€å·²ç”Ÿæˆæˆ–å·²æŸ¥çœ‹çš„ä¿¡æ¯ï¼Œè¿™å¯¹æ¨¡å‹çš„å¤„ç†èƒ½åŠ›æå‡ºäº†è¾ƒé«˜è¦æ±‚ã€‚
* ã€2. æ•°æ®åº“ç±»æ¯”ï¼ˆDatabases and Key-Value Pairsï¼‰ã€‘æ•°æ®åº“é€šå¸¸åŒ…å«ç”± ``é”®ï¼ˆkï¼‰`` å’Œ ``å€¼ï¼ˆvï¼‰`` ç»„æˆçš„é”®å€¼å¯¹ã€‚
    * ä¾‹å¦‚ï¼Œå‡è®¾æœ‰ä¸€ç»„å§“æ°å’Œåå­—çš„é”®å€¼å¯¹ã€‚å¦‚æœæˆ‘ä»¬æŸ¥è¯¢æŸä¸ªç‰¹å®šçš„é”®ï¼ˆå¦‚â€œLiâ€ï¼‰ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°ç›¸åº”çš„å€¼ï¼ˆâ€œMuâ€ï¼‰ã€‚è¿™é‡Œçš„ ``æŸ¥è¯¢ï¼ˆqï¼‰`` å¯ä»¥è¿”å›ä¸€ä¸ªç»“æœï¼Œä¹Ÿå¯ä»¥è¿”å›å¤šä¸ªè¿‘ä¼¼ç»“æœï¼Œå…·ä½“å–å†³äºæ•°æ®åº“ä¸­çš„å†…å®¹ã€‚
    * é‡è¦çš„æ¦‚å¿µï¼š
        * æŸ¥è¯¢å¯ä»¥ä¸ä¾èµ–äºæ•°æ®åº“çš„å¤§å°ï¼Œè¿™è¡¨æ˜æ·±åº¦å­¦ä¹ æ¨¡å‹å¯ä»¥æ‰©å±•åˆ°è¾ƒå¤§çš„æ•°æ®é›†ã€‚
        * æŸ¥è¯¢å¯ä»¥å¾—åˆ°ä¸åŒçš„ç­”æ¡ˆï¼Œè¿™ä¸æ·±åº¦å­¦ä¹ ä¸­çš„æ¨¡å‹é¢„æµ‹ç±»ä¼¼ã€‚
        * æ“ä½œä¸éœ€è¦å¯¹æ•°æ®åº“è¿›è¡Œå¤æ‚çš„å‹ç¼©æˆ–ç®€åŒ–ã€‚

* ã€3. å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ã€‘éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œæ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·è¢«å¼•å…¥ã€‚å®ƒæ¨¡æ‹Ÿäº†æŸ¥è¯¢ä¸ä¸€ç»„é”®å€¼å¯¹ä¹‹é—´çš„å…³ç³»ï¼Œå…è®¸æ¨¡å‹åœ¨è¿›è¡Œå†³ç­–æ—¶é‡ç‚¹å…³æ³¨é‡è¦çš„é”®ï¼ˆ ``k`` ï¼‰åŠå…¶å¯¹åº”çš„å€¼ï¼ˆ ``v`` ï¼‰ã€‚
* å…·ä½“æ¥è¯´ï¼Œæ³¨æ„åŠ›æœºåˆ¶å®šä¹‰å¦‚ä¸‹ï¼š

.. math::

    \textrm{Attention}(\mathbf{q}, \mathcal{D}) \stackrel{\textrm{def}}{=} \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i

* è¿™é‡Œçš„ :math:`\alpha(\mathbf{q}, \mathbf{k}_i)` æ˜¯æ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºæŸ¥è¯¢ä¸æ¯ä¸ªé”®çš„ç›¸å…³æ€§ï¼Œè€Œè¿™äº›æƒé‡å†³å®šäº†å¯¹åº”å€¼çš„é‡è¦æ€§ã€‚
* ã€4. ä¸åŒçš„æƒé‡åˆ†é…æ–¹æ³•ï¼ˆWeighting Mechanismsï¼‰ã€‘å‡ ç§å¸¸è§çš„æƒé‡åˆ†é…æ–¹å¼ï¼š
    * éè´Ÿæƒé‡ï¼šæƒé‡ :math:`\alpha(\mathbf{q}, \mathbf{k}_i)` æ˜¯éè´Ÿçš„ï¼Œè¿™æ„å‘³ç€ç»“æœæ˜¯åœ¨å€¼çš„å‡¸é”¥ä¸­ã€‚
    * å½’ä¸€åŒ–æƒé‡ï¼šæƒé‡ :math:`\alpha(\mathbf{q}, \mathbf{k}_i)` å½¢æˆä¸€ä¸ªå‡¸ç»„åˆï¼Œå³æ‰€æœ‰æƒé‡ä¹‹å’Œä¸º1ã€‚è¿™æ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€å¸¸è§çš„è®¾ç½®ã€‚
    * ç²¾ç¡®åŒ¹é…ï¼šåªæœ‰ä¸€ä¸ªæƒé‡ä¸º1ï¼Œå…¶ä½™ä¸º0ï¼Œè¿™ç±»ä¼¼äºä¼ ç»Ÿçš„æ•°æ®åº“æŸ¥è¯¢ã€‚
    * å¹³å‡æ± åŒ–ï¼šæ‰€æœ‰æƒé‡ç›¸ç­‰ï¼Œè¿™ç›¸å½“äºå¯¹æ•´ä¸ªæ•°æ®åº“è¿›è¡Œå¹³å‡æ± åŒ–ã€‚


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/GdvkVn.png

    Fig. 11.1.1 The attention mechanism computes a linear combination over values :math:`\mathbf{v}_i` via attention pooling, where weights are derived according to the compatibility between a query :math:`\mathbf{q}` and keys :math:`\mathbf{k}_i`





11.1.1. Visualization
"""""""""""""""""""""

* æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å…¶ç›´è§‚æ€§ï¼Œå°¤å…¶æ˜¯åœ¨æƒé‡ä¸ºéè´Ÿå¹¶ä¸”æ€»å’Œä¸º1æ—¶ã€‚
* é€šè¿‡è§‚å¯Ÿæ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ç†è§£å“ªäº›éƒ¨åˆ†å¯¹æ¨¡å‹çš„é¢„æµ‹æœ€ä¸ºé‡è¦ã€‚

.. code-block:: python

    #@save
    def show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5),
                      cmap='Reds'):
        """Show heatmaps of matrices."""
        d2l.use_svg_display()
        num_rows, num_cols, _, _ = matrices.shape
        fig, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize,
                                     sharex=True, sharey=True, squeeze=False)
        for i, (row_axes, row_matrices) in enumerate(zip(axes, matrices)):
            for j, (ax, matrix) in enumerate(zip(row_axes, row_matrices)):
                pcm = ax.imshow(matrix.detach().numpy(), cmap=cmap)
                if i == num_rows - 1:
                    ax.set_xlabel(xlabel)
                if j == 0:
                    ax.set_ylabel(ylabel)
                if titles:
                    ax.set_title(titles[j])
        fig.colorbar(pcm, ax=axes, shrink=0.6);

ä½¿ç”¨::

    attention_weights = torch.eye(10).reshape((1, 1, 10, 10))
    show_heatmaps(attention_weights, xlabel='Keys', ylabel='Queries')

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/8nu6AD.png


11.1.2. Summary
"""""""""""""""

* åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„è®¨è®ºéå¸¸æŠ½è±¡ï¼Œåªæ˜¯æè¿°äº†ä¸€ç§æ± åŒ–æ•°æ®çš„æ–¹æ³•ã€‚
* æˆ‘ä»¬è¿˜æ²¡æœ‰è§£é‡Šè¿™äº›ç¥ç§˜çš„æŸ¥è¯¢ã€é”®å’Œå€¼å¯èƒ½ä»ä½•è€Œæ¥ã€‚
* ä¸€äº›ç›´è§‰å¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼šä¾‹å¦‚ï¼Œåœ¨å›å½’è®¾ç½®ä¸­ï¼ŒæŸ¥è¯¢å¯èƒ½å¯¹åº”äºåº”æ‰§è¡Œå›å½’çš„ä½ç½®ã€‚é”®æ˜¯è§‚å¯Ÿè¿‡å»æ•°æ®çš„ä½ç½®ï¼Œå€¼æ˜¯ï¼ˆå›å½’ï¼‰å€¼æœ¬èº«ã€‚


11.2. Attention Pooling by Similarity
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



* Nadarayaâ€“Watson estimatorsçš„æ ¸å¿ƒä¾èµ–äºä¸€äº›å°†æŸ¥è¯¢ **q** ä¸é”® **k** ç›¸å…³è”çš„ç›¸ä¼¼æ€§å†…æ ¸ :math:`\alpha(\mathbf{q}, \mathbf{k})` 

* ä¸€äº›å¸¸è§çš„å†…æ ¸ï¼š

.. math::

    \begin{aligned}
    \alpha(\mathbf{q}, \mathbf{k}) & = \exp\left(-\frac{1}{2} \|\mathbf{q} - \mathbf{k}\|^2 \right) && \textrm{Gaussian;} \\
    \alpha(\mathbf{q}, \mathbf{k}) & = 1 \textrm{ if } \|\mathbf{q} - \mathbf{k}\| \leq 1 && \textrm{Boxcar;} \\
    \alpha(\mathbf{q}, \mathbf{k}) & = \mathop{\mathrm{max}}\left(0, 1 - \|\mathbf{q} - \mathbf{k}\|\right) && \textrm{Epanechikov.}
    \end{aligned}


æ ¸å¿ƒç‚¹-fromGPT
""""""""""""""

æ ¸å¿ƒæ¦‚å¿µ
++++++++

* ã€Nadaraya-Watson æ ¸å›å½’ã€‘ï¼šç»Ÿè®¡å­¦æ–¹æ³•ï¼Œç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡æŸ¥è¯¢ç‚¹ä¸è®­ç»ƒæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼ˆæˆ–æ ¸å‡½æ•°ï¼‰æ¥è®¡ç®—é¢„æµ‹å€¼ã€‚
* ã€æ ¸å‡½æ•°ã€‘ï¼šæ ¸å‡½æ•°æ˜¯è®¡ç®—ä¸¤ä¸ªç‚¹ï¼ˆæŸ¥è¯¢ç‚¹å’Œé”®ï¼‰ä¹‹é—´ç›¸ä¼¼åº¦çš„å‡½æ•°ã€‚é«˜æ–¯æ ¸ç»™å‡ºä¸€ä¸ªå¹³æ»‘çš„ã€éšç€è·ç¦»å¢åŠ è€Œè¡°å‡çš„ç›¸ä¼¼åº¦ï¼Œè€Œ Boxcar æ ¸ä»…å¯¹åœ¨æŸä¸ªç‰¹å®šèŒƒå›´å†…çš„ç‚¹èµ‹äºˆéé›¶ç›¸ä¼¼åº¦ï¼ŒEpanechikov æ ¸çš„æ•ˆæœç±»ä¼¼ï¼Œä½†å®ƒçš„æˆªæ–­æ›´åŠ æŸ”å’Œã€‚
* ã€ä¸æ³¨æ„åŠ›æœºåˆ¶çš„è”ç³»ã€‘ï¼šåœ¨æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ï¼Œç‰¹åˆ«æ˜¯æ·±åº¦å­¦ä¹ ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œæ–¹å¼ä¸æ­¤ç›¸ä¼¼ï¼šå®ƒæ ¹æ®ä¸åŒè¾“å…¥å¯¹å½“å‰ä»»åŠ¡çš„ç›¸å…³æ€§èµ‹äºˆä¸åŒçš„æƒé‡ã€‚åœ¨è¿™é‡Œï¼Œ"æŸ¥è¯¢"æ˜¯æˆ‘ä»¬æƒ³è¦è¿›è¡Œé¢„æµ‹çš„ç‚¹ï¼Œ"é”®"æ˜¯è®­ç»ƒæ•°æ®ç‚¹ã€‚æ¯ä¸ªè®­ç»ƒç‚¹çš„æ³¨æ„åŠ›æƒé‡æ˜¯é€šè¿‡æŸ¥è¯¢ç‚¹ä¸è®­ç»ƒç‚¹çš„ç›¸ä¼¼åº¦æ¥è®¡ç®—çš„ï¼Œè¿™ä¸ Nadaraya-Watson å›å½’ä¸­çš„æ ¸å‡½æ•°è®¡ç®—æ–¹å¼ç±»ä¼¼ã€‚

å…³é”®è§è§£
++++++++

* æ— éœ€è®­ç»ƒ: Nadaraya-Watson å›å½’ä¸éœ€è¦åƒä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹é‚£æ ·è¿›è¡Œè®­ç»ƒï¼Œæ¨¡å‹é€šè¿‡è®¡ç®—æŸ¥è¯¢ç‚¹ä¸è®­ç»ƒæ•°æ®ç‚¹çš„ç›¸ä¼¼åº¦æ¥ç›´æ¥åšå‡ºé¢„æµ‹ã€‚è¿™æ˜¯ä¸€ç§ç®€å•çš„ã€éå‚æ•°åŒ–çš„æ–¹æ³•ï¼Œä¸éœ€è¦å­¦ä¹ å‚æ•°çš„å¤æ‚ç¥ç»ç½‘ç»œæ¨¡å‹æœ‰æ‰€ä¸åŒã€‚
* æ ¸å‡½æ•°çš„å¯è°ƒæ€§: æ ¸å‡½æ•°çš„å®½åº¦ï¼ˆå°¤å…¶æ˜¯é«˜æ–¯æ ¸ï¼‰å¯¹ä¼°è®¡å‡½æ•°çš„å¹³æ»‘åº¦æœ‰é‡è¦å½±å“ã€‚æ›´çª„çš„æ ¸å‡½æ•°ä¼šå¯¼è‡´ä¼°è®¡ç»“æœæ›´åŠ æ•æ„Ÿäºå±€éƒ¨å˜åŒ–ï¼Œè€Œæ›´å®½çš„æ ¸å‡½æ•°åˆ™ä¼šå¹³æ»‘æ‰å™ªå£°ã€‚


å…³é”®æ€»ç»“
++++++++

* æ—©æœŸçš„æ³¨æ„åŠ›æœºåˆ¶: Nadaraya-Watson å›å½’æ–¹æ³•æ˜¯ç°ä»£æ³¨æ„åŠ›æœºåˆ¶çš„å‰èº«ä¹‹ä¸€ï¼Œå±•ç¤ºäº†å¦‚ä½•åŸºäºæ•°æ®ç‚¹ä¹‹é—´çš„ç›¸ä¼¼æ€§è¿›è¡ŒåŠ æƒé¢„æµ‹ã€‚
* ç®€å•æœ‰æ•ˆçš„ä¼°è®¡: è¿™ç§æ–¹æ³•æ˜¯ä¸€ç§ç®€å•çš„éå‚æ•°æ–¹æ³•ï¼Œé€‚ç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡ï¼Œå°¤å…¶åœ¨æ•°æ®ä¸°å¯Œæ—¶ç‰¹åˆ«æœ‰æ•ˆã€‚
* æ ¸å‡½æ•°çš„é€‰æ‹©: æ ¸å‡½æ•°çš„é€‰æ‹©ï¼Œå°¤å…¶æ˜¯æ ¸å‡½æ•°å®½åº¦çš„è®¾ç½®ï¼Œèƒ½æ˜¾è‘—å½±å“æ¨¡å‹çš„è¡¨ç°ã€‚è¿™ä¸ç°ä»£æ³¨æ„åŠ›æœºåˆ¶å¯¹æ³¨æ„åŠ›å‡½æ•°ç»“æ„çš„æ•æ„Ÿæ€§ç±»ä¼¼ã€‚

ä¸ºä»€ä¹ˆé‡è¦
++++++++++

* è¿™ä¸€éƒ¨åˆ†éå¸¸é‡è¦ï¼Œå› ä¸ºå®ƒå°†ç»å…¸çš„ç»Ÿè®¡å­¦æ–¹æ³•ä¸ç°ä»£æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶è”ç³»èµ·æ¥ã€‚é€šè¿‡äº†è§£æ³¨æ„åŠ›æœºåˆ¶çš„ç®€å•èµ·æºï¼Œè¯»è€…å¯ä»¥æ›´æ¸…æ¥šåœ°ç†è§£ä¸ºä»€ä¹ˆæ³¨æ„åŠ›æœºåˆ¶åœ¨æ·±åº¦å­¦ä¹ ä¸­å¦‚æ­¤å…³é”®ï¼Œå¹¶èƒ½å¤Ÿç†è§£å…¶æ¼”åŒ–è¿‡ç¨‹ã€‚





11.2.1. Kernels and Data
""""""""""""""""""""""""

æœ¬èŠ‚ä¸­å®šä¹‰çš„æ‰€æœ‰å†…æ ¸ :math:`\alpha(\mathbf{q}, \mathbf{k})` éƒ½æ˜¯å¹³ç§»å’Œæ—‹è½¬ä¸å˜çš„ï¼›ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœæˆ‘ä»¬ä»¥ç›¸åŒçš„æ–¹å¼ç§»åŠ¨å’Œæ—‹è½¬ **k** å’Œ **q** ï¼Œåˆ™ :math:`\alpha` çš„å€¼ä¿æŒä¸å˜ã€‚

.. code-block:: python

    # Define some kernels
    def gaussian(x):
        return torch.exp(-x**2 / 2)

    def boxcar(x):
        return torch.abs(x) < 1.0

    def constant(x):
        return 1.0 + 0 * x

    def epanechikov(x):
        return torch.max(1 - torch.abs(x), torch.zeros_like(x))

    fig, axes = d2l.plt.subplots(1, 4, sharey=True, figsize=(12, 3))

    kernels = (gaussian, boxcar, constant, epanechikov)
    names = ('Gaussian', 'Boxcar', 'Constant', 'Epanechikov')
    x = torch.arange(-2.5, 2.5, 0.1)
    for kernel, name, ax in zip(kernels, names, axes):
        ax.plot(x.detach().numpy(), kernel(x).detach().numpy())
        ax.set_xlabel(name)

    d2l.plt.show()

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/ONIbtE.png



11.2.2. Attention Pooling via Nadarayaâ€“Watson Regression
""""""""""""""""""""""""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/lVruiE.png

    ä¸‰ä¸ªéå¹³å‡¡æ ¸ï¼ˆGaussianã€Boxcar å’Œ Epanechikovï¼‰éƒ½ä¼šäº§ç”Ÿç›¸å½“å¯è¡Œçš„ä¼°è®¡ï¼Œä¸çœŸå®å‡½æ•°ç›¸å·®ä¸è¿œã€‚



11.2.3. Adapting Attention Pooling
""""""""""""""""""""""""""""""""""

* ç”¨ä¸åŒå®½åº¦çš„æ ¸æ›¿æ¢é«˜æ–¯æ ¸ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ :math:`\alpha(\mathbf{q}, \mathbf{k}) = \exp\left(-\frac{1}{2 \sigma^2} \|\mathbf{q} - \mathbf{k}\|^2 \right)` ï¼Œå…¶ä¸­ :math:`\sigma^2` ç¡®å®šå†…æ ¸çš„å®½åº¦

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/FVri41.png

    æ ¸è¶Šçª„ï¼Œä¼°è®¡å°±è¶Šä¸å¹³æ»‘ã€‚åŒæ—¶ï¼Œå®ƒèƒ½æ›´å¥½åœ°é€‚åº”å½“åœ°çš„å˜åŒ–ã€‚



11.2.4. Summary
"""""""""""""""

* å­¦ä¹ è¿™ä¸ªåŠä¸ªå¤šä¸–çºªå†å²çš„æ–¹æ³•çš„åŸå› ï¼šé¦–å…ˆï¼Œå®ƒæ˜¯ç°ä»£æ³¨æ„åŠ›æœºåˆ¶æœ€æ—©çš„å…ˆé©±ä¹‹ä¸€ã€‚å…¶æ¬¡ï¼Œå®ƒéå¸¸é€‚åˆå¯è§†åŒ–ã€‚ç¬¬ä¸‰ï¼ŒåŒæ ·é‡è¦çš„æ˜¯ï¼Œå®ƒå±•ç¤ºäº†æ‰‹å·¥æ³¨æ„åŠ›æœºåˆ¶çš„å±€é™æ€§ã€‚æ›´å¥½çš„ç­–ç•¥æ˜¯é€šè¿‡å­¦ä¹ æŸ¥è¯¢å’Œé”®çš„è¡¨ç¤ºæ¥å­¦ä¹ è¯¥æœºåˆ¶ã€‚
* Nadaraya â€“ Watsonå†…æ ¸å›å½’æ˜¯å½“å‰æ³¨æ„æœºåˆ¶çš„æ—©æœŸèµ·æºã€‚å®ƒå¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œå‡ ä¹æ²¡æœ‰åŸ¹è®­æˆ–è°ƒæ•´ï¼Œæ— è®ºæ˜¯ç”¨äºåˆ†ç±»è¿˜æ˜¯å›å½’ã€‚æ³¨æ„åŠ›çš„é‡é‡æ˜¯æ ¹æ®æŸ¥è¯¢å’Œé’¥åŒ™ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼ˆæˆ–è·ç¦»ï¼‰åˆ†é…çš„ï¼Œå¹¶ä¸”æ ¹æ®æœ‰å¤šå°‘ç›¸ä¼¼è§‚å¯Ÿç»“æœã€‚


11.3. Attention Scoring Functions
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/IS0fQU.png

    Computing the output of attention pooling as a weighted average of values, where weights are computed with the attention scoring function :math:`\mathit{a}` and the softmax operation.


æ ¸å¿ƒç‚¹-fromGPT
""""""""""""""

* æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ï¼ˆattention scoring functionsï¼‰æ˜¯è®¡ç®—æ³¨æ„åŠ›æƒé‡çš„æ ¸å¿ƒå‡½æ•°ï¼Œé€šå¸¸ç”¨äºåŠ æƒæ±‚å’Œè¾“å…¥çš„å€¼ï¼ˆvaluesï¼‰ï¼Œä»è€Œäº§ç”Ÿè¾“å‡ºã€‚
* ä¸¤ç§å¸¸è§çš„è¯„åˆ†å‡½æ•°ï¼šç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆDot Product Attentionï¼‰å’ŒåŠ æ³•æ³¨æ„åŠ›ï¼ˆAdditive Attentionï¼‰


11.3.1. Dot Product Attention
"""""""""""""""""""""""""""""

* é«˜æ–¯å†…æ ¸(Gaussian kernel)çš„æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°

.. math::

    a(\mathbf{q}, \mathbf{k}_i) \\
        = -\frac{1}{2} \|\mathbf{q} - \mathbf{k}_i\|^2  \\
        = \mathbf{q}^\top \mathbf{k}_i -\frac{1}{2} \|\mathbf{k}_i\|^2  -\frac{1}{2} \|\mathbf{q}\|^2.


* ç‚¹ç§¯æ³¨æ„åŠ›æ˜¯æœ€å¸¸ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ä¹‹ä¸€ï¼Œå¹¿æ³›åº”ç”¨äºç°ä»£çš„Transformeræ¶æ„ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡è®¡ç®—æŸ¥è¯¢ï¼ˆqueryï¼‰ä¸é”®ï¼ˆkeyï¼‰ä¹‹é—´çš„ç‚¹ç§¯æ¥è¯„ä¼°å®ƒä»¬çš„ç›¸ä¼¼æ€§ã€‚è¿™ä¸ªç›¸ä¼¼æ€§å¾—åˆ†å†é€šè¿‡softmaxå‡½æ•°å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ³¨æ„åŠ›æƒé‡ã€‚
* ç„¶åï¼Œé€šè¿‡å˜å½¢å…¬å¼ç®€åŒ–å¾—åˆ°ç‚¹ç§¯æ³¨æ„åŠ›çš„å…¬å¼ï¼š

.. math::

    a(\mathbf{q}, \mathbf{k}_i) = \mathbf{q}^\top \mathbf{k}_i / \sqrt{d}


* å…¶ä¸­ï¼Œd æ˜¯æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡çš„ç»´åº¦ã€‚è¿™ä¸ªå…¬å¼è¡¨æ˜ï¼Œç‚¹ç§¯æ³¨æ„åŠ›çš„æ ¸å¿ƒæ˜¯è®¡ç®—æŸ¥è¯¢å’Œé”®çš„æ ‡å‡†åŒ–ç‚¹ç§¯ã€‚
* æœ€ç»ˆï¼Œå¾—åˆ°çš„æ³¨æ„åŠ›æƒé‡ :math:`\alpha(\mathbf{q}, \mathbf{k}_i)` éœ€è¦é€šè¿‡softmaxè¿›è¡Œå½’ä¸€åŒ–ï¼š

.. math::

    \alpha(\mathbf{q}, \mathbf{k}_i) \\
        = \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) \\
        = \frac{\exp(\mathbf{q}^\top \mathbf{k}_i / \sqrt{d})}{\sum_{j=1} \exp(\mathbf{q}^\top \mathbf{k}_j / \sqrt{d})}



* è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå¯ä»¥ä¿è¯æ‰€æœ‰çš„æ³¨æ„åŠ›æƒé‡æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”å®ƒä»¬çš„å’Œä¸º1ï¼Œç¡®ä¿äº†åç»­åŠ æƒæ±‚å’Œæ—¶çš„ç¨³å®šæ€§ã€‚



11.3.2. Convenience Functions
"""""""""""""""""""""""""""""

11.3.2.1. Masked Softmax Operation
++++++++++++++++++++++++++++++++++

* åœ¨åºåˆ—æ¨¡å‹ä¸­ï¼Œå¸¸å¸¸éœ€è¦å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦æ©è”½æ“ä½œæ¥å¿½ç•¥å¡«å……ï¼ˆpaddingï¼‰éƒ¨åˆ†å¯¹æ³¨æ„åŠ›è®¡ç®—çš„å½±å“ã€‚
* ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæ‰¹æ¬¡ä¸­æœ‰ä¸‰ä¸ªå¥å­ï¼Œå…¶ä¸­ä¸€ä¸ªå¥å­è¾ƒçŸ­ï¼ŒåŒ…å«å¡«å……ç¬¦ <blank>ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è¿™äº›å¡«å……éƒ¨åˆ†ä¸ä¼šå½±å“æœ€ç»ˆçš„æ³¨æ„åŠ›è®¡ç®—ã€‚æ©è”½æ“ä½œé€šè¿‡å°†å¡«å……éƒ¨åˆ†çš„æ³¨æ„åŠ›æƒé‡è®¾ç½®ä¸ºéå¸¸å°çš„è´Ÿæ•°ï¼ˆå¦‚ :math:`-10^6` ï¼‰ï¼Œä»è€Œä½¿å¾—è¿™äº›éƒ¨åˆ†åœ¨è®¡ç®—ä¸­ä¸ä¼šèµ·ä½œç”¨ã€‚


11.3.2.2. Batch Matrix Multiplication
+++++++++++++++++++++++++++++++++++++

* åœ¨å®é™…åº”ç”¨ä¸­ï¼Œé€šå¸¸ä¼šå¤„ç†å¤šä¸ªæŸ¥è¯¢ã€é”®å’Œå€¼ï¼Œå› æ­¤éœ€è¦å¯¹å¤šä¸ªçŸ©é˜µè¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆBMMï¼‰ã€‚BMM æ˜¯ä¸€ç§é«˜æ•ˆçš„æ“ä½œï¼Œå¯ä»¥åœ¨ä¸€ä¸ªæ­¥éª¤ä¸­åŒæ—¶è®¡ç®—å¤šä¸ªæŸ¥è¯¢ä¸é”®çš„ç‚¹ç§¯ã€‚
* å…¬å¼å¦‚ä¸‹ï¼š

.. math::

    \textrm{BMM}(\mathbf{Q}, \mathbf{K}) \\
        = [\mathbf{Q}_1 \mathbf{K}_1, \mathbf{Q}_2 \mathbf{K}_2, \ldots, \mathbf{Q}_n \mathbf{K}_n]
            \in \mathbb{R}^{n \times a \times c}



11.3.3. Scaled Dot Product Attention
""""""""""""""""""""""""""""""""""""

* ä¸ºäº†æ§åˆ¶ç‚¹ç§¯å€¼çš„è§„æ¨¡ï¼Œé€šå¸¸ä½¿ç”¨â€œç¼©æ”¾â€æ“ä½œï¼Œå°†ç‚¹ç§¯ç»“æœé™¤ä»¥æŸ¥è¯¢å’Œé”®çš„ç»´åº¦çš„å¹³æ–¹æ ¹
* è¿™ç§ç¼©æ”¾æœ‰åŠ©äºé¿å…éšç€å‘é‡ç»´åº¦å¢åŠ ï¼Œç‚¹ç§¯å€¼å˜å¾—è¿‡å¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šçš„é—®é¢˜ã€‚



11.3.4. Additive Attention
""""""""""""""""""""""""""

* å½“æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡çš„ç»´åº¦ä¸ä¸€è‡´æ—¶ï¼Œå¯ä»¥ä½¿ç”¨åŠ æ³•æ³¨æ„åŠ›ã€‚åŠ æ³•æ³¨æ„åŠ›é€šè¿‡å°†æŸ¥è¯¢å’Œé”®å‘é‡ç»“åˆï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ä¸€ä¸ªæƒé‡çŸ©é˜µï¼‰è¿›è¡Œå˜æ¢ï¼Œä¹‹åå†ä½¿ç”¨ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆé€šå¸¸æ˜¯tanhï¼‰è®¡ç®—è¯„åˆ†ã€‚
* å…¶è¯„åˆ†å‡½æ•°ä¸ºï¼š

.. math::

    a(\mathbf q, \mathbf k) = \mathbf w_v^\top \textrm{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R}

* å…¶ä¸­
* :math:`\mathbf{W_q}` å’Œ :math:`\mathbf{W_k}` æ˜¯å­¦ä¹ çš„å‚æ•°
* :math:`\mathbf{w_v}` æ˜¯ç”¨äºç”Ÿæˆæœ€ç»ˆå¾—åˆ†çš„æƒé‡å‘é‡ã€‚
* æœ€åï¼Œé€šè¿‡softmaxå¯¹å¾—åˆ†è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¾—åˆ°æœ€ç»ˆçš„æ³¨æ„åŠ›æƒé‡ã€‚


11.4. The Bahdanau Attention Mechanism
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* åœ¨ç¬¬10.7èŠ‚ä¸­é‡åˆ°æœºå™¨ç¿»è¯‘æ—¶ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåŸºäºä¸¤ä¸ªRNNçš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ çš„ encoderâ€“decoder ä½“ç³»ç»“æ„ï¼ˆSutskeverç­‰ï¼Œ2014ï¼‰ã€‚å…·ä½“è€Œè¨€ï¼ŒRNNç¼–ç å™¨å°†å˜é‡é•¿åº¦åºåˆ—è½¬æ¢ä¸ºå›ºå®šå½¢çŠ¶ä¸Šä¸‹æ–‡å˜é‡ã€‚ç„¶åï¼ŒRNNè§£ç å™¨åŸºäºç”Ÿæˆçš„ä»¤ç‰Œå’Œä¸Šä¸‹æ–‡å˜é‡ç”Ÿæˆè¾“å‡ºï¼ˆç›®æ ‡ï¼‰åºåˆ—ä»¤ç‰Œã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/qahW5M.png

    Fig. 11.4.1 Sequence-to-sequence model. The state, as generated by the encoder, is the only piece of information shared between the encoder and the decoder.

* è¿™å¯¹äºç®€çŸ­åºåˆ—æ¥è¯´æ˜¯å¾ˆåˆç†çš„ï¼Œä½†å¾ˆæ˜æ˜¾ï¼Œå¯¹äºé•¿ç¯‡å°è¯´ï¼Œä¾‹å¦‚ç« èŠ‚ç”šè‡³æ˜¯å¾ˆé•¿çš„å¥å­éƒ½æ˜¯ä¸å¯è¡Œçš„ã€‚æ¯•ç«Ÿï¼Œä¸ä¹…ä¹‹åï¼Œä¸­é—´è¡¨ç¤ºä¸­æ ¹æœ¬æ²¡æœ‰è¶³å¤Ÿçš„â€œç©ºé—´â€æ¥å­˜å‚¨æ‰€æœ‰é‡è¦çš„å†…å®¹ã€‚å› æ­¤ï¼Œè§£ç å™¨å°†æ— æ³•ç¿»è¯‘é•¿è€Œå¤æ‚çš„å¥å­ã€‚



æ ¸å¿ƒç‚¹-fromGPT
""""""""""""""

* ã€å®šä¹‰ã€‘Bahdanau Attention Mechanismï¼šæ˜¯ä¸€ä¸ªåœ¨åºåˆ—åˆ°åºåˆ—ï¼ˆsequence-to-sequence, seq2seqï¼‰æ¨¡å‹ä¸­ä½¿ç”¨çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒèƒ½å¤Ÿæ ¹æ®è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†é€‰æ‹©æ€§åœ°èšç„¦ï¼ˆâ€œæ³¨æ„â€ï¼‰æœ€ç›¸å…³çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨é•¿åºåˆ—çš„æƒ…å†µä¸‹ã€‚

å…³é”®ç‚¹åˆ†æ
++++++++++

* èƒŒæ™¯ï¼ˆSequence-to-Sequenceæ¨¡å‹ï¼‰:åœ¨ç»å…¸çš„seq2seqæ¨¡å‹ä¸­ï¼Œä½¿ç”¨äº†ä¸¤ä¸ªRNNï¼Œä¸€ä¸ªæ˜¯ç¼–ç å™¨ï¼ˆencoderï¼‰ï¼Œå°†è¾“å…¥åºåˆ—ç¼–ç æˆä¸€ä¸ªå›ºå®šç»´åº¦çš„çŠ¶æ€å˜é‡ï¼›å¦ä¸€ä¸ªæ˜¯è§£ç å™¨ï¼ˆdecoderï¼‰ï¼Œæ ¹æ®ç¼–ç å™¨çš„è¾“å‡ºç”Ÿæˆç›®æ ‡åºåˆ—ã€‚è¿™ä¸ªå›ºå®šç»´åº¦çš„çŠ¶æ€å˜é‡é€šå¸¸æ— æ³•å¾ˆå¥½åœ°å¤„ç†é•¿åºåˆ—ï¼Œå› ä¸ºå®ƒå°†æ•´ä¸ªè¾“å…¥åºåˆ—å‹ç¼©æˆä¸€ä¸ªå‘é‡ï¼Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚å› æ­¤ï¼Œæ¨¡å‹å¯¹é•¿å¥å­çš„ç¿»è¯‘æ•ˆæœè¾ƒå·®ã€‚
* Bahdanau Attentionæœºåˆ¶ï¼š **é—®é¢˜** : å½“è¾“å…¥åºåˆ—è¾ƒé•¿æ—¶ï¼Œä¼ ç»Ÿçš„seq2seqæ¨¡å‹ä¸­çš„å›ºå®šçŠ¶æ€ï¼ˆcontext variableï¼‰æ— æ³•åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ï¼Œå¯¼è‡´è§£ç å™¨æ— æ³•å‡†ç¡®ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚ **è§£å†³æ–¹æ¡ˆ** : Bahdanau Attentionæœºåˆ¶æå‡ºäº†åœ¨è§£ç æ—¶ï¼Œä¸æ˜¯ä¾èµ–å›ºå®šçš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œè€Œæ˜¯æ ¹æ®å½“å‰è§£ç å™¨çš„çŠ¶æ€ï¼ˆå³ä¸Šä¸€æ—¶åˆ»çš„éšè—çŠ¶æ€ï¼‰åŠ¨æ€é€‰æ‹©è¾“å…¥åºåˆ—ä¸­æœ€ç›¸å…³çš„éƒ¨åˆ†ã€‚å…·ä½“æ¥è¯´ï¼Œè§£ç å™¨æ¯æ¬¡ç”Ÿæˆä¸€ä¸ªæ–°tokenæ—¶ï¼Œä¼šåŸºäºå½“å‰çš„è§£ç å™¨çŠ¶æ€ï¼ˆä¸Šä¸€æ—¶åˆ»çš„éšè—çŠ¶æ€ï¼‰ä½œä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰ï¼Œåœ¨ç¼–ç å™¨çš„æ‰€æœ‰éšè—çŠ¶æ€ä¸­è®¡ç®—æ³¨æ„åŠ›æƒé‡ï¼Œé€‰æ‹©æœ€ç›¸å…³çš„éƒ¨åˆ†æ¥æ›´æ–°ä¸Šä¸‹æ–‡å‘é‡ã€‚è¿™ä¸ªä¸Šä¸‹æ–‡å‘é‡ï¼ˆcontext variableï¼‰ç„¶åç”¨äºç”Ÿæˆä¸‹ä¸€ä¸ªtokenã€‚

å…³é”®æ¦‚å¿µ
++++++++

* æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰ï¼šé€šè¿‡åŠ¨æ€è®¡ç®—å’Œèšç„¦åœ¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿæ ¹æ®å½“å‰éœ€è¦çš„ä¿¡æ¯æ¥ç”Ÿæˆè¾“å‡ºï¼Œè€Œä¸æ˜¯ä¾èµ–ä¸€ä¸ªå›ºå®šçš„ä¸Šä¸‹æ–‡å‘é‡ã€‚
* åŠ æ€§æ³¨æ„åŠ›ï¼ˆAdditive Attentionï¼‰ï¼šé€šè¿‡è®¡ç®—å½“å‰æŸ¥è¯¢å’Œæ‰€æœ‰é”®ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œç”Ÿæˆæƒé‡å¹¶å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä»¥æ­¤æ¥åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡ã€‚
* åŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆDynamic Contextï¼‰ï¼šä¸æ˜¯ä½¿ç”¨å›ºå®šçš„ä¸Šä¸‹æ–‡ï¼Œè€Œæ˜¯æ ¹æ®è§£ç å™¨çš„å½“å‰çŠ¶æ€å’Œç¼–ç å™¨çš„è¾“å‡ºåŠ¨æ€æ›´æ–°ä¸Šä¸‹æ–‡å‘é‡ã€‚




11.4.1. Model
"""""""""""""

* context variable :math:`\mathbf{c}`
* encoder hidden states :math:`\mathbf{h}_{t}`
* decoder hidden states :math:`\mathbf{s}_{t'-1}` 
* The key idea is that instead of keeping the state, i.e., the ``context variable`` summarizing the source sentence, as fixed, we dynamically update it, as a function of both the original text ( ``encoder hidden states`` ) and the text that was already generated ( ``decoder hidden states`` ). 
* This yields :math:`\mathbf{c}_{t'}` , which is updated after any decoding time step :math:`t'` . Suppose that the input sequence is of length T . In this case the context variable is the output of attention pooling:

.. math::

    \mathbf{c}_{t'} = \sum_{t=1}^{T} \alpha(\mathbf{s}_{t' - 1}, \mathbf{h}_{t}) \mathbf{h}_{t}

* We used :math:`\mathbf{s}_{t' - 1}` as the query, and :math:`\mathbf{h}_{t}` as both the key and the value. 
* Note that :math:`\mathbf{c}_{t'}` is then used to generate the state :math:`\mathbf{s}_{t'}` and to generate a new token.
* In particular, the attention weight :math:`\alpha` is computed using the additive attention scoring function


.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/F6i9sq.png

    Fig. 11.4.2 Layers in an RNN encoderâ€“decoder model with the Bahdanau attention mechanism.




11.4.2. Defining the Decoder with Attention
"""""""""""""""""""""""""""""""""""""""""""

.. code-block:: python

    class AttentionDecoder(d2l.Decoder):  #@save
        """The base attention-based decoder interface."""
        def __init__(self):
            super().__init__()

        @property
        def attention_weights(self):
            raise NotImplementedError

    class Seq2SeqAttentionDecoder(AttentionDecoder):
        def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0):
            super().__init__()
            self.attention = d2l.AdditiveAttention(num_hiddens, dropout)
            self.embedding = nn.Embedding(vocab_size, embed_size)
            self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)
            self.dense = nn.LazyLinear(vocab_size)
            self.apply(d2l.init_seq2seq)

        def init_state(self, enc_outputs, enc_valid_lens):
            # Shape of outputs: (num_steps, batch_size, num_hiddens).
            # Shape of hidden_state: (num_layers, batch_size, num_hiddens)
            outputs, hidden_state = enc_outputs
            return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

        def forward(self, X, state):
            # Shape of enc_outputs: (batch_size, num_steps, num_hiddens).
            # Shape of hidden_state: (num_layers, batch_size, num_hiddens)
            enc_outputs, hidden_state, enc_valid_lens = state
            # Shape of the output X: (num_steps, batch_size, embed_size)
            X = self.embedding(X).permute(1, 0, 2)
            outputs, self._attention_weights = [], []
            for x in X:
                # Shape of query: (batch_size, 1, num_hiddens)
                query = torch.unsqueeze(hidden_state[-1], dim=1)
                # Shape of context: (batch_size, 1, num_hiddens)
                context = self.attention(query, enc_outputs, enc_outputs, enc_valid_lens)
                # Concatenate on the feature dimension
                x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
                # Reshape x as (1, batch_size, embed_size + num_hiddens)
                out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
                outputs.append(out)
                self._attention_weights.append(self.attention.attention_weights)
            # After fully connected layer transformation, shape of outputs:
            # (num_steps, batch_size, vocab_size)
            outputs = self.dense(torch.cat(outputs, dim=0))
            return outputs.permute(1, 0, 2), [enc_outputs, hidden_state, enc_valid_lens]

        @property
        def attention_weights(self):
            return self._attention_weights

ä½¿ç”¨å››ä¸ªåºåˆ—çš„å°åŒ¹é…æ¥æµ‹è¯•å®ç°çš„è§£ç å™¨ï¼Œæ¯ä¸ªåºåˆ—é•¿ä¸ƒä¸ªæ—¶é—´æ­¥é•¿::

    vocab_size, embed_size, num_hiddens, num_layers = 10, 8, 16, 2
    batch_size, num_steps = 4, 7
    encoder = d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)
    decoder = Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens, num_layers)
    X = torch.zeros((batch_size, num_steps), dtype=torch.long)
    state = decoder.init_state(encoder(X), None)
    output, state = decoder(X, state)
    d2l.check_shape(output, (batch_size, num_steps, vocab_size))
    d2l.check_shape(state[0], (batch_size, num_steps, num_hiddens))
    d2l.check_shape(state[1][0], (batch_size, num_hiddens))


11.4.3. Training
""""""""""""""""

æŒ‡å®šè¶…å‚æ•°ï¼Œå®ä¾‹åŒ–å¸¸è§„ç¼–ç å™¨å’Œå¼•èµ·æ³¨æ„çš„è§£ç å™¨ï¼Œç„¶åå°†æ­¤æ¨¡å‹è®­ç»ƒä»¥è¿›è¡Œæœºå™¨ç¿»è¯‘::

    data = d2l.MTFraEng(batch_size=128)
    embed_size, num_hiddens, num_layers, dropout = 256, 256, 2, 0.2
    encoder = d2l.Seq2SeqEncoder(len(data.src_vocab), embed_size, num_hiddens, num_layers, dropout)
    decoder = Seq2SeqAttentionDecoder(len(data.tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
    model = d2l.Seq2Seq(encoder, decoder, tgt_pad=data.tgt_vocab['<pad>'], lr=0.005)
    trainer = d2l.Trainer(max_epochs=30, gradient_clip_val=1, num_gpus=1)
    trainer.fit(model, data)


å°†ä¸€äº›è‹±è¯­å¥å­ç¿»è¯‘æˆæ³•è¯­å¹¶è®¡ç®—å…¶BLEUåˆ†æ•°::

    engs = ['go .', 'i lost .', 'he\'s calm .', 'i\'m home .']
    fras = ['va !', 'j\'ai perdu .', 'il est calme .', 'je suis chez moi .']
    preds, _ = model.predict_step(
        data.build(engs, fras), d2l.try_gpu(), data.num_steps)
    for en, fr, p in zip(engs, fras, preds):
        translation = []
        for token in data.tgt_vocab.to_tokens(p):
            if token == '<eos>':
                break
            translation.append(token)
        print(f'{en} => {translation}, bleu,'
              f'{d2l.bleu(" ".join(translation), fr, k=2):.3f}')
    # è¾“å‡º
    go . => ['va', '!'], bleu,1.000
    i lost . => ["j'ai", 'perdu', '.'], bleu,1.000
    he's calm . => ['il', 'court', '.'], bleu,0.000
    i'm home . => ['je', 'suis', 'chez', 'moi', '.'], bleu,1.000


11.4.4. Summary
"""""""""""""""

* Bahdanau Attentionæœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨ç”Ÿæˆæ¯ä¸ªtokenæ—¶ï¼Œè§£ç å™¨ä¸å†åªä¾èµ–ä¸€ä¸ªå›ºå®šçš„ä¸Šä¸‹æ–‡å‘é‡ï¼Œè€Œæ˜¯é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶åŠ¨æ€åœ°é€‰æ‹©å’Œèšç„¦åœ¨è¾“å…¥åºåˆ—çš„æœ€ç›¸å…³éƒ¨åˆ†ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°å¤„ç†é•¿åºåˆ—ï¼Œé¿å…äº†ä¼ ç»Ÿseq2seqæ¨¡å‹åœ¨å¤„ç†é•¿æ–‡æœ¬æ—¶é‡åˆ°çš„ç“¶é¢ˆã€‚



11.5. Multi-Head Attention
^^^^^^^^^^^^^^^^^^^^^^^^^^

* åœ¨å®è·µä¸­ï¼Œç»™å®šç›¸åŒçš„æŸ¥è¯¢ï¼Œé”®å’Œå€¼ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹ç»“åˆç›¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„ä¸åŒè¡Œä¸ºçš„çŸ¥è¯†ï¼Œä¾‹å¦‚æ•è·å„ç§èŒƒå›´çš„ä¾èµ–æ€§ï¼ˆä¾‹å¦‚ï¼Œè¾ƒçŸ­èŒƒå›´ä¸è¾ƒé•¿èŒƒå›´ï¼‰åœ¨åºåˆ—ä¸­ã€‚å› æ­¤ï¼Œå…è®¸æˆ‘ä»¬çš„æ³¨æ„æœºåˆ¶å…±åŒä½¿ç”¨æŸ¥è¯¢ï¼Œé”®å’Œå€¼çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´å¯èƒ½æ˜¯æœ‰ç›Šçš„ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/01/BGyPHz.png

    Fig. 11.5.1 Multi-head attention, where multiple heads are concatenated then linearly transformed.

* FC: fully connected layers




æ ¸å¿ƒç‚¹-fromGPT
""""""""""""""

å…³é”®ç‚¹åˆ†æ
++++++++++

* ã€èƒŒæ™¯å’ŒåŠ¨æœºã€‘ï¼šå•å¤´æ³¨æ„åŠ›æœºåˆ¶é€šå¸¸ç”¨äºä»æŸ¥è¯¢ï¼ˆqueryï¼‰ã€é”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰ä¸­è®¡ç®—æ³¨æ„åŠ›ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æ¨¡å‹èƒ½å¤Ÿä»åŒä¸€æ³¨æ„åŠ›æœºåˆ¶ä¸­è·å¾—ä¸åŒçš„è¡Œä¸ºè¡¨ç°ï¼Œæ¯”å¦‚æ•æ‰ä¸åŒèŒƒå›´çš„ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼ŒçŸ­ç¨‹ä¾èµ–å’Œé•¿ç¨‹ä¾èµ–ï¼‰ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç›®æ ‡ï¼ŒMulti-Head Attentionæœºåˆ¶è®©æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šä¸ªå­ç©ºé—´ä¸­å¹¶è¡Œè®¡ç®—ä¸åŒçš„æ³¨æ„åŠ›ï¼Œä»¥ä¾¿æ•æ‰æ›´å¤šçš„ä¿¡æ¯ã€‚æ¯ä¸ªå¤´ï¼ˆheadï¼‰ç‹¬ç«‹å¤„ç†ä¸€ä¸ªæ³¨æ„åŠ›æ± åŒ–è¿‡ç¨‹ï¼Œå¹¶ä¸”å¯ä»¥å…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ã€‚
* ã€å¤šå¤´æ³¨æ„åŠ›çš„ä¼˜åŠ¿ã€‘ï¼šæ¯ä¸ªå¤´å¯ä»¥å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†ï¼Œä»è€Œèƒ½å¤Ÿæ•æ‰åˆ°ä¸åŒçš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤æ‚çš„ã€é•¿åºåˆ—æ—¶ï¼Œå®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ•è·ä¸åŒçš„ä¾èµ–å…³ç³»ã€‚é€šè¿‡å¹¶è¡Œè®¡ç®—å¤šä¸ªå¤´ï¼Œæ¨¡å‹å¯ä»¥é«˜æ•ˆåœ°åŒæ—¶å­¦ä¹ å¤šä¸ªè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
* ã€å®ç°ä¸­çš„ä¼˜åŒ–ã€‘ï¼šåœ¨å®é™…å®ç°ä¸­ï¼Œé€‰æ‹©ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆscaled dot product attentionï¼‰ä½œä¸ºæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›å‡½æ•°ï¼Œä»¥é¿å…è®¡ç®—æˆæœ¬å’Œå‚æ•°é‡çš„å‰§çƒˆå¢é•¿ã€‚ä¸ºäº†ä¿è¯è®¡ç®—çš„é«˜æ•ˆæ€§ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼çš„è¾“å‡ºç»´åº¦é€šå¸¸ä¼šè®¾ç½®ä¸º :math:`p_q = p_k = p_v = \frac{p_o}{h}` ï¼Œå…¶ä¸­ :math:`p_o` æ˜¯æœ€ç»ˆè¾“å‡ºçš„ç»´åº¦ï¼Œ :math:`h` æ˜¯å¤´çš„æ•°é‡ã€‚è¿™æ ·å¯ä»¥åœ¨å¹¶è¡Œè®¡ç®—æ—¶ä¿æŒè®¡ç®—å’Œå†…å­˜æ•ˆç‡ã€‚


å…³é”®æ¦‚å¿µ
++++++++

* æ³¨æ„åŠ›å¤´ï¼ˆAttention Headï¼‰ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—ä¸€ä¸ªæ³¨æ„åŠ›è¿‡ç¨‹ï¼Œå…³æ³¨è¾“å…¥åºåˆ—çš„ä¸åŒéƒ¨åˆ†ï¼Œæœ€ç»ˆå°†å¤šä¸ªå¤´çš„è¾“å‡ºè¿›è¡Œæ‹¼æ¥å¹¶çº¿æ€§å˜æ¢å¾—åˆ°æœ€ç»ˆçš„ç»“æœã€‚
* å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attentionï¼‰ï¼šé€šè¿‡å¤šä¸ªå¤´çš„å¹¶è¡Œè®¡ç®—ï¼Œå¯ä»¥åœ¨ä¸åŒçš„å­ç©ºé—´ä¸­å­¦ä¹ åˆ°æ›´å¤šçš„ä¿¡æ¯ï¼Œæé«˜æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ã€‚
* ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆScaled Dot Product Attentionï¼‰ï¼šé€šå¸¸ç”¨äºæ¯ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—ï¼Œé‡‡ç”¨ç‚¹ç§¯æ–¹å¼ï¼Œå¹¶é€šè¿‡ç¼©æ”¾å› å­æ¥é¿å…ç‚¹ç§¯è¿‡å¤§å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚



11.5.1. Model
"""""""""""""

* ã€å…·ä½“å®ç°ã€‘ï¼šåœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­ï¼Œé¦–å…ˆé€šè¿‡å¯¹æŸ¥è¯¢ã€é”®å’Œå€¼è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå°†å®ƒä»¬æ˜ å°„åˆ°ä¸åŒçš„å­ç©ºé—´ï¼ˆè¿™ä¸€æ­¥é€šè¿‡hä¸ªä¸åŒçš„çº¿æ€§å˜æ¢å®ç°ï¼‰ï¼Œæ¯ä¸ªå¤´ä¼šå¾—åˆ°ä¸€ä¸ªç‹¬ç«‹çš„æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚ä¹‹åï¼Œå°†è¿™äº›å¤´çš„ç»“æœå¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ã€‚


* query: :math:`\mathbf{q} \in \mathbb{R}^{d_q}`
* key: :math:`\mathbf{k} \in \mathbb{R}^{d_k}`
* value: :math:`\mathbf{v} \in \mathbb{R}^{d_v}`
* attention head: :math:`\mathbf{h}_i  (i = 1, \ldots, h)`

* è®¡ç®—è¿‡ç¨‹çš„å…¬å¼

.. math::

    \mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v}

* å…¶ä¸­
* :math:`f` is attention pooling,
* :math:`\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}, \mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}, and \mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}` are learnable parameters


* æœ€ç»ˆçš„å¤šå¤´æ³¨æ„åŠ›è¾“å‡ºæ˜¯å°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥åœ¨ä¸€èµ·åï¼Œå†é€šè¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ç”Ÿæˆï¼š

.. math::

    \mathbf W_o \begin{bmatrix}
        \mathbf h_1\\\vdots\\\mathbf h_h
    \end{bmatrix} \in \mathbb{R}^{p_o}

* :math:`\mathbf{W}_o` æ˜¯ç”¨äºå°†æ‹¼æ¥åçš„ç»“æœè¿›è¡Œçº¿æ€§å˜æ¢çš„æƒé‡çŸ©é˜µ



11.5.2. Implementation
""""""""""""""""""""""

.. code-block:: python

    class MultiHeadAttention(d2l.Module):  #@save
        """Multi-head attention."""
        def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):
            super().__init__()
            self.num_heads = num_heads
            self.attention = d2l.DotProductAttention(dropout)
            self.W_q = nn.LazyLinear(num_hiddens, bias=bias)
            self.W_k = nn.LazyLinear(num_hiddens, bias=bias)
            self.W_v = nn.LazyLinear(num_hiddens, bias=bias)
            self.W_o = nn.LazyLinear(num_hiddens, bias=bias)

        def forward(self, queries, keys, values, valid_lens):
            # Shape of queries, keys, or values:
            # (batch_size, no. of queries or key-value pairs, num_hiddens)
            # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)
            # After transposing, shape of output queries, keys, or values:
            # (batch_size * num_heads, no. of queries or key-value pairs,
            # num_hiddens / num_heads)
            queries = self.transpose_qkv(self.W_q(queries))
            keys = self.transpose_qkv(self.W_k(keys))
            values = self.transpose_qkv(self.W_v(values))

            if valid_lens is not None:
                # On axis 0, copy the first item (scalar or vector) for num_heads
                # times, then copy the next item, and so on
                valid_lens = torch.repeat_interleave(
                    valid_lens, repeats=self.num_heads, dim=0)

            # Shape of output: (batch_size * num_heads, no. of queries,
            # num_hiddens / num_heads)
            output = self.attention(queries, keys, values, valid_lens)
            # Shape of output_concat: (batch_size, no. of queries, num_hiddens)
            output_concat = self.transpose_output(output)
            return self.W_o(output_concat)

ä¸ºäº†å…è®¸å¤šä¸ªå¤´éƒ¨çš„å¹¶è¡Œè®¡ç®—ï¼Œä¸Šè¿° MultiHeadAttention ç±»ä½¿ç”¨ä¸‹é¢å®šä¹‰çš„ä¸¤ä¸ªæ¢ä½æ–¹æ³•::

    @d2l.add_to_class(MultiHeadAttention)  #@save
    def transpose_qkv(self, X):
        """Transposition for parallel computation of multiple attention heads."""
        # Shape of input X: (batch_size, no. of queries or key-value pairs,
        # num_hiddens). Shape of output X: (batch_size, no. of queries or
        # key-value pairs, num_heads, num_hiddens / num_heads)
        X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)
        # Shape of output X: (batch_size, num_heads, no. of queries or key-value
        # pairs, num_hiddens / num_heads)
        X = X.permute(0, 2, 1, 3)
        # Shape of output: (batch_size * num_heads, no. of queries or key-value
        # pairs, num_hiddens / num_heads)
        return X.reshape(-1, X.shape[2], X.shape[3])

    @d2l.add_to_class(MultiHeadAttention)  #@save
    def transpose_output(self, X):
        """Reverse the operation of transpose_qkv."""
        X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])
        X = X.permute(0, 2, 1, 3)
        return X.reshape(X.shape[0], X.shape[1], -1)


ç¤ºä¾‹æµ‹è¯•æˆ‘ä»¬å®ç°çš„ MultiHeadAttention ç±»::

    num_hiddens, num_heads = 100, 5
    attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)
    batch_size, num_queries, num_kvpairs = 2, 4, 6
    valid_lens = torch.tensor([3, 2])
    X = torch.ones((batch_size, num_queries, num_hiddens))
    Y = torch.ones((batch_size, num_kvpairs, num_hiddens))
    d2l.check_shape(attention(X, Y, Y, valid_lens),
                    (batch_size, num_queries, num_hiddens))




11.5.3. Summary
"""""""""""""""

* å¤šå¤´æ³¨æ„åŠ›é€šè¿‡æŸ¥è¯¢ï¼Œé”®å’Œå€¼çš„ä¸åŒè¡¨ç¤ºå­ç©ºé—´ç»“åˆäº†ç›¸åŒæ³¨æ„åŠ›é›†åˆçš„çŸ¥è¯†ã€‚è¦å¹³è¡Œè®¡ç®—å¤šå¤´æ³¨æ„çš„å¤šä¸ªå¤´éƒ¨ï¼Œéœ€è¦è¿›è¡Œé€‚å½“çš„å¼ é‡æ“ä½œã€‚
* Multi-head Attentionæœºåˆ¶é€šè¿‡å¹¶è¡Œè®¡ç®—å¤šä¸ªå¤´æ¥ä»ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ä¸­å­¦ä¹ æ³¨æ„åŠ›ï¼Œæ•æ‰è¾“å…¥çš„å¤šç§ç‰¹å¾å’Œä¾èµ–å…³ç³»ï¼Œå°¤å…¶åœ¨é•¿åºåˆ—çš„æƒ…å†µä¸‹å…·æœ‰é‡è¦ä½œç”¨ã€‚
* é€šè¿‡é€‚å½“çš„å¼ é‡æ“ä½œï¼ˆtensor manipulationï¼‰ï¼Œå¯ä»¥é«˜æ•ˆåœ°è®¡ç®—å¤šä¸ªå¤´ï¼Œé¿å…è®¡ç®—æˆæœ¬å’Œå‚æ•°é‡çš„è¿‡åº¦å¢é•¿ã€‚




11.6. Self-Attention and Positional Encoding
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* è‡ªæ³¨æ„åŠ›æœºåˆ¶æ˜¯Transformeræ¨¡å‹çš„æ ¸å¿ƒï¼Œå¹¿æ³›åº”ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸï¼Œè€Œä½ç½®ç¼–ç ç”¨äºè¡¥å¿è‡ªæ³¨æ„åŠ›æœºåˆ¶æ— æ³•æ•æ‰åºåˆ—ä¸­ä½ç½®ä¿¡æ¯çš„ä¸è¶³ã€‚

11.6.1. Self-Attention
""""""""""""""""""""""

* è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å…³é”®æ˜¯æ¯ä¸ªè¾“å…¥åºåˆ—ä¸­çš„tokenéƒ½å¯ä»¥æ ¹æ®å…¶æŸ¥è¯¢ï¼ˆqueryï¼‰ä¸å…¶ä»–æ‰€æœ‰tokençš„é”®ï¼ˆkeyï¼‰è¿›è¡Œè®¡ç®—ï¼Œä»è€Œå†³å®šå¦‚ä½•æ›´æ–°è¯¥tokençš„è¡¨ç¤ºã€‚
* å…·ä½“æ¥è¯´ï¼Œæ¯ä¸ªtokençš„æŸ¥è¯¢ä¼šä¸å…¶ä»–æ‰€æœ‰tokençš„é”®è¿›è¡ŒåŒ¹é…ï¼Œå¾—å‡ºå…¼å®¹æ€§åˆ†æ•°ï¼Œç„¶ååŸºäºè¿™äº›åˆ†æ•°ï¼ŒæŒ‰æƒé‡æ±‚å’Œæ‰€æœ‰tokençš„å€¼ï¼ˆvalueï¼‰ï¼Œä»¥ç”Ÿæˆæœ€ç»ˆçš„tokenè¡¨ç¤ºã€‚
* è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹ï¼š
    * ç»™å®šè¾“å…¥åºåˆ— :math:`\mathbf{x}_1, \ldots, \mathbf{x}_n` ï¼Œæ¯ä¸ªtokençš„è¡¨ç¤ºä¼šç»“åˆæ•´ä¸ªåºåˆ—ä¸­å…¶ä»–tokençš„è¡¨ç¤ºï¼Œè®¡ç®—å…¶åŠ æƒå’Œã€‚
    * è¾“å‡ºæ˜¯ä¸€ä¸ªä¸è¾“å…¥é•¿åº¦ç›¸åŒçš„åºåˆ— :math:`\mathbf{y}_1, \ldots, \mathbf{y}_n`



11.6.2. Comparing CNNs, RNNs, and Self-Attention
""""""""""""""""""""""""""""""""""""""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/02/nTybnr.png

    Fig. 11.6.1 Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.


* è‡ªæ³¨æ„åŠ›çš„è®¡ç®—å¤æ‚åº¦ä¸º :math:`\mathcal{O}(n^2d)` ï¼Œå…¶ä¸­ n æ˜¯åºåˆ—çš„é•¿åº¦ï¼Œd æ˜¯æ¯ä¸ªtokençš„ç»´åº¦ã€‚è¿™ä½¿å¾—è‡ªæ³¨æ„åŠ›åœ¨å¤„ç†éå¸¸é•¿çš„åºåˆ—æ—¶å˜å¾—éå¸¸æ…¢ï¼Œå› ä¸ºè®¡ç®—é‡æ˜¯äºŒæ¬¡å¢é•¿çš„ã€‚

* CNN
    * CNNä¸»è¦ç”¨äºå¤„ç†å±€éƒ¨ç‰¹å¾ï¼Œé€šè¿‡å·ç§¯æ“ä½œæ•æ‰å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
    * å…¶è®¡ç®—å¤æ‚åº¦ä¸º :math:`\mathcal{O}(knd^2)`
    * å…¶ä¸­ k æ˜¯å·ç§¯æ ¸çš„å¤§å°ã€‚
    * CNNå…·æœ‰è¾ƒçŸ­çš„è·¯å¾„é•¿åº¦ï¼Œä½†è®¡ç®—è¿‡ç¨‹ä¸­å­˜åœ¨å±‚çº§ç»“æ„ï¼Œå› æ­¤æœ€å¤§è·¯å¾„é•¿åº¦ä¸º :math:`\mathcal{O}(n/k)`
* RNN
    * RNNæŒ‰é¡ºåºå¤„ç†æ¯ä¸ªtoken
    * å…¶è®¡ç®—å¤æ‚åº¦ä¸º :math:`\mathcal{O}(nd^2)`
    * RNNçš„æœ€å¤§è·¯å¾„é•¿åº¦ä¸º :math:`\mathcal{O}(n)`
    * ç”±äºå…¶é¡ºåºä¾èµ–æ€§ï¼Œæ— æ³•å¹¶è¡ŒåŒ–è®¡ç®—ã€‚
* Self-Attention
    * ç›¸æ¯”äºCNNå’ŒRNNï¼Œè‡ªæ³¨æ„åŠ›èƒ½å¤Ÿå¹¶è¡Œè®¡ç®—ï¼Œå¹¶ä¸”æ¯ä¸ªtokenéƒ½èƒ½ç›´æ¥ä¸å…¶ä»–tokenå»ºç«‹è¿æ¥ï¼ˆå³è·¯å¾„é•¿åº¦ä¸º :math:`\mathcal{O}(1)` ï¼‰
    * ä½†æ˜¯ï¼Œè®¡ç®—å¤æ‚åº¦æ˜¯ :math:`\mathcal{O}(n^2d)` ï¼Œå› æ­¤å¯¹äºé•¿åºåˆ—æ¥è¯´è®¡ç®—é‡éå¸¸å¤§ã€‚


.. note:: All in all, both CNNs and self-attention enjoy parallel computation and self-attention has the shortest maximum path length. However, the quadratic computational complexity with respect to the sequence length makes self-attention prohibitively slow for very long sequences.


11.6.3. Positional Encoding
"""""""""""""""""""""""""""

* è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸åŒ…å«é¡ºåºä¿¡æ¯ï¼Œå› æ­¤æ— æ³•ç›´æ¥æ•æ‰åºåˆ—ä¸­tokençš„ç›¸å¯¹ä½ç½®ã€‚ä¸ºäº†å¼¥è¡¥è¿™ä¸€ç‚¹ï¼Œå¼•å…¥äº†ä½ç½®ç¼–ç ï¼Œå®ƒä¸ºæ¯ä¸ªtokenæ·»åŠ äº†é¢å¤–çš„ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥æ„ŸçŸ¥åºåˆ—çš„é¡ºåºã€‚


11.6.3.1. Absolute Positional Information
+++++++++++++++++++++++++++++++++++++++++


* ä¸€ç§åŸºäºæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„å›ºå®šä½ç½®ç¼–ç çš„ç®€å•æ–¹æ¡ˆ

.. math::

    \begin{aligned} 
        p_{i, 2j} &= \sin\left(\frac{i}{10000^{2j/d}}\right),\\
        p_{i, 2j+1} &= \cos\left(\frac{i}{10000^{2j/d}}\right)
    \end{aligned}


.. code-block:: python

    class PositionalEncoding(nn.Module):  #@save
        """Positional encoding."""
        def __init__(self, num_hiddens, dropout, max_len=1000):
            super().__init__()
            self.dropout = nn.Dropout(dropout)
            # Create a long enough P
            self.P = torch.zeros((1, max_len, num_hiddens))
            X = torch.arange(max_len, dtype=torch.float32).reshape(
                -1, 1) / torch.pow(10000, torch.arange(
                0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)
            self.P[:, :, 0::2] = torch.sin(X)
            self.P[:, :, 1::2] = torch.cos(X)

        def forward(self, X):
            X = X + self.P[:, :X.shape[1], :].to(X.device)
            return self.dropout(X)


11.6.3.2. Relative Positional Information
+++++++++++++++++++++++++++++++++++++++++

* é™¤äº†ç»å¯¹ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç ä¹Ÿèƒ½æ•æ‰ç›¸å¯¹ä½ç½®å…³ç³»ã€‚
* é€šè¿‡å¯¹ä½ç½®ç¼–ç è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œå¯ä»¥è¡¨ç¤ºä»»æ„ä¸¤ä¸ªtokenä¹‹é—´çš„ç›¸å¯¹ä½ç½®åç§»ï¼Œä»è€Œä½¿æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ åˆ°åŸºäºç›¸å¯¹ä½ç½®çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚


.. math::

    \begin{aligned}
        \begin{bmatrix} \cos(\delta \omega_j) & \sin(\delta \omega_j) \\  -\sin(\delta \omega_j) & \cos(\delta \omega_j) \\ \end{bmatrix}
        \begin{bmatrix} p_{i, 2j} \\  p_{i, 2j+1} \\ \end{bmatrix}
        =&\begin{bmatrix} \cos(\delta \omega_j) \sin(i \omega_j) + \sin(\delta \omega_j) \cos(i \omega_j) \\  -\sin(\delta \omega_j) \sin(i \omega_j) + \cos(\delta \omega_j) \cos(i \omega_j) \\ \end{bmatrix}\\
        =&\begin{bmatrix} \sin\left((i+\delta) \omega_j\right) \\  \cos\left((i+\delta) \omega_j\right) \\ \end{bmatrix}\\
        =&\begin{bmatrix} p_{i+\delta, 2j} \\  p_{i+\delta, 2j+1} \\ \end{bmatrix},
    \end{aligned}

* å¯¹äºä»»ä½•å›ºå®šä½ç½®åç§» :math:`\delta` ï¼Œä½ç½® :math:`i + \delta` çš„ä½ç½®ç¼–ç å¯ä»¥ç”¨ä½ç½® :math:`i` çš„çº¿æ€§æŠ•å½±è¡¨ç¤º
* å…¶ä¸­ :math:`\omega_j = 1/10000^{2j/d}`
* å¯¹äºä»»ä½•å›ºå®šåç§» :math:`\delta` ä»»ä½•ä¸€å¯¹ :math:`(p_{i, 2j}, p_{i, 2j+1})` å¯ä»¥çº¿æ€§åœ°æŠ•å½±åˆ° :math:`(p_{i+\delta, 2j}, p_{i+\delta, 2j+1})`


11.6.4. Summary
"""""""""""""""

* è‡ªæ³¨æ„åŠ›æœºåˆ¶ä½¿å¾—æ¯ä¸ªtokenå¯ä»¥ä¸å…¶ä»–æ‰€æœ‰tokenè¿›è¡Œç›´æ¥äº¤äº’ï¼Œä»è€Œæ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚ç›¸æ¯”äºCNNå’ŒRNNï¼Œè‡ªæ³¨æ„åŠ›è®¡ç®—å¯ä»¥å¹¶è¡ŒåŒ–ï¼Œæœ€å¤§è·¯å¾„é•¿åº¦æœ€çŸ­ï¼Œä½†ç”±äºè®¡ç®—å¤æ‚åº¦æ˜¯äºŒæ¬¡çš„ï¼Œé•¿åºåˆ—å¤„ç†çš„å¼€é”€è¾ƒå¤§ã€‚
* ä½ç½®ç¼–ç é€šè¿‡ä¸ºæ¯ä¸ªtokenæ·»åŠ ä½ç½®ä¿¡æ¯ï¼Œä½¿å¾—è‡ªæ³¨æ„åŠ›æœºåˆ¶èƒ½å¤Ÿå¤„ç†é¡ºåºé—®é¢˜ã€‚ä½ç½®ç¼–ç å¯ä»¥æ˜¯ç»å¯¹çš„ï¼Œä¹Ÿå¯ä»¥æ˜¯ç›¸å¯¹çš„ï¼Œå¸®åŠ©æ¨¡å‹æ•æ‰åºåˆ—ä¸­çš„ä½ç½®å…³ç³»ã€‚
* è¿™ä¸¤ç§æœºåˆ¶ç»“åˆèµ·æ¥ï¼Œæ„æˆäº†Transformerçš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ï¼Œä½¿å¾—å®ƒåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†æ˜¾è‘—çš„æˆåŠŸã€‚




11.7. The Transformer Architecture
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* Transformer å°±æ˜¯ å †å è‡ªæ³¨æ„åŠ›æœºåˆ¶ + æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œï¼Œé€šè¿‡ Encoder ç¼–ç è¾“å…¥åºåˆ—ï¼Œå†é€šè¿‡ Decoder é€æ­¥è‡ªå›å½’åœ°ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œæ•´å¥—æ¶æ„çµæ´»ä¸”é«˜åº¦å¹¶è¡ŒåŒ–ï¼Œæ˜¯ç°ä»£å¤§å¤šæ•°å¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€ã€‚


11.7.1. Model
"""""""""""""

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/tlm6Qx.png

    Fig. 11.7.1 The Transformer architecture.

* æ•´ä½“ç»“æ„
    - Transformer æ˜¯ä¸€ç§ Encoder-Decoder æ¶æ„ï¼Œæ•´ä½“åˆ†ä¸ºä¸¤å¤§å—ï¼š
        - Encoderï¼ˆç¼–ç å™¨ï¼‰
        - Decoderï¼ˆè§£ç å™¨ï¼‰
    - å®ƒæ˜¯ åºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼ˆseq2seq learningï¼‰ çš„ä¸€ç§æ”¹è¿›æ–¹æ³•ï¼Œç›¸æ¯”äºä¹‹å‰æµè¡Œçš„ Bahdanau Attentionï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ï¼ŒTransformer å®Œå…¨åŸºäºè‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ æœºåˆ¶æ¥å¤„ç†è¾“å…¥å’Œè¾“å‡ºåºåˆ—ã€‚

* è¾“å…¥å¤„ç†
    - è¾“å…¥çš„ source sequenceï¼ˆè¾“å…¥åºåˆ—ï¼‰ å’Œ target sequenceï¼ˆè¾“å‡ºåºåˆ—ï¼‰ éƒ½è¦ å…ˆç»è¿‡ embeddingï¼ˆè¯å‘é‡åµŒå…¥ï¼‰
    - ç„¶åï¼Œä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ è¢«åŠ åˆ° embedding ä¸Šï¼Œå‘Šè¯‰æ¨¡å‹å•è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œå› ä¸º Transformer æœ¬èº«ä¸å…·å¤‡åºåˆ—é¡ºåºæ„ŸçŸ¥èƒ½åŠ›
    - ä¹‹åï¼Œè¿™äº›åŠ äº†ä½ç½®ç¼–ç çš„å‘é‡ä¼šåˆ†åˆ«è¢«é€è¿› Encoder å’Œ Decoder

* Encoder ç»“æ„
    * Encoder æ˜¯ç”± å¤šä¸ªç›¸åŒçš„å±‚å åœ¨ä¸€èµ·ï¼Œæ¯ä¸€å±‚åŒ…å« ä¸¤ä¸ªå­å±‚ï¼ˆsublayersï¼‰ï¼š
        1. å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼ˆMulti-Head Self-Attentionï¼‰  
            - è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè¾“å…¥åºåˆ—å†…éƒ¨çš„å„ä¸ªè¯ç›¸äº’å…³æ³¨ï¼Œæ•æ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
            - è¿™é‡Œçš„ Queryã€Keyã€Value éƒ½æ¥è‡ªä¸Šä¸€å±‚ Encoder çš„è¾“å‡ºã€‚
        2. å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰  
            - é€ä½ç½®åº”ç”¨çš„å…¨è¿æ¥ç½‘ç»œï¼Œä½œç”¨æ˜¯å¢åŠ æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚

    * æ®‹å·®è¿æ¥ï¼ˆResidual Connectionï¼‰ å’Œ Layer Normalization
        - æ¯ä¸ªå­å±‚å¤–é¢éƒ½æœ‰ä¸€ä¸ª æ®‹å·®è¿æ¥ï¼ˆå’Œ ResNet ç±»ä¼¼ï¼‰ï¼Œå½¢å¼æ˜¯ï¼š :math:`\mathbf{x} + \textrm{sublayer}(\mathbf{x})`
        - ä¿è¯è¾“å…¥å’Œè¾“å‡ºç»´åº¦ä¸€è‡´ï¼ˆ :math:`\mathbb{R}^d` ï¼‰ï¼Œä¾¿äºç›´æ¥ç›¸åŠ ã€‚
        - æ®‹å·®ä¹‹åå†åš LayerNormï¼Œæœ‰åŠ©äºè®­ç»ƒç¨³å®šæ€§ã€‚

    * Encoder æœ€ç»ˆè¾“å‡ºï¼šæ¯ä¸ªä½ç½®ä¸€ä¸ª d ç»´å‘é‡ï¼Œè¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­æ¯ä¸ªè¯çš„è¡¨ç¤ºã€‚

* Decoder ç»“æ„
    * Decoder ä¹Ÿç”± å¤šä¸ªç›¸åŒå±‚å †å ï¼Œä½†å®ƒæ¯” Encoder å¤šäº†ä¸€ä¸ªå­å±‚ï¼Œæ€»å…± ä¸‰ä¸ªå­å±‚ï¼š
        1. Masked å¤šå¤´è‡ªæ³¨æ„åŠ›å±‚ï¼ˆMasked Multi-Head Self-Attentionï¼‰
            - ä¹Ÿæ˜¯ Queryã€Keyã€Value éƒ½æ¥è‡ª Decoder çš„å‰ä¸€å±‚è¾“å‡ºã€‚
            - ä½†åŠ äº† maskï¼Œé˜²æ­¢æ¨¡å‹çœ‹åˆ°å½“å‰æ—¶é—´æ­¥ä¹‹åçš„ tokenï¼Œä¿è¯è‡ªå›å½’ï¼ˆautoregressiveï¼‰æ€§è´¨ã€‚
        2. Encoder-Decoder Attention
           - è¿™æ˜¯ Decoder ç‰¹æœ‰çš„ã€‚
           - Query æ¥è‡ª Decoderï¼Œè€Œ Key å’Œ Value æ¥è‡ª Encoder è¾“å‡ºï¼Œå®ç°è¾“å…¥è¾“å‡ºåºåˆ—ä¹‹é—´çš„è”ç³»ã€‚
        3. å‰é¦ˆç¥ç»ç½‘ç»œ

    * åŒæ ·ï¼Œæ¯ä¸ªå­å±‚ä¹Ÿå¸¦æœ‰ï¼š
        - æ®‹å·®è¿æ¥ï¼ˆè¾“å…¥ + å­å±‚è¾“å‡ºï¼‰
        - Layer Normalization

* æ€»ç»“
    - Transformer æ•´ä¸ªè®¾è®¡ç†å¿µï¼š
        - å®Œå…¨åŸºäº Attentionï¼Œæ²¡æœ‰ RNN/CNN
        - ä½ç½®ç¼–ç è¡¥å……åºåˆ—é¡ºåºä¿¡æ¯
        - æ®‹å·®è¿æ¥ + LayerNorm ä¿è¯è®­ç»ƒæ·±å±‚ç½‘ç»œæ—¶çš„ç¨³å®šæ€§
        - Encoder ç”¨äºç†è§£è¾“å…¥åºåˆ—ï¼ŒDecoder ç”¨äºç”Ÿæˆè¾“å‡ºåºåˆ—
        - Decoder çš„ Masked Attention ä¿è¯ è‡ªå›å½’ç”Ÿæˆï¼ˆåªçœ‹å½“å‰å’Œä¹‹å‰çš„ tokenï¼‰


* å®šä¹‰ï¼šæ®‹å·®è¿æ¥(Residual Connection)
    - æ·±åº¦ç¥ç»ç½‘ç»œä¸­å¸¸ç”¨çš„ä¸€ç§æŠ€å·§ï¼Œç‰¹åˆ«æ˜¯åœ¨ ResNetï¼ˆæ®‹å·®ç½‘ç»œï¼‰ å’Œ Transformer é‡Œç”¨å¾—å¾ˆå¤š
    - æ ¸å¿ƒæ€æƒ³ï¼šæ®‹å·®è¿æ¥å°±æ˜¯â€œè·³è¿‡ä¸€å±‚â€ç›´æ¥æŠŠè¾“å…¥åŠ åˆ°è¾“å‡ºä¸Šã€‚
    - å…¬å¼: :math:`\mathbf{y} = \mathbf{x} + \textrm{F}(\mathbf{x})`
    - ç›´è§‚ç†è§£: æ¨¡å‹å­¦çš„ä¸æ˜¯ç›´æ¥è¾“å‡ºï¼Œè€Œæ˜¯â€œè¾“å…¥å’Œè¾“å‡ºçš„å·®å€¼â€ï¼Œå³ residualï¼ˆæ®‹å·®ï¼‰
        + ä¼ ç»Ÿç½‘ç»œ::

            è¾“å…¥ x â†’ ç¥ç»ç½‘ç»œå±‚ â†’ è¾“å‡º y

        + æ®‹å·®è¿æ¥::

            è¾“å…¥ x â†’ ç¥ç»ç½‘ç»œå±‚ â†’ å¾—åˆ° F(x)
                        â†˜ åŠ ä¸Šè¾“å…¥ x
            è¾“å‡ºï¼šy = F(x) + x

    - ä¸ºä»€ä¹ˆè¦è¿™æ ·è®¾è®¡
        + è§£å†³æ·±åº¦ç½‘ç»œè®­ç»ƒå›°éš¾
            * éšç€å±‚æ•°åŠ æ·±ï¼Œå®¹æ˜“å‡ºç° æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾
            * æ®‹å·®è¿æ¥å¯ä»¥è®©æ¢¯åº¦ ç›´æ¥ä»åå±‚ä¼ åˆ°å‰å±‚ï¼Œç¼“è§£è¿™ä¸ªé—®é¢˜
        + é¿å…æ€§èƒ½é€€åŒ–
            * å®é™…ä¸Šï¼Œå¦‚æœç½‘ç»œæ²¡å­¦åˆ°ä»€ä¹ˆæœ‰ç”¨çš„ä¸œè¥¿ï¼Œæ®‹å·®è¿æ¥è‡³å°‘å¯ä»¥è®©ç½‘ç»œå­¦åˆ°â€œæ’ç­‰æ˜ å°„â€ï¼ˆå³è¾“å‡ºå’Œè¾“å…¥ä¸€æ ·ï¼‰
            * è¿™æ ·ç½‘ç»œè‡³å°‘ä¸ä¼šæ¯”æµ…å±‚ç½‘ç»œè¡¨ç°æ›´å·®ï¼Œé¿å…â€œåŠ å±‚åè€Œæ•ˆæœä¸‹é™â€çš„é—®é¢˜




11.7.2. Positionwise Feed-Forward Networks
""""""""""""""""""""""""""""""""""""""""""

* ä½ç½®å‰é¦ˆç½‘ç»œï¼ˆPositionwise Feed-Forward Networkï¼‰
* ä¸ºä»€ä¹ˆç§°ä¸º"positionwise"ï¼ˆä½ç½®å‰é¦ˆï¼‰
    * å…³é”®ç‚¹ï¼š
        * æ¯ä¸ªä½ç½®çš„å‘é‡æ˜¯ç‹¬ç«‹ã€é€ä¸ªä½ç½®å¤„ç†çš„ï¼
        * åŒä¸€ä¸ª FFNï¼ˆåŒæ ·çš„å‚æ•° $W_1, b_1, W_2, b_2$ï¼‰è¢«åº”ç”¨åˆ°åºåˆ—ä¸­çš„ æ¯ä¸€ä¸ªä½ç½®ã€‚
    * â†’ è¿™å°±å« positionwise â€”â€” å¯¹æ¯ä¸ªâ€œä½ç½®â€å•ç‹¬åº”ç”¨ç›¸åŒçš„ FFNã€‚
        * åºåˆ—æœ‰å¾ˆå¤š tokenï¼Œæ¯ä¸ª token æœ‰è‡ªå·±çš„è¡¨ç¤ºï¼ˆembedding å‘é‡ï¼‰ã€‚
        * æ‰€æœ‰ token çš„å‘é‡éƒ½ç”¨åŒä¸€ä¸ª FFN è¿›è¡Œå¤„ç†ï¼Œä½†æ¯ä¸ª token æ˜¯å•ç‹¬å¤„ç†ï¼Œä¸è€ƒè™‘åˆ«çš„ä½ç½®çš„ä¿¡æ¯ã€‚
* è¾“å…¥å’Œè¾“å‡ºçš„å½¢çŠ¶ï¼š
    * è¾“å…¥Xçš„å½¢çŠ¶æ˜¯(batch size, number of time steps or sequence length in tokens, number of hidden units or feature dimension)ã€‚
    * è¾“å‡ºYçš„å½¢çŠ¶æ˜¯(batch size, number of time steps, ffn_num_outputs)


ç»“æ„::

    class PositionWiseFFN(nn.Module):  #@save
        """The positionwise feed-forward network."""
        def __init__(self, ffn_num_hiddens, ffn_num_outputs):
            super().__init__()
            self.dense1 = nn.LazyLinear(ffn_num_hiddens)
            self.relu = nn.ReLU()
            self.dense2 = nn.LazyLinear(ffn_num_outputs)

        def forward(self, X):
            return self.dense2(self.relu(self.dense1(X)))

ä½¿ç”¨::

    ffn = PositionWiseFFN(4, 8)
    ffn.eval()
    ffn(torch.ones((2, 3, 4)))[0]
    # è¾“å‡º
    tensor([[ 0.6300,  0.7739,  0.0278,  0.2508, -0.0519,  0.4881, -0.4105,  0.5163],
            [ 0.6300,  0.7739,  0.0278,  0.2508, -0.0519,  0.4881, -0.4105,  0.5163],
            [ 0.6300,  0.7739,  0.0278,  0.2508, -0.0519,  0.4881, -0.4105,  0.5163]],
           grad_fn=<SelectBackward0>)


11.7.3. Residual Connection and Layer Normalization
"""""""""""""""""""""""""""""""""""""""""""""""""""

* é€šè¿‡å±‚è§„èŒƒåŒ–å’Œæ‰¹é‡è§„èŒƒåŒ–æ¯”è¾ƒäº†ä¸åŒç»´åº¦ä¸Šçš„è§„èŒƒåŒ–::

    ln = nn.LayerNorm(2)
    bn = nn.LazyBatchNorm1d()
    X = torch.tensor([[1, 2], [2, 3]], dtype=torch.float32)
    # Compute mean and variance from X in the training mode
    print('layer norm:', ln(X), '\nbatch norm:', bn(X))







11.7.4. Encoder
"""""""""""""""

11.7.5. Decoder
"""""""""""""""

11.7.6. Training
""""""""""""""""

11.7.7. Summary
"""""""""""""""




11.8. Transformers for Vision
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

11.8.1. Model
11.8.2. Patch Embedding
11.8.3. Vision Transformer Encoder
11.8.4. Putting It All Together
11.8.5. Training
11.8.6. Summary and Discussion



11.9. Large-Scale Pretraining with Transformers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

11.9.1. Encoder-Only
11.9.2. Encoderâ€“Decoder
11.9.3. Decoder-Only
11.9.4. Scalability
11.9.5. Large Language Models
11.9.6. Summary and Discussion








Part 3: Scalability, Efficiency, and Applications
=================================================

























































