# 2210.17323_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers

* [https://arxiv.org/abs/2210.17323](https://arxiv.org/abs/2210.17323)
* GitHub: [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)
* 组织: 奥地利-苏黎世联邦理工学院


## Abstract

* GPTQ 
    * 一种新的**权重量化方法**
    * a new one-shot weight quantization method based on approximate second-order information, 
        * that is both highly-accurate and highly-efficient.
    * 能在**只用一次计算**的情况下，把模型压缩得很小（每个权重只用 3~4 比特表示），而且基本不影响准确率。
* 它可以在约 4 小时内把一个有 **1750 亿参数**的 GPT 模型压缩完，甚至可以让这个巨大的模型在**一张 GPU 上运行**。
* 它比以前的方法压缩效果好一倍多。即使把权重压缩到 **2-bit** 或 **三值化（ternary）** 也还能保持不错的效果。
* 实验表明
    * 在高端 GPU（如 A100）上能加速约 **3.25 倍**
    * 在便宜一些的 GPU（如 A6000）上可达 **4.5 倍**


## 1. Introduction


1. **背景和问题**：大型Transformer生成模型（如GPT-3，1750亿参数）性能强大，但体积巨大，推理（使用模型做预测）时需要极高算力和显存，普通GPU难以承载，成本高。

2. **现有解决方案的不足**：模型压缩（特别是后训练量化post-training quantization，PTQ）可以降低资源需求，但之前方法对超大模型效果有限，尤其在压缩比高（3-4位量化）时准确率下降明显。

3. **本文贡献**：提出一种新的高效后训练量化方法叫GPTQ，能在几小时内对超大模型（OPT-175B，BLOOM-176B）进行3-4位压缩，几乎不损失准确率。

4. **效果和应用**：GPTQ压缩后的 OPT-175B 模型可在单张NVIDIA A100或两张更便宜的A6000 GPU上运行，并且速度提升3-4倍。还支持极端压缩到2位或三值。

5. **创新点和意义**：首次实现了数百亿参数级别模型的高精度低位量化，开启了让大型模型更广泛应用的可能。

6. **限制**：当前方法对乘法运算速度提升有限，因主流硬件架构尚不支持混合精度操作（如 FP16 x INT4）；另外，我们当前的结果未包含激活量化，因为激活并非目标场景中的主要瓶颈

* 总结：这篇论文提出了一个实用且高效的模型压缩方法，使超大语言模型的推理更省资源、更快，并保证准确率，推动了大模型的落地应用。


## 2. Related Work

1. **量化方法分两类**
    * 训练时量化（需要大量重新训练）
    * 训练后量化（只用少量数据和时间对预训练模型进行量化）
    * 本文重点关注训练后量化。

2. **训练后量化**：
    * 大多针对视觉模型，常用方法是分层或分块量化。
    * 介绍了几种经典方法（AdaRound、BitSplit、AdaQuant、BRECQ、OBQ），
    * 它们在中等规模模型上效果好，但难以扩展到超大模型。

3. **超大模型量化**：
    * 随着大型语言模型（如BLOOM、OPT-175B）出现
    * 研究者开始探索适合它们的高效量化方法（如ZeroQuant、LLM.int8()、nuQmm）
    * 这些方法都采用较简单的四舍五入策略以保证速度，且在规模和时间上有一定限制。

4. **本文贡献**：提出的GPTQ方法能在大模型上高效实现更复杂、更准确的量化，压缩率是之前方法的两倍，同时保持相似准确度。


## 3. Background

1. **分层量化（Layer-Wise Quantization）**：
    * 量化方法是对神经网络中每一层的权重分别进行处理，通过最小化量化后输出与原始输出的差异（平方误差）来优化量化权重。

2. **最佳脑量化（Optimal Brain Quantization，OBQ）**：
   * OBQ 是一种针对分层量化的优化方法，它逐行处理权重，逐个权重量化，
   * 并在量化每个权重时，动态调整其他未量化的权重来补偿误差。
   * 这个过程基于一个二次优化目标，用数学方法高效更新权重，避免了重复计算。
   * OBQ 在中等规模模型（如ResNet-50）上效率较高，
       * 但其计算复杂度随权重矩阵大小呈三次方增长，导致在超大模型（数十亿参数）上计算代价极高。



## 4. The GPTQ Algorithm

* 这段内容讲的是 GPTQ 算法的三个关键优化步骤及其整体流程，
* 核心目的是高效且准确地对大模型中的权重进行后训练量化
* 具体简述如下：
    1. **任意顺序量化（Step 1）**
       * 传统方法按“误差最小”的贪心顺序量化权重，但作者发现，对大模型来说，随意固定顺序量化效果差别不大。
       * 于是GPTQ统一按列顺序对所有行权重量化，这样只需计算一次关键矩阵的逆，大幅减少计算复杂度，从而节省大量时间。
    2. **懒批量更新（Step 2）**
       * 直接更新算法很慢，瓶颈在于内存带宽不足。
       * GPTQ改为一次批量处理128列，延迟统一更新权重和相关矩阵，极大提升GPU利用率和速度，适合超大模型。
    3. **Cholesky分解稳定化（Step 3）**
       * 反复矩阵更新会带来数值不稳定，尤其大模型容易导致算法失效。
       * GPTQ利用Cholesky分解预先计算需要的矩阵行信息，结合轻微的对角线加权（阻尼），保证数值稳定，提升鲁棒性和速度

* **总结：** GPTQ通过“任意顺序量化+批量懒更新+Cholesky数值稳定”三大技术，实现在超大模型上高效准确的后训练权重量化。



## 5. Experimental Validation

1. **实验目标**：验证GPTQ量化方法在准确度和速度上的表现，尤其是在大规模语言模型上的应用效果。

2. **实验设置**：用PyTorch实现GPTQ，基于单块GPU（NVIDIA A100 80GB）进行量化，采用标准的异步逐行量化方法，使用通用文本数据（非特定任务数据）进行校准，实现真正的“零样本”量化。

3. **对比基线**：主要比较GPTQ与当前主流的简单四舍五入方法（RTN）以及更复杂但计算慢的其他先进量化方法（AdaRound、BRECQ、OBQ等）。

4. **小模型量化效果**：GPTQ在4位量化时性能接近最优方法，3位稍逊，但速度大幅快于传统方法，支持快速量化，方便扩展到更大模型。

5. **大模型运行时间**：GPTQ能在几分钟到几小时内完成从十亿到上百亿参数模型的全量化，而其他方法处理大模型耗时极长，难以实际使用。

6. **语言生成任务效果**：GPTQ在多种语言任务上表现优于RTN，尤其是大模型在低位宽（3-4位）量化下仍保持较好性能，且模型越大，量化难度反而越低。

7. **超大模型（175B参数）量化**：GPTQ在4位量化下性能几乎无损，3位量化下仍能保持合理效果，且通过调整量化粒度（分组大小）还能进一步提升准确率，接近未量化模型。


* 总结来说，GPTQ是一种高效且准确的后训练量化方法，能够快速且稳定地对超大规模语言模型进行低位宽量化，明显优于现有简单方法，同时在精度和速度上兼顾，适合大规模模型实用部署。



## 6. Summary and Limitations

* 作者介绍了一种叫 GPTQ 的方法，用来高效量化大型语言模型到3或4位，压缩后模型速度更快，准确率损失很小，使得更多人能使用这些大模型。
* 但也有局限：这种方法主要通过减少内存传输提速，并没有减少计算量；只针对生成任务，没有涉及激活量化。未来可以通过优化GPU代码等方式继续改进。























