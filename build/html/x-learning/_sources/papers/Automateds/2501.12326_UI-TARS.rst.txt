2501.12326_UI-TARS: Pioneering Automated GUI Interaction with Native Agents
###########################################################################

* https://arxiv.org/abs/2501.12326
* GoogleScholar(star: 9)
* 组织: 字节，清华
* GitHub: https://github.com/bytedance/UI-TARS 


Abstract
========

* UI-TARS 是一种 端到端的 GUI 代理模型，仅通过 屏幕截图 作为输入，模拟人类的交互方式（键盘、鼠标操作等）。
* 它的最大特点是 完全不依赖外部大模型（如 GPT-4o），也不需要复杂的提示词 (prompt) 和预设工作流，而是直接通过自身的模型能力执行任务。
* 实验表明，UI-TARS 在 10+ 个 GUI 代理基准测试中 达到了 SOTA（State of the Art，最优性能）。
* UI-TARS 的核心创新
    - 增强感知能力(Enhanced Perception)：训练于 大规模 GUI 截图数据集，能更精准地理解 UI 组件，并进行精确的 UI 元素标注。
    - 统一的交互建模(Unified Action Modeling)：将不同平台（如 Windows、Android）的交互方式 标准化，通过 大规模交互数据 学习精确的 UI 组件定位和交互方式。
    - 系统 2 级推理（System-2 Reasoning）：采用更 高级的推理能力 来执行多步决策，包括：
        + 任务分解（Task Decomposition）
        + 反思性思考（Reflection Thinking）
        + 里程碑识别（Milestone Recognition）
    - 基于反思的在线迭代训练(Iterative Training with Reflective Online Traces)：通过 数百台虚拟机 自动收集新的交互数据，进行数据筛选和优化，反思过去的错误，持续学习和自我优化，减少人为干预


基准测试::

    # GUI 自动化 & 计算机视觉
    GUI-Odyssey
    OSWorld (Screenshot 15 steps)
    ScreenSpot-Pro
    ScreenSpot-v2
    ScreenQA-Short

    # Web 自动化 & AI 浏览器交互
    MM2Web-Website
    MM2Web-Task
    MM2Web-Domain
    VisualWebBench

    # Android 设备控制
    AndroidControl-Low
    AndroidControl-High


1. Introduction
===============

本章主要讨论自主智能体（Autonomous Agents），特别是它们在图形用户界面（GUI）中的应用和发展。

背景: GUI 代理(GUI Agents)的重要性
----------------------------------

* 自主智能体可以感知环境、做决策、执行动作，以完成特定任务。而在 GUI 交互方面，它们面临重大挑战，因为 GUI 主要依赖按钮、文本框、图像等视觉元素，而不是纯文本或结构化数据（如 HTML 代码）。

传统 GUI 代理方法的局限性
-------------------------

* 混合方法
    - 过去的 GUI 代理主要采用基于文本的混合方法，即：
        - 读取 HTML 结构、可访问性树（Accessibility Tree）。
        - 结合视觉模型来分析 UI。
    - 问题
        - 平台兼容性差：不同平台的 HTML 结构、可访问性数据不同，导致泛化能力弱。
        - 系统权限要求高：许多方法需要访问底层系统信息，受限较多。
        - 不够灵活：需要大量人工设计（handcrafted rules），不适应变化。

* 模块化架构
    - 许多 GUI 代理使用模块化架构，把不同的功能拆分为：
        - 视觉语言模型（VLM，例如 GPT-4o）：用于理解 UI 组件。
        - 任务规划模块：负责逻辑推理和操作执行。
        - 记忆模块：存储过去的交互信息。
    - 问题
        - 依赖专家规则：需要大量人为设计，难以适应新任务。
        - 不够端到端（End-to-End）：各模块之间的衔接可能导致误差累积，影响整体性能。

原生 GUI 代理(Native GUI Agent)
-------------------------------

* 为了克服上述问题，研究者正在推动两个核心转变：
* 从文本+视觉方法 → 纯视觉方法
    - 过去的 GUI 代理需要解析 HTML，但新方法完全依赖屏幕截图作为输入，类似人类视觉感知 GUI。
    - 好处：
        - 不受平台限制（适用于不同操作系统）。
        - 降低系统访问权限要求（无需 HTML 解析）。
        - 更符合人类的认知方式。

* 从模块化框架 → 端到端模型
    - 模块化方法：各个子模块独立训练，不利于全局优化。
    - 端到端方法：所有组件（感知、推理、记忆、行动）整合到同一架构中，使模型能基于数据直接学习，而非依赖人工规则。

当前端到端 GUI 代理面临的挑战
-----------------------------

* 尽管端到端方法概念上更先进，但仍然存在两个核心难点：
* GUI 本身的复杂性
    - 信息密度高：现代 GUI 界面包含大量信息，代理必须准确识别 UI 组件及其功能。
    - 需要复杂的推理能力：
        - 规划多步交互（比如填表、提交表单）。
        - 记忆过去的操作，避免重复错误。
        - 处理低级执行（比如精确点击屏幕坐标、输入文本）。

* 数据瓶颈
    - 端到端模型需要大规模高质量数据：
    - 过去的模块化方法使用不同的数据集分别训练子模块，但端到端方法需要完整的操作轨迹数据（action traces），包括 UI 变化、点击位置、决策逻辑等。
    - 历史上缺乏此类数据，导致端到端方法难以泛化到复杂任务。


UI-TARS: 新一代原生 GUI 代理
----------------------------

* 为了克服上述挑战，本文提出了一种新模型UI-TARS，它有以下几个核心创新点
* 1）增强 GUI 视觉感知
    - 使用大规模 GUI 截图数据集，结合元数据（如 UI 组件类型、边界框、文本内容）。
    - 任务包括：
        - 元素描述（提供结构化的 UI 组件信息）。
        - 密集标注（dense captioning）（整体 UI 解析，包括层次结构、交互关系）。
        - 状态变化检测（识别 UI 界面的细微变化）。
        - 问答任务（提升对 GUI 视觉推理能力）。
        - 视觉标记（set-of-mark prompting）（将 UI 组件与特定功能关联）。

* 2）统一动作建模
    - 设计标准化的跨平台动作表示，确保不同平台上的相同操作（如“点击按钮”）具有一致性。
    - 训练大规模 GUI 操作轨迹数据，改进多步任务的执行能力。
    - 精准 UI 元素定位：数据集包含 UI 组件及其空间坐标，提高交互精度。

* 3）系统-2 推理（深度推理能力）
    - 收集6600万条 GUI 教程，用于训练模型进行任务推理。
    - 结合多种推理模式：
        - 任务分解（Task Decomposition）。
        - 里程碑识别（Milestone Recognition）。
        - 试错（Trial & Error）。
        - 反思（Reflection）。
    - 显式思考过程：UI-TARS 在每次操作前生成“思维轨迹”（类似人类思考后再执行）。

* 4）自适应学习（Iterative Refinement）
    - 动态数据采集：
        - 通过虚拟机探索 GUI 任务，生成自动标注的数据。
        - 结合规则过滤、VLM 评分、人类审核，确保数据质量。
    - 错误纠正机制：
        - 训练模型自我反思，纠正错误：
        - 错误修正（标注错误并提供正确操作）。
        - 任务恢复（模拟任务失败后如何重新调整）。


总结
----

* 本文介绍了 GUI 代理的发展趋势，并提出了新一代原生 GUI 代理 UI-TARS，它具备：
    - 纯视觉感知（不依赖 HTML）。
    - 端到端任务执行（整合感知、推理、记忆、行动）。
    - 自适应学习（动态数据采集+错误修正）。
* 实验表明，它在多个 GUI 任务中超越现有基准，为未来智能交互系统提供了更强的能力。


2. Evolution Path of GUI Agents
===============================

.. note:: 本章主要讨论 GUI 代理（Graphical User Interface Agents） 的发展历程及其在自动化工作流中的作用，重点介绍了从 基于规则的代理（Rule-based Agents）到 本地代理模型（Native Agent Model）乃至 主动学习和终身学习代理（Active and Lifelong Agents）的演进。

* GUI Agents 是自动化工作流的重要组成部分，能够简化重复性任务，提高工作效率，减少人为干预。它们最初是用来模拟人机交互的工具，但逐渐发展为能自主学习、适应任务、甚至作为“协作者”参与任务执行的智能系统。


2.1 Rule-based Agents
---------------------


Stage 1: Rule-based Agents
^^^^^^^^^^^^^^^^^^^^^^^^^^

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/o8trS1.png

    Figure 2:The evolution path for GUI agents.

- 典型代表：RPA（机器人流程自动化）。
- 这些代理依赖 预定义规则 来执行任务，比如点击按钮、输入数据等。
- 主要问题：
    - 刚性强：必须提前定义规则，无法适应新任务或界面变化。
    - 缺乏学习能力：不能从经验中学习，任何更改都需要人工干预。
    - 对 API 依赖高：需要直接访问 API 或系统权限，因此在受限环境下无法使用。

🔹 转向 GUI 交互方式
    - 为了克服这些局限，代理开始通过视觉方式与界面交互，而不依赖底层 API，这提高了适应性，使代理能够处理 陌生 UI 或新开发的软件。



2.2 From Modular Agent Framework to Native Agent Model
------------------------------------------------------

Stage 2: Agent Framework
^^^^^^^^^^^^^^^^^^^^^^^^

- 这些代理利用 大模型（LLM/VLM），如 GPT-4，来增强任务执行的灵活性。
- 典型技术：
    - AutoGPT、LangChain：集成多个外部工具，提高交互灵活性。
    - Agent Workflow Memory（AWM）：利用记忆模块来优化任务执行。
    - ReAct 框架：结合推理和多步规划，提高任务适应性。
    - 多代理协作：通过多个智能体合作提高成功率，如 MobileExperts。
  
🔹 主要问题
    - 仍依赖人工规则和工作流设计：每当任务、界面或使用场景变化时，都需要重新设计工作流和提示词（prompt）。
    - 难以自适应学习：无法基于新数据更新自身，只能依靠手工优化提示词或代码。
    - 模块协调难度大：不同模块（视觉解析、记忆、规划）需要额外的桥接代码，容易出错，难以大规模部署。



Stage 3: Native Agent Model
^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 这一阶段的代理是 数据驱动 的，不再依赖人工规则，而是通过端到端学习，统一感知、推理、记忆和行动，具备更强的泛化能力。

🔹 关键特性
    - 统一学习和适应性：代理可以从交互数据中学习，并在环境变化时自我更新，而非手工设计。
    - 减少人工干预：不需要人工编写详细规则，而是通过大量数据训练，自动学习任务流程。
    - 强泛化能力：知识内嵌于模型参数中，能够跨任务迁移，如从某些软件的 UI 操作中学习经验并应用到新任务。

🔹 代表性模型
    - Claude Computer-Use（Anthropic）
    - Aguvis
    - ShowUI
    - OS-Atlas
    - Octopus v2-4
  
* 这些模型主要基于 视觉-语言大模型（VLMs），专门针对 GUI 交互进行训练，提升了适应性和自主性。

2.3 Active and Lifelong Agent (Prospect)
----------------------------------------


Stage 4: Action and Lifelong Agent
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 目前代理仍然需要大量 人工标注数据 进行训练，而未来的目标是让代理主动学习，摆脱对人类监督的依赖。

🔹 关键特性
    - 自主探索环境：代理可以自己提出任务、尝试执行，并根据结果优化自身能力。
    - 自我奖励机制：通过强化学习，代理可以根据成功率自动调整行动策略，而不是依赖人工反馈。
    - 持续学习：像人类一样，代理可以通过 终身学习 迭代优化，不断扩展知识和技能。

🔹 核心区别
    - 本地代理模型（Native Agent） 仍然依赖人工提供数据进行训练。
    - 主动学习代理（Active Agents） 能够自己探索、执行任务并优化自身能力，摆脱人工数据限制。

总结
----

* GUI 代理从 基于规则的自动化 发展到 数据驱动的自主智能体，并逐步迈向 主动学习和终身进化。  
* 核心趋势是 减少人工干预，提升 泛化能力 和 自主学习能力，最终让代理能够在真实环境中持续优化自身。




3. Core Capabilities of Native Agent Model
==========================================

* 本章主要介绍了 Native Agent Model（原生智能体模型） 的核心能力及其评估方法，重点关注 GUI Agent（Graphical User Interface Agents，图形用户界面智能体） 在处理交互界面时的关键技术。



.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/cLJ3XM.png

    Figure 3:An overview of core capabilities and evaluation for GUI agents.


3.1 Core Capabilities
---------------------

原生智能体模型通过将以往代理框架中的模块化组件内化，逐步转向端到端（end-to-end）的结构。它的核心能力包括::

    - 感知（Perception）
    - 行动（Action）
    - 推理（Reasoning，包括系统1 & 系统2 思维）
    - 记忆（Memory）

1 感知(Perception)
^^^^^^^^^^^^^^^^^^^^

* GUI 代理的感知能力指的是对 用户界面（UI） 的准确理解，包括以下几点：

* 1.结构化文本（Structured Text）  
    - 早期的 GUI 代理只能处理文本输入，所以它们会将 GUI 页面转换为 HTML、DOM（文档对象模型）或可访问性树的结构化文本。  
    - 一些方法使用 DOM 精简（DOM distillation）技术减少噪音，提高对 UI 的理解。  

* 2.视觉截图（Visual Screenshot）  
    - 近年来，计算机视觉（CV）和视觉语言模型（VLM）取得进展，
    - 使智能体可以通过 OCR（光学字符识别） 和 GUI 元素检测模型（如 ICONNet、DINO）来理解屏幕上的视觉内容。  
    - 这些方法可以识别 GUI 中的交互元素，并通过描述这些元素来增强理解（如 SeeAct 模型）。  

* 3.综合界面建模（Comprehensive Interface Modeling）  
    - 结合结构化文本 + 视觉截图 + 语义元素来构建更完整的界面理解。  
    - 例如，Uground 训练 GUI 视觉锚定模型，而 OSCAR 结合 Windows API 提供的 A11y 树来增强语义理解。  

* 4.实时交互能力（Real-time Interaction）  
    - 界面是动态变化的，GUI 代理必须实时监控 UI 变化，如识别加载图标、处理未响应的情况等，以确保交互准确性。  
    - 这种感知能力可以让智能体保持对 UI 状态的感知，并做出相应的调整，提高交互体验。  

2 行动(Action)
^^^^^^^^^^^^^^^^

* GUI 代理的行动能力需要适应不同平台（如移动端、桌面端、网页）。关键点包括：
* 1.统一且多样的行动空间（Unified and Diverse Action Space）  
    - GUI 代理需要标准化各种操作，如 点击、输入、滚动、拖拽，并支持更复杂的 API 调用、代码解释和 CLI（命令行界面）操作。  
    - 动作可以分为：
        - 原子动作（Atomic Actions）：单步操作，如点击按钮。  
        - 组合动作（Compositional Actions）：一系列原子动作的组合，如填写表单并提交。  

* 2.坐标定位挑战（Challenges in Grounding Coordinates）  
    - 由于不同设备的 屏幕比例、UI 布局 各异，GUI 代理需要精准识别交互元素的位置，确保点击、滑动等操作准确无误。  
    - 需要先进的视觉技术来解析 GUI 元素的空间位置。  

3 推理(Reasoning): 系统 1 & 2 思维
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* GUI 代理的推理能力仿照人类的两种认知方式：
    - 系统 1（System 1）：直觉性、快速、自动化思维，适用于简单、重复的任务（如点击按钮、提交表单）。  
    - 系统 2（System 2）：慢速、分析性、逻辑性思维，适用于复杂任务（如错误排查、规划多步骤操作）。  

* 系统 1（快速推理）
    - 识别模式，执行直觉性操作，如“按回车提交表单”或“点击下一步按钮”。  
    - 适用于 UI 操作中简单、重复性的任务，但不能处理复杂任务。  

* 系统 2（深度推理）
    - 通过 思维链（Chain of Thought, CoT） 和 反思机制（Reflection） 进行复杂的多步骤规划。  
    - 关键能力：
        - 任务拆解（Task Decomposition）：将复杂任务分解成多个子任务，例如依次填写表单中的各个字段。  
        - 长期一致性（Long-term Consistency）：保持任务目标不变，防止中途偏离。  
        - 里程碑识别（Milestone Recognition）：跟踪任务进度，确保任务按计划进行。  
        - 试错机制（Trial and Error）：在不确定情况下尝试不同策略，提高成功率。  
        - 反思能力（Reflection）：总结经验，避免重复错误，提高未来决策能力。  

* UI-TARS 模型 强调系统 2 推理能力，使其能够：
    - 细化目标任务，分解为小步骤（CoT）。  
    - 采用反思机制，持续优化任务执行方式，提高复杂任务的完成能力。  

4 记忆(Memory)
^^^^^^^^^^^^^^^^

* GUI 代理的记忆系统分为：
    1. 短期记忆（Short-term Memory）
        - 存储任务执行中的临时信息，如当前操作历史、界面状态等，确保智能体能实时适应 UI 变化。  
    2. 长期记忆（Long-term Memory）：
        - 存储过去的任务经验、交互记录，提高未来任务执行的效率。
        - 例如：
        - OS-copilot 通过积累用户偏好和操作经验，提高任务执行效率。  
        - Cradle 通过长期记忆增强多任务处理能力。  
        - API 驱动的 Web 代理（Song et al., 2024） 利用长期知识优化复杂 Web 任务。  

* 原生智能体模型的长期记忆被编码到内部参数中，使其能够通过 上下文学习（ICL） 或 思维链推理（CoT reasoning） 进行任务回忆和优化。  



3.2 Capability Evaluation
-------------------------

* 为了衡量 GUI 代理的能力，研究者们设计了多个基准测试，包括：
* 1.感知评估（Perception Evaluation）
    - 测试智能体对 UI 的理解，如 VisualWebBench（评估网页理解）、WebSRC（网页结构理解）等。  
* 2.坐标定位评估（Grounding Evaluation）
    - 测试智能体在不同 GUI 布局中能否正确识别交互元素的坐标，如 ScreenSpot 系列。  
* 3.智能体能力评估（Agent Capability Evaluation）：
    - 离线评估（Offline Evaluation）：在静态环境下评估 GUI 代理的任务执行能力，如 AITW、Mind2Web 等。  
    - 在线评估（Online Evaluation）：在动态环境中测试 GUI 代理能否完成真实任务，如 BrowserGym。  



总结
----

* 这篇文章描述了原生智能体模型的核心能力及其评估方式，强调了：
    1. 感知能力：结合结构化文本、视觉截图、界面建模，实现精准 GUI 解析。  
    2. 行动能力：通过统一的动作空间，适配不同 GUI 交互方式，并解决坐标定位问题。  
    3. 推理能力：结合 系统 1（直觉决策） 和 系统 2（深度推理），提升任务执行能力。  
    4. 记忆能力：利用短期和长期记忆增强智能体的持续学习能力。  
    5. 评估标准：通过一系列基准测试，衡量智能体的 UI 感知、定位、决策和任务执行能力。  

* 这篇文章的核心思想是
    - 原生智能体模型应该具备更接近人类认知的感知、行动、推理和记忆能力，以实现更智能的 GUI 交互。


4. UI-TARS
==========

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/6a2oK4.png

    Figure 4: Overview of UI-TARS. We illustrate the architecture of the model and its core capabilities.

4.1 Architecture Overview
-------------------------

- UI-TARS 通过观察（设备截图）、推理（思考）和执行动作来完成任务。
- 该模型采用迭代交互方式，即：  
    1. 读取任务指令
    2. 接收当前屏幕截图（观察 :math:`o_i`）
    3. 结合历史交互信息（ :math:`(o_1, a_1), (o_2, a_2), ..., (o_{i-1}, a_{i-1})`）
    4. 生成下一步动作（ :math:`a_i`）并执行
    5. 设备返回新的观察（ :math:`o_{i+1}`）
    6. 反复循环，直到任务完成

* 引入“系统 2”推理
    - 传统 GUI 代理模型可能直接从观察 (:math:`o_i`) 生成动作 (:math:`a_i`)，
    - 但 UI-TARS 在此基础上增加了“思考步骤（thoughts, t_i）”：
        - 在执行动作 `a_i` 之前，UI-TARS 先生成 `t_i`，即一系列推理思维。
        - 这种思维类似于人类的“系统 2 思考”（即深度、慎重的决策）。
        - 通过这种方式，UI-TARS 具备了更强的推理能力，可以在复杂或模糊的环境下做出更合理的决策。
    - 这一机制受到 ReAct 框架（Yao et al., 2023）的启发，但相比 ReAct，UI-TARS 的思维机制更加结构化和目标导向。

* 高效的记忆管理
    - UI-TARS 需要在有限的token 预算内（如 32k 序列长度）保持高效运行，因此：
    - 只保留最近 `N` 轮交互（即 :math:`(o_{n-i}, t_{n-i}, a_{n-i})_{i=1}^N`）。
    - 旧的交互不会完全丢弃，而是作为短期记忆存储，确保模型的长期学习能力。

* 总结
    - UI-TARS 通过截图感知、思考推理、迭代交互来自主执行任务。
    - 其“系统 2”推理能力使其在复杂环境下更可靠。
    - 通过限制历史观察数量，模型能在高效运行的同时维持长期记忆能力。

4.2 Enhancing GUI Perception
----------------------------

* 本章主要讲如何增强 UI-TARS 对 GUI（图形用户界面）的感知能力，即让它更好地理解界面上的各种元素、布局和交互状态。

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/aioD4k.png

    Figure 5:Data example of perception and grounding data.

* GUI 感知挑战
    - 1)截图稀缺性：普通场景的图片数据很多，但 GUI 特定的截图数据较少，导致训练数据有限。  
    - 2)信息密度高，精确度要求高：
        - GUI 图像比普通照片更结构化，一个界面可能包含上百个 UI 元素（如按钮、文本框）。
        - 这些元素的空间关系和功能交互很复杂，模型必须同时理解它们的位置、大小、层级等信息。
        - 许多 UI 元素（如 10×10 像素的小图标）很小，传统模型很难准确识别和定位。
    - 解决方案：
        - UI-TARS 直接处理 GUI 截图，而不依赖传统的模块化感知模型
        - 利用大规模数据集训练，使其具备更高效的感知能力。


* 数据收集方法
    - 1)构建大规模 GUI 数据集：
        - 采集 网站、APP、操作系统 的截图及其元数据。
        - 使用解析工具自动提取 UI 元素信息（如元素类型、层级、边界框、文本内容）。
        - 结合 自动爬取 + 人工探索，收集主界面和嵌套界面。
    - 2)自底向上的数据构建方式：
        - 先识别小的 UI 元素，再逐步扩展到整个界面，确保既能精确识别组件，又能理解复杂布局。
    - 核心数据类型（图 5）：
        1. 元素描述
        2. 密集标注（Dense Captioning）
        3. 状态变化描述
        4. 问答（QA）
        5. Set-of-Mark（SoM）可视标记

* 关键数据增强技术
    - 1)元素描述
        - 为了更准确地识别 UI 元素，UI-TARS 生成 结构化的元素描述，包括：
        - 1.元素类型（按钮、文本框、滚动条等）。
        - 2.视觉描述（形状、颜色、文本内容、样式）。
        - 3.位置信息（相对其他元素的位置）。
        - 4.功能描述（该元素的作用和交互方式）。
    - 2)密集标注（Dense Captioning）
        - 目标：生成整个界面的详细描述，包括元素、布局、空间关系。
        - 具体方法：
            - 先对每个 UI 元素生成单独描述。
            - 对嵌入的图片（缺乏元数据）生成视觉描述。
            - 结合这些信息，生成完整的 GUI 结构化描述。
        - 训练方式：UI-TARS 仅接收图片输入，目标是生成对应的详细描述。
    - 3)状态变化描述（State Transition Captioning）
        - 目标：理解 UI 状态的变化，区分用户操作 vs. 非交互变化（如动画、屏幕刷新）。
        - 方法：
            - 让 UI-TARS 观察两个连续的截图，分析它们的差异：
            - 交互变化（用户点击按钮、输入文本）。
            - 非交互变化（界面动画、内容刷新）。
        - 训练 UI-TARS 预测视觉变化及其可能的原因。
    - 4)问答（QA）
        - 增强 UI-TARS 的推理能力，让它能回答涉及 GUI 理解的问题，例如：
        - 界面内容理解（某个按钮在哪里？）
        - 元素识别（这个界面里有哪些文本框？）
        - 关系推理（哪个元素与 "提交" 按钮最接近？）
    - 5) Set-of-Mark (SoM) 视觉标记
        - 在 GUI 截图上绘制可视化标记（不同形状、颜色、大小）。
        - UI-TARS 通过这些标记，更清晰地关联 UI 元素，从而提高界面理解能力。
        - 与其他任务结合：
            - 例如，在 QA 任务中，模型可以被训练成描述被标记的元素。

* 主要贡献
    - 构建了大规模 GUI 数据集，填补了数据稀缺问题。
    - 采用多种数据增强技术（密集标注、状态变化、QA、视觉标记）。
    - 提升 UI-TARS 的 GUI 认知能力，让它不仅能识别界面元素，还能理解界面结构、状态变化，并进行推理。

* 总结
    - 这部分内容的核心是 UI-TARS 如何更好地理解 GUI 界面：
    - 通过大规模截图数据训练，使其掌握 GUI 结构和交互方式。
    - 采用元素描述、密集标注、状态变化建模、QA 以及 SoM 视觉标记等方法，让它能准确识别、描述和推理 GUI 界面。


4.3 Unified Action Modeling and Grounding
-----------------------------------------

* 本章主要讲的是 GUI 自动化交互系统（UI-TARS）如何建模和执行用户操作，包括 统一的操作空间（Unified Action Space）、操作轨迹收集（Action Trace Collection） 和 提升操作定位能力（Improving Grounding Ability）。 

1. 统一的操作空间(Unified Action Space)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 目标：定义一个标准化的操作集合，使得 AI 代理能跨平台（桌面、移动、Web）执行相同的操作。  

* 跨平台标准化：
    - 比如在 Windows 叫 “click”，在移动端叫 “tap”，但它们的本质是一样的——点击一个 GUI 元素。
    - UI-TARS 统一了这些语义相同的操作。
* 平台专属操作：不同平台有独特操作，比如：
    - 桌面：快捷键（Hotkey）、右键点击（RightClick）
    - 移动端：长按（LongPress）、返回（PressBack）
* 终止操作：
    - `Finished()`: 任务完成
    - `CallUser()`: 需要用户介入（比如登录验证）

* 这个统一的操作定义让 UI-TARS 可以在不同系统上执行任务，同时保持一致性。

2. 操作轨迹收集(Action Trace Collection)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 目标：让 UI-TARS 通过模仿人类行为（行为克隆，Behavior Cloning）学习如何执行多步任务，而不仅仅是单个动作。
* 多步骤轨迹数据的可用性有限，而这些数据在历史上一直记录不足且很稀疏。为了解决这个问题，我们依赖两个主要数据源
* 数据来源：
    1. 人工标注的数据：
        - 使用专门的标注工具收集用户在各种软件、网站上的操作数据。
        - 任务先由专家制定，并由标注员执行，确保高质量。
    2. 开源数据集：
        - 整合了多个已有的数据集（如 MM-Mind2Web、GUIAct、AndroidControl），
        - 并将不同的数据格式标准化，使它们能够兼容 UI-TARS 的操作定义。

* 数据示例：
    - Web 端：采集了 1480 万 条操作轨迹，平均每条轨迹有 6.7 步操作。
    - 移动端：有 250 万 条轨迹，每条 9.6 步操作。
    - 桌面端：数据相对较少，仅 110 万 条轨迹。

这些数据让 UI-TARS 能够学会如何完成 完整的任务序列，而不仅仅是执行单个操作，比如：
> 打开浏览器 → 搜索关键词 → 点击搜索结果 → 滚动页面 → 填写表单 → 提交表单  


.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/YE3KUM.png

    Table 1: Unified action space for different platforms.

3. 提升操作定位能力(Improving Grounding Ability)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

* 目标：确保 UI-TARS 能够精准定位 GUI 界面中的元素，进行点击、拖拽等操作。
* 核心问题：
    - 传统方法：依赖单独的目标检测模型来识别 UI 元素，但可能不够精准。
    - UI-TARS 方案：
        - 直接从 截图和元数据 预测 GUI 元素的位置。
        - 采用 归一化坐标，确保不同分辨率的屏幕都能适配。
        - 例如，给定描述 “右上角的红色 Submit 按钮”，模型需要预测它的坐标。
* 数据增强：
    - 结合多个开源数据集（如 SeeClick、MultiUI、Rico-SCA 等），确保 UI-TARS 能处理 不同风格的 GUI 设计。
    - 通过 计算 GUI 元素的中心点坐标，让模型学会精准定位。
* 效果：
    - 这些改进提升了 UI-TARS 在 GUI 交互中的 精度和适应性，让它能更可靠地模拟人类的操作流程。

总结
^^^^

1. 统一操作空间
    - 让 UI-TARS 能跨平台执行 GUI 操作（点击、拖拽、滚动等）。
    - 设计终止操作 ``Finished()`` 和 ``CallUser()``。
2. 操作轨迹收集
    - 训练模型学习 多步任务执行，不仅是单步动作。
    - 结合 人工标注数据 + 开源数据集，提升学习效果。
3. 操作定位
    - 让模型能精准识别 GUI 元素的位置，提高操作的稳定性。
    - 采用截图 + 元数据的方法，确保模型能跨设备适配 GUI 变化。

应用场景
^^^^^^^^

- 自动化软件测试（UI 自动化操作）
- 智能 UI 助手（自动填表、操作电脑或手机界面）
- 无障碍技术（帮助视障用户操作 GUI）
- AI 自动执行 Web 操作（如自动预定机票、填写表单等）




4.4 Infusing System-2 Reasoning
-------------------------------

* 本章主要讲的是如何让 UI-TARS（一个自动 GUI 交互的 AI 系统）具备更高级的推理能力（System-2 级别的推理）。
* 它通过两个核心方法实现这一目标::

    1. 通过 GUI 教程增强推理能力（Reasoning Enrichment with GUI Tutorials）
    2. 通过思维增强（Thought Augmentation）来激发推理能力  

Reasoning Enrichment with GUI Tutorials
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- 使用公开的图文教程（如 MINT 和 OmniCorpus 数据集）来构建 AI 的 GUI 交互知识，并学习任务执行的逻辑推理模式。  
- 由于这些数据集含有大量噪声，研究者设计了一套三步筛选流程：
    1. 粗粒度筛选：使用 fastText 训练分类器，初步筛选出类似教程的数据。
    2. 精细筛选：用 LLM（大模型）进一步去除误判，确保只保留真正的 GUI 教程。
    3. 去重和优化：去掉重复数据、广告，并让 LLM 重新表述文本，提高质量。  
- 最终，研究者构建了一个包含 600 万个高质量 GUI 教程的数据集，每个教程平均包含 510,510 个文本 token 和 3.3 张图片。

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/G6AHyL.png

    Figure 6:Various reasoning patterns in our augmented thought.


Reasoning Stimulation with Thought Augmentation
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

- 原始的 UI 交互数据集（见 4.3 章节）主要记录的是观察（observation）和动作（action）的序列，如 :math:`(o_{i-1}, a_{i-1}, o_{i}, a_{i}, …)` ，但缺少明确的推理过程。  
- 研究者通过思维标注（thought annotation）的方式，在数据中加入推理过程（thought），形成新的数据格式：:math:`(o_{i-1}, t_{i-1}, a_{i-1}, o_{i}, t_{i}, a_{i}, …)`
    - 其中 t 代表系统的推理过程，使得模型可以显式表达自己的决策逻辑，更好地对齐任务目标。  
- 推理标注的方法：
    1. ActRe 方法：
        - 使用 VLM（视觉语言模型）基于前序观察、推理和动作，迭代生成新的推理步骤，确保推理是逻辑自洽的。
        - 例如： :math:`t_n = VLM(\text{instruction}, (o_1, t_1, a_1), (o_2, t_2, a_2), ..., o_n, a_n)`
        - 目标是让 AI 具备更高级的推理能力（System-2），包括：
            - 任务分解（Task Decomposition）：把复杂任务拆解成小步骤。
            - 长期一致性（Long-term Consistency）：确保模型在多步骤任务中保持一致目标。
            - 里程碑识别（Milestone Recognition）：识别任务中的关键进展。
            - 试错能力（Trial and Error）：在不确定的情况下进行假设、测试和调整。
            - 反思（Reflection）：识别错误并自我调整，提高适应能力。  
    2. Thought Bootstrapping 方法：  
        - 由于 ActRe 方法是基于已知的正确动作生成推理，可能会导致模型“迎合”正确答案，而不是基于真正的推理过程决定行动。  
        - 为了避免这种问题，研究者使用 Bootstrapping 方法，在不知道正确答案的情况下生成推理，
        - 然后从多个推理-动作对中选择真正导致正确决策的推理，使推理更具因果关系。  
- 双语标注：研究者在中英文数据上同时进行标注，以提升模型的跨语言泛化能力。  
- 训练策略：尽管所有数据都带有推理标注，但在训练时仍然保留了一部分只包含观察和动作的数据，以提高泛化能力。  


关键理解
^^^^^^^^

- System-2 推理：AI 不仅仅依靠直觉（System-1），还要具备复杂的逻辑推理能力。  
- 数据增强：通过 GUI 教程和推理标注，帮助 AI 学会分步执行任务，并能解释自己的决策过程。  
- 思维增强方法：
    - ActRe（基于 VLM 推理）：模拟系统的思考过程，让推理更符合逻辑。
    - Bootstrapping（去除迎合答案的偏差）：确保推理是因果性的，而不是仅仅匹配正确答案。  
- 多种推理模式：
    - 任务分解：拆解复杂任务。
    - 长期一致性：保持目标不变。
    - 里程碑识别：理解任务阶段。
    - 试错：在不确定的情况下做出合理决策。
    - 反思：发现并修正错误。  

总结
^^^^

* 这部分的核心思想是让 UI-TARS 具备更强的逻辑推理能力，而不仅仅是执行 GUI 任务。
* 研究者通过高质量 GUI 教程和思维增强技术，让 AI 在 GUI 操作中能进行有条理的思考、拆解任务、反思错误，并保持长期目标一致性，从而更好地完成复杂交互任务。


4.5 Learning from Prior Experience in Long-term Memory
------------------------------------------------------

* 本章主要讨论如何让 GUI 代理（如 UI-TARS）通过长期记忆和自我调整来改进自身能力，以更高效地完成 GUI 任务。

* 为什么 GUI 代理需要长期记忆？
    - 相比于 LLM 拥有大量的文本数据用于训练，GUI 代理缺乏大规模、标准化的真实交互数据，导致它们难以有效扩展和泛化。
    - 为了解决这个问题，UI-TARS 采用长期记忆来存储并学习过去的交互经验，使其能够更适应不同任务。

* 如何利用长期记忆改进 UI-TARS？
    - UI-TARS 通过 半自动化数据采集、筛选和优化 过程来学习：
    1. 数据采集：记录与真实设备的交互过程。
    2. 数据筛选：去除低质量数据，仅保留有价值的交互信息。
    3. 长期记忆存储：不断积累经验，优化未来的操作。


Online Trace Bootstrapping
^^^^^^^^^^^^^^^^^^^^^^^^^^

* 核心思想：通过迭代训练的方式，让模型不断改进。
* 具体流程：

1. 执行任务并生成操作轨迹：
    - 任务目标（instruction）可以是人类标注的，也可以是模型生成的。
    - 代理（M_n）在 GUI 环境中执行任务，产生一组原始操作轨迹（𝒯_raw）。
  
2. 数据筛选与优化：
    - 采用多层过滤机制，去掉低质量数据：
        1. 基于规则的奖励（如删除冗余操作）。
        2. VLM 评分（视觉语言模型给轨迹打分，低分轨迹被删除）。
        3. 人工审核（审查错误步骤，保留有效部分）。
    - 过滤后的数据集（𝒯_filtered）用于下一轮微调（FineTune）。

3. 迭代优化：
    - 每一轮优化后，模型（:math:`M_{n+1}`）变得更智能，并用于生成新的交互数据，不断扩展和优化 GUI 任务的数据集。


.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/dlMtW1.png

    Figure 7:Overview of the online bootstrapping process.

Reflection Tuning
^^^^^^^^^^^^^^^^^

* 问题：在实际部署中，代理可能会陷入错误循环（如重复点击无响应按钮），但大部分离线训练数据是理想化的，缺乏错误及其纠正示例，使得代理难以学会如何纠正错误。

* 解决方案：
    1. 让模型经历真实错误并学会纠正：
        - 记录模型在执行过程中出现的错误(t, a)，并由人工标注正确的思考方式（:math:`t^*`）和正确的操作（:math:`a^*`）。
        - 形成“错误-纠正”数据对，训练代理如何从错误中恢复。
    2. 进一步扩展反思能力：
        - 如果错误已经发生，下一步应该如何补救？（如误关网页后，应重新打开）。
        - 让代理学会在出错后进行自我调整，而不是陷入死循环。

Agent DPO
^^^^^^^^^

* 问题：SFT（监督微调）只使用正确数据，而忽略错误数据，导致模型无法有效避免错误。
* 解决方案：
    1. 采用 DPO（Direct Preference Optimization，直接偏好优化），同时利用错误和正确数据来优化模型决策。
    2. 核心思想：
        - 在某个状态 s_τ 下，代理执行了错误操作 a_τ，后来被修正为正确的 a_τ'。
        - 计算两者的偏好概率 P_θ(a_τ' ≻ a_τ | s_τ)，确保模型更倾向于选择 a_τ' 而非 a_τ。
        - 通过优化 DPO 目标函数，使模型倾向于更优决策。

总结
^^^^

- 长期记忆：UI-TARS 通过存储和学习历史数据，提高任务泛化能力。
- 在线数据增强：代理不断执行任务，生成新数据，并通过筛选提升质量。
- 反思调整：模型学习如何识别错误并进行自我纠正，提高任务适应性。
- 直接偏好优化（DPO）：利用错误数据优化决策过程，使模型避免低效操作。

* 最终，UI-TARS 通过这些方法，逐步提升 GUI 任务的执行能力，使其更加智能化和高效。




4.6 Training
------------

* 本章描述了 UI-TARS 训练的 三阶段流程，其目的是 提升模型在 GUI 交互任务上的能力，并确保与现有方法（如 Aguvis 和 OS-Atlas）进行公平对比。
* 模型的基础是 Qwen-2-VL（一个视觉语言模型，VLM），整个训练过程中使用了 大约 500 亿个 token 的数据。每个阶段都逐步引入 更高质量的数据，以提升模型在 复杂推理任务 上的表现。  


训练的三大阶段
^^^^^^^^^^^^^^

1. 持续预训练阶段（Continual Pre-training Phase）
    - 使用 完整数据集（除反思调优数据外）进行训练，采用 固定学习率。
    - 目标是让模型学习 GUI 交互的基础知识：
        - 感知（perception）：理解 GUI 元素的视觉和语义信息。
        - 对齐（grounding）：将文本或指令与 GUI 组件对应起来。
        - 动作轨迹（action traces）：学习如何执行 GUI 操作，如点击、滑动等。

2. 退火阶段(Annealing Phase)
    - 选取 高质量子集（包括感知、对齐、动作轨迹和反思调优数据）。
    - 退火过程的作用：
        - 调整学习动态，让模型更聚焦于关键任务。
        - 优化决策策略，让其更适应真实的 GUI 交互。
    - 经过此阶段的模型被命名为 UI-TARS-SFT（Supervised Fine-Tuning）。

3. DPO 阶段（DPO Phase）
    - 使用 在线自举（bootstrapping）数据 训练，数据包含 带标注的反思对比数据（reflective pairs）。
    - 目标是让模型强化 优选决策（optimal actions），并惩罚 次优决策（suboptimal actions）。
    - 这个阶段提升模型的 上下文感知能力，使其能做出更精准的 GUI 操作决策。
    - 经过此阶段的最终模型被命名为 UI-TARS-DPO。


总结
^^^^

- 训练从 全面学习 → 精调优化 → 强化决策能力，层层递进。
- UI-TARS-SFT 是经过精调的版本，UI-TARS-DPO 则是最终版本，具有更强的决策优化能力。
- 关键点是 逐步提升数据质量，确保模型能适应 真实 GUI 交互场景。


5. Experiment
=============

* 本章关于 UI-TARS（一个用于自动 GUI 交互的 AI 模型）的实验评估部分，主要探讨其在不同任务上的表现，包括感知、定位和代理能力。
* 关键结论
    - UI-TARS 在 感知、定位和代理能力 方面全面超越了现有 SOTA 模型（GPT-4o、Claude、Aguvis 等）。
    - 更大规模的模型（UI-TARS-72B）在复杂任务中的表现更优，尤其在在线交互任务中优势更加明显。
    - DPO 训练策略 显著提升了 UI-TARS 在动态任务中的推理和执行能力。
    - 在网页和移动端操作任务上，UI-TARS 展现了极高的适应性和泛化能力，说明其在 自动 GUI 交互 领域具有广泛应用潜力。

6. Conclusion
=============

* 在本文中，我们介绍了 UI-TARS，这是一种将感知、动作、推理和记忆集成到可扩展且自适应框架中的原生 GUI 代理模型。
* UI-TARS 在 OSWorld 等具有挑战性的基准测试中实现了最先进的性能，优于 Claude 和 GPT-4o 等现有系统。
* 我们介绍了几项新颖的创新，包括增强感知、统一动作建模、系统 2 推理和使用在线跟踪进行迭代细化，所有这些创新都使代理能够在最少的人为监督下有效处理复杂的 GUI 任务。
* 我们还回顾了 GUI 代理的演进路径，从基于规则的系统到自适应原生模型。
* 我们根据人为干预的程度和泛化能力将开发过程划分为关键阶段，强调从基于文本的方法到纯视觉、端到端代理模型的过渡。
* 我们还探索了原生代理模型的核心功能，包括感知、动作、推理和记忆，这些功能为 GUI 代理的未来发展奠定了基础。
* 展望未来，虽然原生代理代表着一次重大的飞跃，但未来在于主动学习和终身学习的融合，代理通过持续的现实世界互动自主地推动自己的学习。





























