# 2410.00037_Moshi: a speech-text foundation model for real-time dialogue

* [https://arxiv.org/abs/2410.00037](https://arxiv.org/abs/2410.00037)
* GitHub: [github.com/kyutai-labs/moshi](https://github.com/kyutai-labs/moshi)
* 组织: Kyutai(法国人工智能研究实验室)
* 引用: 157(2025-06-30)
* 在线体验：[https://moshi.chat](https://moshi.chat)





## Abstract

* Keywords: speech, text, multimodal, foundation, spoken dialogue

* Moshi 是一个语音对话模型，它能实现真正像人一样的语音对话，不需要传统的“语音活动检测→语音识别→文本对话→语音合成”的流程。
* 传统方法的问题是：
    1. 延迟高，互动不流畅；
    2. 中间用文字交流，会丢失情绪、语气等非语言信息；
    3. 需要人为分清“谁在说话”，但现实对话中常有打断、重叠等情况。
* Moshi 用“语音生成语音”的方式解决这些问题：
    * 它从语言模型出发，直接生成语音；
    * 模型同时建模自己和用户的语音流，实现并行对话，不需要区分轮次；
    * 还加入了“内心独白”机制，先生成文字，再辅助生成语音，提升语言质量；
    * 最终实现了 160–200 毫秒低延迟的实时双工语音对话（双方可同时说话）。


## 1.Introduction

### ✅ **背景问题**

* 传统语音助手（如 Siri、Alexa）通常流程是：
    1. 识别唤醒词；
    2. 将语音转为文字（ASR）；
    3. 文字理解（NLU）；
    4. 生成回答文字（NLG）；
    5. 再转回语音（TTS）；
* 这个流程存在三个问题：
    * **延迟高**（通常几秒，而自然对话反应时间是几百毫秒）；
    * **信息丢失**（只处理文本，忽略情绪、语调、环境声等非文字信息）；
    * **回合式对话**（你说完我说，不能打断、重叠说话，不自然）。

### ✅ **Moshi的创新点**

1. **语音直达语音（Speech-to-Speech）**：
      * 不再中间必须转成文字；
      * 能处理情绪、口音、背景声等非文本内容；
      * 实时处理对话，延迟只有 160 毫秒，比人类平均反应更快。

2. **双流处理器（Multi-stream）**：
      * 同时处理输入和输出语音；
      * 不再区分“谁先说”，可以自然重叠说话、打断等。

3. **三个主要组件**：
      * **Helium**：7B 参数的大语言模型，负责文本推理；
      * **Mimi**：将语音转成离散音频token并还原的神经音频编码器；
      * **Moshi 架构**：一个层次化、流式音频语言模型，支持长时间上下文对话（比如连续5分钟）。

4. **Inner Monologue 技术**：
      * 模型先预测文本，再生成音频；
      * 提高语音的准确性与连贯性，还能实现流式语音识别（ASR）和语音合成（TTS）。


### ✅ **效果**

* Moshi 在语音理解、回答准确性、语音质量等方面效果领先；
* 同时支持流式交互，能自然应对重叠语音；
* 是第一个“**全双工**”（始终在听和说）的语音对话大模型。


---



## 2.Related Work

### 一、语音语言模型（Audio Language Modeling）

```note
语音语言模型最初用于理解语音，现在正逐步发展为可以像语言模型一样生成语音的系统，
关键技术包括自监督学习、语音token化和分层建模，目标是实现高质量、可控、实时的语音生成。
```


* 早期的语音基础模型，如ASR、说话人验证和语音分类，主要借助**自监督学习** 取得进展。
    * 自监督学习：允许学习通用且具有判别性的语音表示
* 随着生成式 LLM 的兴起
    * 后续研究开始将语音理解和语音生成结合，引入“音频语言建模”概念：
    * 把语音编码为离散的“语音token”，就能把语音生成看作“语言建模”
* **AudioLM**结合了**语义 token(semantic token)** 与神经音频编解码器(neural audio codec)产生的**声学 token(acoustic token)**
    * 使得模型能表达任意声音、录音条件和非语音声音。
    * 这类音频语言模型重塑了语音生成领域的技术水平，覆盖从文本转语音（TTS）到语音翻译和语音增强等任务。
* **主要挑战**：
   * 音频数据长且复杂，不如文本紧凑；
   * Autoregressive 方式预测多个 token 时计算量很大；
   * 扩展到几分钟长度的语音很难。
* **应对挑战的方法**：
   * **层次化建模**（逐层生成语义 + 声学 token）；
   * **延迟并行预测**（Copet 2023）减少生成步骤；
   * **小型嵌套Transformer**提升每步建模效率；
   * **扩散模型**虽能压缩表示，但不能实时生成。
* **目前限制**：
   * 仅语音模型学不到事实知识和推理能力；
   * 所以发展出**语音-文本联合模型**，希望结合文本模型的知识与语音模型的生成能力。



### 二、语音-文本联合模型（Speech-text Models）

```note
Moshi 提出的 Inner Monologue 机制，让语音-文本模型可以统一建模、实时生成、多说话人交互，并保持语音质量，是目前最先进的实时语音对话方案之一。
```

* 定义：什么是语音-文本联合模型？
    * 这些模型通常**以一个已经训练好的文本语言模型为基础**，通过微调来让它能处理语音
    * 比如：
        * 预测语音（生成语音）；
        * 同时训练语音和文本任务（如语音识别ASR、语音合成TTS、语音翻译等）。

* 常见模型的做法：
    * **AudioPaLM、VoxTLM**：把语音转换为语义token，加入到文本模型中，进行联合训练。
    * **Spirit-LM**：在一个序列中通过对齐信息，在语音token和文本token之间切换，从而学到统一的语言理解。
    * **Spectron、SpeechGPT**：采用“**链式生成（Chain-of-Modality）**”，
        * 先生成文字，再用这些文字作为提示生成语音。
        * 好处是能更精准控制语音内容；
        * 缺点是**不能实时对话**，因为说话前必须先生成整段文字。
    * 也有模型（如 PSLM）尝试**并行生成文本与语音token**，但会牺牲语音质量。

* Moshi 的创新点 —— **Inner Monologue**：
    * 提出一种新的结构，叫做“内心独白”（**Inner Monologue**）：
      * 把语音分为三层：**文本 → 语义 → 声学 token**，层层递进。
      * 每次只生成一帧语音，**支持实时生成**；
      * 同时支持多种声线、情绪、说话条件；
      * 不需要像 Chain-of-Modality 那样等完整文字生成完。
* Moshi 的技术优势：
    * 不像 Spirit-LM 那样需要在语音/文本之间切换，Moshi 可以统一建模；
    * 不像 PSLM 那样并行预测，Moshi 的**分层结构避免序列太长**；
    * 支持**流式TTS/ASR**；
    * Moshi 采用**多流架构**，可以建模**说话重叠、中断、插话** 等复杂对话现象。


### 三、语音对话模型（Spoken Dialogue Models）

```note
Moshi 是第一个真正支持实时、全双工、有知识、有情感表达能力的语音对话大模型。
```

### 📌语音对话的挑战：

* 语音对话很难做，因为它要求模型：
    1. **实时运行**（边听边说，不能卡顿）
    2. **语音对语音**交流（不仅识别说什么，还要捕捉情绪、语气等“副语言信息”）
    3. **具备知识和推理能力**（能给出有逻辑、有帮助的回答）
    4. **支持“全双工”交流**（用户和系统可以同时说话，不像传统对话那样轮流说）

### 📌现有方法的不足：

* **Spectron**：依赖文本大模型，有知识能力，但不能实时说话（必须先生成完整文本，再转语音）。
* **PSLM**：语音和文本同时生成，能更快，但语音质量差，还依赖ASR，会丢失语气等信息。
* **Wang等人提出的方法**：ASR + 文本生成 + TTS 串联工作，虽然更复杂，但也不能很好处理说话重叠。
* **dGSLM**：唯一能双向同时说话的模型，但只是原型，不能在线运行，也没有语音知识或丰富音色。

### ✅Moshi的优势：

* **真正的全双工对话**：同时监听和说话，支持打断、重叠等真实交流场景。
* **多模态理解与生成**：同时处理语义和声学信息，既懂语言，又能表达情绪/语调。
* **支持实时生成**：每一步都能输出一个语音帧，无需先生成文本。
* **具备知识能力**：得益于底层文本模型（Helium）的预训练。
* **情绪与语音多样性强**：能生成不同情绪、声线、场景下的语音。


## 3.Model

### 3.1 Overview


![](https://img.zhaoweiguo.com/uPic/2025/06/OucvjN.png)

Figure 1:Overview of Moshi. Moshi is a speech-text foundation model which enables real-time spoken dialogue. The main components of Moshi’s architecture are: a bespoke text language model backbone (Helium, see Section 3.2); a neural audio codec with residual vector quantization and with semantic knowledge distilled from a self-supervised speech model (Mimi, Section 3.3); the streaming, hierarchical generation of semantic and acoustic tokens for both the user and Moshi, along with time-aligned text tokens for Moshi when using Inner Monologue (Section 3.4).


#### 🔧 架构主要包括 5 个核心模块：

1. **Helium（文本大模型）**
   * 从零构建的文本语言模型，提供强大的推理能力。
   * 类似 ChatGPT 的文本理解和生成能力。

2. **Mimi（音频编码器）**
   * 一个神经音频编解码器，将语音转换为语义+声学 token。
   * 使用残差向量量化（RVQ）和知识蒸馏，压缩语音信息并保留关键特征。

3. **Inner Monologue（内部独白机制）**
   * 在训练和生成过程中，同时处理文本、语义和声学 token。
   * 让模型既能生成语音，又能利用文本的语言知识。

4. **Multi-stream（多流架构）**
   * 同时处理“用户的语音输入”和“Moshi 的语音输出”。
   * 不需要明确的轮流说话，支持**双向实时语音对话**（full-duplex）。

5. **Depth Transformer（流式 Transformer）**
   * 用于处理长序列的语音和文本 token。
   * 支持分层自回归建模和**流式推理**（边输入边生成）。



### 3.2 The Helium Text Language Model

#### 3.2.1 Architecture

* **Helium** 是一个基于 Transformer 的自回归文本语言模型（类似 GPT），它有以下特点：
* **改进架构**：
    * 使用 **RMSNorm** 替代 LayerNorm，提升训练稳定性。
    * 使用 **RoPE 位置编码**，更好支持长上下文。
    * 使用 **FlashAttention** 加速训练。
    * **前馈网络改成 GLU（门控线性单元）+ SiLU 激活函数**，增强非线性表达能力。
* **分词器**：基于 SentencePiece 的 unigram 模型，词表大小为 32,000，专为英文设计，数字被拆成单个字符。
* **优化器**：使用 AdamW 和 **余弦学习率衰减**。
* **上下文长度**：最大 4096 tokens。

> Helium 是 Moshi 模型的“文字理解大脑”，也是它预训练的基础。


![](https://img.zhaoweiguo.com/uPic/2025/06/46qnfd.jpg)

Table 1: Models’ hyper-parameters.


#### 3.2.2 Pre-training data filtering

* 由于高质量文本数据量有限（如 Wikipedia、StackExchange），Helium 使用了大量网页爬虫数据（CommonCrawl）
* 但这些数据需要清洗：
    1. **去重**：
       * 使用 FNV-1a 哈希和 Bloom Filter 去除重复行；
       * 用 fastText 分类器进一步清除“模糊重复”的内容（例如网页模板）。
    2. **语言识别**：
       * 用 fastText 判断文档语言，只保留英文，要求置信度 > 0.85。
    3. **质量过滤**：
       * 训练 fastText 分类器，学习区分高质量 vs 随机网页行；
       * 打分并过滤掉低质量文档，保留更接近 Wikipedia、Wikibooks 等高质量来源的文本。



### 3.3 Audio Tokenization

```note
Mimi 是一个能同时编码语义与声学信息、并支持流式处理的神经音频编码器。
```

![](https://img.zhaoweiguo.com/uPic/2025/06/FlCAyU.png)

Figure 2:Architecture and training of Mimi, our neural audio codec, with its split residual vector quantization. During training (blue part, top), we distill non-causal embeddings from WavLM (Chen et al., 2022) into a single vector quantizer which produces semantic tokens, and is combined with separate acoustic tokens for reconstruction.

* **Mimi 是一个神经音频编码器（codec）**，用于把连续的语音波形信号转换为离散的 **音频 token**（tokenization），供后续模型处理。这类似于图像的离散化（如 VQ-VAE）或文字的分词。
* 它的核心功能包括：
    * **高保真语音还原**：音质清晰，适合语音合成；
    * **嵌入语义信息**：适合语言理解/对话生成任务；
    * **可实时推理**：可流式地进行语音 token 编码和解码。

* **两类音频 token **
    * **Acoustic token（声学 token）**：
        * 表示语音的细节信息；
        * 可以重建高质量音频；
        * 用于 TTS 或音乐生成等任务；
    * **Semantic token（语义 token）**：
        * 表示语音的语言信息；
        * 不能还原音频，但可表达内容含义；
        * 类似“音频版文字 token”，可支持无文本语音生成。

* **混合 token 问题：**
    * 之前的模型通常先提取 semantic token，再用它引导生成 acoustic token；
    * 但这种方式 **不能实时运行**，因为：
        * semantic token 是非因果（non-causal）的，必须整段语音提取；
        * 分别使用两个编码器，计算开销大。

* **Mimi 的创新点：**
    * 借鉴 SpeechTokenizer 的思路，将语义信息“蒸馏”进因果模型生成的 token 中；
    * 实现了 **声学和语义 token 的统一建模**，支持实时编码和解码；
    * 既保留了语义信息，又能逐步生成高质量音频，适合流式语音生成任务。




#### 3.3.1 Architecture

```note
Mimi 的基础架构是一种流式的神经音频编码器，结合残差量化、因果卷积和自动编码器设计，可高效将语音压缩成离散 token，同时保持高音质的还原能力。
**Mimi 在因果自动编码器的基础上引入 Transformer、可控量化率和纯对抗训练等关键技术，不仅支持流式语音压缩，还显著提升了低比特率下的主观音质与语义保真。**
```

* 这段内容详细描述了 **Mimi 音频编码器的基础架构**
* 核心思想是借鉴已有的高质量神经音频编码器（如 SoundStream 和 Encodec），并在此基础上加入残差量化器以提高压缩效率与重建质量。
* 🎯 目标
    * 将 **单声道语音波形**（24kHz）压缩为可离散的、可用于神经网络处理的表示（即**音频token**），
    * 并能流式实时还原为高质量语音。

* 架构组成
    * 自动编码器 (Autoencoder)
        * 编码器部分使用 **SeaNet** 架构，由一系列 **残差卷积模块** 构成：
    * 解码器
        * 使用反卷积（Transposed Convolution）逐步还原语音；
        * 支持 **流式语音重建**。
    * 残差向量量化器（RVQ）
        * 将高维连续的 latent 表征转化为离散的 **音频 token**；
        * 使用 **Q 个量化器**（多级量化），每个量化器有 **NA 个词条**；
          * 输出为大小为 $S \times Q$ 的离散索引矩阵，每个位置取值为 {1, ..., NA}；
          * 这种离散表示称为 **Acoustic Tokens**（音频token）；
        * RVQ 机制：逐层对残差进行量化，减少信息损失。


##### Transformer-based bottleneck.

* **目的：**
    * 增强压缩语音的表达能力，提升音质，并辅助语义信息提取（如语音转文字或理解）。

* **设计细节：**
    * 在**quantization前后** 各插入一个 Transformer 模块；
    * 每个 Transformer：
        * 8 层、8 个注意力头；
        * 使用 RoPE（旋转位置编码）；
        * 上下文窗口为 **250帧 ≈ 20 秒**；
        * 激活函数为 GELU；
        * 维度为 `model_dim=512`，MLP层为 `2048`；
        * 使用 LayerScale（对残差分量加权）初始化为 0.01；
        * 全部使用因果掩码（causal masking），**可流式处理**。
* **效果：**
    * 有助于还原更高质量音频；
    * 编码器侧 Transformer 还能提升语义 distillation 效果。

##### Causality and streaming.

* 整个架构为**完全因果性设计**，可以 **边收音边编码并播放**；
* 输入一段 **80ms 音频帧**，Mimi 即可生成对应 latent 表征 → 解码为 80ms 输出音频；
* 支持**实时语音压缩与重建**。

##### Optimization.

* 因引入 Transformer，需要：
  * 使用 AdamW 优化器；
  * 仅对 Transformer 参数加 `weight_decay=0.05`；
* 训练超参包括：
    * 学习率：8e-4；
    * β1=0.5，β2=0.9；
    * EMA 权重衰减：0.99；
    * 批次大小：128；
    * 音频窗口：12秒，训练4M步；
    * Transformer上下文限制：10秒。

##### Quantization rate.

* 使用 **8个量化器 (Q=8)**，每个有 **2048 个词条 (NA=2048)**；
* 总帧率为 12.5Hz → **比特率约为 1.1kbps**；
* 在进入量化器前将维度从 512 投影到 256，解码前恢复到 512；
* 训练时仅 **50% 概率使用量化**（其余直接用浮点表征）：
    * 避免过度依赖量化，提升模型鲁棒性；
    * 实验证明：在低比特率下此策略能显著提升音质（尤其是**客观指标**）。

##### Adversarial-only training.


* 标准训练方式：
  * Mel谱重建损失（内容保真）；
  * 多尺度 STFT 判别器（感知音质）；
* **实验突破**：
  * 尝试**完全移除重建损失**，仅保留：
    * 特征损失（Feature loss）；
    * 判别器损失（Discriminator loss）；
  * 结果：虽然**客观指标变差**，但**主观听感显著提升**（参见 Table 4）；
  * 类似策略在带宽增强场景中也被成功验证（如 Tagliasacchi et al., 2020）。


#### 3.3.2 Learning semantic-acoustic tokens with a split RVQ

* 为什么需要语义 token？
    * **背景问题：**
        * 原始的 acoustic tokens（纯音频特征）能很好重建高质量音频，但 **不能表达语言语义**；
        * 语义 tokens（如从自监督语音模型中提取）对语言内容建模更好，但无法还原高质量语音。
* 现有方法：直接把语义信息蒸馏到第一级 RVQ 中
    * **蒸馏来源**：使用 WavLM-large（微软自监督模型）提取 1024 维语义表示，采样率 50Hz；
    * **目标对齐**：Mimi 采样率为 12.5Hz，因此：
        * 需将输入音频下采样到 16kHz；
        * WavLM 输出后再做非因果平均池化（`stride=4, kernel=8`）→ 与 Mimi 输出对齐；
    * **训练方式：**
        * 对 RVQ 的第一级输出做线性映射到 1024维；
        * 与处理后的 WavLM 表征做余弦距离 loss（即 distillation loss）；
    * **问题：**
        * 语义蒸馏能**增强语音的辨音能力（phonetic discriminability）**；
        * 但却**损害了音频重建质量**，因为：
            * 后续量化器的 residual 已被语义 token “污染”，难以还原真实音频；
            * 第一级量化器要同时承载语义与音质，任务冲突。

* 创新方法：Split RVQ（分裂式残差向量量化器）
    * **核心思想**：将语义 token 和音频 token 分开量化！
          * 用一个**独立的普通 VQ**专门做语义 token（用于蒸馏）；
          * 剩下的音频信息交由**7 层 RVQ**处理（不需要保留语义）；
          * 二者**并行量化**，结果再相加作为整体表示；
    * **优势**：
        * **语义模块只学语义，不干扰音质重建**；
        * **RVQ 专注于音频细节保真**；
        * 提升语音辨识度与音质间的平衡。


### 3.4 Generative Audio Modeling

* 我们在原始的 Helium 模型基础上做了扩展，加入了对 Mimi 编码器生成的音频 token 的建模能力。为了实现更真实的对话效果，我们不仅建模一条音频流，而是同时建模两条音频流：一条代表用户说话，另一条代表系统（AI）说话。
* 此外，我们还引入了一个新功能叫 “内心独白”（Inner Monologue），即在系统端同时建模文字和音频，以提升对话质量。


#### 3.4.1 Hierarchical autoregressive modeling with RQ-Transformer

* 这段内容讲的是如何用 **RQ-Transformer（Residual Quantization Transformer）** 对多模态（主要是音频token）进行高效建模

![](https://img.zhaoweiguo.com/uPic/2025/06/zorFG5.png)

Figure 3:Architecture of the RQ-Transformer. The RQ-Transformer breaks down a flattened sequence of length K⋅S into S timesteps for a large Temporal Transformer which produces a context embedding used to condition a smaller Depth Transformer over K steps. This allows scaling to longer sequences by increasing S—or to a higher depth by increasing K— than modeling the flattened sequence with a single model. In this figure, we use K=4 for the sake of illustration.


##### 🔧 问题背景：

* 音频比文字更长：比如用 Mimi 编码器编码音频时，每秒要处理 **100 个 token**（远高于文字的 3~4 个 token），所以用传统方法处理音频，计算量太大，**不适合实时对话生成**。
* 音频是多通道的（比如多个 codebook），相比单一路的文字，还更复杂。

##### 🧠 目标：

* 我们不只建模一条序列，而是建模**多条子序列**（比如多个音频 codebook 的输出 + 可选的文字 token）。
* 每个时间步包含多个子 token。


##### 🌲 解决方案：RQ-Transformer（分层自回归建模）

```note
RQ-Transformer 通过时间-层次解耦的双 Transformer 架构，高效地建模语音 token 的时序与结构特性，并用参数定制化优化每个子序列的生成质量，同时保证推理效率。
```

* **关键思想**：把每一帧的音频 token 序列拆成两维建模：
    * RQ-Transformer 由两个 Transformer 模型组成
        * **时间维（Temporal Transformer）**：处理每个时间步的上下文（例如从第1秒到第s-1秒说了什么）。
        * **深度维（Depth Transformer）**：在当前时间步内，预测多个 codebook 的 token（比如音频的多个“子通道”）。
* 重点
    * 时间变换器 (Temporal Transformer) 的步数始终等于 S ，而不是 K⋅S，所以计算量 较小且稳定
    * 深度变换器 (Depth Transformer) 的步数最多为 K，即每个时间步中最多预测 K 个子 token。
 

##### 🧱 定义：

* Temporal Transformer 表示为函数 $\mathrm{Tr}_{\mathrm{Temp}}$
* Depth Transformer 表示为 $\mathrm{Tr}_{\mathrm{Depth}}$
* 对于每一个时间步 $s \in [1, S]$，我们用以下公式表示该步所有子序列的联合状态。
$V_s = (V_{s,1}, ..., V_{s,K})$


##### 🧠 Temporal Transformer 的工作方式

* 在时间步 $s$，Temporal Transformer 接收历史所有步的输出作为输入：
* $z_s = \mathrm{Tr}_{\mathrm{Temp}}(V_0, ..., V_{s-1}) \in \mathbb{R}^d$
* 这个 $z_s$ 是当前时间步的上下文向量。


##### 🧱 Depth Transformer 的工作方式

* 对于每一个子序列索引 $k > 1$，Depth Transformer 接收：
    * 上下文向量 $z_s$
    * 当前步之前的所有子序列 token：$(V_{s,1}, ..., V_{s,k-1})$
* 并输出一个 **logits 向量**：
    * $l_{s,k} = \mathrm{Tr}_{\mathrm{Depth}}(z_s, V_{s,1}, ..., V_{s,k-1}) \in \mathbb{R}^{N_k}$
    * 对于第一个子序列 $k = 1$，使用一个独立的线性层：
        * $l_{s,1} = \mathrm{Lin}(z_s) \in \mathbb{R}^{N_1}$

##### 整合

* 我们训练这三个模块（Temporal Transformer、Depth Transformer 和线性层 Lin）来使 softmax 后的结果逼近真实的 token 分布：
$$
\begin{cases}
\mathrm{softmax}(l_{s,1}) \approx \mathbb{P}(V_{s,1} \mid V_0, ..., V_{s-1}) \\
\mathrm{softmax}(l_{s,k}) \approx \mathbb{P}(V_{s,k} \mid V_0, ..., V_{s-1}, V_{s,1}, ..., V_{s,k-1}), \quad \text{if } k > 1
\end{cases}
$$


##### ⚙️ 模型效率设计

* **Temporal Transformer 的步数始终为 S（音频的时间步数）**
  * 而不是 $K \cdot S$，因此计算量不随 codebook 数增加而线性上升。
  * 每步输入为上一时间步 $V_{s-1}$ 所有子序列对应的 K 个 embedding 向量之和。
* **Depth Transformer 最多运行 K 次（每个时间步内预测 K 个子 token）**
  * 每步输入为 $z_s + \text{embedding}(V_{s,k-1})$



#### 3.4.2 Audio modeling

* 这段讲的是如何**用 RQ-Transformer** 来建模**音频 token**，
* 特别是用 **Mimi 编码器**生成的多子序列（codebooks）音频 token。

##### 🎯 目标：

* 音频经过 Mimi 编码器后，被分成 **Q=8 条子序列**（即 8 个 codebook），每秒 12.5 步（frame）。
* 其中，第 1 条子序列代表 **语义信息**（semantic），其余是 **声学特征**（acoustic features）。


##### 🧠 做法：

* 把这些音频子序列插入到我们之前定义的多序列 $V$ 中，交给 **RQ-Transformer** 来建模。

##### 🔁 Acoustic Delay（声学延迟）：

* 原本是直接把 V=A（即当前时间步的语义和声学 token 都一起输入），但这种方式生成效果不太好。
* 改进方案：**延迟声学 token** 的输入，比如把第 s 步的声学 token 放到第 s+1 步再输入。
* 为什么要延迟？
    * 这样可以**减弱每个时间点内部不同子序列之间的依赖**，让模型更容易学习。
    * 大模型专注于时间上的语义变化，小模型处理细节特征，更高效。
    * 文中引用了多个论文，也实验证明延迟后生成质量更高。

##### 🚀 创新点：

1. **首次实现了语义与声学 token 的联合流式生成**（以前是先生成语义，再补声学）。
2. 在 Depth Transformer 中，为每个子序列 **使用了独立的参数**，提升了建模质量。



#### 3.4.3 Multi-stream modeling

* 这段话讲的是 **多音频流建模（Multi-stream modeling）**，用于模拟一个包含双方对话的真实交流场景。
* 在对话中，只建模一个人的音频是不够的。所以，Moshi 框架扩展到了**双人的音频建模**：
    * 输入是两个音频流：一个是 Moshi（模型）的说话音频，另一个是用户的说话音频。
    * 对这两个音频流都加入“音频延迟”（为了考虑说话的先后顺序），然后合并起来作为模型的输入。
    * 图中展示的是这个合并后的序列，模型逐步处理每一步的音频“Token”。
    * 推理时，**Moshi 部分（图中虚线以下）是模型生成的**，而**用户部分（虚线以上）是实际输入的**。
    * 这种方式可以让模型处理“双方讲话可能重叠”的情况。
* 这段在讲如何让模型能听懂和应对**两人交谈、甚至是说话重叠**的对话场景。

![](https://img.zhaoweiguo.com/uPic/2025/06/JP5OJZ.png)

Figure 4:Representation of the joint sequence modeled by Moshi. Each column represents the tokens for a given step in the joint sequence (Vs,k) described in Equation 6 with an acoustic delay τ=1, e.g. the input of the Temporal Transformer for this step. Tokens are predicted from bottom to top in the Depth Transformer. At inference time, tokens under the dashed line (corresponding to Moshi) are sampled, while those above are fed from the user. This design allows for our model to handle overlapping speech turns.


#### 3.4.4 Inner Monologue(内心独白)

* 主要讲的是 **Moshi 模型如何通过“内心独白”（Inner Monologue）机制，同时建模文本与语音流，提高语音生成质量，并实现可切换的流式语音识别（ASR）和语音合成（TTS）功能。**
* **核心思想**：Moshi 让模型“听见自己在说什么”
* **什么是 Inner Monologue（内心独白）？**
    * 模型在说话时，不只生成语音，还生成对应的文字（用 Whisper 识别自己说的话）。
    * 这个文字流（文本 token）作为一种“语言支架”，帮助模型说得更清楚、更符合语言规范。
    * 这个机制只对 Moshi 自己使用，对用户的语音流不做实时转写。
        * 因为实时转录难度高，且这不符合我们端到端的语音到语音建模目标

##### Aligning text and audio tokens.

* 目标：将文本 token 与每秒 12.5 帧的音频 token 对齐
* 模型使用 Whisper 给出的单词时间戳，把文本 token 精准对齐到这些帧。
* 为了让模型知道哪里是“空白”或“说完了”，引入了两种特殊 token：
    * `PAD`：表示填充（静音）
    * `EPAD`：表示某个词结束
* 在英语中对话语料中，约65%的帧都是填充帧（PAD token），说明词之间的停顿是常见现象。


##### Deriving streaming ASR and TTS.

* **一套机制，切换成流式 ASR 或 TTS**
* 通过控制文本 token 和语音 token 的“谁在前谁在后”，可以让模型：
    * 只生成文本 → 流式语音识别（ASR）
    * 只用文本来引导语音 → 流式语音合成（TTS）
* **不用改模型结构或训练数据，只改一个“延迟”参数。**


##### Joint sequence modeling for Moshi.

* 每一时刻，模型预测一个包含多个流的“联合序列”，包括：
    1. Moshi 的文本 token
    2. Moshi 的语义 token
    3. Moshi 的延迟音频 token
    4. 用户的语义 token
    5. 用户的延迟音频 token
* 总共有 `2Q+1` 条流（Q=8），如图4所示。


* 给定延迟步数 $\tau$，语音分帧维度 $Q$，第 $s$ 步的联合序列 $V_s$ 包括以下内容：

* 公式：
$$
\left\{
\begin{array}{lll}
V_{s,1} &= W_s & \text{aligned text tokens.} \\
V_{s,2} &= A_{s,1} & \text{semantic tokens of Moshi.} \\
V_{s,1+q} &= A_{s-\tau,q} \quad \text{if} \quad s \geq \tau+1, 1 < q \leq Q & \text{delayed acoustic tok. of Moshi.} \\
V_{s,1+Q+1} &= A^{\prime}_{s,1} & \text{semantic tokens of other.} \\
V_{s,1+Q+q} &= A^{\prime}_{s-\tau,q} \quad \text{if} \quad s \geq \tau+1, 1 < q \leq Q & \text{delayed acoustic tok. of other.} \\
\end{array}
\right.
$$

* 其中
$$
K = 2Q + 1 \quad \text{with } Q = 8
$$


* 参数说明
    * $V_{s,k}$：第 $s$ 个时间步、第 $k$ 个子序列的 token。
    * $W_s$：对齐的文本 token 序列（Moshi 说的文字）。
    * $A_{s,1}$：Moshi 的语义 token（表示语义内容）。
    * $A_{s-\tau,q}$：Moshi 的延迟音频 token，第 $q$ 个（表示具体发音细节），仅在 $s \geq \tau + 1$ 且 $1 < q \leq Q$ 时有效。
    * $A'_{s,1}$：对方说话者的语义 token。
    * $A'_{s-\tau,q}$：对方说话者的延迟音频 token，第 $q$ 个，条件同上。
    * $\tau$：延迟步数（控制文本和音频之间的时间差）。
    * $Q$：每个时间步包含的音频 token 数量，实验中取 $Q=8$。
    * $K = 2Q + 1$：总的子序列流数量，包括文本流、两个说话者的语义流和音频流。

##### Inference of Moshi.

* 只生成 Moshi 的文本和语音部分，用户部分用真实音频代替。
* 模型可以同时听和说，也可以静音（生成“自然的静音波形”）。
* 利用文本流控制行为，比如插入 `EPAD` 可以让 Moshi 立即开口说话。

##### 🎯 总结一句话：

**Moshi 在生成语音时，也生成自己说的文字，通过这种“内心独白”机制提高语音质量，并可以灵活切换成语音识别或语音合成。**


## 4. Datasets and Training


### 4.1 Text Data

* **12.5%** 来自高质量数据源：Wikipedia、Wikibooks、Wikisource、Wikinews、StackExchange、科学文章（peS2o）。
* **87.5%** 来自 CommonCrawl 网络数据，使用特定过滤方法筛选。
* Wikipedia 用了 5 个不同年份的数据（2017、2018、2019、2021、2022）。

### 4.2 Audio Data

* **700万小时**音频（以英语为主）通过 Whisper large-v3 转录，用于预训练（不区分说话人）。
* **Fisher 数据集**（2000小时电话对话，左右声道分开录）用于训练模型能“听说同步”（多流能力）。
* **170小时高质量多说话人对话**用于微调多流 TTS 模型，不直接训练 Moshi，但对其提升效果。
* 所有音频处理为单声道、24kHz采样率。
* 为生成时间戳，使用 whisper-timestamped 工具。

### 4.3 Speech-Text Instruct Data

* 文本指令集（如 Open Hermes）不适合 TTS，因此使用 Helium 生成真实对话的文字，再用多流 TTS 合成语音。
* 合成语音数据达 **2万小时**，Moshi 的声音来自同一个演员（录了70+种说话风格），用户声音随机。
* 对话生成方式包括：
    * 根据 Wikipedia/StackExchange 生成知识问答类对话；
    * 让用户请求 Moshi 用特定声音（如愤怒或像海盗）说话；
    * 角色扮演情景（如“开心的侦探”）；
    * 训练 Moshi 识别错误、拼写问题、误导性提问（如“埃菲尔铁塔在北京吗？”）；
    * 生成安全对话，避免 Moshi 回答不当或不合规内容。



### 4.4 Training Stages and Hyper-parameters


#### Helium pre-training.

* 使用纯文本训练语言模型 Helium，共训练 50 万步。
* 用 AdamW 优化器 + 余弦学习率调度 + warmup。
* 每批数据包含 420 万个 token，运行在 H100 GPU 上。

#### Moshi pre-training.

* Temporal Transformer 初始化自 Helium，Depth Transformer 随机初始化。
* 数据：单通道的音频序列 + 随机延迟的文本（mask 掉30%）。
* 总共训练 100 万步，其中一半 batch 是纯文本以防遗忘。
* 使用两个优化器分别处理文本与音频，调节学习率以保持平衡。

#### Moshi post-training.

* 使用说话人分离（diarization），将音频分成主说话人和其他人两个通道。
* 文本只保留主说话人内容。
* 固定延迟为 0，训练 10 万步，批次为 8 小时音频。

#### Moshi finetuning.

* 真实多说话人对话
    * 使用 Fisher 数据集，让模型学习真实多轮对话。
    * 批量大小为 40 分钟音频，训练 1 万步。
    * 不再使用纯文本 batch。
* instruct
    * 使用合成 instruct 数据集，把主说话人设置为“助手”。
    * 批量大小为 2.7 小时音频，训练 3 万步。
    * 增强用户音频流：随机调整音量、加背景噪音、模拟回声和混响等。

#### TTS Training.

* 使用延迟 2 秒的音频流进行训练，目标是生成 instruct 微调数据。
* 注意：Moshi 本身并未使用该 TTS 的有监督多流数据进行训练。


#### Training loss.

* 同时对文本 token 和音频 token 建模。
* 文本 loss 与音频 loss 加权求和（文本重要性高，语义音频 token 权重为 100，其他为 1）。


## 5. Evaluation

### 5.1 Text Language Modeling












