# 2009.03300_MMLU: Measuring Massive Multitask Language Understanding


* [https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)
* 组织: UC Berkeley, Columbia University, UChicago, UIUC
* GitHub: [https://github.com/hendrycks/test](https://github.com/hendrycks/test)
    * 全部数据集: https://people.eecs.berkeley.edu/~hendrycks/data.tar
* 数据集:
    * https://openaipublic.blob.core.windows.net/simple-evals/mmlu.csv
    * https://openaipublic.blob.core.windows.net/simple-evals/mmlu_{language}.csv
* 相关链接
    * https://github.com/openai/simple-evals

* 简介:
    * Massive Multitask Language Understanding
    * 类型：多任务理解和推理
    * 多任务的评估基准，涵盖57个不同领域的任务，包括人文、科学、数学、社会科学等。每个任务包含多个选择题，旨在测试模型在多种学科和任务上的广泛理解和推理能力。


## Abstract

* 我们设计了一个新测试，用来评估文本模型在多任务上的准确性，涵盖了57个任务，比如数学、历史、计算机科学和法律等。要在这个测试中表现好，模型需要具备广泛的知识和解决问题的能力。我们发现，大多数模型表现接近随机猜测，只有最大的GPT-3模型平均提升了约20个百分点。但即使是最好的模型，在所有任务上离专家水平还有很大差距，而且经常不知道自己错了。在一些重要领域（比如道德和法律）上，表现仍接近随机。这个测试可以全面评估模型的知识广度和深度，并找出它的不足。


## 1.Introduction

* 尽管NLP模型在一些评测中已经超过人类，但它们在整体语言理解上仍不如人类，说明现有评测标准不够全面。
* 早期的评测如GLUE和SuperGLUE，很快被模型攻克。为了更准确评估模型的实际知识和推理能力，研究者设计了一个新基准：**MMLU（Massive Multitask Language Understanding）**，它覆盖了**57个学科**，从小学水平到专业级，涉及STEM、人文、社科、法律、伦理等领域，只用**零样本和少样本**方式测试，以更贴近人类评估。
* 结果发现，像GPT-3这种大模型在一些学科（如历史）表现很好，但在数学、物理和伦理等方面接近随机水平。而且，GPT-3对自己知道或不知道的事情也缺乏准确判断。
* 这个新测试揭示了当前模型在知识理解和运用上的重要短板，有助于推动更全面的模型评估与改进。

## 2.Related Work

### 1.1. **预训练**

* NLP 领域主流做法是用大量文本（比如书籍和网站）来预训练大模型。
* 这些模型学到了很多知识，甚至能像知识库一样使用。
* 过去主要靠微调模型来完成任务，但现在像 GPT-3 这样的模型，仅用少量例子（few-shot）就能有很好的表现，甚至不需要微调。
* 这样可以设计更多样的任务，避免模型靠数据中的“巧合线索”来得分。

### 2.2. **评估基准**

* 最近的很多评估数据集是为了测试模型的常识和基本推理能力，比如 HellaSwag、Physical IQA 和 CosmosQA。
    * 但这些任务相对简单，相当于小孩子能做的。
* 本文提出的任务更难，包含了人类需要专门学习才能掌握的知识。
* 有人提议用自然语言生成（NLG）来评估模型，但它评估难、没有统一标准。
    * 所以本文使用更容易评估的方式：多项选择题的分类准确率。
* 现有的问答数据集太简单，或只考语言理解（比如阅读理解）。
    * 本文则涵盖更多、更难的学科，不止考语言理解能力。


## 3.A Multitask Test

这段内容介绍了一个**大规模多任务测试**，用来评估语言模型对各种真实世界知识的理解能力。

### 🌐 测试概述

* **测试内容**：包括各类选择题，覆盖**人文、社会科学、自然科学、工程和其他专业知识**。
* **任务总数**：57个，题目总量15908
* **题目来源**：研究生和本科生从网络上公开获取，如GRE、USMLE、大学课程、牛津大学出版社图书等。
* **题目分级**：每个科目按难度划分（小学、高中、大学、专业级别）。


### 🔍 数据集结构

* 收集的 15908 道题，分为少样本开发集、验证集和测试集
    * Few-shot开发集：每个科目5题
    * 验证集：1540
    * 测试集：14079
    * **每个科目**至少有100道测试题。

### 🧠 人类表现

* 普通人（如Amazon Mechanical Turk）平均答对率：**34.5%**
* 高水平人类（如医学执照考试优秀者）答对率高达：**约89.8%**


### 🎯 评估目标

该测试**不仅考察常识和语言理解**，更考察模型对**各种真实文本与知识的广泛理解能力**，是衡量AI语言理解能力的重要标准。


### 🧩 各学科示例

* **人文学科**：法学、哲学、历史，涉及推理、规范理解、伦理问题等。
* **社会科学**：经济学、社会学、政治学、心理学等，需融合世界知识与推理能力。
* **STEM**（理工科）：物理、数学、计算机等，考察数学表达与链式推理。
* **其他**：如专业医学、商业、全球知识（如各国贫困率）等。


### 总结

* 这是一个极其庞大且多元的测试，旨在挑战AI模型的广泛知识覆盖能力、推理水平和真实世界理解力。


## 4.Experiments

![](https://img.zhaoweiguo.com/uPic/2025/06/DIQlge.jpg)

Table 1:Average weighted accuracy for each model on all four broad disciplines. All values are percentages. Some models proposed in the past few months can move several percent points beyond random chance. GPT-3 uses few-shot learning and UnifiedQA is tested under distribution shift.


## 5.Discussion

### 1. 多模态理解

* 很多重要概念不仅仅靠文字表达，还需要图片、音频或互动等形式。
* 当前像 GPT-3 这样的模型只处理文字，所以评估任务也只用了文字。
* 但随着模型开始支持多模态输入，评测方式也应随之改变，比如可以用像“Turk Test”这样的任务来考察模型是否能理解多种信息形式。


### 2. 互联网作为训练集

* 不同于传统方法，这里的模型不是靠人工标注的大量训练题来学习，而是通过阅读互联网的大量文本“自学”的。
* 就像人们读书获得知识一样，模型通过“预训练”获取世界知识，而不是单纯靠练习题。

### 3. 类似人类的学习方式

* 传统机器学习靠做题学知识，但人类主要通过读书和听讲获取知识。
* 比如法律领域，有海量的法条书籍，但可用的练习题却很少。因此，模型应在预训练阶段尽量“读”更多的高质量文本，而不是靠有限的题库。
* 为了更真实地评估模型，作者设置了 zero-shot、few-shot 和 transfer 等测试方式，并把开发集（dev）、验证集（val）、测试集（test）分开使用，避免训练集和测试集“长得一模一样”的问题。


### 4. 模型的局限性

* 目前的大模型仍有许多不足，比如：
    * 难以理解人的价值观和道德判断（在法律和伦理题上表现差）
    * 数学和理科题表现很差，连基础计算都做不好
    * 所有科目都达不到专家水平，整体表现仍低于人类
* 作者尝试通过“专业领域的数据微调”来提升模型，比如用法律案例对 RoBERTa 模型继续训练，但效果有限，只提升了几个百分点，说明光靠加数据也不够。

### 5. 扩大规模未必是答案

* 虽然放大模型能带来一些提升，但同时也需要大量的数据。
* 然而在冷门知识领域（比如专业法律），可用的优质文本资源很少。所以，单靠“变大”可能无法解决模型在某些领域的不足。


## 6.Conclusion

* 他们提出了一个新测试，用来评估语言模型在预训练时学到的知识是否真的能用上。这个测试覆盖了57个不同难度的学科，比以前的测试更全面。
* 他们发现，虽然模型在这个测试上的表现有进步，但总体表现不均衡，在具体任务上表现一般。而且，模型在需要计算的任务上表现差，也不擅长处理道德和法律等社会相关的问题。
* 这个测试有助于研究人员发现模型的不足，更清楚地了解当前模型的实际能力。













