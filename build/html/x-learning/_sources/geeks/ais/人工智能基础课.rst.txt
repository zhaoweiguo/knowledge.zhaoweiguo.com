人工智能基础课
##############

.. raw:: html

   <details>
   <summary>目录</summary>

.. sidebar:: 目录

    .. contents::

.. raw:: html

   </details>


数学基础
========

线性代数
--------

::

    线性代数（linear algebra）
    范数（norm）
    正交（orthogonality）

    线性空间（linear space）
    内积空间（inner product space）


L^p范数（norm）::

    对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值
    L^p范数:
      1. L^1:
      范数计算的是向量所有元素绝对值的和
      2. L^2:
      范数计算的是通常意义上的向量长度
      3. L^∞:
      范数计算的则是向量中最大元素的取值

内积（inner product）::

    计算的则是两个向量之间的关系
    对应元素乘积的求和。
    内积能够表示两个向量之间的相对位置，即向量之间的夹角。

正交（orthogonality）::

    一种特殊的情况是内积为 0，即 ⟨x,y⟩=0
    在二维空间上，这意味着两个向量的夹角为 90 度，即相互垂直。
    在高维空间上，这种关系被称为正交（orthogonality）。
    如果两个向量正交，说明他们线性无关，相互独立，互不影响。

线性空间（linear space）::

    有一个集合，它的元素都是具有相同维数的向量（可以是有限个或无限个），
    并且定义了加法和数乘等结构化的运算，这样的集合就被称为线性空间（linear space）

内积空间（inner product space）::

    定义了内积运算的线性空间则被称为内积空间（inner product space）

.. note:: 在线性空间中，任意一个向量代表的都是 n 维空间中的一个点；反过来， 空间中的任意点也都可以唯一地用一个向量表示。两者相互等效。


正交基（orthogonal basis）::

    在内积空间中，一组两两正交的向量构成这个空间的正交基（orthogonal basis）

标准正交基（orthonormal basis）::

    假若正交基中基向量的 L^2 范数都是单位长度 1，这组正交基就是标准正交基（orthonormal basis）


概率论
------

::

    概率论（probability theory）

    “频率学派”（frequentist probability）
    条件概率（conditional probability）
    联合概率（joint probability）
    全概率公式（law of total probability）

    古典概率模型

    “逆概率” 
    贝叶斯公式:
        H(Hypothesis) 和 D(Data)

    先验概率（prior probability）
        P(H): 预先设定的假设成立的概率
    似然概率（likelihood function）
        P(D∣H): 在假设成立的前提下观测到结果的概率
    后验概率（posterior probability）
        P(H∣D): 在观测到结果的前提下假设成立的概率

    随机变量（random variable）

    相关系数（correlation coefficient）


概率的估计有两种方法::

    1. 最大似然估计法（maximum likelihood estimation）
        频率学派对概率的理解方式
    2. 最大后验概率法（maximum a posteriori estimation）
        贝叶斯学派对概率的理解方式


.. note:: 概率论的一个重要应用是描述随机变量（random variable）

根据取值空间的不同，随机变量可以分成两类::

    1. 离散型随机变量（discrete random variable）
    2. 连续型随机变量（continuous random variable）

.. note:: 离散变量的每个可能的取值都具有大于 0 的概率，取值和概率之间一一对应的关系就是离散型随机变量的分布律，也叫『概率质量函数（probability mass function）』。概率质量函数在连续型随机变量上的对应就是『概率密度函数（probability density function）』

重要的离散分布包括::

    1. 两点分布（Bernoulli distribution）
      适用于随机试验的结果是二进制的情形，事件发生 / 不发生的概率分别为 p/(1−p)。
      任何只有两个结果的随机试验都可以用两点分布描述
      如：抛掷一次硬币的结果就可以视为等概率的两点分布
    2. 二项分布（Binomial distribution）
      将满足参数为 p 的两点分布的随机试验独立重复 n 次
      事件发生的次数即满足参数为 (n,p) 的二项分布。
    3. 泊松分布（Poisson distribution）
      放射性物质在规定时间内释放出的粒子数所满足的分布，

重要的连续分布则包括::

    1. 均匀分布（uniform distribution）
      在区间 (a, b) 上满足均匀分布的连续型随机变量，其概率密度函数为 1 / (b - a)，
      这个变量落在区间 (a, b) 内任意等长度的子区间内的可能性是相同的。
    2. 指数分布（exponential distribution）
      满足参数为 θ 指数分布的随机变量只能取正值

    3. 正态分布（normal distribution）
      正态分布是最常见最重要的一种分布，自然界中的很多现象都近似地服从正态分布。

.. note:: 除了概率质量函数 / 概率密度函数之外，另一类描述随机变量的参数是其数字特征。数字特征是用于刻画随机变量某些特性的常数

数字特征包括::

    1. 数学期望（expected value）
        即均值，体现的是随机变量可能取值的加权平均，
        即根据每个取值出现的概率描述作为一个整体的随机变量的规律
    2. 方差（variance）
        表示的则是随机变量的取值与其数学期望的偏离程度。
        方差较小意味着随机变量的取值集中在数学期望附近，
        方差较大则意味着随机变量的取值比较分散。
    3. 协方差（covariance）
        数学期望和方差描述的都是单个随机变量的数字特征，
        如果要描述两个随机变量之间的相互关系，就需要用到协方差和相关系数。
        协方差度量了两个随机变量之间的线性相关性，
        即变量 Y 能否表示成以另一个变量 X 为自变量的 aX+b 的形式

        注: 无论是协方差还是相关系数，刻画的都是线性相关的关系

全概率公式:

.. math::

    P(A)=\sum\limits_{i=1}^N P(A|B_i) * P(B_i)
    \sum\limits_{i=1}^N P(B_i) = 1


贝叶斯公式:

.. math::

    P(B_i|A) = \frac{P(A|B_i)*P(B_i)}{\sum\limits_{j=1}^N P(A|B_j)*P(B_j)}

数理统计
--------

::

    数理统计（mathematical statistics）

    样本（sample）
    总体（population）

    参数估计（estimation theory）
    假设检验（hypothesis test）


.. note:: 数理统计（mathematical statistics）根据观察或实验得到的数据来研究随机现象，并对研究对象的客观规律做出合理的估计和判断。


数理统计与概率论的区别::

    数理统计以概率论为理论基础，但两者之间存在方法上的本质区别。

    概率论作用的前提是随机变量的分布已知，根据已知的分布来分析随机变量的特征与规律；
    数理统计的研究对象则是未知分布的随机变量，研究方法是对随机变量进行独立重复的观察，
        根据得到的观察结果对原始分布做出推断。



.. math::

    样本均值: \bar{X} = \frac{1}{n}\sum\limits_{i=1}^n(X_i)

    样本方差: S^2 = \frac{1}{n-1}\sum\limits_{i=1}^n(X_i-\bar{X})^2


统计推断的基本问题可以分为两大类::

    1. 参数估计（estimation theory）
    2. 假设检验（hypothesis test）

参数估计
^^^^^^^^

参数估计是通过随机抽取的样本来估计总体分布的方法，又可以进一步划分为::

    1. 点估计（point estimation）
    2. 区间估计（interval estimation）

估计量的评价通常要考虑以下三个基本标准::

    1. 无偏性：估计量的数学期望等于未知参数的真实值
    2. 有效性：无偏估计量的方差尽可能小
    3. 一致性：当样本容量趋近于无穷时，估计量依概率收敛于未知参数的真实值

泛化误差的构成可以分为三部分::

    1. 偏差（bias）
        表示算法预测值和真实结果之间的偏离程度，刻画的是模型的欠拟合特性
    2. 方差（variance）
        表示数据的扰动对预测性能的影响，刻画的是模型的过拟合特性
    3. 噪声（noise）
        表示在当前学习任务上能够达到的最小泛化误差，刻画的是任务本身的难度

    对任何实际的模型来说，偏差和方差都难以实现同时优化，反映出欠拟合与过拟合之间难以调和的矛盾

最优化方法
----------

::

    拉格朗日乘子（Lagrange multiplier）
    
    一阶导数（first-order derivative）
    二阶导数（second-order derivative）



* 要实现最小化或最大化的函数被称为目标函数（objective function）或评价函数

根据约束条件的不同，最优化问题可以分为::

    1. 无约束优化（unconstrained optimization）
    2. 约束优化（constrained optimization）
        如: 线性规划（linear programming）

当可用的训练样本有多个时，样本的使用模式就分为两种::

    1. 批处理模式（batch processing）
    2. 随机梯度下降法（stochastic gradient descent）

::

    一阶导数描述的是目标函数如何随输入的变化而变化
    二阶导数描述的则是一阶导数如何随输入的变化而变化，提供了关于目标函数曲率（curvature）的信息。
        曲率影响的是目标函数的下降速度。
        当曲率为正时，目标函数会比梯度下降法的预期下降得更慢；
        反之，当曲率为负时，目标函数则会比梯度下降法的预期下降得更快。

寻找最小值点的方法::

    1. 梯度下降法（gradient descent）
    2. 牛顿法（Newton's method）
    3. “置信域方法”（trust region）
    4. “启发式算法”（heuristics）


启发式算法实例包括::

    1. 模拟生物进化规律的遗传算法（genetic algorithm）
    2. 模拟统计物理中固体结晶过程的模拟退火算法（simulated annealing）
    3. 模拟低等动物产生集群智能的蚁群算法（ant colony optimization）


优化算法分类::

    精确算法(绝对最优解)
        精确算法包括: 线性规划、动态规划、整数规划和分支定界法
    启发式算法（近似算法）

领域搜索算法分为两类::

    局部搜索法(爬山算法)
    指导性搜索法(SA、GA、EP、ES和TS)

群体智能算法::
    
    粒子群算法(PSO),蚁群算法(ACO),
    人工蜂群算法(ABC),人工鱼群算法(AFSA),
    混洗蛙跳算法(SFLA),烟花算法(FWA),
    细菌觅食优化(BFO),萤火虫算法(FA);

信息论
------

* 1948 年，供职于美国贝尔实验室的物理学家克劳德・香农发表了著名论文《通信的数学理论》（A Mathematical Theory of Communication），给出了对信息这一定性概念的定量分析方法，标志着信息论作为一门学科的正式诞生。定义了 “熵” 这一信息论中最基本最重要的概念。“熵” 这个词来源于另一位百科全书式的科学家约翰・冯诺伊曼，他的理由是没人知道熵到底是什么。虽然这一概念已经在热力学中得到了广泛使用，但直到引申到信息论后，熵的本质才被解释清楚，即一个系统内在的混乱程度。


形式逻辑
--------

* 1956 年召开的达特茅斯会议宣告了人工智能的诞生。在人工智能的襁褓期，各位奠基者们，包括约翰·麦卡锡、赫伯特·西蒙、马文·明斯基等未来的图灵奖得主，他们的愿景是让“具备抽象思考能力的程序解释合成的物质如何能够拥有人类的心智”。

古希腊哲学家亚里士多德提出并流传至今的三段论，它由两个前提和一个结论构成::

    1. 科学是不断发展的
    2. 人工智能是科学
    3. 所以，人工智能是不断发展的

按照优先级由高到低排列，逻辑联结词包括以下五种::

    1. 否定(¬)：复合命题 ¬P 表示否定命题 P 的真值的命题，即“非 P” 
    2. 合取(∧)：复合命题 P∧Q 表示命题 P 和命题 Q 的合取，即“P 且 Q”
    3. 析取(∨)：复合命题 P∨Q 表示命题 P 或命题 Q 的析取，即“P 或 Q”
    4. 蕴涵(→)：复合命题 P→Q 表示命题 P 是命题 Q 的条件，即“如果 P，那么 Q”
    5. 等价(↔)：复合命题 P↔Q 表示命题 P 和命题 Q 相互蕴涵，即“如果 P，那么 Q 且如果 Q，那么 P”

产生式系统包括规则库、事实库和推理机三个基本部分::

    1. 规则库是专家系统的核心与基础，
        存储着以产生式形式表示的规则集合，
        其中规则的完整性、准确性和合理性都将对系统性能产生直接的影响。
    2. 事实库存储的是输入事实、中间结果与最终结果，
        当规则库中的某条产生式的前提可与事实库中的某些已知事实匹配时，
        该产生式就被激活，其结论也就可以作为已知事实存储在事实库中。
    3. 推理机则是用于控制和协调规则库与事实库运行的程序，
        包括了推理方式和控制策略。

推理的方式可以分为三种：正向推理、反向推理和双向推理::

    1. 正向推理采用的是自底向上的方式，
        即从已知事实出发，通过在规则库中不断选择匹配的规则前件，得到匹配规则的后件，
        进而推演出目标结论。
    2. 反向推理采用的是自顶向下的方式，
        即从目标假设出发，通过不断用规则库中规则的后件与已知事实匹配，
        选择出匹配的规则前件，进而回溯已知事实。
    3. 双向推理则是综合利用正向推理和反向推理，
        使推理从自顶向下和自底向上两个方向进行，直到在某个中间点汇合，这种方式具有更高的效率。

* 1900 年，德国数学家大卫·希尔伯特在巴黎国际数学家代表大会上提出了 20 世纪 23 个最重要的数学问题，其中的第二问题便是算术公理系统的无矛盾性。
* 1931 年，奥地利数学家库尔特·哥德尔对这个问题给出了否定的答案，即第一不完备性定理：『在任何包含初等数论的形式系统中，都必定存在一个不可判定命题。』



机器学习
========

从方法论的角度看，机器学习是计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的学科。





















