

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Build a Large Language Model (From Scratch) &mdash; 新溪-gordon V2025.06 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="动手学深度学习(Dive into Deep Learning)" href="Dive_into_Deep_Learning.html" />
    <link rel="prev" title="scikit-learn 1.3.2" href="../scikit-learn.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>
  <script src="../../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.06
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../book.html">书籍</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../bookreview.html">书</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../bookreviews/2021.html">2021年看的书</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/cryptography-graph.html">图解密码技术</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/deep-learning-with-python.html">Deep learning with Python</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/how-networks-work.html">网络是怎么连接的</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/building-microservices.html">微服务设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/microservice_design_principle_and_architecture.html">微服务设计原理与架构</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/microservice-governance.html">微服务治理: 体系、架构及实践</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/TCPIP-ILLustrated-Volume1.html">TCP/IP ILLustrated Volume 1: The Protocols</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/SRE.html">SRE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/the-site-reliability-workbook.html">The Site Reliability Workbook</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/bitcoin-and-cryptocurrency-technologies.html">Bitcoin and Cryptocurrency Technologies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/2021s/other.html">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../bookreviews/detail.html">详情</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id3">编码实践</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id4">设计模式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id5">工程实践</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id6">领域驱动设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id7">产品与需求</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id8">开发文化</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id9">管理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id10">科幻小说</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../bookreviews/detail.html#id11">其他相关</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../booklist.html">要看的书</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/classic.html">经典书</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/classic.html#it-core">IT Core</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/classic.html#id3">编译原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/classic.html#id4">组成原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/classic.html#it">IT 设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/classic.html#id5">管理</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/AI.html">AI 相关</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/AI.html#id2">推荐系统</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/IT.html">IT相关</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/IT.html#id2">区块链</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/IT.html#id3">统计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/IT.html#id4">网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/IT.html#id5">实时协同</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/methodology.html">方法论</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/fiction.html">小说</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/fiction.html#science-fiction">科幻小说science fiction</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklists/source.html">书籍来源</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklists/sources/geek.html">极客来源</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklist.html#id3">数学基础</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../booklist.html#id4">语言经典</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../booklist.html#c">C 语言</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../booklist.html#id5">计算机经典</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../booklist.html#id6">计算机语言设计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../booklist.html#id7">其他</a></li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../ai.html">ai</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html">Microsoft: AI-For-Beginners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#i-introduction-to-ai">I Introduction to AI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#ii-symbolic-ai">II Symbolic AI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#iii-introduction-to-neural-networks">III Introduction to Neural Networks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#iv-computer-vision">IV Computer Vision</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#v-nlp">V. NLP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#vi-other">VI. Other</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-AI-For-Beginners.html#vii-ethics">VII. Ethics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html">Microsoft: Machine Learning for Beginners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#regression">2. Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#web-app">3. Web-App</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#classification">4. Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#clustering">5. Clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#nlp">6. NLP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#timeseries">7. TimeSeries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#reinforcement">8. Reinforcement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-ML-for-Beginners.html#real-world">9. Real-World</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html">Microsoft: Generative AI For Beginners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#id2">第一章: 简介</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#llms">第二章: 不同的 LLMs对比</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#ai">第三章: AI安全</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#id3">第四章: 提示工程基础</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#id4">第五章: 提示工程进阶</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Generative-AI-For-Beginners.html#id5">第八章: 创建搜索应用</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Getting-Started-with-OpenCV.html">Getting Started with OpenCV</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-PyTorch.html">Microsoft Learn: Introduction to PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-PyTorch.html#id2">简介</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-NLP-with-PyTorch.html">Microsoft Learn: Introduction to NLP with PyTorch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-NLP-with-PyTorch.html#id2">简介</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-NLP-with-PyTorch.html#representing-text-as-tensors">2. Representing text as Tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-NLP-with-PyTorch.html#bag-of-words-and-tf-idf-representations">3. Bag-of-Words and TF-IDF representations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Microsoft-Learn-Introduction-to-NLP-with-PyTorch.html#embeddings">4. Embeddings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../HuggingFace-Learn.html">HuggingFace: Learn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Kaggle-Learn.html">Kaggle Learn</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Kaggle-Learn.html#intermediate-machine-learning">Intermediate Machine Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Kaggle-Learn.html#intro-to-deep-learning">Intro to Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Kaggle-Learn.html#id4">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../scikit-learn.html">scikit-learn 1.3.2</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Build a Large Language Model (From Scratch)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#understanding-llm">1. Understanding LLM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#working-with-text-data">2. Working with Text Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#coding-attention-mechanisms">3. Coding Attention Mechanisms</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implementing-a-gpt-model-from-scratch-to-generate-text">4 Implementing a GPT model from Scratch To Generate Text</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pretraining-on-unlabeled-data">5 Pretraining on Unlabeled Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fine-tuning-for-classification">6 Fine-tuning for classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#fine-tuning-to-follow-instructions">7 Fine-tuning to follow instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-a-introduction-to-pytorch">Appendix A. Introduction to PyTorch</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-b-references-and-further-reading">Appendix B. References and Further Reading</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-c-exercise-solutions">Appendix C. Exercise Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-d-adding-bells-and-whistles-to-the-training-loop">Appendix D. Adding Bells and Whistles to the Training Loop</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-e-parameter-efficient-fine-tuning-with-lora">appendix E Parameter-efficient fine- tuning with LoRA</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id8">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="Dive_into_Deep_Learning.html">动手学深度学习(Dive into Deep Learning)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="Dive_into_Deep_Learning.html#id2">前言</a></li>
<li class="toctree-l4"><a class="reference internal" href="Dive_into_Deep_Learning.html#part-1-basics-and-preliminaries">Part 1: Basics and Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="Dive_into_Deep_Learning.html#part-2-modern-deep-learning-techniques">Part 2: Modern Deep Learning Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="Dive_into_Deep_Learning.html#part-3-scalability-efficiency-and-applications">Part 3: Scalability, Efficiency, and Applications</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../architecture.html">架构相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84.html">微服务设计原理与架构</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84.html#id3">微服务建模</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84.html#id17">服务拆分与集成</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84.html#id18">微服务架构关键要素</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86%E4%B8%8E%E6%9E%B6%E6%9E%84.html#id21">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html">领域驱动设计精粹</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id4">第 2 章 运用限界上下文与通用语言进行战略设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id6">第 3 章 运用子域进行战略设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id7">第 4 章 运用上下文映射进行战略设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id8">第 5 章 运用聚合进行战术设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id9">第 6 章 运用领域事件进行战术设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1%E7%B2%BE%E7%B2%B9.html#id10">第 7 章 加速和管理工具</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/%E5%AE%9E%E7%8E%B0%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1.html">实现领域驱动设计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../architectures/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1.html">领域驱动设计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../optimize.html">优化相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../optimizes/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.html">性能之巅</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../optimizes/%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85.html#id3">书评</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../protocol.html">协议相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html">UNIX 网络编程卷1</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id3">第1章 简介</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#tcp-udpsctp">第2章 传输层: TCP, UDP和SCTP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id8">第3章 套接字编程简介</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id14">第4章 基本TCP套接字编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id16">第5章 TCP客户/服务器程序示例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#i-o-selectpoll">第6章 I/O复用: select和poll函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id24">第7章 套接字选项</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#udp">第8章 基本UDP套接字编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id26">第9章 基本SCTP套接字编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id27">第11章 名字与地址转换</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#ipv4ipv6">第12章 IPv4与IPv6的互操作性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#inetd">第13章 守护进程和inetd超级服务器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id31">第14章 高级I/O函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#unix">第15章 Unix域协议</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id35">第16章 非阻塞式I/O</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#ioctl">第17章 ioctl操作</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id36">第18章 路由套接字</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id37">第19章 密钥管理套接字</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id38">第20章 广播</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id41">第21章 多播</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id47">第22章 高级UDP套接字编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id51">第23章 高级SCTP套接字编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id52">第24章 带外数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id57">第25章 信号驱动式I/O</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id58">第26章 线程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#ip">第27章 IP选项</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id66">第28章 原始套接字</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id70">第29章 数据链路访问</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id74">第30章 客户/服务器程序设计范式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#a-ipv4-ipv6-icmpv4icmpv6">附录A IPv4, IPv6, ICMPv4和ICMPv6</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/UNIX%20%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%8D%B71.html#id77">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71.html">TCP/IP 详情-卷1</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71.html#id3">第1章 概 述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71.html#id4">第2章 链 路 层</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71.html#ip">第3章 IP:网际协议</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71.html#id15">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71-%E7%AC%AC2%E7%89%88.html">TCP:IP 详情卷1-第2版</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../protocols/TCP-IP%20%E8%AF%A6%E6%83%85%E5%8D%B71-%E7%AC%AC2%E7%89%88.html#id3">第1章 概 述</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../edge.html">边缘相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../edges/%E9%9B%BE%E8%AE%A1%E7%AE%97%E4%B8%8E%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%EF%BC%9A%E5%8E%9F%E7%90%86%E5%8F%8A%E8%8C%83%E5%BC%8F.html">雾计算与边缘计算: 原理及范式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html">边缘计算入门 20 课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id3">第 01 课: 边缘计算深度调研</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id17">第 02 课: 云走向边缘, 云将无处不在</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id27">第 03 课: 信通院-边缘计算发展现状与趋势展望</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#edgerec">第 04 课: EdgeRec: 边缘计算在推荐系统中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id51">第 05 课: 阿里云边缘云原生应用实践应用实践</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#kubeedge-sedna-0-1">第 06 课: KubeEdge 子项目 Sedna 0.1 发布</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#superedge">第 07 课: 用 SuperEdge 统管边缘设备和机器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#k8s-10">第 08 课: 如何使用 k8s 管理 10 万边缘节点</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#ai">第 09 课: 边云协同-打通 AI 最后一公里</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#edgeadm-k8s">第 10 课: 用 edgeadm 一键安装边缘 K8s 集群</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#kubeedge-10086">第 11 课: 基于 KubeEdge 实现 10086 客服云边协同平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#volcano">第 12 课: Volcano 架构设计与原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id57">第 13 课: 一文读懂 SuperEdge 的云边隧道</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id58">第 14 课: 打破内网壁垒-从云端一次添加上千边缘节点</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id59">第 15 课: 一文读懂 SuperEdge 边缘容器架构与原理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id60">第 16 课: 2020 十大边缘计算开源项目</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#addon-superedge-k8s">第 17 课: Addon SuperEdge 让原生 K8s 管理边缘应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id63">第 18 课: SuperEdge 云边隧道新特性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id64">第 19 课: 《深入理解边缘计算》</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#fabedge">第 20 课: FabEdge 边缘网络方案</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id65">第 21 课: 边缘计算云原生开源方案选型比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E5%85%A5%E9%97%A8%2020%20%E8%AF%BE.html#id66">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html">边缘计算方法与工程实践</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id3">第1章 边缘计算综述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id4">第2章 边缘计算基础资源架构技术</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id5">第3章 边缘计算软件架构</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id7">第4章 边缘计算安全管理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id23">第5章 边缘计算应用案例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../edges/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%8E%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5.html#id39">第6章 边缘计算发展展望</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../iot.html">物联网相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html">图解物联网</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id3">第1章 物联网的基础知识</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id4">第 2 章 物联网的架构</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id5">第 3 章 物联网设备</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id9">第 4 章 先进的感测技术</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id12">第 5 章 物联网服务的系统开发</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id14">第 6 章 物联网与数据分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id15">第 7 章 物联网与可穿戴设备</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E5%9B%BE%E8%A7%A3%E7%89%A9%E8%81%94%E7%BD%91.html#id16">第 8 章 物联网与机器人</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../iots/%E7%89%A9%E8%81%94%E7%BD%91%E8%AE%BE%E8%AE%A1.html">物联网设计</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../iots/%E7%89%A9%E8%81%94%E7%BD%91%E8%AE%BE%E8%AE%A1.html#id3">第一部分 原型阶段</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../iots/%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E8%AE%BE%E8%AE%A1%E7%89%A9%E8%81%94%E7%BD%91.html">自己动手设计物联网</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../lang.html">编程语言相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../langs/C%20%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1.html">C 程序设计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../geek.html">极客时间</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/principle.html">[重要]编程基础</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.html">深入浅出计算机组成原理</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.html#id4">指令和运算</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.html#id15">处理器</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86.html#id35">书籍</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html">网络编程实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html#id4">第一模块: 基础篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html#id28">第二模块: 提高篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html#id48">第三模块: 性能篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html#id62">第四模块: 实战篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98.html#id63">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html">趣谈 Linux 操作系统</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id2">第二部分 系统初始化 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id10">第三部分 进程管理 (10 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id34">第四部分 内存管理 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id51">第五部分 文件系统 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id60">第六部分 输入输出系统 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id69">第七部分 进程间通信 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id83">第八部分 网络系统 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id90">第九部分 虚拟化 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id100">第十部分 容器化 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id105">实战串讲篇 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%B6%A3%E8%B0%88%20Linux%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F.html#id106">学习攻略</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html">编译原理实战课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id4">语法分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id7">语义分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id10">运行时机制</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#ir">中间代码 IR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id11">代码优化</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id12">代码生成</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#ast">解析树和 AST 的区别</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#golang">Golang</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#erlang">Erlang</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id15">并发</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#meta-programming">元编程-Meta-Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id19">泛型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id20">函数式编程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id21">远程办公</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id22">如何学习</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id23">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%E5%AE%9E%E6%88%98%E8%AF%BE.html#id24">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html">操作系统实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id4">整体设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id6">程序的基石：硬件</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id7">同步原语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id8">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E5%86%99%E4%B8%80%E9%97%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.html">手把手带你写一门编程语言</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E5%86%99%E4%B8%80%E9%97%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.html#id4">开篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E5%86%99%E4%B8%80%E9%97%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.html#id5">词法分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E5%86%99%E4%B8%80%E9%97%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.html#id6">语法分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E5%86%99%E4%B8%80%E9%97%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80.html#id7">语义分析</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html">计算机基础实战课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id3">课程设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id4">01以史为鉴 (3讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#mini-cpu-9">02硬件-芯片(手写mini CPU) (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id11">03环境准备 (2讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id12">04语言与指令 (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id13">05应用与内存 (8讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id30">06国庆策划 (3讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#io-6">07IO与文件 (6讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id36">08综合应用 (6讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id44">09结束语 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/principles/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E8%AF%BE.html#id45">10技术雷达 (5讲)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/architecture.html">架构相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E.html">左耳听风-陈皓</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E.html#id4">程序员如何用技术变现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E.html#id19">05 _ 何为技术领导力</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%B7%A6%E8%80%B3%E5%90%AC%E9%A3%8E.html#id25">06 _ 如何才能拥有技术领导力</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html">许式伟的架构课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id4">编程语言</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id9">操作系统</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id10">外置存储</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id11">需求分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id12">详细设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id13">导致故障的因素</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id14">软件架构</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id15">架构设计文档</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id20">软件质量管理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id21">软件工程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id22">架构设计的优劣</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%B8%E5%BC%8F%E4%BC%9F%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id23">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html">软件工程之美</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/md/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8Esummary.html">软件工程之美summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/md/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html">软件工程之美</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html#id3">基础理论 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html#id20">需求分析篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html#id26">系统设计篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E4%B9%8B%E7%BE%8E.html#id31">开发编码篇 (7 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E7%BE%8E.html">设计模式之美</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html">DDD 实战课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html#id8">基础篇 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html#id30">02进阶篇 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html#id61">03实战篇 (10 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/DDD%20%E5%AE%9E%E6%88%98%E8%AF%BE.html#id107">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90.html">架构实战案例解析</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90.html#id4">01概述篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90.html#id12">02业务架构篇 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90.html#id47">03技术架构篇 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E8%A7%A3%E6%9E%90.html#id79">总结篇 (2 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html">乔新亮的 CTO 成长复盘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html#id3">00开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html#id4">01对个人认知的复盘 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html#id11">02对管理工作的复盘 (10 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html#id22">03对专业成长的复盘 (10 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E4%B9%94%E6%96%B0%E4%BA%AE%E7%9A%84%20CTO%20%E6%88%90%E9%95%BF%E5%A4%8D%E7%9B%98.html#id34">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1.html">如何落地业务建模</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1.html#id4">旧约: “前云时代” 的领域驱动设计 (11 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1.html#id6">深度答疑专题 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E5%A6%82%E4%BD%95%E8%90%BD%E5%9C%B0%E4%B8%9A%E5%8A%A1%E5%BB%BA%E6%A8%A1.html#id7">新约: 云时代的业务建模 (2 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html">郭东白的架构课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id3">我的收获</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id4">课程设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id5">00开篇词|没有战略意图,就成不了一个顶尖的架构师</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id6">01模块一:生存法则 (15 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id45">02模块二:创造价值 (21讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id98">03模块三:职业成长 (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id113">04模块四:思考力 (11讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id138">05结束语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/architectures/%E9%83%AD%E4%B8%9C%E7%99%BD%E7%9A%84%E6%9E%B6%E6%9E%84%E8%AF%BE.html#id139">06加餐</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/architectures/%E6%9D%8E%E6%99%BA%E6%85%A7%20%C2%B7%20%E9%AB%98%E5%B9%B6%E5%8F%91%E6%9E%B6%E6%9E%84%E5%AE%9E%E6%88%98%E8%AF%BE.html">李智慧 · 高并发架构实战课</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/secure.html">安全</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html">实用密码学</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id4">00开篇词 _ 人人都要会点密码学</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id5">01 | 学习密码学有什么用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id6">02 | 单向散列函数: 如何保证信息完整性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id7">03 | 如何设置合适的安全强度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id11">04 | 选择哈希算法应该考虑哪些因素</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id14">05|如何有效避免长度延展攻击</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id15">06|对称密钥: 如何保护私密数据</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id18">07 | 怎么选择对称密钥算法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#ecb">09 | 为什么ECB模式不安全</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#cbc">10 | 怎么防止数据重放攻击CBC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id32">11 | 怎么利用解密端攻击</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id39">12 | 怎么利用加密端攻击</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id45">13 | 如何防止数据被调包</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id50">14 | 加密数据能够自我验证吗</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#aead">15 | AEAD 有哪些安全陷阱</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id57">16 | 为什么说随机数都是骗人的</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id65">17 | 加密密钥是怎么来的</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id69">18 | 如何管理对称密钥</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id74">19|量子时代,你准备好了吗</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/%E5%AE%9E%E7%94%A8%E5%AF%86%E7%A0%81%E5%AD%A6.html#id78">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/secures/Web%20%E5%AE%89%E5%85%A8%E6%94%BB%E9%98%B2%E5%AE%9E%E6%88%98.html">Web 安全攻防实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/Web%20%E5%AE%89%E5%85%A8%E6%94%BB%E9%98%B2%E5%AE%9E%E6%88%98.html#id2">1. 前端基础</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/secures/Web%20%E5%AE%89%E5%85%A8%E6%94%BB%E9%98%B2%E5%AE%9E%E6%88%98.html#id5">2. Web安全之后端安全</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/testing.html">测试相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/testings/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E5%85%A5%E9%97%A8%E8%AF%BE.html">接口测试入门课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E5%85%A5%E9%97%A8%E8%AF%BE.html#id3">点评</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E5%85%A5%E9%97%A8%E8%AF%BE.html#id4">开篇词 | 把接口测试这件小事做深/做透</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E5%85%A5%E9%97%A8%E8%AF%BE.html#id5">01 | 基础: 跳出细节看全局</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E6%8E%A5%E5%8F%A3%E6%B5%8B%E8%AF%95%E5%85%A5%E9%97%A8%E8%AF%BE.html#id6">02 | 方法论: 没有任何文档, 怎么才能快速了解接口的信息</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html">程序员的测试课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html#id5">基础篇 (11 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html#id36">应用篇 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html#id40">03扩展篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E6%B5%8B%E8%AF%95%E8%AF%BE.html#id41">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html">软件测试 52 讲</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id3">01测试基础知识篇 (11讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#gui-10">02GUI自动化测试篇 (10讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#api-3">03API自动化测试篇 (3讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id14">04代码测试篇 (3讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id16">05性能测试篇 (7讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id17">06测试数据准备篇 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id18">07测试基础架构篇 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id19">08测试新技术篇 (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id24">09测试人员的互联网架构核心知识篇 (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/testings/%E8%BD%AF%E4%BB%B6%E6%B5%8B%E8%AF%95%2052%20%E8%AE%B2.html#id25">10特别放送篇 (8讲)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/cloudnative.html">云原生</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html">容器实战高手课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#namespace">Namespace</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#cgroups">Cgroups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#linux-kernel">Linux Kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#load-average">Load Average</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#memory-cgroup">Memory Cgroup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#id4">存储</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#network">Network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#id5">容器安全</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#k8s">k8s</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/cloudnatives/%E5%AE%B9%E5%99%A8%E5%AE%9E%E6%88%98%E9%AB%98%E6%89%8B%E8%AF%BE.html#id6">思考</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/manager.html">管理&amp;长成</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html">跟着高手学复盘</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html#id3">01基础概念篇 (3 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html#id8">02实操流程篇 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html#id64">03实战案例篇 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html#id76">结束语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E8%B7%9F%E7%9D%80%E9%AB%98%E6%89%8B%E5%AD%A6%E5%A4%8D%E7%9B%98.html#id77">春节荐书</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html">程序员进阶攻略</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id4">启程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id7">修炼</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id22">修行</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id92">徘徊</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id124">寻路</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E7%A8%8B%E5%BA%8F%E5%91%98%E8%BF%9B%E9%98%B6%E6%94%BB%E7%95%A5.html#id137">蜕变</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html">10x 程序员工作法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id3">思考框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id4">四个思考原则</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id5">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id6">一. 以终为始</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id12">二. 任务分解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id17">三. 沟通反馈</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id18">四. 自动化</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id19">五. 综合运用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id20">好书推荐</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/10x%20%E7%A8%8B%E5%BA%8F%E5%91%98%E5%B7%A5%E4%BD%9C%E6%B3%95.html#id28">提问</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html">大厂晋升指南</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id4">晋升原则</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id5">晋升逻辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id6">能力模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id7">职级档次</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#p7">P7</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#p8">P8</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#p9">P9</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#p10-p11">P10/P11</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#ppt">面评技巧-PPT框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id18">面评技巧-PPT 讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id19">面评技巧-PPT 答辩</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id20">面评技巧-注意点</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id24">面评技巧-技术大会</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id25">面评技巧-其他</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id26">学习方法-指导原则</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id27">学习方法-找时间：海绵学习法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id28">学习方法-学什么：三段分解法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id29">学习方法-怎么学</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id33">学习方法-保证效果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id34">做事方法-总</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#kpi-okr">做事方法-KPI&amp;OKR</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#c">做事方法-3C 方案设计法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#pdca">做事方法-PDCA执行法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#w">做事方法-5W根因分析法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#s">做事方法-5S 问题处理法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#d">做事方法-4D 总结法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id35">做事方法-金字塔汇报法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id37">做事方法-四线复盘法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id38">专项提升-业务</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#w1h8c1d">专项提升-业务:5W1H8C1D 分析法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#aarrr">专项提升-业务:AARRR 漏斗模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id39">专项提升-业务:宝洁战略模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id40">专项提升-管理</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id41">专项提升-管理:管理四象限</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id42">专项提升-管理:管理五模式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id43">别人的心得</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id44">其他</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id46">10000小时定律</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id47">领域分层图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id48">参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/managers/%E5%A4%A7%E5%8E%82%E6%99%8B%E5%8D%87%E6%8C%87%E5%8D%97.html#id53">其他</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/analysis.html">数据分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html">数据分析实战 45 讲</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id4">思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id5">开篇词 | 你为什么需要数据分析能力</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id10">01基础篇 (16 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id24">02算法篇 (20 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id37">03实战篇 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id44">04工作篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%AE%9E%E6%88%9845%E8%AE%B2.html#id47">结束语</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html">数据分析思维课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id4">思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id5">00开篇词 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id7">01数据分析基础 (11 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id34">02数据算法基础 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id50">03如何用数据说话 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id68">04分析工具 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id74">05特别放送 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/analysis/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%80%9D%E7%BB%B4%E8%AF%BE.html#id75">其他</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/ai.html">AI 相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E8%AF%BE.html">人工智能基础课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E8%AF%BE.html#id4">数学基础</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80%E8%AF%BE.html#id12">机器学习</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F.html">推荐系统三十六式</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F.html#id4">内容推荐</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F.html#id9">近邻推荐</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F.html#id18">矩阵分解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B8%89%E5%8D%81%E5%85%AD%E5%BC%8F.html#id19">个人成长</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html">AI 大模型之美</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html#id2">课前必读 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html#id5">基础知识篇: 探索大型语言模型的能力 (8 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html#nlp-10">实战提高篇一: 利用NLP技术完成高级任务 (10讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html#id42">实战提高篇(二) 大型语音与图像模型的应用 (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B9%8B%E7%BE%8E.html#id55">扩展</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2040%20%E8%AE%B2.html">机器学习 40 讲</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2040%20%E8%AE%B2.html#id3">01机器学习概观 (10 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2040%20%E8%AE%B2.html#id5">02统计机器学习模型 (18 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2040%20%E8%AE%B2.html#id6">03概率图模型 (14 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html">PyTorch 深度学习实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id2">开篇词 | 如何高效入门 PyTorch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id3">01基础篇 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id5">02模型训练篇 (12 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id26">03实战篇 (9 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id56">加餐| 基础模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/PyTorch%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98.html#id60">结束语| 人生充满选择, 选择与努力同样重要</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%20GPT%20%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8%E8%AF%BE.html">零基础 GPT 应用入门课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%20GPT%20%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8%E8%AF%BE.html#id2">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%20GPT%20%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8%E8%AF%BE.html#id7">基础速通 (3讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%20GPT%20%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8%E8%AF%BE.html#id10">黄金密钥 (7讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%20GPT%20%E5%BA%94%E7%94%A8%E5%85%A5%E9%97%A8%E8%AF%BE.html#id18">综合实战 (6讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html">AI 大模型系统实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id2">热身篇 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id6">架构基础篇 (6讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%B3%BB%E7%BB%9F%E5%AE%9E%E6%88%98.html#id33">技术原理篇 (5讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html">AI 绘画核心技术与实战</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html#id2">开篇词 (2讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html#ai-4">热身篇:AI 绘画初体验 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html#ai-9">基础篇:AI 绘画原理揭秘 (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html#dall-e-2-stable-diffusion-5">进阶篇:从 DALL-E 2 到 Stable Diffusion (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/AI%20%E7%BB%98%E7%94%BB%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html#ai-8">综合演练篇:AI 绘画高手养成计划 (8讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">零基础实战机器学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/ais/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%AE%9E%E6%88%98%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html#id3">08 | 模型优化1: 怎么用特征工程提高模型效率</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/blockchain.html">区块链</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/blockchains/%E8%AF%B4%E9%80%8F%E5%8C%BA%E5%9D%97%E9%93%BE.html">说透区块链</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/blockchains/%E8%AF%B4%E9%80%8F%E5%8C%BA%E5%9D%97%E9%93%BE.html#id3">数字人民币</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/blockchains/%E8%AF%B4%E9%80%8F%E5%8C%BA%E5%9D%97%E9%93%BE.html#id4">书籍</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/coding.html">代码精进</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E4%B9%8B%E4%B8%91.html">代码之丑</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E4%B9%8B%E4%B8%91.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E4%B9%8B%E4%B8%91.html#id4">13 类典型坏味道 (13 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E4%B9%8B%E4%B8%91.html#id5">延伸阅读 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E4%B9%8B%E4%B8%91.html#id10">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/codings/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%B9%8B%E7%BE%8E.html">软件设计之美</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/md/%E8%BD%AF%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%B9%8B%E7%BE%8E.html">软件设计之美</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF.html">代码精进之路</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF.html#id3">01第一模块: 代码 “规范” 篇 (16 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF.html#id19">02第二模块: 代码 “经济” 篇 (14 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/codings/%E4%BB%A3%E7%A0%81%E7%B2%BE%E8%BF%9B%E4%B9%8B%E8%B7%AF.html#id48">03第三模块: 代码 “安全” 篇 (14 讲)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/lang.html">编程语言</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/langs/Go%E8%AF%AD%E8%A8%80%E6%A0%B8%E5%BF%8336%E8%AE%B2.html">Go 语言核心 36 讲</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html">TonyBai Go语言第一课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id2">课程设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id3">00开篇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id5">01入门篇: 勤加练手 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id6">02基础篇: “脑勤” 多理解 (20 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id8">03核心篇: “脑勤 +” 洞彻核心 (5 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id16">04实战篇: 打通“最后一公里” (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id17">大咖助阵</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id18">加餐</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/langs/Tony-Bai-Go-%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id26">泛型</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/langs/Python%20%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E6%88%98.html">Python 核心技术与实战</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/product.html">产品&amp;运营</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html">梁宁-产品思维 30 讲</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id4">发刊词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id7">模块一: 同理心</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id23">模块二: 机会判断</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id31">模块三: 系统能力</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id45">模块四: 用户体验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id66">模块五: 创新模式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id73">产品世界观</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id80">彩蛋</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/products/%E6%A2%81%E5%AE%81%E3%83%BB%E4%BA%A7%E5%93%81%E6%80%9D%E7%BB%B4%2030%20%E8%AE%B2.html#id90">参考</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/interview.html">面试</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html">后端工程师的高阶面经</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#id5">01微服务架构 (10讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#mysql-13">数据库与MySQL (13讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#id269">消息队列 (10讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#id321">缓存 (9讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#nosql-5">NoSQL (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/interviews/%E5%90%8E%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%AB%98%E9%98%B6%E9%9D%A2%E7%BB%8F.html#id394">结束语</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/softengineering.html">软件工程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html">说透敏捷</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id3">开篇词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id4">原理篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id14">实战篇 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id26">策略篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id29">管理篇 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/softengineers/%E8%AF%B4%E9%80%8F%E6%95%8F%E6%8D%B7.html#id32">结束语</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../geeks/other.html">其它</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html">互联网人的英语私教课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#ksa">KSA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id4">独立主格结构</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id5">介词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id6">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id8">关键英语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#vs">并列句 VS 复杂句</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id9">单词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id10">英语谚语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id11">常用短语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id12">口语专用词汇</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id13">好的英文网站</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id15">其他</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id16">会不会阅读</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id17">词汇学习</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#paraphrase">paraphrase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id18">动词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id19">其他</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%BA%E7%9A%84%E8%8B%B1%E8%AF%AD%E7%A7%81%E6%95%99%E8%AF%BE.html#id20">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E4%BB%8E%200%20%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F.html">从 0 打造音视频直播系统</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BB%8E%200%20%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F.html#webrtc-1-1-23">WebRTC 1 对 1 通话 (23 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BB%8E%200%20%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F.html#webrtc-7">WebRTC 多人音视频实时通话 (7 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BB%8E%200%20%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F.html#id22">支持上万人同时在线的直播系统 (8 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%BB%8E%200%20%E6%89%93%E9%80%A0%E9%9F%B3%E8%A7%86%E9%A2%91%E7%9B%B4%E6%92%AD%E7%B3%BB%E7%BB%9F.html#id27">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html">快手·音视频技术入门课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html#id3">开篇基础 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html#id29">流媒体技术速成 (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html#ffmpeg-api-4">FFmpeg API 应用 (4讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html#ffmpeg-2">FFmpeg 社区“玩法” (2讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E5%BF%AB%E6%89%8B%C2%B7%E9%9F%B3%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E8%AF%BE.html#id55">结束语 | 音视频技术更宠爱脚踏实地的人</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E6%94%BB%E5%85%8B%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF.html">攻克视频技术</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%94%BB%E5%85%8B%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF.html#id3">图像基础和前处理 (3 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%94%BB%E5%85%8B%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF.html#id12">视频编码 (5讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%94%BB%E5%85%8B%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF.html#id22">参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%94%BB%E5%85%8B%E8%A7%86%E9%A2%91%E6%8A%80%E6%9C%AF.html#id23">评论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html">搞定音频技术</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id3">音频基础 (4 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id13">02音频降噪 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id15">03回声消除 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id18">04音频网络传输 (3 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id22">05空间音频 (2 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E6%90%9E%E5%AE%9A%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF.html#id24">06音频特效生成与算法 (3 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html">专利写作第一课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id3">开篇词 | 写专利, 将是知识工作者的核心产出</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id4">01 _ 为什么我推荐互联网人要积极写专利</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id5">02 _ 奖金是专利写作中最不值得一提的事儿</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#keyperson">03 _ 找到KeyPerson利益点, 提升专利通过率</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#prd-1">04 _ 像写PRD一样, 撰写专利交底书1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#prd-2">05 _ 像写PRD一样, 撰写专利交底书2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id14">06 _ 如何把常见的生活问题变成专利(案例-节假日不响起闹钟)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id23">07 _ 专利创新的步伐不必迈得特别大</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id28">08 _ 那些异想天开的专利是怎么诞生的</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/%E4%B8%93%E5%88%A9%E5%86%99%E4%BD%9C%E7%AC%AC%E4%B8%80%E8%AF%BE.html#id32">答疑 _ 专利申请十大常见问题</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/WebAssembly%20%E5%85%A5%E9%97%A8%E8%AF%BE.html">WebAssembly 入门课</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/WebAssembly%20%E5%85%A5%E9%97%A8%E8%AF%BE.html#id2">课前必读</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/WebAssembly%20%E5%85%A5%E9%97%A8%E8%AF%BE.html#id9">01核心原理篇 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/WebAssembly%20%E5%85%A5%E9%97%A8%E8%AF%BE.html#id21">02应用篇 (6 讲)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../geeks/others/WebAssembly%20%E5%85%A5%E9%97%A8%E8%AF%BE.html#id22">03实战篇 (6 讲)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../geeks/others/other.html">其他</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../matter.html">Matter 协议</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../matters/matter.html">Matter Core</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-1-introduction">Chapter 1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-2-architecture">Chapter 2. Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#overview">2.1. Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#layered-architecture">2.2. Layered Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#network-topology">2.3. Network Topology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#scoped-names">2.4. Scoped names</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#identifiers">2.5. Identifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-identity">2.6. Device identity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#security">2.7. Security</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-commissioning">2.8. Device Commissioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#sleepy-end-device-sed">2.9. Sleepy End Device (SED)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#data-model-root">2.10. Data Model Root</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#stack-limits">2.11. Stack Limits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#list-of-provisional-items">2.12. List of Provisional Items</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-3-cryptographic-primitives">Chapter 3. Cryptographic Primitives</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-4-secure-channel">Chapter 4. Secure Channel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#general-description">4.1. General Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#ipv6-reachability">4.2. IPv6 Reachability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#discovery">4.3. Discovery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-frame-format">4.4. Message Frame Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-counters">4.5. Message Counters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-processing">4.6. Message Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-security">4.7. Message Security</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-privacy">4.8. Message Privacy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-exchanges">4.9. Message Exchanges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#secure-channel-protocol">4.10. Secure Channel Protocol</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-reliability-protocol-mrp">4.11. Message Reliability Protocol (MRP)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#unicast-communication">4.12. Unicast Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#session-establishment">4.13. Session Establishment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#group-communication">4.14. Group Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#group-key-management">4.15. Group Key Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-counter-synchronization-protocol-mcsp">4.16. Message Counter Synchronization Protocol(MCSP)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#bluetooth-transport-protocol-btp">4.17. Bluetooth Transport Protocol (BTP)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-5-commissioning">Chapter 5. Commissioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#onboarding-payload">5.1. Onboarding Payload</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#initiating-commissioning">5.2. Initiating Commissioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#user-directed-commissioning">5.3. User Directed Commissioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-discovery">5.4. Device Discovery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#commissioning-flows">5.5. Commissioning Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#administrator-assisted-commissioning-flows">5.6. Administrator Assisted Commissioning Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-commissioning-flows">5.7. Device Commissioning Flows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#in-field-upgrade-to-matter">5.8. In-field Upgrade to Matter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-6-device-attestation-and-operational-credentials">Chapter 6. Device Attestation and Operational Credentials</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#common-conventions">6.1. Common Conventions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-attestation">6.2. Device Attestation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#certification-declaration">6.3. Certification Declaration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#node-operational-credentials-specification">6.4. Node Operational Credentials Specification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#operational-certificate-encoding">6.5. Operational Certificate Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#access-control">6.6. Access Control</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-7-data-model-specification">Chapter 7. Data Model Specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#practical-information">7.1. Practical Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#data-qualities">7.2. Data Qualities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#conformance">7.3. Conformance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#element">7.4. Element</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#fabric">7.5. Fabric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#access">7.6. Access</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#other-qualities">7.7. Other Qualities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#node">7.8. Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#endpoint">7.9. Endpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#cluster">7.10. Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#command">7.11. Command</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#attribute">7.12. Attribute</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#global-elements">7.13. Global Elements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#event">7.14. Event</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-type">7.15. Device Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#non-standard">7.16. Non-Standard</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#data-field">7.17. Data Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#data-types">7.18. Data Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#manufacturer-specific-extensions">7.19. Manufacturer Specific Extensions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-8-interaction-model-specification">Chapter 8. Interaction Model Specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#id17">8.1. Practical Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#concepts">8.2. Concepts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#status-and-interaction">8.3. Status and Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#read-interaction">8.4. Read Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#subscribe-interaction">8.5. Subscribe Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#report-transaction">8.6. Report Transaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#write-interaction">8.7. Write Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#invoke-interaction">8.8. Invoke Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#common-action-information-blocks-and-paths">8.9. Common Action Information Blocks and Paths</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#status-codes">8.10. Status Codes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-9-system-model-specification">Chapter 9. System Model Specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#id18">9.1. Practical Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#endpoint-composition">9.2. Endpoint Composition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#interaction-model-relationships">9.3. Interaction Model Relationships</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#binding-relationship">9.4. Binding Relationship</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#descriptor-cluster">9.5. Descriptor Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#binding-cluster">9.6. Binding Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#label-cluster">9.7. Label Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#fixed-label-cluster">9.8. Fixed Label Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#user-label-cluster">9.9. User Label Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#access-control-cluster">9.10. Access Control Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#group-relationship">9.11. Group Relationship</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#bridge-for-non-matter-devices">9.12. Bridge for non-Matter devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#bridged-device-basic-information-cluster">9.13. Bridged Device Basic Information Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#actions-cluster">9.14. Actions Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#proxy-architecture">9.15. Proxy Architecture</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-10-interaction-model-encoding-specification">Chapter 10. Interaction Model Encoding Specification</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#id23">10.1. Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#messages">10.2. Messages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#id24">10.3. Data Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#sample-cluster">10.4. Sample Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#information-blocks">10.5. Information Blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#message-definitions">10.6. Message Definitions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-11-service-and-device-management">Chapter 11. Service and Device Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#basic-information-cluster">11.1. Basic Information Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#group-key-management-cluster">11.2. Group Key Management Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#localization-configuration-cluster">11.3. Localization Configuration Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#time-format-localization-cluster">11.4. Time Format Localization Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#unit-localization-cluster">11.5. Unit Localization Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#power-source-configuration-cluster">11.6. Power Source Configuration Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#power-source-cluster">11.7. Power Source Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#network-commissioning-cluster">11.8. Network Commissioning Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#general-commissioning-cluster">11.9. General Commissioning Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#diagnostic-logs-cluster">11.10. Diagnostic Logs Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#general-diagnostics-cluster">11.11. General Diagnostics Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#software-diagnostics-cluster">11.12. Software Diagnostics Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#thread-network-diagnostics-cluster">11.13. Thread Network Diagnostics Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#wi-fi-network-diagnostics-cluster">11.14. Wi-Fi Network Diagnostics Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#ethernet-network-diagnostics-cluster">11.15. Ethernet Network Diagnostics Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#time-synchronization">11.16. Time Synchronization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#node-operational-credentials-cluster">11.17. Node Operational Credentials Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#administrator-commissioning-cluster">11.18. Administrator Commissioning Cluster</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#over-the-air-ota-software-update">11.19. Over-the-Air (OTA) Software Update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#over-the-air-ota-software-update-file-format">11.20. Over-the-Air (OTA) Software Update File Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#bulk-data-exchange-protocol-bdx">11.21. Bulk Data Exchange Protocol (BDX)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#distributed-compliance-ledger">11.22. Distributed Compliance Ledger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-12-multiple-fabrics">Chapter 12. Multiple Fabrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#multiple-fabrics">12.1. Multiple Fabrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#chapter-13-security-requirements">Chapter 13. Security Requirements</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#device-vs-node">13.2. Device vs. Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#factory-reset">13.4. Factory Reset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#threats-and-countermeasures">13.7. Threats and Countermeasures</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-a-tag-length-value-tlv-encoding-format">Appendix A: Tag-length-value (TLV) Encoding Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-1-scope-purpose">A.1. Scope &amp; Purpose</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-2-tags">A.2. Tags</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-9-length-encoding">A.9. Length Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-10-end-of-container-encoding">A.10. End of Container Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-11-value-encodings">A.11. Value Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#a-12-tlv-encoding-examples">A.12. TLV Encoding Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-b-tag-length-value-tlv-schema-definitions">Appendix B: Tag-length-value (TLV) Schema Definitions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#b-1-introduction">B.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#b-2-definitions">B.2. Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#b-3-types">B.3. Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#b-4-pseudo-types">B.4. Pseudo-Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#b-5-qualifiers">B.5. Qualifiers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-c-tag-length-value-tlv-payload-text-representation-format">Appendix C: Tag-length-value (TLV) Payload Text Representation Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#c-1-introduction">C.1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#c-3-examples">C.3. Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-d-status-report-messages">Appendix D: Status Report Messages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/matter.html#d-3-message-format">D.3. Message Format</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-e-matter-specific-asn-1-object-identifiers-oids">Appendix E: Matter-Specific ASN.1 Object Identifiers (OIDs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-f-cryptographic-test-vectors-for-some-procedures">Appendix F: Cryptographic test vectors for some procedures</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/matter.html#appendix-g-minimal-resource-requirements">Appendix G: Minimal Resource Requirements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html">Matter协议分析</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#id2">简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#id3">算法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#ecc">椭圆曲线密码学 (ECC) 原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#bridge">Bridge</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#factory-data">Factory Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#id4">安全</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#pase">PASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#id5">配网过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#case">CASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#group">Group</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#cluster">cluster</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#ota">OTA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/Matter%E5%8D%8F%E8%AE%AE%E5%88%86%E6%9E%90.html#id6">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../matters/chatgpt.html">chatGPT学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-01-introduction-document">Chapter 01 — Introduction Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-02-architecture-document">Chapter 02 — Architecture Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-03-cryptographic-primitives-document">Chapter 03 — Cryptographic Primitives Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-04-secure-channel-document">Chapter 04 — Secure Channel Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-05-commissioning-document">Chapter 05 — Commissioning Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-06-device-attestation-document">Chapter 06 — Device Attestation Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-07-data-model-document">Chapter 07 — Data Model Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-08-interaction-model-document">Chapter 08 — Interaction Model Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-09-system-model-document">Chapter 09 — System Model Document</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id2">概述和定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id3">设备类型和服务类型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id4">特征</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id5">系统模型实例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-10-interaction-encoding-document">Chapter 10 — Interaction Encoding Document</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id6">概述和定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id7">数据类型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id8">交互编码格式</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-11-device-management-document">Chapter 11 — Device Management Document</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id9">概述和定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id10">设备组成</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id11">设备状态</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../matters/chatgpt.html#id12">设备操作</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-12-multiple-fabrics-document">Chapter 12 — Multiple Fabrics Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#chapter-13-security-requirements-document">Chapter 13 — Security Requirements Document</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#appendix-a-tag-length-value-tlv-encoding-format">Appendix A: Tag-length-value (TLV) Encoding Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#appendix-b-tag-length-value-tlv-schema-definitions">Appendix B: Tag-length-value (TLV) Schema Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#appendix-c-tag-length-value-tlv-payload-text-representation-format">Appendix C: Tag-length-value (TLV) Payload Text Representation Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../matters/chatgpt.html#appendix-d-status-report-messages">Appendix D: Status Report Messages</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../rfc.html">rfc</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html">RFC791: IP: INTERNET PROTOCOL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#preface">PREFACE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#introduction">1.  INTRODUCTION</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#overview">2.  OVERVIEW</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#relation-to-other-protocols">2.1.  Relation to Other Protocols</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#model-of-operation">2.2.  Model of Operation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#function-description">2.3.  Function Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#gateways">2.4.  Gateways</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#specification">3.  SPECIFICATION</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#internet-header-format">3.1.  Internet Header Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#discussion">3.2  Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#interfaces">3.3  Interfaces</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#appendix-a-examples-scenarios">APPENDIX A:  Examples &amp; Scenarios</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#minimal-data-carrying-internet-datagram">minimal data carrying internet datagram</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#moderate-size-internet-datagram-452-data-octets">moderate size internet datagram (452 data octets)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#datagram-containing-options">datagram containing options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#appendix-b-data-transmission-order">APPENDIX B:  Data Transmission Order</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0791-IP%20Spec.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc0792-ICMP.html">RFC792: ICMP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc0792-ICMP.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html">RFC3569: An Overview of Source-Specific Multicast (SSM)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#terminology">2.  Terminology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#any-source-multicast-asm">Any-Source Multicast (ASM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#source-specific-multicast-ssm">Source-Specific Multicast (SSM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#source-filtered-multicast-sfm">Source-Filtered Multicast (SFM)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#the-igmp-pim-sm-msdp-mbgp-protocol-suite-for-asm">3.  The IGMP/PIM-SM/MSDP/MBGP Protocol Suite for ASM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#problems-with-current-architecture">4.  Problems with Current Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#source-specific-multicast-ssm-benefits-and-requirements">5.  Source Specific Multicast (SSM): Benefits and Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#ssm-framework">6.  SSM Framework</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#address-allocation">6.1.  Address Allocation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#session-description-and-channel-discovery">6.2.  Session Description and Channel Discovery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#ssm-aware-applications">6.3.  SSM-Aware Applications</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#igmpv3-mldv2-host-reporting-and-querier">6.4.  IGMPv3/MLDv2 Host Reporting and Querier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#pim-ssm-routing">6.5.  PIM-SSM Routing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#interoperability-with-existing-multicast-service-models">7.  Interoperability with Existing Multicast Service Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#id2">应用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#ssm">SSM示例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc3569-SSM.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc4301.html">RFC4301: Security Architecture for the IP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc4301.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc4302-IP%20Authentication%20Header.html">RFC4302: IP Authentication Header</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc4302-IP%20Authentication%20Header.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc4303.html">RFC4303: IP Encapsulating Security Payload (ESP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ips/rfc4303.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ips/rfc4693-CIDR.html">RFC4693: Classless Inter-domain Routing (CIDR)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html">RFC3306: Unicast-Prefix-based IPv6 Multicast Addresses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#ipv6">IPv6 多播地址中前缀长度的取值范围</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#id2">多播地址的分配规则</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#motivation">2. Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#multicast-address-format">4. Multicast Address Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#ssm-source-specific-multicast-addresses">6. SSM(Source-Specific Multicast Addresses)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#examples">7. Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc3306-Unicast-Prefix-based%20IPv6%20Multicast%20Addresses.html#id3">参考</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html">RFC4007: IPv6 Scoped Address Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#address-scope">4.  Address Scope</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#scope-zones">5.  Scope Zones</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#zone-indices">6.  Zone Indices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#sending-packets">7.  Sending Packets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#receiving-packets">8.  Receiving Packets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#forwarding">9.  Forwarding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#routing">10.  Routing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#textual-representation">11.  Textual Representation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#examples">Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4007-IPv6%20Scoped%20Address%20Architecture.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html">RFC4291: IP Version 6 Addressing Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#id2">学习</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#keypoints">keypoints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#id3">地址类型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#id4">地址分配</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#ipv6-addressing">2.  IPv6 Addressing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#addressing-model">2.1.  Addressing Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#text-representation-of-addresses">2.2.  Text Representation of Addresses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#text-representation-of-address-prefixes">2.3.  Text Representation of Address Prefixes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#address-type-identification">2.4. Address Type Identification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#unicast-addresses">2.5.  Unicast Addresses</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#anycast-addresses">2.6.  Anycast Addresses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#multicast-addresses">2.7.  Multicast Addresses</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#pre-defined-multicast-addresses">2.7.1.  Pre-Defined Multicast Addresses</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#a-node-s-required-addresses">2.8. A Node’s Required Addresses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#security-considerations">3. Security Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#appendix-a-creating-modified-eui-64-format-interface-identifiers">Appendix A: Creating Modified EUI-64 Format Interface Identifiers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#links-or-nodes-with-ieee-eui-64-identifiers">Links or Nodes with IEEE EUI-64 Identifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#links-or-nodes-with-ieee-802-48-bit-macs">Links or Nodes with IEEE 802 48-bit MACs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#links-with-other-kinds-of-identifiers">Links with Other Kinds of Identifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#links-without-identifiers">Links without Identifiers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc4291-IPv6%20Addressing%20Architecture.html#id5">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc6437.html">RFC6437: IPv6 Flow Label Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc6437.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html">RFC7346: IPv6 Multicast Address Scopes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#definition-of-ipv6-multicast-address-scopes-updates-rfc-4291">2.  Definition of IPv6 Multicast Address Scopes (Updates RFC 4291)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#definition-of-realm-local-scopes">3.  Definition of Realm-Local Scopes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#definition-of-realm-local-scope-for-ieee-802-15-4">5.  Definition of Realm-Local Scope for IEEE 802.15.4</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7346-IPv6%20Multicast%20Address%20Scopes.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc7707.html">RFC7707: Network Reconnaissance in IPv6 Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc7707.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html">RFC8200 Internet Protocol, Version 6 (IPv6) Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#introduction">1.  Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#changes-from-ipv4-to-ipv6">changes from IPv4 to IPv6</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#related-rfc">related RFC</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#terminology">2.  Terminology</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#ipv6-header-format">3.  IPv6 Header Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#ipv6-extension-headers">4.  IPv6 Extension Headers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#extension-header-order">4.1.  Extension Header Order</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#options">4.2.  Options</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#hop-by-hop-options-header">4.3.  Hop-by-Hop Options Header</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#routing-header">4.4.  Routing Header</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#fragment-header">4.5.  Fragment Header</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#destination-options-header">4.6.  Destination Options Header</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#no-next-header">4.7.  No Next Header</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#defining-new-extension-headers-and-options">4.8.  Defining New Extension Headers and Options</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#packet-size-issues">5.  Packet Size Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#flow-labels">6.  Flow Labels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#traffic-classes">7.  Traffic Classes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#upper-layer-protocol-issues">8.  Upper-Layer Protocol Issues</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#upper-layer-checksums">8.1.  Upper-Layer Checksums</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#maximum-packet-lifetime">8.2.  Maximum Packet Lifetime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#maximum-upper-layer-payload-size">8.3.  Maximum Upper-Layer Payload Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#responding-to-packets-carrying-routing-headers">8.4.  Responding to Packets Carrying Routing Headers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#iana-considerations">9.  IANA Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#security-considerations">10. Security Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#same-with-ipv4">same with ipv4</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#compare-with-ipv4">compare with ipv4</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#references">11. References</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#appendix-a-formatting-guidelines-for-options">Appendix A.  Formatting Guidelines for Options</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#appendix-b-changes-since-rfc-2460">Appendix B.  Changes Since RFC 2460</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8200-IPv6%20Spec.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/ipv6/rfc8201.html">RFC8201: Path MTU Discovery for IP version 6</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/ipv6/rfc8201.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/tcps/rfc9293-TCP.html">RFC9293: Transmission Control Protocol (TCP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/tcps/rfc9293-TCP.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/tcps/rfc0768-UDP.html">RFC0768: User Datagram Protocol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/tcps/rfc0768-UDP.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html">rfc7230: HTTP/1.1: Message Syntax and Routing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#id2">定义</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#hop-by-hop-and-end-to-end">hop-by-hop and end-to-end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#inbound-and-outbound">Inbound and Outbound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#head-of-line-hol-blocking-problem">head-of-line (HOL) blocking problem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#abnf">ABNF语法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#introduction">1.  Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#syntax-notation">1.2.  Syntax Notation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#architecture">2.  Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#client-server-messaging">2.1.  Client/Server Messaging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#implementation-diversity">2.2.  Implementation Diversity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#intermediaries">2.3.  Intermediaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#caches">2.4.  Caches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#conformance-and-error-handling">2.5.  Conformance and Error Handling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#protocol-versioning">2.6. Protocol Versioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#uniform-resource-identifiers">2.7. Uniform Resource Identifiers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#message-format">3. Message Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#start-line">3.1. Start Line</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#header-fields">3.2. Header Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#message-body">3.3. Message Body</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#handling-incomplete-messages">3.4. Handling Incomplete Messages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#transfer-codings">4. Transfer Codings</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#chunked-transfer-coding">4.1.  Chunked Transfer Coding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#compression-codings">4.2. Compression Codings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#te">4.3. TE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#trailer">4.4. Trailer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#message-routing">5. Message Routing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#identifying-a-target-resource">5.1.  Identifying a Target Resource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#connecting-inbound">5.2.  Connecting Inbound</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#request-target">5.3.  Request Target</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#host">5.4. Host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#effective-request-uri">5.5. Effective Request URI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#associating-a-response-to-a-request">5.6. Associating a Response to a Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#message-forwarding">5.7. Message Forwarding</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#connection-management">6. Connection Management</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#connection">6.1.  Connection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#establishment">6.2. Establishment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#persistence">6.3. Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#concurrency">6.4. Concurrency</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#failures-and-timeouts">6.5. Failures and Timeouts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#tear-down">6.6. Tear-down</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#upgrade">6.7. Upgrade</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#abnf-list-extension-rule">7. ABNF List Extension: #rule</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#iana-considerations">8.  IANA Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#header-field-registration">8.1.  Header Field Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#uri-scheme-registration">8.2.  URI Scheme Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#internet-media-type-registration">8.3.  Internet Media Type Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#transfer-coding-registry">8.4.  Transfer Coding Registry</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#content-coding-registration">8.5.  Content Coding Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7230%20HTTP-Message%20Syntax%20and%20Routing.html#upgrade-token-registry">8.6.  Upgrade Token Registry</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7231%20HTTP-Semantics%20and%20Content.html">rfc7231: HTTP/1.1: Semantics and Content</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7232%20HTTP-Conditional%20Requests.html">rfc7232: HTTP/1.1: Conditional Requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7233%20HTTP-Range%20Requests.html">rfc7233: HTTP/1.1: Range Requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7234%20HTTP-Caching.html">rfc7234: HTTP/1.1: Caching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/obsoleted-rfc7235%20HTTP-Authentication.html">rfc7235: HTTP/1.1: Authentication</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/rfc9110-HTTP%20Semantics.html">rfc9110: HTTP Semantics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http/rfc9110-HTTP%20Semantics.html#introduction">1. Introduction</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/rfc9111-HTTP%20Caching.html">rfc9111: HTTP Caching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http/rfc9112-HTTP1.1.html">rfc9112: HTTP/1.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http3/rfc9000.html">RFC9000: QUIC: A UDP-Based Multiplexed and Secure Transport</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http3/rfc9000.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http3/rfc9001.html">RFC9001: Using TLS to Secure QUIC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http3/rfc9002.html">RFC9002: QUIC Loss Detection and Congestion Control</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http3/rfc9002.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http3/rfc9114.html">RFC9114: HTTP/3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http3/rfc9114.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/http3/rfc9204.html">RFC9204: QPACK: Field Compression for HTTP/3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/http3/rfc9204.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/dns/rfc1035.html">RFC1035: DOMAIN NAMES-IMPLEMENTATION AND SPECIFICATION</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/dns/rfc2782.html">RFC2782: DNS SRV</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html">RFC6762: mDNS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id2">收集</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#chatgpt">chatGPT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id3">规范和要求</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id4">实现和应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id5">安全性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id6">性能和可扩展性</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#mdns-names">3.  mDNS Names</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#reverse-address-mapping">4. Reverse Address Mapping</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#querying">5. Querying</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#one-shot-mdns-queries">5.1.  One-Shot mDNS Queries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#continuous-mdns-querying">5.2.  Continuous mDNS Querying</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#multiple-questions-per-query">5.3.  Multiple Questions per Query</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#questions-requesting-unicast-responses">5.4.  Questions Requesting Unicast Responses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#direct-unicast-queries-to-port-5353">5.5.  Direct Unicast Queries to Port 5353</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#responding">6. Responding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#common">common</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#negative-responses">6.1.  Negative Responses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#responding-to-address-queries">6.2.  Responding to Address Queries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#traffic-reduction">7. Traffic Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#probing-and-announcing-on-startup">8. Probing and Announcing on Startup</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#conflict-resolution">9. Conflict Resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#resource-record-ttl-values-and-cache-coherency">10. Resource Record TTL Values and Cache Coherency</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#source-address-check">11.  Source Address Check</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#special-characteristics-of-mdns-domains">12.  Special Characteristics of mDNS Domains</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#enabling-and-disabling-mdns">13.  Enabling and Disabling mDNS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#considerations-for-multiple-interfaces">14.  Considerations for Multiple Interfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#considerations-for-multiple-responders-on-the-same-machine">15.  Considerations for Multiple Responders on the Same Machine</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#mdns-character-set">16.  mDNS Character Set</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#mdns-message-size">17.  mDNS Message Size</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#mdns-message-format">18.  mDNS Message Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id-query-identifier">18.1.  ID (Query Identifier)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#qr-query-response-bit">18.2.  QR (Query/Response) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#opcode">18.3.  OPCODE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#aa-authoritative-answer-bit">18.4.  AA (Authoritative Answer) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#tc-truncated-bit">18.5.  TC (Truncated) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#rd-recursion-desired-bit">18.6.  RD (Recursion Desired) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#ra-recursion-available-bit">18.7.  RA (Recursion Available) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#z-zero-bit">18.8.  Z (Zero) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#ad-authentic-data-bit">18.9.  AD (Authentic Data) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#cd-checking-disabled-bit">18.10.  CD (Checking Disabled) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#rcode-response-code">18.11.  RCODE (Response Code)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#repurposing-of-top-bit-of-qclass-in-question-section">18.12.  Repurposing of Top Bit of qclass in Question Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#repurposing-of-top-bit-of-rrclass-in-resource-record-sections">18.13.  Repurposing of Top Bit of rrclass in Resource Record Sections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#name-compression">18.14.  Name Compression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id7">18.5.  TC (Truncated) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id8">18.6.  RD (Recursion Desired) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id9">18.7.  RA (Recursion Available) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id10">18.8.  Z (Zero) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id11">18.9.  AD (Authentic Data) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id12">18.10.  CD (Checking Disabled) Bit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id13">18.11.  RCODE (Response Code)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id14">18.12.  Repurposing of Top Bit of qclass in Question Section</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id15">18.13.  Repurposing of Top Bit of rrclass in Resource Record Sections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id16">18.14.  Name Compression</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#summary-of-differences-between-mdns-and-unicast-dns">19.  Summary of Differences between mDNS and Unicast DNS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#ipv6-considerations">20.  IPv6 Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#security-considerations">21.  Security Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#iana-considerations">22. IANA Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-a-design-rationale-for-choice-of-udp-port-number">Appendix A. Design Rationale for Choice of UDP Port Number</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-b-design-rationale-for-not-using-hashed-multicast-addresses">Appendix B. Design Rationale for Not Using Hashed Multicast Addresses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-c-design-rationale-for-maximum-multicast-dns-name-length">Appendix C. Design Rationale for Maximum Multicast DNS Name Length</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-d-benefits-of-multicast-responses">Appendix D. Benefits of Multicast Responses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-e-design-rationale-for-encoding-negative-responses">Appendix E. Design Rationale for Encoding Negative Responses</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-f-use-of-utf-8">Appendix F. Use of UTF-8</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-g-private-dns-namespaces">Appendix G. Private DNS Namespaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#appendix-h-deployment-history">Appendix H.  Deployment History</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6762.html#id17">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html">RFC6763: DNS-Based Service Discovery</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#id2">收集</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#design-goals">3.  Design Goals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#service-instance-enumeration-browsing">4.  Service Instance Enumeration (Browsing)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#structured-service-instance-names">4.1.  Structured Service Instance Names</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#user-interface-presentation">4.2.  User Interface Presentation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#internal-handling-of-names">4.3.  Internal Handling of Names</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#service-instance-resolution">5. Service Instance Resolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#data-syntax-for-dns-sd-txt-records">6. Data Syntax for DNS-SD TXT Records</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#general-format-rules-for-dns-txt-records">6.1. General Format Rules for DNS TXT Records</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#dns-sd-txt-record-size">6.2. DNS-SD TXT Record Size</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#dns-txt-record-format-rules-for-use-in-dns-sd">6.3. DNS TXT Record Format Rules for Use in DNS-SD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#rules-for-keys-in-dns-sd-key-value-pairs">6.4. Rules for Keys in DNS-SD Key/Value Pairs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#rules-for-values-in-dns-sd-key-value-pairs">6.5. Rules for Values in DNS-SD Key/Value Pairs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#example-txt-record">6.6. Example TXT Record</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#version-tag">6.7. Version Tag</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#service-instances-with-multiple-txt-records">6.8. Service Instances with Multiple TXT Records</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#id3">7. Service Names</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#selective-instance-enumeration-subtypes">7.1. Selective Instance Enumeration (Subtypes)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#service-name-length-limits">7.2. Service Name Length Limits</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#flagship-naming">8. Flagship Naming</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#service-type-enumeration">9. Service Type Enumeration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#populating-the-dns-with-information">10. Populating the DNS with Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#domain-enumeration">11. Domain Enumeration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#dns-additional-record-generation">12.  DNS Additional Record Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#ptr-records">12.1.  PTR Records</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#srv-records">12.2.  SRV Records</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#txt-records">12.3.  TXT Records</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#other-record-types">12.4.  Other Record Types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#working-examples">13.  Working Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#ipv6-considerations">14.  IPv6 Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#security-considerations">15.  Security Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#iana-considerations">16.  IANA Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-a-rationale-for-using-dns-as-a-basis-for-service-discovery">Appendix A.  Rationale for Using DNS as a Basis for Service Discovery</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-b-ordering-of-service-instance-name-components">Appendix B.  Ordering of Service Instance Name Components</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#b-1-semantic-structure">B.1.  Semantic Structure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#b-2-network-efficiency">B.2.  Network Efficiency</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#b-3-operational-flexibility">B.3.  Operational Flexibility</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-c-what-you-see-is-what-you-get">Appendix C.  What You See Is What You Get</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-d-choice-of-factory-default-names">Appendix D.  Choice of Factory-Default Names</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-e-name-encodings-in-the-domain-name-system">Appendix E.  Name Encodings in the Domain Name System</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#appendix-f-continuous-live-update-browsing-model">Appendix F.  “Continuous Live Update” Browsing Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc6763.html#id4">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/dns/rfc8766.html">RFC8766: Discovery Proxy for Multicast DNS-Based Service Discovery</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/dns/rfc8766.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc2974-Session%20Announcement%20Protocol.html">RFC2974: Session Announcement Protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html">RFC3261: SIP: Session Initiation Protocol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#overview-of-sip-functionality">2 Overview of SIP Functionality</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#overview-of-operation">4 Overview of Operation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#structure-of-the-protocol">5 Structure of the Protocol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#definitions">6 Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#sip-messages">7 SIP Messages</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#requests">7.1 Requests</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#responses">7.2 Responses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#header-fields">7.3 Header Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#bodies">7.4 Bodies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#framing-sip-messages">7.5 Framing SIP Messages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#general-user-agent-behavior">8 General User Agent Behavior</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#uac-behavior">8.1 UAC Behavior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#uas-behavior">8.2 UAS Behavior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#redirect-servers">8.3 Redirect Servers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#canceling-a-request">9 Canceling a Request</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#client-behavior">9.1 Client Behavior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#server-behavior">9.2 Server Behavior</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#registrations">10 Registrations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#overview">10.1 Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#constructing-the-register-request">10.2 Constructing the REGISTER Request</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#processing-register-requests">10.3 Processing REGISTER Requests</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#querying-for-capabilities">11 Querying for Capabilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#dialogs">12 Dialogs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#creation-of-a-dialog">12.1 Creation of a Dialog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#requests-within-a-dialog">12.2 Requests within a Dialog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#termination-of-a-dialog">12.3 Termination of a Dialog</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#initiating-a-session">13 Initiating a Session</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id3">13.1 Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#uac-processing">13.2 UAC Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#uas-processing">13.3 UAS Processing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#modifying-an-existing-session">14 Modifying an Existing Session</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id4">14.1 UAC Behavior</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id5">14.2 UAS Behavior</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#terminating-a-session">15 Terminating a Session</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#terminating-a-session-with-a-bye-request">15.1 Terminating a Session with a BYE Request</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#proxy-behavior">16 Proxy Behavior</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id8">16.1 Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#stateful-proxy">16.2 Stateful Proxy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#request-validation">16.3 Request Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#route-information-preprocessing">16.4 Route Information Preprocessing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#determining-request-targets">16.5 Determining Request Targets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#request-forwarding">16.6 Request Forwarding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#response-processing">16.7 Response Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#processing-timer-c">16.8 Processing Timer C</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#handling-transport-errors">16.9 Handling Transport Errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#cancel-processing">16.10 CANCEL Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#stateless-proxy">16.11 Stateless Proxy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#summary-of-proxy-route-processing">16.12 Summary of Proxy Route Processing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#transactions">17 Transactions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#client-transaction">17.1 Client Transaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#server-transaction">17.2 Server Transaction</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#transport">18 Transport</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#common-message-components">19 Common Message Components</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#sip-and-sips-uniform-resource-indicators">19.1 SIP and SIPS Uniform Resource Indicators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#option-tags">19.2 Option Tags</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#tags">19.3 Tags</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id10">20 Header Fields</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#accept">20.1 Accept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#accept-encoding">20.2 Accept-Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#accept-language">20.3 Accept-Language</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#alert-info">20.4 Alert-Info</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#allow">20.5 Allow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#authentication-info">20.6 Authentication-Info</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#authorization">20.7 Authorization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#call-id">20.8 Call-ID</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#call-info">20.9 Call-Info</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#contact">20.10 Contact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#content-disposition">20.11 Content-Disposition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#content-encoding">20.12 Content-Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#content-language">20.13 Content-Language</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#content-length">20.14 Content-Length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#content-type">20.15 Content-Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#cseq">20.16 CSeq</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#date">20.17 Date</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#error-info">20.18 Error-Info</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#expires">20.19 Expires</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#from">20.20 From</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#in-reply-to">20.21 In-Reply-To</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#max-forwards">20.22 Max-Forwards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#min-expires">20.23 Min-Expires</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#mime-version">20.24 MIME-Version</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#organization">20.25 Organization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#priority">20.26 Priority</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#proxy-authenticate">20.27 Proxy-Authenticate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#proxy-authorization">20.28 Proxy-Authorization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#proxy-require">20.29 Proxy-Require</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#record-route">20.30 Record-Route</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#reply-to">20.31 Reply-To</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id11">20.32 Require</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#retry-after">20.33 Retry-After</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#route">20.34 Route</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#server">20.35 Server</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#subject">20.36 Subject</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#supported">20.37 Supported</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#timestamp">20.38 Timestamp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#to">20.39 To</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#unsupported">20.40 Unsupported</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#user-agent">20.41 User-Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#via">20.42 Via</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#warning">20.43 Warning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#www-authenticate">20.44 WWW-Authenticate</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#response-codes">21 Response Codes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#provisional-1xx">21.1 Provisional 1xx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#successful-2xx">21.2 Successful 2xx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#redirection-3xx">21.3 Redirection 3xx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#request-failure-4xx">21.4 Request Failure 4xx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#server-failure-5xx">21.5 Server Failure 5xx</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#global-failures-6xx">21.6 Global Failures 6xx</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#usage-of-http-authentication">22 Usage of HTTP Authentication</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#framework">22.1 Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#user-to-user-authentication">22.2 User-to-User Authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#proxy-to-user-authentication">22.3 Proxy-to-User Authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#the-digest-authentication-scheme">22.4 The Digest Authentication Scheme</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#s-mime">23 S/MIME</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#s-mime-certificates">23.1 S/MIME Certificates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#s-mime-key-exchange">23.2 S/MIME Key Exchange</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#securing-mime-bodies">23.3 Securing MIME bodies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#sip-header-privacy-and-integrity-using-s-mime-tunneling-sip">23.4 SIP Header Privacy and Integrity using S/MIME: Tunneling SIP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#id12">24 Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#registration">24.1 Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#session-setup">24.2 Session Setup</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#augmented-bnf-for-the-sip-protocol">25 Augmented BNF for the SIP Protocol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#security-considerations-threat-model-and-security-usage-recommendations">26 Security Considerations: Threat Model and Security Usage Recommendations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#iana-considerations">27 IANA Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#changes-from-rfc-2543">28 Changes From RFC 2543</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#major-functional-changes">28.1 Major Functional Changes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#minor-functional-changes">28.2 Minor Functional Changes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3261-SIP-Session%20Initiation%20Protocol.html#a-table-of-timer-values">A Table of Timer Values</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html">RFC3550: RTP: A Transport Protocol for Real-Time Applications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-use-scenarios">2. RTP Use Scenarios</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#simple-multicast-audio-conference">2.1 Simple Multicast Audio Conference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#audio-and-video-conference">2.2 Audio and Video Conference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#mixers-and-translators">2.3 Mixers and Translators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#layered-encodings">2.4 Layered Encodings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#definitions">3. Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#byte-order-alignment-and-time-format">4. Byte Order, Alignment, and Time Format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-data-transfer-protocol">5. RTP Data Transfer Protocol</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-fixed-header-fields">5.1 RTP Fixed Header Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#multiplexing-rtp-sessions">5.2 Multiplexing RTP Sessions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#profile-specific-modifications-to-the-rtp-header">5.3  Profile-Specific Modifications to the RTP Header</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-control-protocol-rtcp">6.  RTP Control Protocol – RTCP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-packet-format">6.1  RTCP Packet Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-transmission-interval">6.2  RTCP Transmission Interval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-packet-send-and-receive-rules">6.3  RTCP Packet Send and Receive Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#sender-and-receiver-reports">6.4  Sender and Receiver Reports</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#sdes-source-description-rtcp-packet">6.5  SDES: Source Description RTCP Packet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#bye-goodbye-rtcp-packet">6.6  BYE: Goodbye RTCP Packet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#app-application-defined-rtcp-packet">6.7  APP: Application-Defined RTCP Packet</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-translators-and-mixers">7.  RTP Translators and Mixers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#general-description">7.1  General Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-processing-in-translators">7.2  RTCP Processing in Translators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-processing-in-mixers">7.3  RTCP Processing in Mixers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#cascaded-mixers">7.4  Cascaded Mixers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#ssrc-identifier-allocation-and-use">8.  SSRC Identifier Allocation and Use</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#security">9.  Security</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#confidentiality">9.1 Confidentiality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#authentication-and-message-integrity">9.2 Authentication and Message Integrity</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#congestion-control">10. Congestion Control</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-over-network-and-transport-protocols">11. RTP over Network and Transport Protocols</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#summary-of-protocol-constants">12. Summary of Protocol Constants</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtcp-packet-types">12.1 RTCP Packet Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#sdes-types">12.2 SDES Types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#rtp-profiles-and-payload-format-specifications">13. RTP Profiles and Payload Format Specifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#appendix-a-algorithms">Appendix A.   Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3550-RTP-A%20Transport%20Protocol%20for%20Real-Time%20Applications.html#appendix-b-changes-from-rfc-1889">Appendix B.   Changes from RFC 1889</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html">RFC3551: RTP Profile for Audio and Video Conferences with Minimal Control</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#rtp-and-rtcp-packet-forms-and-protocol-behavior">2. RTP and RTCP Packet Forms and Protocol Behavior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#registering-additional-encodings">3.  Registering Additional Encodings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#audio">4.  Audio</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#encoding-independent-rules">4.1  Encoding-Independent Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#operating-recommendations">4.2  Operating Recommendations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#guidelines-for-sample-based-audio-encodings">4.3  Guidelines for Sample-Based Audio Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#guidelines-for-frame-based-audio-encodings">4.4  Guidelines for Frame-Based Audio Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#audio-encodings">4.5 Audio Encodings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#video">5.  Video</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#payload-type-definitions">6.  Payload Type Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#rtp-over-tcp-and-similar-byte-stream-protocols">7.  RTP over TCP and Similar Byte Stream Protocols</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#port-assignment">8.  Port Assignment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc3551-RTP%20Profile%20for%20Audio%20and%20Video%20Conferences%20with%20Minimal%20Control.html#changes-from-rfc-1890">9.  Changes from RFC 1890</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html">RFC6184: RTP Payload Format for H.264 Video</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#introduction">1. Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#the-h-264-codec">1.1.  The H.264 Codec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#parameter-set-concept">1.2.  Parameter Set Concept</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#network-abstraction-layer-unit-types">1.3.  Network Abstraction Layer Unit Types</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#conventions">2. Conventions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#scope">3. Scope</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#definitions-and-abbreviations">4. Definitions and Abbreviations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#definitions">4.1.  Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#abbreviations">4.2.  Abbreviations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#rtp-payload-format">5. RTP Payload Format</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#rtp-header-usage">5.1.  RTP Header Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#payload-structures">5.2.  Payload Structures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#nal-unit-header-usage">5.3.  NAL Unit Header Usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#packetization-modes">5.4.  Packetization Modes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#decoding-order-number-don">5.5.  Decoding Order Number (DON)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#single-nal-unit-packet">5.6.  Single NAL Unit Packet</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#aggregation-packets">5.7.  Aggregation Packets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#fragmentation-units-fus">5.8.  Fragmentation Units (FUs)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#packetization-rules">6. Packetization Rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#de-packetization-process">7. De-Packetization Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#payload-format-parameters">8. Payload Format Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#media-type-registration">8.1.  Media Type Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#sdp-parameters">8.2.  SDP Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#examples">8.3.  Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#parameter-set-considerations">8.4.  Parameter Set Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#decoder-refresh-point-procedure-using-in-band-transport-of-parameter-sets-informative">8.5.  Decoder Refresh Point Procedure Using In-Band Transport of Parameter Sets (Informative)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#informative-appendix-application-examples">12. Informative Appendix: Application Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#video-telephony-according-to-annex-a-of-itu-t-recommendation-h-241">12.1.  Video Telephony According to Annex A of ITU-T Recommendation H.241</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#video-telephony-no-slice-data-partitioning-no-nal-unit-aggregation">12.2.  Video Telephony, No Slice Data Partitioning, No NAL Unit Aggregation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#video-telephony-interleaved-packetization-using-nal-unit-aggregation">12.3.  Video Telephony, Interleaved Packetization Using NAL Unit Aggregation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#video-telephony-with-data-partitioning">12.4.  Video Telephony with Data Partitioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#video-telephony-or-streaming-with-fus-and-forward-error-correction">12.5.  Video Telephony or Streaming with FUs and Forward Error Correction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#low-bitrate-streaming">12.6.  Low Bitrate Streaming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#robust-packet-scheduling-in-video-streaming">12.7.  Robust Packet Scheduling in Video Streaming</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#informative-appendix-rationale-for-decoding-order-number">13. Informative Appendix: Rationale for Decoding Order Number</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc6184-RTP%20Payload%20Format%20for%20H.264%20Video.html#example-of-robust-packet-scheduling">13.3.  Example of Robust Packet Scheduling</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html">RFC7826: Real-Time Streaming Protocol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#protocol-overview">2.  Protocol Overview</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#presentation-description">2.1.  Presentation Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#session-establishment">2.2.  Session Establishment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#media-delivery-control">2.3.  Media Delivery Control</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#session-parameter-manipulations">2.4. Session Parameter Manipulations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#media-delivery">2.5. Media Delivery</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#session-maintenance-and-termination">2.6. Session Maintenance and Termination</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#extending-rtsp">2.7. Extending RTSP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#document-conventions">3. Document Conventions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#terminology">3.2. Terminology</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#protocol-parameters">4. Protocol Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-version">4.1.  RTSP Version</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-iri-and-uri">4.2.  RTSP IRI and URI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#session-identifiers">4.3.  Session Identifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#media-time-formats">4.4.  Media-Time Formats</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#feature-tags">4.5.  Feature Tags</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-body-tags">4.6.  Message Body Tags</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#media-properties">4.7.  Media Properties</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-message">5. RTSP Message</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-types">5.1. Message Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-headers">5.2. Message Headers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-body">5.3. Message Body</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-length">5.4. Message Length</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#general-header-fields">6. General-Header Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#request">7. Request</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#request-line">7.1.  Request Line</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#request-header-fields">7.2.  Request-Header Fields</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#response">8. Response</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#status-line">8.1.  Status-Line</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#response-headers">8.2.  Response Headers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#id2">9. Message Body</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-body-header-fields">9.1.  Message Body Header Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#id3">9.2. Message Body</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#message-body-format-negotiation">9.3. Message Body Format Negotiation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#connections">10. Connections</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#reliability-and-acknowledgements">10.1. Reliability and Acknowledgements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#using-connections">10.2. Using Connections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#closing-connections">10.3. Closing Connections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#timing-out-connections-and-rtsp-messages">10.4.  Timing Out Connections and RTSP Messages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#showing-liveness">10.5.  Showing Liveness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#use-of-ipv6">10.6.  Use of IPv6</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#overload-control">10.7.  Overload Control</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#capability-handling">11. Capability Handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#pipelining-support">12. Pipelining Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#method-definitions">13. Method Definitions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#options">13.1.  OPTIONS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#describe">13.2. DESCRIBE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#setup">13.3.  SETUP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#play">13.4.  PLAY</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#play-notify">13.5.  PLAY_NOTIFY</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#pause">13.6.  PAUSE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#teardown">13.7.  TEARDOWN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#get-parameter">13.8.  GET_PARAMETER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#redirect">13.10.  REDIRECT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#embedded-interleaved-binary-data">14. Embedded (Interleaved) Binary Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#proxies">15. Proxies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#proxies-and-protocol-extensions">15.1.  Proxies and Protocol Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#multiplexing-and-demultiplexing-of-messages">15.2.  Multiplexing and Demultiplexing of Messages</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#caching">16. Caching</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#validation-model">16.1.  Validation Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#invalidation-after-updates-or-deletions">16.2.  Invalidation after Updates or Deletions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#status-code-definitions">17. Status Code Definitions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#header-field-definitions">18. Header Field Definitions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#pipelined-requests">18.33.  Pipelined-Requests</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#security-framework">19. Security Framework</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-and-http-authentication">19.1.  RTSP and HTTP Authentication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-over-tls">19.2. RTSP over TLS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#security-and-proxies">19.3. Security and Proxies</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#syntax">20. Syntax</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#base-syntax">20.1.  Base Syntax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#rtsp-protocol-definition">20.2.  RTSP Protocol Definition</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#security-considerations">21. Security Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#signaling-protocol-threats">21.1.  Signaling Protocol Threats</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-a-examples">Appendix A.  Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-1-media-on-demand-unicast">A.1.  Media on Demand (Unicast)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-2-media-on-demand-using-pipelining">A.2.  Media on Demand Using Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-3-secured-media-session-for-on-demand-content">A.3.  Secured Media Session for On-Demand Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-4-media-on-demand-unicast">A.4.  Media on Demand (Unicast)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-5-single-stream-container-files">A.5.  Single-Stream Container Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-6-live-media-presentation-using-multicast">A.6.  Live Media Presentation Using Multicast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#a-7-capability-negotiation">A.7.  Capability Negotiation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-b-rtsp-protocol-state-machine">Appendix B.  RTSP Protocol State Machine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#b-1-states">B.1.  States</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#b-2-state-variables">B.2.  State Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#b-3-abbreviations">B.3.  Abbreviations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#b-4-state-tables">B.4.  State Tables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-c-media-transport-alternatives">Appendix C.  Media-Transport Alternatives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-1-rtp">C.1.  RTP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-2-rtp-over-tcp">C.2. RTP over TCP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-3-handling-media-clock-time-jumps-in-the-rtp-media-layer">C.3.  Handling Media-Clock Time Jumps in the RTP Media Layer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-4-handling-rtp-timestamps-after-pause">C.4. Handling RTP Timestamps after PAUSE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-5-rtsp-rtp-integration">C.5.  RTSP/RTP Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-6-scaling-with-rtp">C.6.  Scaling with RTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-7-maintaining-npt-synchronization-with-rtp-timestamps">C.7.  Maintaining NPT Synchronization with RTP Timestamps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-8-continuous-audio">C.8.  Continuous Audio</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-9-multiple-sources-in-an-rtp-session">C.9.  Multiple Sources in an RTP Session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-10-usage-of-ssrcs-and-the-rtcp-bye-message-during-an-rtsp-session">C.10.  Usage of SSRCs and the RTCP BYE Message during an RTSP Session</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#c-11-future-additions">C.11.  Future Additions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-d-use-of-sdp-for-rtsp-session-descriptions">Appendix D. Use of SDP for RTSP Session Descriptions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#d-1-definitions">D.1.  Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#d-2-aggregate-control-not-available">D.2.  Aggregate Control Not Available</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#d-3-aggregate-control-available">D.3.  Aggregate Control Available</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#d-4-grouping-of-media-lines-in-sdp">D.4.  Grouping of Media Lines in SDP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#d-5-rtsp-external-sdp-delivery">D.5.  RTSP External SDP Delivery</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-e-rtsp-use-cases">Appendix E. RTSP Use Cases</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#e-1-on-demand-playback-of-stored-content">E.1.  On-Demand Playback of Stored Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#e-2-unicast-distribution-of-live-content">E.2.  Unicast Distribution of Live Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#e-3-on-demand-playback-using-multicast">E.3.  On-Demand Playback Using Multicast</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#e-4-inviting-an-rtsp-server-into-a-conference">E.4.  Inviting an RTSP Server into a Conference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#e-5-live-content-using-multicast">E.5.  Live Content Using Multicast</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-f-text-format-for-parameters">Appendix F.  Text Format for Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-g-requirements-for-unreliable-transport-of-rtsp">Appendix G.  Requirements for Unreliable Transport of RTSP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-h-backwards-compatibility-considerations">Appendix H.  Backwards-Compatibility Considerations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#h-1-play-request-in-play-state">H.1.  Play Request in Play State</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#h-2-using-persistent-connections">H.2.  Using Persistent Connections</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#appendix-i-changes">Appendix I.  Changes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#i-1-brief-overview">I.1.  Brief Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc7826-Real-Time%20Streaming%20Protocol.html#i-2-detailed-list-of-changes">I.2.  Detailed List of Changes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html">RFC8866: SDP-Session Description Protocol</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#glossary-of-terms">2. Glossary of Terms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#examples-of-sdp-usage">3. Examples of SDP Usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#session-initiation">3.1. Session Initiation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#streaming-media">3.2. Streaming Media</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#email-and-the-world-wide-web">3.3. Email and the World Wide Web</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#multicast-session-announcement">3.4. Multicast Session Announcement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#requirements-and-recommendations">4. Requirements and Recommendations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#media-and-transport-information">4.1. Media and Transport Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#timing-information">4.2. Timing Information</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#sdp-specification">5. SDP Specification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#protocol-version-v">5.1. Protocol Version (“v=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#origin-o">5.2. Origin (“o=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#session-name-s">5.3. Session Name (“s=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#session-information-i">5.4. Session Information (“i=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#uri-u">5.5. URI (“u=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#email-address-and-phone-number-e-and-p">5.6. Email Address and Phone Number (“e=” and “p=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#connection-information-c">5.7. Connection Information (“c=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#bandwidth-information-b">5.8. Bandwidth Information (“b=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#time-active-t">5.9. Time Active (“t=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#repeat-times-r">5.10. Repeat Times (“r=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#time-zone-adjustment-z">5.11. Time Zone Adjustment (“z=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#encryption-keys-k">5.12. Encryption Keys (“k=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#attributes-a">5.13. Attributes (“a=”)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#media-descriptions-m">5.14. Media Descriptions (“m=”)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#sdp-attributes">6. SDP Attributes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#cat-category">6.1. cat (Category)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#keywds-keywords">6.2. keywds (Keywords)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#tool">6.3. tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#ptime-packet-time">6.4. ptime (Packet Time)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#maxptime-maximum-packet-time">6.5. maxptime (Maximum Packet Time)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#rtpmap">6.6. rtpmap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#media-direction-attributes">6.7. Media Direction Attributes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#orient-orientation">6.8. orient (Orientation)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#type-conference-type">6.9. type (Conference Type)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#charset-character-set">6.10. charset (Character Set)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#sdplang-sdp-language">6.11. sdplang (SDP Language)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#lang-language">6.12. lang (Language)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#framerate-frame-rate">6.13. framerate (Frame Rate)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#quality">6.14. quality</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#fmtp-format-parameters">6.15. fmtp (Format Parameters)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#security-considerations">7. Security Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#iana-considerations">8. IANA Considerations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#sdp-grammar">9. SDP Grammar</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/streamings/rfc8866-SDP-Session%20Description%20Protocol.html#summary-of-changes-from-rfc-4566">10. Summary of Changes from RFC 4566</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc7350.html">rfc7350: DTLS as Transport for STUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc5769.html">RFC5769: Test Vectors for STUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc5780.html">rfc5780: NAT Behavior Discovery Using STUN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc7443.html">rfc7443: ALPN Labels for STUN Usages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc7635.html">rfc7635: STUN Extension for Third-Party Authorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/STUNs/rfc8489.html">RFC8489: STUN - Session Traversal Utilities for NAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/x.509/rfc5280.html">RFC5280: Internet X.509 PKIC and CRL Profile</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/x.509/rfc5280.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/x.509/rfc5652.html">RFC5652: Cryptographic Message Syntax (CMS)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/x.509/rfc5652.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/x.509/rfc5912.html">RFC5912: New ASN.1 Modules for the PKIX</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/x.509/rfc5912.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html">RFC2045: (MIME) Part One: Format of Internet Message Bodies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#introduction">1.  Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#definitions-conventions-and-generic-bnf-grammar">2.  Definitions, Conventions, and Generic BNF Grammar</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#mime-header-fields">3.  MIME Header Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#mime-version-header-field">4.  MIME-Version Header Field</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#id2">示例</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-type-header-field">5.  Content-Type Header Field</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#syntax-of-the-content-type-header-field">5.1 Syntax of the Content-Type Header Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-type-defaults">5.2 Content-Type Defaults</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-transfer-encoding-header-field">6. Content-Transfer-Encoding Header Field</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-transfer-encoding-syntax">6.1.  Content-Transfer-Encoding Syntax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-transfer-encodings-semantics">6.2.  Content-Transfer-Encodings Semantics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#new-content-transfer-encodings">6.3.  New Content-Transfer-Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#interpretation-and-use">6.4.  Interpretation and Use</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#translating-encodings">6.5.  Translating Encodings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#canonical-encoding-model">6.6.  Canonical Encoding Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#quoted-printable-content-transfer-encoding">6.7.  Quoted-Printable Content-Transfer-Encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#base64-content-transfer-encoding">6.8.  Base64 Content-Transfer-Encoding</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-id-header-field">7.  Content-ID Header Field</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content-description-header-field">8.  Content-Description Header Field</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#additional-mime-header-fields">9.  Additional MIME Header Fields</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#appendix-a-collected-grammar">Appendix A – Collected Grammar</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#content">content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#encoding">encoding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#id">id</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#description">description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#mime-extension-field">MIME-extension-field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2045-MIME1.html#id3">通用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html">RFC2046: (MIME) Part Two: Media Types</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#overview-of-the-initial-top-level-media-types">3. Overview Of The Initial Top-Level Media Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#discrete-media-type-values">4. Discrete Media Type Values</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#text-media-type">4.1.  Text Media Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#image-media-type">4.2.  Image Media Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#audio-media-type">4.3.  Audio Media Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#video-media-type">4.4.  Video Media Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#application-media-type">4.5.  Application Media Type</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#composite-media-type-values">5. Composite Media Type Values</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#multipart-media-type">5.1.  Multipart Media Type</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#message-media-type">5.2 Message Media Type</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2046-MIME2.html#experimental-media-type-values">6.  Experimental Media Type Values</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html">RFC2047: (MIME) Part Three: Message Header Extensions for Non-ASCII Text</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#syntax-of-encoded-words">2. Syntax of encoded-words</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#character-sets">3. Character sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#encodings">4. Encodings</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#use-of-encoded-words-in-message-headers">5. Use of encoded-words in message headers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#support-of-encoded-word-s-by-mail-readers">6. Support of ‘encoded-word’s by mail readers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#conformance">7. Conformance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2047-MIME3.html#examples">8. Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html">RFC2048: (MIME) Part Four: Registration Procedures</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#media-type-registration">2. Media Type Registration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#registration-trees-and-subtype-names">2.1.  Registration Trees and Subtype Names</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#registration-requirements">2.2 Registration Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#registration-procedure">2.3 Registration Procedure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#comments-on-media-type-registrations">2.4 Comments on Media Type Registrations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#location-of-registered-media-type-list">2.5 Location of Registered Media Type List</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#iana-procedures-for-registering-media-types">2.6.  IANA Procedures for Registering Media Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#change-control">2.7.  Change Control</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#registration-template">2.8 Registration Template</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#external-body-access-types">3. External Body Access Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2048-MIME4.html#transfer-encodings">4. Transfer Encodings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html">RFC2049: (MIME) Part Five: Conformance Criteria and Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html#mime-conformance">2.  MIME Conformance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html#guidelines-for-sending-email-data">3.  Guidelines for Sending Email Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html#canonical-encoding-model">4.  Canonical Encoding Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/MIME/rfc2049-MIME5.html#appendix-a-a-complex-multipart-example">Appendix A – A Complex Multipart Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/rfc1123.html">rfc1123</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/rfc3232.html">RFC3232: ASSIGNED NUMBERS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/rfc3232.html#rfc1700">RFC1700</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/rfc3339.html">rfc3339</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/rfc3339.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../rfcs/rfc5234-ABNF.html">RFC5234: Augmented BNF for Syntax Specifications: ABNF</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../rfcs/rfc5234-ABNF.html#id2">示例</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../iana.html">iana</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../IEEE.html">IEEE</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../IEEEs/normal.html">常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../IEEEs/754.html">IEEE 754</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/754.html#id1">工具</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/754.html#id2">📦 IEEE 754 定义的内容包括：</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../IEEEs/754.html#id3">✅ 1. <strong>浮点数格式</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../IEEEs/754.html#id4">✅ 2. <strong>浮点数结构（三部分）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../IEEEs/754.html#id5">✅ 3. <strong>特殊值支持</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/754.html#id6">📚 衍生标准（对深度学习重要）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../IEEEs/754.html#fp4">FP4</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../IEEEs/802.3.html">IEEE 802.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/802.3.html#ethernet-ii">Ethernet II</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../IEEEs/802.11.html">802.11: Wireless LAN &amp; Mesh</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/802.11.html#protocol">Protocol</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/802.11.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../IEEEs/802.15.html">802.15: Wireless PAN</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../IEEEs/802.15.html#low-rate-wireless-pan">802.15.4: Low-Rate wireless PAN</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ITU.html">ITU</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ITUs/normal.html">常用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/normal.html#id3">电信标准化</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../ITUs/normal.html#id4">研究组</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ITUs/X%20Series.html">X-Series</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/X%20Series.html#directory">DIRECTORY</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/X%20Series.html#asn-1">ASN.1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/X%20Series.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ITUs/G%20Series.html">G-Series</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/G%20Series.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../ITUs/H%20Series.html">H-Series</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ITUs/H%20Series.html#id2">参考</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../ISO.html">ISO</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../ISOs/normal.html">常用</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ISOs/iso10646.html">ISO/IEC 10646: Universal coded character set (UCS)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../ISOs/iso13818.html">ISO/IEC 13818</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../ISOs/iso13818.html#part-1-systems">Part 1: Systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../ISOs/iso13818.html#part-6-extensions-for-dsm-cc">Part 6: Extensions for DSM-CC</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../GB.html">中标</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../GBs/T28181.html">GB/T28181安全技术视频监控联网系统信息传输, 交换, 控制技术要求</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../pep.html">pep</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../peps/pep-3333.html">pep-3333</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-3333.html#id3">背景与动机</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-3333.html#id4">pep-3333主要变化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-3333.html#id5">规范概述</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-3333.html#id6">应用程序端</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-3333.html#id7">服务器端</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-3333.html#id8">中间件</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../peps/pep-0440.html">pep-0440</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0440.html#id2">简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0440.html#id3">基本格式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0440.html#id4">版本号的比较</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../peps/pep-0420.html">PEP 420 – Implicit Namespace Packages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0420.html#id2">核心要点</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0420.html#id3">主要优势</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0420.html#id4">当前现有方案</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#pkgutil-style-namespace-packages">pkgutil-style namespace packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#pkg-resources-style-namespace-packages">pkg_resources-style namespace packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#id5">两方案的不足</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0420.html#specification">Specification 规范</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#id6">说明</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#id7">命名空间包和常规包之间的区别</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../peps/pep-0420.html#examples">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#nested-namespace-packages">Nested namespace packages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../peps/pep-0420.html#dynamic-path-computation">Dynamic path computation</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../paper.html">论文</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id3">通用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/normals/normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/normals/normal.html#id3">如何看一个论文是不是重要</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/normals/website.html">学术网站</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/normals/website.html#id3">学术搜索平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/normals/website.html#id5">资源共享</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/normals/website.html#id6">论文数据库</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#agents">Agents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2210.03629_ReAct.html">2210.03629_ReAct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2303.08268_Chat-with-the-Environment.html">2303.08268_Chat-with-the-Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2303.08268_Chat-with-the-Environment.html#id2">正文</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2303.11366_Reflexion.html">2303.11366_Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2303.16434_TaskMatrix.AI.html">2303.16434_TaskMatrix.AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2303.16434_TaskMatrix.AI.html#id2">大脑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2303.16434_TaskMatrix.AI.html#id3">接口平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2303.16434_TaskMatrix.AI.html#api">API 选择器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2304.03442_Generative-Agents.html">2304.03442_Generative-Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2304.03442_Generative-Agents.html#generative-agent-architecture">Generative Agent Architecture</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2307.07924_ChatDev.html">2307.07924_ChatDev: Communicative Agents for Software Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2308.00352_MetaGPT.html">2308.00352_MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2308.04026_AgentSims.html">2308.04026_AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2308.08155_AutoGen.html">2308.08155_AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2308.10848_AgentVerse.html">2308.10848_AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2308.10848_AgentVerse.html#id2">理念</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2310.06117_Step-Back.html">2310.06117_Step-Back: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2402.18679_MetaGPT_DI.html">2402.18679_MetaGPT_DI: Data Interpreter: An LLM Agent For Data Science</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2402.18679_MetaGPT_DI.html#introduction">INTRODUCTION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2407.07061_IoA.html">2407.07061_IoA: Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2407.07061_IoA.html#overview-of-ioa">2.1 OVERVIEW OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2407.07061_IoA.html#architecture-of-ioa">2.2 ARCHITECTURE OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2407.07061_IoA.html#key-mechanisms">2.3 KEY MECHANISMS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2407.07061_IoA.html#putting-it-all-together">2.5 Putting It All Together</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2408.08435_ADAS.html">2408.08435_ADAS: Automated Design of Agentic Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2408.08435_ADAS.html#prompt">Prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2410.17238_SELA.html">2410.17238_SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.17238_SELA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.17238_SELA.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.17238_SELA.html#method">3 Method</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2410.10762_AFlow.html">2408.08435_ADAS: Automating Agentic Workflow Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.10762_AFlow.html#introduce">Introduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.10762_AFlow.html#preliminary">PRELIMINARY</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2410.21012_FACT.html">2410.21012_FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2410.21012_FACT.html#introduce">Introduce</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2504.01990_foundation-agents.html">2504.01990_Advances and Challenges in Foundation Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agents/2506.12508_AgentOrchestra.html">2506.12508_AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2506.12508_AgentOrchestra.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2506.12508_AgentOrchestra.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2506.12508_AgentOrchestra.html#agentorchestra">3.AgentOrchestra</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agents/2506.12508_AgentOrchestra.html#experiments">4.Experiments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#agent-aios">视觉 Agent&amp;AIOS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2312.13771_AppAgent.html">2312.13771_AppAgent: Multimodal Agents as Smartphone Users</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2312.13771_AppAgent.html#environment-and-action-space">3.1 Environment and Action Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2312.13771_AppAgent.html#exploration-phase">3.2 Exploration Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2312.13771_AppAgent.html#deployment-phase">3.3 Deployment Phase</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html">2402.07939_UFO: A UI-Focused Agent for Windows OS Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#the-design-of-ufo">3.The Design of UFO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#experiment">4.Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#limitations-lessons-learned">5.Limitations &amp; Lessons Learned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2402.07939_UFO.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2406.01014_Mobile-Agent-v2.html">2406.01014_Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html">2501.11733_Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#mobile-agent-e">2. Mobile-Agent-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#conclusion-and-future-work">6. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-a-full-trajectory-comparison-example-with-previous-sota">Appendix A Full Trajectory Comparison Example with Previous SOTA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-b-error-recovery-with-escalation-to-manager">Appendix B Error Recovery with Escalation to Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-c-remaining-limitations">Appendix C Remaining Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-d-all-tasks-in-mobile-eval-e-benchmark">Appendix D All Tasks in Mobile-Eval-E Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-e-atomic-operation-space">Appendix E Atomic Operation Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-f-full-list-of-self-evolved-shortcuts">Appendix F Full list of Self-Evolved Shortcuts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-g-full-list-of-self-evolved-tips">Appendix G Full list of Self-Evolved Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html">2501.12326_UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#evolution-path-of-gui-agents">2. Evolution Path of GUI Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#core-capabilities-of-native-agent-model">3. Core Capabilities of Native Agent Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#ui-tars">4. UI-TARS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#experiment">5. Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2501.12326_UI-TARS.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html">2502.14282_PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#pc-agent">2. PC-Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#related-work">4. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2502.14282_PC-Agent.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html">2403.16971_AIOS: LLM Agent Operating System</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#the-architecture-of-aios">2. The Architecture of AIOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#aios-kernel">3. AIOS Kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2403.16971_AIOS.html#appendix-e-discussion">Appendix E Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html">2504.14603_UFO2: The Desktop AgentOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#background">2.Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#system-design-of-ufo2">3.System Design of UFO2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#picture-in-picture-interface">4.Picture-in-Picture Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#implementation-and-specialized-engineering-design">5.Implementation and Specialized Engineering Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#evaluation">6.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#discussion-future-work">7.Discussion &amp; Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Agent_Visions/2504.14603_UFO2.html#conclusion">9.Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id4">大模型调优</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2101.00190_Prefix-Tuning.html">2101.00190_Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2103.10385_p-tuning.html">2103.10385_p-tuning: GPT Understands, Too</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2104.08691_Prompt_Tuning.html">2104.08691_Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2106.09685_LoRA.html">2106.09685_LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2401.01335_Self-Play.html">2401.01335_Self-Play: Fine-Tuning Converts Weak Language Models to Strong Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2402.09353_DoRA.html">2402.09353_DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2402.12354_LoRA%2B.html">2402.12354_LoRA+: Efficient Low Rank Adaptation of Large Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.03507_GaLore.html">2403.03507_GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.13372_LlamaFactory.html">2403.13372_LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.13372_LlamaFactory.html#id2">竞争框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.13372_LlamaFactory.html#efficient-fine-tuning-techniques">3. Efficient Fine-Tuning Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.13372_LlamaFactory.html#llamafactory-framework">4 LlamaFactory Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/2403.13372_LlamaFactory.html#conclusion-and-future-work">6 Conclusion and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html">2203.02155_Training language models to follow instructions with human feedback(InstructGPT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#methods-and-experimental-details">3. Methods and experimental details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#discussion">5. Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#appendix-a-additional-prompt-data-details">Appendix A Additional prompt data details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#appendix-b-additional-human-data-collection-details">Appendix B Additional human data collection details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#appendix-c-additional-model-details">Appendix C Additional model details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2203.02155_InstructGPT.html#appendix-d-automatic-evaluation-details">Appendix D Automatic evaluation details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2305.20050_LetsVerifyStepbyStep.html">2305.20050_Let’s Verify Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2305.20050_LetsVerifyStepbyStep.html#id2">1. 研究背景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2305.20050_LetsVerifyStepbyStep.html#id3">2. 监督方法对比</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2305.20050_LetsVerifyStepbyStep.html#id4">3. 核心发现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2305.20050_LetsVerifyStepbyStep.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html">2408.03314_Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#how-to-scale-test-time-computation-optimally">3. How to Scale Test-Time Computation Optimally</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#scaling-test-time-compute-via-verifiers">5. Scaling Test-Time Compute via Verifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#refining-the-proposal-distribution">6. Refining the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#id7">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html">2412.14135_Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#fromgpt">FromGPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#id2">3. Policy Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#id3">4. Reward Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#id5">5. Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#id8">6. Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#open-source-o1-project">7 Open-source o1 Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/a/2412.14135_Scaling_of_Search_and_Learning.html#future-directions">8. Future Directions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id5">分布式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/normal.html">通用</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1701.06538_MoE.html">1701.06538_MoE: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html">1806.03377_PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#background-related-work">2. Background &amp; Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#parallel-training-in-pipedream">3. Parallel Training in PipeDream</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#implementation">4. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1806.03377_PipeDream.html#evaluation">5. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html">1811.06965_GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#the-gpipe-library">2. The GPipe Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#performance-analyses">3. Performance Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#image-classification">4. Image Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#massive-massively-multilingual-machine-translation">5. Massive Massively Multilingual Machine Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1811.06965_GPipe.html#design-features-and-trade-offs">6. Design Features and Trade-Offs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html">1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html#background-and-challenges">2. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1909.08053_Megatron-LM.html#model-parallel-transformers">3. Model Parallel Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html">19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#background-and-related-work">2. BACKGROUND AND RELATED WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#pipeline-parallelism">3. 流水线并行(PIPELINE PARALLELISM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#id5">4. 实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/1910_PipeDream2.html#id6">6. 结论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2006.15704DataParallel.html">2006.15704_PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2006.16668_GShard.html">2006.16668_GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2006.09503_PipeDream-2BW.html">2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2006.09503_PipeDream-2BW.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2104.04473_Megatron-LM2.html">2104.04473_Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2104.04473_Megatron-LM2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html">2205.14135_FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#background">2 Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#flashattention-algorithm-analysis-and-extensions">3. FLASHATTENTION: Algorithm, Analysis, and Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#limitations-and-future-directions">5. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#appendix-b-algorithm-details">Appendix B Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#appendix-c-proofs">Appendix C Proofs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2205.14135_FlashAttention.html#appendix-d-extension-details">Appendix D Extension Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html">2307.08691_FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#flashattention-2-algorithm-parallelism-and-work-partitioning">3. FlashAttention-2: Algorithm, Parallelism, and Work Partitioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#empirical-validation">4. Empirical Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMFineTunes/Parallelism/2307.08691_FlashAttention2.html#discussion-and-future-directions">5. Discussion and Future Directions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#llm-nlp">LLM NLP</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html">18xx_GPT1: Improving Language Understanding by Generative Pre-Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#framework">3. Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#analysis">5 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#id3">引文口碑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/18_GPT1.html#id4">要点解读</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/1810.04805_BERT.html">1810.04805_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/1810.04805_BERT.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/1810.04805_BERT.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/1810.04805_BERT.html#bert">3 BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/1810.04805_BERT.html#appendix-a-additional-details-for-bert">Appendix A Additional Details for BERT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/19_GPT2.html">19xx_GPT2: Language Models are Unsupervised Multitask Learners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/19_GPT2.html#the-illustrated-gpt-2">The Illustrated GPT-2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/19_GPT2.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2012.00413_CPM.html">2012.00413_CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2302.13971_LLaMA.html">2302.13971_LLaMA: Open and Efficient Foundation Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2307.09288_Llama2.html">2307.09288_Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html">2309.16609_Qwen Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#pretraining">2. Pretraining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#alignment">3. Alignment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#code-qwen-specialized-model-for-coding">4. CODE-QWEN: SPECIALIZED MODEL FOR CODING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#math-qwen-specialized-model-for-mathematics-reasoning">5. MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#a-1-more-training-details">A.1 MORE TRAINING DETAILS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2309.16609_Qwen.html#a-2-evaluation">A.2 EVALUATION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2401.14196_DeepSeek-Coder.html">2401.14196_DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2404.06395_MiniCPM.html">2404.06395_MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2404.06395_MiniCPM.html#two-stage-pre-training-strategy">5. Two Stage Pre-training Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2404.06395_MiniCPM.html#model">6. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2404.06395_MiniCPM.html#minicpm-family">7 MiniCPM Family</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2405.04434_DeepSeek-V2.html">2405.04434_DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2406.12793_ChatGLM.html">2406.12793_ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html">2407.10671_Qwen2 Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#tokenizer-model">2. Tokenizer &amp; Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2407.10671_Qwen2.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html">2412.15115_Qwen2.5</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#architecture-and-tokenizer">2. Architecture and Tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2412.15115_Qwen2.5.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html">2505.09388_Qwen3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMs/2505.09388_Qwen3.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#llm-moe">LLM MoE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMoEs/2410.07490_MoDEM.html">2410.07490_MoDEM: Mixture of Domain Expert Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMoEs/2408.15664_AUXILIARY-LOSS-FREE_LB.html">2408.15664_AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#llm">LLM 多模态</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html">2304.08485_LLaVA: Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#gpt-assisted-visual-instruction-data-generation">3. GPT-assisted Visual Instruction Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#visual-instruction-tuning">4. Visual Instruction Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2304.08485_LLaVA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2308.12966_Qwen-VL.html">2308.12966_Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2308.12966_Qwen-VL.html#methodology">Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2308.12966_Qwen-VL.html#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2308.12966_Qwen-VL.html#evaluation">Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2308.12966_Qwen-VL.html#b-data-format-details-of-training">B. Data Format Details of Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html">2310.03744_LLaVA2: Improved Baselines with Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#empirical-evaluation">4. Empirical Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#open-problems-in-lmms">5. Open Problems in LMMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#a-implementation-details">A. Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2310.03744_LLaVA2.html#b-qualitative-results">B. Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html">2312.07533_VILA: On Pre-training for Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#on-pre-training-for-visual-language-models">3. On Pre-training for Visual Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2312.07533_VILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2403.05525_DeepSeek-VL.html">2403.05525_DeepSeek-VL: Towards Real-World Vision-Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2403.05525_DeepSeek-VL.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html">2408.01800_MiniCPM-V: A GPT-4V Level MLLM on Your Phone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#model-architecture">3. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#end-side-deployment">5. End-side Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#experiments">6. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2408.01800_MiniCPM-V.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html">2409.17146_Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#data">3. Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#ablations">6. Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-a-model-details">Appendix A: Model Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-b-training-details">Appendix B: Training Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-c-evaluation-results">Appendix C: Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-d-result-details">Appendix D: Result Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-e-ablations-details">Appendix E Ablations Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-f-data-details">Appendix F Data Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-g-dataset-examples">Appendix G Dataset Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-h-related-work">Appendix H Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html">2411.00774_Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html#model">2. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html#experience">3. Experience</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2411.00774_Freeze-Omni.html#conclusion-and-future-work">4. Conclusion and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html">2412.04468_NVILA: Efficient Frontier Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#more-capabilities">4. More Capabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2412.04468_NVILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html">2502.13923_Qwen2.5-VL</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2502.13923_Qwen2.5-VL.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html">2503.20215_Qwen2.5-Omni Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#archtecture">2. Archtecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#id2">3 预训练</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#post-training">4 后训练（Post-training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#id4">3. Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#results-and-analyses">5. Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id6">LLM 音频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2005.08100_Conformer.html">2005.08100_Conformer: Convolution-augmented Transformer for Speech Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html">2112.02418_YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#id1">关键概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#yourtts-model">2. YourTTS Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#results-and-discussion">4. Results and Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#zero-shot-voice-conversion">5. Zero-Shot Voice Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#speaker-adaptation">6. Speaker Adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2112.02418_YourTTS.html#conclusions-limitations-and-future-work">7. Conclusions, limitations and future work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html">2212.04356_whisper: Robust Speech Recognition via Large-Scale Weak Supervision</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#analysis-and-ablations">4. Analysis and Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#limitations-and-future-work">6. Limitations and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#conclusions">7. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#a-evaluation-datasets">A. Evaluation Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#b-compared-models">B Compared Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2212.04356_whisper.html#c-text-standardization">C. Text Standardization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html">2301.02111_Vall-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#background-speech-quantization">3. Background: Speech Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#id9">4. VALL-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2301.02111_Vall-E.html#conclusion-limitations-and-future-work">6. Conclusion, Limitations, and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html">2303.03926_VALL-E_X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#cross-lingual-codec-language-model">3 Cross-Lingual Codec Language Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#vall-e-x-application">4. VALL-E X Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2303.03926_VALL-E_X.html#a-appendix">A. Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html">2406.05370_VALL-E2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#id5">3. VALL-E 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2406.05370_VALL-E2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html">2407.05407_CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#cosyvoice-a-scalable-tts-model-using-supervised-semantic-tokens">2. CosyVoice: A Scalable TTS model using Supervised Semantic Tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#dataset">3. Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#experimental-settings">4. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.05407_CosyVoice.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html">2407.10759_Qwen2-Audio Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2407.10759_Qwen2-Audio.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html">2410.00037_Moshi: a speech-text foundation model for real-time dialogue</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#model">3.Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#datasets-and-training">4. Datasets and Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2410.00037_Moshi.html#evaluation">5. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html">2412.10117_CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#instroduction">1. Instroduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#id5">2. CosyVoice 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#experimental-settings">3. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#experimental-results">4. Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2412.10117_CosyVoice2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html">2501.06282_MinMo: A Multimodal Large Language Model for Seamless Voice Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#instruction">1.Instruction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#id9">3.MinMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#conclusion">5.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2501.06282_MinMo.html#a-prompts-for-voice-understanding-tasks">A. Prompts for Voice Understanding Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html">2505.02707_Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#voila-voice-language-foundation-models">3. Voila: Voice-Language Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.02707_Voila.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html">2505.17589_CosyVoice3: Towards In-the-wild Speech Generation via Scaling-up and Post-training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#id3">2.CosyVoice 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#the-multilingual-data-pipeline">3.The Multilingual Data Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#experimental-settings">4.Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#experimental-results">5.Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#conclusion">6.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMAudio/2505.17589_CosyVoice3.html#limitations">7.Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id7">LLM强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMRLs/1703.03864_EvolutionStrategies.html">1703.03864_Evolution Strategies: as a Scalable Alternative to Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html">2504.02495_DeepSeek-GRM: Inference-Time Scaling for Generalist Reward Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#self-principled-critique-tuning-spct">3. Self-Principled Critique Tuning (SPCT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#inference-time-scaling-with-spct">4. Inference-Time Scaling with SPCT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#results-on-reward-modeling-benchmarks">5. Results on Reward Modeling Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#a-additional-related-work">A. Additional Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#b-limitations-and-future-directions">B. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMRLs/2504.02495_DeepSeek_GRM.html#g-prompt-templates">G. Prompt Templates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMRLs/2504.13958_ToolRL.html">2504.13958_ToolRL: Reward is All Tool Learning Needs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id8">LLM 量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/0normal.html#id2">混合精度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/0normal.html#id3">浮点数格式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/0normal.html#weight-only-quantization">weight-only quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html">2110.02861_bitsandbytes: 8-bit Optimizers via Block-wise Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#background">1. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#bit-optimizers">2. 8-bit Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#bit-vs-32-bit-optimizer-performance-for-common-benchmarks">3. 8-bit vs 32-bit Optimizer Performance for common Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#analysis">4. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2110.02861_bitsandbytes.html#related-work">5. Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html">2206.01861_ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#relative-work">2. Relative Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#background-and-challenges">3. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#methodology">4. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#results">5. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#appendix-a-background">Appendix A Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.01861_ZeroQuant.html#appendix-d-details-about-system-optimization">Appendix D Details about System Optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html">2206.09557_LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#design-methodology-of-lut-gemm">3. Design Methodology of LUT-GEMM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#accelerating-quantized-opt-175b">5. Accelerating Quantized OPT-175B</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#appendix-a-llm-inference-latency-breakdown">Appendix A LLM Inference Latency Breakdown</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2206.09557_LUT-GEMM.html#appendix-b-detailed-implementation">Appendix B Detailed Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html">2208.07339_LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#id1">相关参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#int8-matrix-multiplication-at-scale">3. Int8 Matrix Multiplication at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#emergent-large-magnitude-features-in-transformers-at-scale">4. Emergent Large Magnitude Features in Transformers at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#discussion-and-limitations">6. Discussion and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#broader-impacts">7. Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2208.07339_LLM.int8.html#id17">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html">2209.05433_FP8: FP8 Formats For Deep Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#aspects-of-fp8-usage-in-deep-learning">2. Aspects of FP8 Usage in Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#fp8-binary-interchange-format">3. FP8 Binary Interchange Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#id3">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#empirical-results">4. Empirical Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2209.05433_FP8.html#conclusions">5. Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html">2210.17323_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#background">3. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#the-gptq-algorithm">4. The GPTQ Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#experimental-validation">5. Experimental Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2210.17323_GPTQ.html#summary-and-limitations">6. Summary and Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html">2211.10438_SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#review-of-quantization-difficulty">3. Review of Quantization Difficulty</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#id9">4. SmoothQuant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2211.10438_SmoothQuant.html#appendix-a-discussion-on-weight-only-quantization">Appendix A. Discussion on Weight-Only Quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html">2305.14314_QLoRA: Efficient Finetuning of Quantized LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#qlora-finetuning">3. QLoRA Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#qlora-vs-standard-finetuning">4. QLoRA vs. Standard Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#pushing-the-chatbot-state-of-the-art-with-qlora">5. Pushing the Chatbot State-of-the-art with QLoRA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#qualitative-analysis">6. Qualitative Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#related-work">7. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2305.14314_QLoRA.html#limitations-and-discussion">8. Limitations and Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html">2306.00978_AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#awq-activation-aware-weight-quantization">3. AWQ: Activation-aware Weight Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#tinychat-mapping-awq-onto-edge-platforms">4. TinyChat: Mapping AWQ onto Edge Platforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2306.00978_AWQ.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html">2309.05516_AutoRound: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#methodology">3. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMQuantizations/2309.05516_AutoRound.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id9">LLM 闭源模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMCommercials/2303.08774_GPT4.html">2303.08774_GPT-4 Technical Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html">2312.11805_Gemini: A Family of Highly Capable Multimodal Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#model-architecture">2. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#training-infrastructure">3. Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#post-training-models">6. Post-Training Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#responsible-deployment">7. Responsible Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/LLMCommercials/2312.11805_Gemini.html#discussion-and-conclusion">8. Discussion and Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMCommercials/2403.05530_Gemini1.5.html">2403.05530_Gemini1.5: Unlocking multimodal understanding across millions of tokens of context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMCommercials/2503.20020_Gemini2.html">2503.20020_Gemini2: Gemini Robotics: Bringing AI into the Physical World</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#d">3D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html">2003.08934_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#neural-radiance-field-scene-representation">3. Neural Radiance Field Scene Representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#volume-rendering-with-radiance-fields">4. Volume Rendering with Radiance Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#optimizing-a-neural-radiance-field">5. Optimizing a Neural Radiance Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#result">6. Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2003.08934_NeRF.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html">2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#id1">概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#geometric-priors-for-vp-detection">3. Geometric priors for VP detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2203.08586_VanishingPointEstimation.html#conclusion-and-limitations">5. Conclusion and limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html">2312.14132_DUSt3R: Geometric 3D Vision Made Easy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#id2">相关概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#experiments-with-dust3r">4. Experiments with DUSt3R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-a">Appendix A <strong>附录概览</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-b-qualitative-results">Appendix B.  Qualitative results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-c-extended-related-work">Appendix C. Extended Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-d-multi-view-pose-estimation">Appendix D. 多视角姿态估计（Multi-view Pose Estimation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-e-visual-localization">Appendix E. 视觉定位（Visual Localization）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2312.14132_DUSt3R.html#appendix-f-training-details">Appendix F. Training details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html">2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#id1">前言</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#id2">🧠 思维导图式总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#related-works">2. Related works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#id3">🧠 总结思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#appendix-a-additional-qualitative-results">Appendix A Additional Qualitative Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#b-fast-reciprocal-matching">B. Fast Reciprocal Matching</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#c-coarse-to-fine">C. Coarse-to-Fine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2406.09756_MASt3R.html#d-detailed-experimental-settings">D. Detailed experimental settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html">2412.09401_SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#id1">术语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#id14">6. 致谢</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#appendix-a-implementation-details">Appendix A Implementation details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#appendix-b-details-for-experimental-settings">Appendix B Details for experimental settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#appendix-c-additional-comparisons-and-analyses">Appendix C Additional comparisons and analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.09401_SLAM3R.html#d-more-visual-results">D. More visual results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html">2412.12392_MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#gpt">GPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#id1">先验知识</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#limitations-and-future-work">5. Limitations and Future Work（局限与未来工作）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#conclusion">🧾 6. Conclusion（总结）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#id32">🧠 总结一句话版：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#initialisation">8. Initialisation（初始化）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#runtime-breakdown">9. Runtime Breakdown（运行时分析）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#evaluation-setup">10. Evaluation Setup（评估设置）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2412.12392_MASt3R-SLAM.html#id35">11. EuRoC 结果总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html">2503.11651_VGGT: Visual Geometry Grounded Transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#discussions">5. Discussions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#appendix-a-formal-definitions">Appendix A Formal Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#appendix-c-additional-experiments">Appendix C Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#appendix-d-qualitative-examples">Appendix D Qualitative Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/3D/2503.11651_VGGT.html#appendix-e-related-work">Appendix E Related Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id10">LLM 安全</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/LLMSecuritys/2312.06674_Llama_Guard.html">2312.06674_Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#benchmarking">Benchmarking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#id2">评测标准</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#accuracy">准确率(Accuracy)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#precision">精确率(Precision, 精准率)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#recall">召回率(Recall)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#f1-score">F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/0normal.html#id3">可视化精度和召回率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html">2009.03300_MMLU: Measuring Massive Multitask Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#a-multitask-test">3.A Multitask Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2009.03300_MMLU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2103.03874_MATH.html">2103.03874_MATH: Measuring Mathematical Problem Solving With the MATH Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html">2311.12022_GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#data-collection">2.Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#dataset-analysis">3.Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#baseline">4.Baseline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#related-work">5.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12022_GPQA.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html">2311.12983_GAIA: a benchmark for General AI Assistants</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#related-work">2.Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#id3">3.GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#llms-results-on-gaia">4.LLMs results on GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#appendix-c-extended-description-of-gaia">Appendix C Extended description of GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2311.12983_GAIA.html#appendix-d-extended-description-of-our-question-design-framework">Appendix D Extended description of our question design framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html">2404.07972_OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#osworld-environment">2. OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#osworld-benchmark">3. OSWORLD Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#benchmarking-llm-and-vlm-agent-baselines">4. Benchmarking LLM and VLM Agent Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#analysis">5. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#a-details-of-osworld-environment">A. Details of OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#c-details-of-baseline-methods">C. Details of Baseline Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2404.07972_OSWorld.html#d-examples-of-qualitative-analysis">D. Examples of Qualitative Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html">2411.04368_SimpleQA: Measuring short-form factuality in large language models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html#data-collection-and-verification">2.Data Collection and Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html#measuring-calibration">4.Measuring calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2411.04368_SimpleQA.html#appendix-b-guessing-strategy-and-f-score">Appendix B Guessing strategy and F-score</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html">2501.14249_HLE: Humanity’s Last Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#dataset">3.Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#evaluation">4.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Benchmarkings/2501.14249_HLE.html#discussion">5.Discussion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id11">数据集&amp;数据蒸馏</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/DataSets/normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/normal.html#dataset-distillation">Dataset distillation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/DataSets/1811.10959v3_Dataset_Distillation.html">1811.10959v3_Dataset Distillation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/1811.10959v3_Dataset_Distillation.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/1811.10959v3_Dataset_Distillation.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/1811.10959v3_Dataset_Distillation.html#introduction">1. INTRODUCTION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/1811.10959v3_Dataset_Distillation.html#approach">3. APPROACH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html">2112.15093_CTR: Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#datasets">3. Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#baselines">4. Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#an-empirical-study">5. An Empirical Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#appendix-a-details-of-prab">Appendix A Details of PRAB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2112.15093_CTR.html#appendix-c-visualization-of-failure-cases">Appendix C Visualization of Failure Cases.</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/DataSets/2502.20653_Dataset_Distillation.html">2502.20653_Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2502.20653_Dataset_Distillation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2502.20653_Dataset_Distillation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2502.20653_Dataset_Distillation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/DataSets/2502.20653_Dataset_Distillation.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#framework">Framework</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html">1712.05889_Ray: A Distributed Framework for Emerging AI Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#motivation-and-requirements">2. Motivation and Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#programming-and-computation-model">3. Programming and Computation Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#architecture">4. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#discussion-and-experiences">7 Discussion and Experiences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1712.05889_Ray.html#conclusion">8. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html">1910.02054_DeepSpeed_ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#extended-introduction">1. Extended Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#where-did-all-the-memory-go">3 Where Did All the Memory Go?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#zero-insights-and-overview">4 ZeRO: Insights and Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-dp">5 Deep Dive into ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-r">6 Deep Dive into ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-dp">7 Communication Analysis of ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-r">8. Communication Analysis of ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#step-towards-1-trillion-parameters">9. Step Towards 1 Trillion Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#implementation-and-evaluation">10. Implementation and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/1910.02054_DeepSpeed_ZeRO.html#concluding-remarks">11. Concluding Remarks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/19XX_PyTorch.html">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/20XX_Transformers.html">Transformers: State-of-the-Art Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html">2210.XX_Ray v2 Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#architecture-overview">Architecture Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#object-management">Object Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#task-management">Task Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#resource-management-and-scheduling">Resource Management and Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#actor-management">Actor management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#global-control-service">Global Control Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#cluster-management">Cluster Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2210.XX_Ray_v2.html#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html">2309.06180_Efficient Memory Management for Large Language Model Serving with PagedAttention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#memory-challenges-in-llm-serving">3. Memory Challenges in LLM Serving</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#method">4. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#implementation">5. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#evaluation">6. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#ablation-studies">7. Ablation Studies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/Frameworks/2309.06180_vLLM.html#conclusion">10. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#ml">ML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2112.09332_WebGPT.html">2112.09332_WebGPT: Browser-assisted question-answering with human feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2203.11147_GopherCite.html">2203.11147_GopherCite: Teaching language models to support answers with verified quotes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2305.14251_FActScore.html">2305.14251_FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2304.09848_Generative_Search.html">2304.09848_Generative_Search: Evaluating Verifiability in Generative Search Engines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2307.02185_Citation.html">2307.02185_Citation: A Key to Building Responsible and Accountable Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2307.16883_HAGRID.html">2307.16883_HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLs/2305.14627_ALCE.html">2305.14627_ALCE: Enabling Large Language Models to Generate Text with Citations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLs/2305.14627_ALCE.html#nli">NLI 在引用质量评估中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLs/2305.14627_ALCE.html#prompt">论文中用的prompt</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#id12">ML 多模态相关</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html">2108.03353_ Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#dataset-creation">3. Dataset Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#model-design">4. Model Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2108.03353_Screen2Words.html#id3">其它</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html">2209.08199_ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#problem-setting-tasks-and-metrics">3. Problem Setting: Tasks and Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#data-annotation">4. Data Annotation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#dataset-analysis">5. Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#experiments-and-baselines">6. Experiments and Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#limitations">8. Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#ethical-considerations">9. Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#a-data-annotation-details">A. Data Annotation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2209.08199_ScreenQA.html#b-data-examples">B. Data Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html">2212.06817_RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#preliminaries">3. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#system-overview">4. System Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#rt-1-robotics-transformer">5. RT-1: ROBOTICS TRANSFORMER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#experiments">6. EXPERIMENTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#conclusions-limitations-and-future-work">7. CONCLUSIONS, LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#b-model-card">B. MODEL CARD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#c-model-and-data">C. MODEL AND DATA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2212.06817_RT-1.html#d-experiments">D. EXPERIMENTS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html">2401.10935_SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#screenspot-a-grounding-benchmark">4. ScreenSpot: A Grounding Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#ethical-considerations">Ethical considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#a-details-of-seeclick-pre-training">A. Details of SeeClick Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#b-screenspot-annotation-evaluation">B ScreenSpot Annotation &amp; Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2401.10935_SeeClick.html#c-downstream-agent-tasks">C. Downstream Agent Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html">2402.04615_ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#automatic-data-generation">3. Automatic data generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#data-mixtures">4. Data Mixtures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#experiments-and-results">5. Experiments and Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#a-definitions-of-metrics">A Definitions of Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#b-screen-schema-examples">B. Screen Schema Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#c-prompts-for-llm-generated-content">C. Prompts For LLM Generated Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#d-screen-navigation-generated-examples">D. Screen Navigation Generated Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#f-screenqa-short-answers-generation">F. ScreenQA Short Answers Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#g-complex-question-answering-datasets">G. Complex Question Answering Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2402.04615_ScreenAI.html#h-new-benchmarks-repositories">H. New Benchmarks Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLMultimodals/2411.02059_TableGPT2.html">2411.02059_TableGPT2: A Large Multimodal Model with Tabular Data Integration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLMultimodals/2411.02059_TableGPT2.html#abstract">Abstract</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#ml-vision">ML Vision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/1506.02640_YOLO.html">1506.02640_You Only Look Once: Unified, Real-Time Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/1506.02640_YOLO.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/1612.08242_YOLO9000.html">1612.08242_YOLO9000: Better, Faster, Stronger</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/1612.08242_YOLO9000.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/1804.02767_YOLOv3.html">1804.02767_YOLOv3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2004.10934_YOLOv4.html">2004.10934_YOLOv4: Optimal Speed and Accuracy of Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2004.10934_YOLOv4.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html">2205.00159_SVTR: Scene Text Recognition with a Single Visual Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html#method">2. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2205.00159_SVTR.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2207.02696_YOLOv7.html">2207.02696_YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2207.02696_YOLOv7.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2303.05499_GroundingDINO.html">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2304.08485_VisualInstructionTuning.html">2304.08485_Visual Instruction Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2402.13616_YOLOv9.html">2402.13616_YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2402.13616_YOLOv9.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2405.14458_YOLOv10.html">2405.14458_YOLOv10: Real-Time End-to-End Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2405.14458_YOLOv10.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html">2411.15858_SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#id1">定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#methods">3. Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/MLVisions/2411.15858_SVTRv2.html#more-detail-of-real-world-datasets">8. More detail of real-world datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#rag">RAG</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2005.11401_RAG_for_KI_NLP_task.html">2005.11401_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html">2312.10997_Retrieval-Augmented Generation for Large Language Models: A Survey</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#ii-overview-of-rag">II. Overview of RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#iii-retrieval">III. Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#iv-generation">IV. Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#v-augmentation-process-in-rag">V. Augmentation process in RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#vi-task-and-evaluation">VI. Task and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2312.10997_RAG_for_LLM.html#vii-discussion-and-future-prospects">VII. Discussion and Future Prospects</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2401.15884_CRAG.html">2401.15884_CRAG: Corrective Retrieval Augmented Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2403.14403_Adaptive-RAG.html">2403.14403_Adaptive-RAG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2404.16130_GRAG.html">2404.16130_From Local to Global: A Graph RAG Approach to Query-Focused Summarization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2404.16130_GRAG.html#id2">简介</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2404.16130_GRAG.html#id3">相关的技术讨论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/graphrag.html">GraphRAG 官方文档</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/graphrag.html#indexing">Indexing</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/graphrag.html#query">Query</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2406.13213_Multi-Meta-RAG.html">2406.13213_Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html">2410.10450_KBLaM: Knowledge Base augmented Language Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#background">3. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#augmenting-llm-with-the-kb">4. Augmenting LLM with the KB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#kb-instruction-tuning">5. KB instruction tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#experiments">6. EXPERIMENTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#conclusion">7. CONCLUSION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#limitations-and-future-work">8. LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#appendix-b-ablation-study">Appendix B Ablation study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#appendix-c-sample-kb">Appendix C Sample KB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#sample-q-a">SAMPLE Q&amp;A</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#prompt">PROMPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2410.10450_KBLaM.html#sample-output">SAMPLE OUTPUT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html">2504.03137_LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#related-work">Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#preliminaries">Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#methodology">Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#experiments">Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/RAGs/2504.03137_LightPROF.html#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#tools">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Tools/2205.00445_MRKL.html">2205.00445_MRKL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Tools/2302.04761_Toolformer.html">2302.04761_Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/Tools/2303.17580_HuggingGPT.html">2303.17580_HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#agi">AGI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/AGIs/1905.10985_AI-GA.html">1905.10985_AI-GA: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/AGIs/2408.06292_AI-Scientist.html">2408.06292_The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../paper.html#others">others</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html">A PAINLESS GUIDE TO CRC ERROR DETECTION ALGORITHMS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#the-basic-idea-behind-crc-algorithms">The Basic Idea Behind CRC Algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#polynomical-arithmetic">Polynomical Arithmetic</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#binary-arithmetic-with-no-carries">Binary Arithmetic with No Carries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id2">一个可用的实例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#choosing-a-poly">Choosing A Poly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-straightforward-crc-implementation">A Straightforward CRC Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-table-driven-implementation">A Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-slightly-mangled-table-driven-implementation">A Slightly Mangled Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/others/Distributed%20Representations%20of%20Sentences%20and%20Documents.html">Distributed Representations of Sentences and Documents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../papers/TODO.html">TODO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/TODO.html#id2">大模型</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../papers/TODO.html#id3">别人的收集</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../tmp.html">临时</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html">学习记录</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#id3">局域网内的服务发现会有什么方法</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#mdns">mDNS协议</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#id4">命令工具</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#id5">组播&amp;广播</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#ipv4">IPv4多播转发</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#id6">多播功能</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../tmps/%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95.html#id7">任播</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../book.html">书籍</a> &raquo;</li>
        
          <li><a href="../../ai.html">ai</a> &raquo;</li>
        
      <li>Build a Large Language Model (From Scratch)</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/books/ais/2024/Build_LLM_From_Scratch.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">Build a Large Language Model (From Scratch)</a><ul>
<li><a class="reference internal" href="#understanding-llm">1. Understanding LLM</a></li>
<li><a class="reference internal" href="#working-with-text-data">2. Working with Text Data</a><ul>
<li><a class="reference internal" href="#understanding-word-embeddings">2.1 Understanding word embeddings</a></li>
<li><a class="reference internal" href="#tokenizing-text">2.2 Tokenizing text</a></li>
<li><a class="reference internal" href="#converting-tokens-into-token-ids">2.3 Converting tokens into token IDs</a></li>
<li><a class="reference internal" href="#adding-special-context-tokens">2.4 Adding special context tokens</a></li>
<li><a class="reference internal" href="#byte-pair-encoding">2.5 Byte pair encoding</a></li>
<li><a class="reference internal" href="#data-sampling-with-a-sliding-window">2.6 Data sampling with a sliding window</a></li>
<li><a class="reference internal" href="#creating-token-embeddings">2.7 Creating token embeddings</a></li>
<li><a class="reference internal" href="#encoding-word-positions">2.8 Encoding word positions</a></li>
<li><a class="reference internal" href="#summary">2.9 Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#coding-attention-mechanisms">3. Coding Attention Mechanisms</a><ul>
<li><a class="reference internal" href="#the-problem-with-modeling-long-sequences">3.1 The problem with modeling long sequences</a></li>
<li><a class="reference internal" href="#capturing-data-dependencies-with-attention-mechanisms">3.2 Capturing data dependencies with attention mechanisms</a></li>
<li><a class="reference internal" href="#attending-to-different-parts-of-the-input-with-self-attention">3.3 Attending to different parts of the input with self-attention</a><ul>
<li><a class="reference internal" href="#a-simple-self-attention-mechanism-without-trainable-weights">3.3.1 A simple self-attention mechanism without trainable weights</a></li>
<li><a class="reference internal" href="#computing-attention-weights-for-all-input-tokens">3.3.2 Computing attention weights for all input tokens</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementing-self-attention-with-trainable-weights">3.4 Implementing self-attention with trainable weights</a><ul>
<li><a class="reference internal" href="#computing-the-attention-weights-step-by-step">3.4.1 Computing the attention weights step by step</a></li>
<li><a class="reference internal" href="#implementing-a-compact-self-attention-python-class">3.4.2 Implementing a compact self-attention Python class</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hiding-future-words-with-causal-attention">3.5 Hiding future words with causal attention</a><ul>
<li><a class="reference internal" href="#applying-a-causal-attention-mask">3.5.1 Applying a causal attention mask</a></li>
<li><a class="reference internal" href="#masking-additional-attention-weights-with-dropout">3.5.2 Masking additional attention weights with dropout</a></li>
<li><a class="reference internal" href="#implementing-a-compact-causal-attention-class">3.5.3 Implementing a compact causal attention class</a></li>
</ul>
</li>
<li><a class="reference internal" href="#extending-single-head-attention-to-multi-head-attention">3.6 Extending single-head attention to multi-head attention</a><ul>
<li><a class="reference internal" href="#stacking-multiple-single-head-attention-layers">3.6.1 Stacking multiple single-head attention layers</a></li>
<li><a class="reference internal" href="#implementing-multi-head-attention-with-weight-splits">3.6.2 Implementing multi-head attention with weight splits</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id2">3.7 Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementing-a-gpt-model-from-scratch-to-generate-text">4 Implementing a GPT model from Scratch To Generate Text</a><ul>
<li><a class="reference internal" href="#coding-an-llm-architecture">4.1 Coding an LLM architecture</a></li>
<li><a class="reference internal" href="#normalizing-activations-with-layer-normalization">4.2 Normalizing activations with layer normalization</a></li>
<li><a class="reference internal" href="#implementing-a-feed-forward-network-with-gelu-activations">4.3 Implementing a feed forward network with GELU activations</a></li>
<li><a class="reference internal" href="#adding-shortcut-connections">4.4 Adding shortcut connections</a></li>
<li><a class="reference internal" href="#connecting-attention-and-linear-layers-in-a-transformer-block">4.5 Connecting attention and linear layers in a transformer block</a></li>
<li><a class="reference internal" href="#coding-the-gpt-model">4.6 Coding the GPT model</a></li>
<li><a class="reference internal" href="#generating-text">4.7 Generating text</a></li>
<li><a class="reference internal" href="#id3">4.8 Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pretraining-on-unlabeled-data">5 Pretraining on Unlabeled Data</a><ul>
<li><a class="reference internal" href="#evaluating-generative-text-models">5.1 Evaluating generative text models</a><ul>
<li><a class="reference internal" href="#using-gpt-to-generate-text">5.1.1 Using GPT to generate text</a></li>
<li><a class="reference internal" href="#calculating-the-text-generation-loss">5.1.2 Calculating the text generation loss</a><ul>
<li><a class="reference internal" href="#backpropagation">Backpropagation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-entropy-loss">Cross entropy loss</a></li>
<li><a class="reference internal" href="#perplexity">Perplexity</a></li>
<li><a class="reference internal" href="#calculating-the-training-and-validation-set-losses">5.1.3 Calculating the training and validation set losses</a><ul>
<li><a class="reference internal" href="#the-cost-of-pretraining-llms">The cost of pretraining LLMs</a></li>
<li><a class="reference internal" href="#training-with-variable-lengths">Training with variable lengths</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#training-an-llm">5.2 Training an LLM</a><ul>
<li><a class="reference internal" href="#adamw">AdamW</a></li>
<li><a class="reference internal" href="#simple-plot">simple plot</a></li>
<li><a class="reference internal" href="#id4">小结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decoding-strategies-to-control-randomness">5.3 Decoding strategies to control randomness</a><ul>
<li><a class="reference internal" href="#temperature-scaling">5.3.1 Temperature scaling</a></li>
<li><a class="reference internal" href="#top-k-sampling">5.3.2 Top-k sampling</a></li>
<li><a class="reference internal" href="#modifying-the-text-generation-function">5.3.3 Modifying the text generation function</a></li>
</ul>
</li>
<li><a class="reference internal" href="#loading-and-saving-model-weights-in-pytorch">5.4 Loading and saving model weights in PyTorch</a></li>
<li><a class="reference internal" href="#loading-pretrained-weights-from-openai">5.5 Loading pretrained weights from OpenAI</a></li>
<li><a class="reference internal" href="#id5">5.6 Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tuning-for-classification">6 Fine-tuning for classification</a><ul>
<li><a class="reference internal" href="#different-categories-of-fine-tuning">6.1 Different categories of fine-tuning</a><ul>
<li><a class="reference internal" href="#choosing-the-right-approach">CHOOSING THE RIGHT APPROACH</a></li>
</ul>
</li>
<li><a class="reference internal" href="#preparing-the-dataset">6.2 Preparing the dataset</a></li>
<li><a class="reference internal" href="#creating-data-loaders">6.3 Creating data loaders</a></li>
<li><a class="reference internal" href="#initializing-a-model-with-pretrained-weights">6.4 Initializing a model with pretrained weights</a></li>
<li><a class="reference internal" href="#adding-a-classification-head">6.5 Adding a classification head</a><ul>
<li><a class="reference internal" href="#output-layer-nodes">OUTPUT LAYER NODES</a></li>
<li><a class="reference internal" href="#fine-tuning-selected-layers-vs-all-layers">FINE-TUNING SELECTED LAYERS VS. ALL LAYERS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#calculating-the-classification-loss-and-accuracy">6.6 Calculating the classification loss and accuracy</a></li>
<li><a class="reference internal" href="#finetuning-the-model-on-supervised-data">6.7 Finetuning the model on supervised data</a><ul>
<li><a class="reference internal" href="#choosing-the-number-of-epochs">CHOOSING THE NUMBER OF EPOCHS</a></li>
</ul>
</li>
<li><a class="reference internal" href="#using-the-llm-as-a-spam-classifier">6.8 Using the LLM as a spam classifier</a></li>
<li><a class="reference internal" href="#id6">Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tuning-to-follow-instructions">7 Fine-tuning to follow instructions</a><ul>
<li><a class="reference internal" href="#introduction-to-instruction-fine-tuning">7.1 Introduction to instruction fine-tuning</a></li>
<li><a class="reference internal" href="#preparing-a-dataset-for-supervised-instruction-fine-tuning">7.2 Preparing a dataset for supervised instruction fine-tuning</a></li>
<li><a class="reference internal" href="#organizing-data-into-training-batches">7.3 Organizing data into training batches</a><ul>
<li><a class="reference internal" href="#why-replacement-by-100">why replacement by -100</a></li>
</ul>
</li>
<li><a class="reference internal" href="#creating-data-loaders-for-an-instruction-dataset">7.4 Creating data loaders for an instruction dataset</a></li>
<li><a class="reference internal" href="#loading-a-pretrained-llm">7.5 Loading a pretrained LLM</a></li>
<li><a class="reference internal" href="#fine-tuning-the-llm-on-instruction-data">7.6 Fine-tuning the LLM on instruction data</a></li>
<li><a class="reference internal" href="#extracting-and-saving-responses">7.7 Extracting and saving responses</a></li>
<li><a class="reference internal" href="#evaluating-the-fine-tuned-llm">7.8 Evaluating the fine-tuned LLM</a></li>
<li><a class="reference internal" href="#conclusions">7.9 Conclusions</a></li>
<li><a class="reference internal" href="#id7">Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-a-introduction-to-pytorch">Appendix A. Introduction to PyTorch</a><ul>
<li><a class="reference internal" href="#a-1-what-is-pytorch">A.1 What is PyTorch</a></li>
<li><a class="reference internal" href="#a-2-understanding-tensors">A.2 Understanding tensors</a></li>
<li><a class="reference internal" href="#a-3-seeing-models-as-computation-graphs">A.3 Seeing models as computation graphs</a></li>
<li><a class="reference internal" href="#a-4-automatic-differentiation-made-easy">A.4 Automatic differentiation made easy</a><ul>
<li><a class="reference internal" href="#partial-derivatives-and-gradients">Partial derivatives and gradients</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-5-implementing-multilayer-neural-networks">A.5 Implementing multilayer neural networks</a></li>
<li><a class="reference internal" href="#a-6-setting-up-efficient-data-loaders">A.6 Setting up efficient data loaders</a></li>
<li><a class="reference internal" href="#a-7-a-typical-training-loop">A.7 A typical training loop</a></li>
<li><a class="reference internal" href="#a-8-saving-and-loading-models">A.8 Saving and loading models</a></li>
<li><a class="reference internal" href="#a-9-optimizing-training-performance-with-gpus">A.9 Optimizing training performance with GPUs</a><ul>
<li><a class="reference internal" href="#a-9-1-pytorch-computations-on-gpu-devices">A.9.1 PyTorch computations on GPU devices</a></li>
<li><a class="reference internal" href="#a-9-2-single-gpu-training">A.9.2 Single-GPU training</a></li>
<li><a class="reference internal" href="#a-9-3-training-with-multiple-gpus">A.9.3 Training with multiple GPUs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#a-10-summary">A.10 Summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-b-references-and-further-reading">Appendix B. References and Further Reading</a><ul>
<li><a class="reference internal" href="#chapter-1-understanding-llm">Chapter 1: Understanding LLM</a></li>
<li><a class="reference internal" href="#chapter-2-working-with-text-data">Chapter 2: Working with Text Data</a></li>
<li><a class="reference internal" href="#chapter-3-coding-attention-mechanisms">Chapter 3: Coding Attention Mechanisms</a></li>
<li><a class="reference internal" href="#chapter-4-implementing-a-gpt-model">Chapter 4: Implementing a GPT model</a></li>
<li><a class="reference internal" href="#chapter-5-pretraining-on-unlabeled-data">Chapter 5: Pretraining on Unlabeled Data</a></li>
<li><a class="reference internal" href="#chapter-6-fine-tuning-for-classification">Chapter 6: Fine-tuning for classification</a></li>
<li><a class="reference internal" href="#chapter-7-fine-tuning-to-follow-instructions">Chapter 7: Fine-tuning to follow instructions</a></li>
<li><a class="reference internal" href="#appendix-a-pytorch">Appendix A: PyTorch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-c-exercise-solutions">Appendix C. Exercise Solutions</a></li>
<li><a class="reference internal" href="#appendix-d-adding-bells-and-whistles-to-the-training-loop">Appendix D. Adding Bells and Whistles to the Training Loop</a><ul>
<li><a class="reference internal" href="#d-1-learning-rate-warmup">D.1 Learning rate warmup</a></li>
<li><a class="reference internal" href="#d-2-cosine-decay">D.2 Cosine decay</a></li>
<li><a class="reference internal" href="#d-3-gradient-clipping">D.3 Gradient clipping</a></li>
<li><a class="reference internal" href="#d-4-the-modified-training-function">D.4 The modified training function</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-e-parameter-efficient-fine-tuning-with-lora">appendix E Parameter-efficient fine- tuning with LoRA</a><ul>
<li><a class="reference internal" href="#e-1-introduction-to-lora">E.1 Introduction to LoRA</a></li>
<li><a class="reference internal" href="#e-2-preparing-the-dataset">E.2 Preparing the dataset</a></li>
<li><a class="reference internal" href="#e-3-initializing-the-model">E.3 Initializing the model</a></li>
<li><a class="reference internal" href="#e-4-parameter-efficient-finetuning-with-lora">E.4 Parameter-efficient finetuning with LoRA</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8">其他</a></li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="build-a-large-language-model-from-scratch">
<h1>Build a Large Language Model (From Scratch)<a class="headerlink" href="#build-a-large-language-model-from-scratch" title="此标题的永久链接">¶</a></h1>
<ul class="simple">
<li><p>GitHub(相关代码): <a class="reference external" href="https://github.com/rasbt/LLMs-from-scratch">https://github.com/rasbt/LLMs-from-scratch</a></p></li>
<li><p>【其他参考】The Illustrated Transformer(图的形式讲解transformer，涉及到本书要讲的qkv计算、多head注意力等): <a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><dl class="simple">
<dt>【其他代码】实现了类似功能: <a class="reference external" href="https://github.com/jingyaogong/minimind">https://github.com/jingyaogong/minimind</a></dt><dd><ul>
<li><p>多模态版: <a class="reference external" href="https://github.com/jingyaogong/minimind-v">https://github.com/jingyaogong/minimind-v</a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<section id="understanding-llm">
<h2>1. Understanding LLM<a class="headerlink" href="#understanding-llm" title="此标题的永久链接">¶</a></h2>
<p>章节:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>1.1 What is an LLM?
1.2 Applications of LLMs
1.3 Stages of building and using LLMs
1.4 Using LLMs for different tasks
1.5 Utilizing large datasets
1.6 A closer look at the GPT architecture
1.7 Building a large language model
1.8 Summary
</pre></div>
</div>
<figure class="align-default">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/uwgOhZ.png" src="https://img.zhaoweiguo.com/uPic/2024/11/uwgOhZ.png" />
</figure>
<p>AI including:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">machine</span> <span class="n">learning</span>
<span class="n">deep</span> <span class="n">learning</span>

<span class="n">rule</span><span class="o">-</span><span class="n">based</span> <span class="n">systems</span>
<span class="n">genetic</span> <span class="n">algorithms</span>
<span class="n">expert</span> <span class="n">systems</span>
<span class="n">fuzzy</span> <span class="n">logic</span>
<span class="n">symbolic</span> <span class="n">reasoning</span>
</pre></div>
</div>
<ul class="simple">
<li><p>traditional machine learning: human experts might manually extract features from email text such as the frequency of certain trigger words (“prize,” “win,” “free”), the number of exclamation marks, use of all uppercase words, or the presence of suspicious links.</p></li>
<li><p>deep learning: does not require manual feature extraction. This means that human experts do not need to identify and select the most relevant features for a deep learning model.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>不管是traditional machine learning还是deep learning，都需要人工收集一些数据，哪此邮件是spam，哪些不是spam。但 traditional machine learning 还需要专家进行人工的特征提取，对垃圾邮件事例特征有：win, free单词很多，有可疑链接，使用大写字母……</p>
</div>
<figure class="align-default" id="id10">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/ntzxz1.png" src="https://img.zhaoweiguo.com/uPic/2024/11/ntzxz1.png" />
<figcaption>
<p><span class="caption-text">Figure 1.3</span><a class="headerlink" href="#id10" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>The two most popular categories of finetuning LLMs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">instruction</span><span class="o">-</span><span class="n">finetuning</span>
<span class="mf">2.</span> <span class="n">finetuning</span> <span class="k">for</span> <span class="n">classification</span> <span class="n">tasks</span>
</pre></div>
</div>
<figure class="align-default" id="id11">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Pg4s7f.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Pg4s7f.png" />
<figcaption>
<p><span class="caption-text">Figure 1.4 A simplified depiction of the original transformer architecture, which is a deep learning model for language translation. The transformer consists of two parts, an encoder that processes the input text and produces an embedding representation (a numerical representation that captures many different factors in different dimensions) of the text that the decoder can use to generate the translated text one word at a time. Note that this figure shows the final stage of the translation process where the decoder has to generate only the final word (“Beispiel”), given the original input text (“This is an example”) and a partially translated sentence (“Das ist ein”), to complete the translation.</span><a class="headerlink" href="#id11" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>A key component of transformers and LLMs is the self-attention mechanism, which allows the model to weigh the importance of different words or tokens in a sequence relative to each other.</p></li>
</ul>
<figure class="align-default" id="id12">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/DMgI9v.png" src="https://img.zhaoweiguo.com/uPic/2024/11/DMgI9v.png" />
<figcaption>
<p><span class="caption-text">Figure 1.5 A visual representation of the transformer’s <code class="docutils literal notranslate"><span class="pre">encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">decoder</span></code> submodules. On the left, the encoder segment exemplifies BERT-like LLMs, which focus on masked word prediction and are primarily used for tasks like text classification. On the right, the decoder segment showcases GPT-like LLMs, designed for generative tasks and producing coherent text sequences.</span><a class="headerlink" href="#id12" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id13">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/jiIOSX.png" src="https://img.zhaoweiguo.com/uPic/2024/11/jiIOSX.png" />
<figcaption>
<p><span class="caption-text">Table 1.1 The pretraining dataset of the popular GPT-3 LLM</span><a class="headerlink" href="#id13" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">*</span> <span class="n">Wikipedia</span> <span class="n">corpus</span> <span class="n">consists</span> <span class="n">of</span> <span class="n">English</span><span class="o">-</span><span class="n">language</span> <span class="n">Wikipedia</span>
<span class="o">*</span> <span class="n">Books1</span> <span class="ow">is</span> <span class="n">likely</span> <span class="n">a</span> <span class="n">sample</span> <span class="kn">from</span><span class="w"> </span><span class="nn">Project</span> <span class="n">Gutenberg</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">gutenberg</span><span class="o">.</span><span class="n">org</span><span class="o">/</span>
<span class="o">*</span> <span class="n">Books2</span> <span class="ow">is</span> <span class="n">likely</span> <span class="kn">from</span><span class="w"> </span><span class="nn">Libgen</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">en</span><span class="o">.</span><span class="n">wikipedia</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">wiki</span><span class="o">/</span><span class="n">Library_Genesis</span>
<span class="o">*</span> <span class="n">CommonCrawl</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">filtered</span> <span class="n">subset</span> <span class="n">of</span> <span class="n">the</span> <span class="n">CommonCrawl</span> <span class="n">database</span><span class="p">:</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">commoncrawl</span><span class="o">.</span><span class="n">org</span><span class="o">/</span>
<span class="o">*</span> <span class="n">WebText2</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">text</span> <span class="n">of</span> <span class="n">web</span> <span class="n">pages</span> <span class="kn">from</span><span class="w"> </span><span class="nn">all</span> <span class="n">outbound</span> <span class="n">Reddit</span> <span class="n">links</span> <span class="kn">from</span><span class="w"> </span><span class="nn">posts</span> <span class="k">with</span> <span class="mi">3</span><span class="o">+</span> <span class="n">upvotes</span><span class="o">.</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The ability to perform tasks that the model wasn’t explicitly trained to perform is called an “emergent behavior(涌现行为).”“涌现行为” 指的是模型在规模增大后自发展现出一些未明确训练的能力（如推理、算术等），这些能力不是直接由个体参数或特定任务驱动的，而是系统在规模和复杂度增加时的自发产物。</p></li>
</ul>
<figure class="align-default" id="id14">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/VoBxrx.png" src="https://img.zhaoweiguo.com/uPic/2024/11/VoBxrx.png" />
<figcaption>
<p><span class="caption-text">Figure 1.9 The stages of building LLMs covered in this book include implementing the LLM architecture and data preparation process, pretraining an LLM to create a foundation model, and finetuning the foundation model to become a personal assistant or text classifier.</span><a class="headerlink" href="#id14" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="working-with-text-data">
<h2>2. Working with Text Data<a class="headerlink" href="#working-with-text-data" title="此标题的永久链接">¶</a></h2>
<figure class="align-default" id="id15">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/tfp8db.png" src="https://img.zhaoweiguo.com/uPic/2024/11/tfp8db.png" />
<figcaption>
<p><span class="caption-text">Figure 2.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a general text dataset, and finetuning it on a labeled dataset. This chapter will explain and code the data preparation and sampling pipeline that provides the LLM with the text data for pretraining.</span><a class="headerlink" href="#id15" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="understanding-word-embeddings">
<h3>2.1 Understanding word embeddings<a class="headerlink" href="#understanding-word-embeddings" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id16">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/enlX0I.png" src="https://img.zhaoweiguo.com/uPic/2024/11/enlX0I.png" />
<figcaption>
<p><span class="caption-text">Figure 2.2 Deep learning models cannot process data formats like video, audio, and text in their raw form. Thus, we use an embedding model to transform this raw data into a dense vector representation that deep learning architectures can easily understand and process. Specifically, this figure illustrates the process of converting raw data into a three-dimensional numerical vector.</span><a class="headerlink" href="#id16" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for retrieval- augmented generation.</p></li>
</ul>
<figure class="align-default" id="id17">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/unvhYf.png" src="https://img.zhaoweiguo.com/uPic/2024/11/unvhYf.png" />
<figcaption>
<p><span class="caption-text">Figure 2.3 If word embeddings are two-dimensional, we can plot them in a two-dimensional scatterplot for visualization purposes as shown here. When using word embedding techniques, such as Word2Vec, words corresponding to similar concepts often appear close to each other in the embedding space. For instance, different types of birds appear closer to each other in the embedding space compared to countries and cities.</span><a class="headerlink" href="#id17" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Word embeddings can have varying dimensions, from one to thousands. As shown in Figure 2.3, we can choose two-dimensional word embeddings for visualization purposes.</p></li>
</ul>
</section>
<section id="tokenizing-text">
<h3>2.2 Tokenizing text<a class="headerlink" href="#tokenizing-text" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id18">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Ujvr7k.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Ujvr7k.png" />
<figcaption>
<p><span class="caption-text">Figure 2.4 A view of the text processing steps covered in this section in the context of an LLM. Here, we split an input text into individual tokens, which are either words or special characters, such as punctuation characters. In upcoming sections, we will convert the text into token IDs and create token embeddings.</span><a class="headerlink" href="#id18" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="converting-tokens-into-token-ids">
<h3>2.3 Converting tokens into token IDs<a class="headerlink" href="#converting-tokens-into-token-ids" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id19">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/OUFhDM.png" src="https://img.zhaoweiguo.com/uPic/2024/11/OUFhDM.png" />
<figcaption>
<p><span class="caption-text">Figure 2.6 We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value. The depicted vocabulary is purposefully small for illustration purposes and contains no punctuation or special characters for simplicity.</span><a class="headerlink" href="#id19" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id20">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/AmM7uq.png" src="https://img.zhaoweiguo.com/uPic/2024/11/AmM7uq.png" />
<figcaption>
<p><span class="caption-text">Figure 2.7 Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs. The vocabulary is built from the entire training set and can be applied to the training set itself and any new text samples. The depicted vocabulary contains no punctuation or special characters for simplicity.</span><a class="headerlink" href="#id20" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id21">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/RAYbq0.png" src="https://img.zhaoweiguo.com/uPic/2024/11/RAYbq0.png" />
<figcaption>
<p><span class="caption-text">Figure 2.8 Tokenizer implementations share two common methods: an encode method and a decode method. The encode method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text.</span><a class="headerlink" href="#id21" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="adding-special-context-tokens">
<h3>2.4 Adding special context tokens<a class="headerlink" href="#adding-special-context-tokens" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id22">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/FS7Wg5.png" src="https://img.zhaoweiguo.com/uPic/2024/11/FS7Wg5.png" />
<figcaption>
<p><span class="caption-text">Figure 2.9 We add special tokens to a vocabulary to deal with certain contexts. For instance, we add an <code class="docutils literal notranslate"><span class="pre">&lt;|unk|&gt;</span></code> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary. Furthermore, we add an <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> token that we can use to separate two unrelated text sources.</span><a class="headerlink" href="#id22" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id23">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/sm50GQ.png" src="https://img.zhaoweiguo.com/uPic/2024/11/sm50GQ.png" />
<figcaption>
<p><span class="caption-text">Figure 2.10 When working with multiple independent text source, we add <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> tokens between these texts. These <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code> tokens act as markers, signaling the start or end of a particular segment, allowing for more effective processing and understanding by the LLM.</span><a class="headerlink" href="#id23" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>additional special tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="p">[</span><span class="n">BOS</span><span class="p">]</span> <span class="p">(</span><span class="n">beginning</span> <span class="n">of</span> <span class="n">sequence</span><span class="p">)</span>
<span class="mf">2.</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span> <span class="p">(</span><span class="n">end</span> <span class="n">of</span> <span class="n">sequence</span><span class="p">)</span>
<span class="mf">3.</span> <span class="p">[</span><span class="n">PAD</span><span class="p">]</span> <span class="p">(</span><span class="n">padding</span><span class="p">)</span>
<span class="mf">4.</span> <span class="o">|</span><span class="n">endoftext</span><span class="o">|</span>
<span class="mf">5.</span> <span class="o">|</span><span class="n">unk</span><span class="o">|</span>
</pre></div>
</div>
</section>
<section id="byte-pair-encoding">
<h3>2.5 Byte pair encoding<a class="headerlink" href="#byte-pair-encoding" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id24">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/OcQqcV.png" src="https://img.zhaoweiguo.com/uPic/2024/11/OcQqcV.png" />
<figcaption>
<p><span class="caption-text">Figure 2.11 BPE tokenizers break down unknown words into subwords and individual characters. This way, a BPE tokenizer can parse any word and doesn’t need to replace unknown words with special tokens, such as <code class="docutils literal notranslate"><span class="pre">&lt;|unk|&gt;</span></code>.</span><a class="headerlink" href="#id24" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>The original BPE tokenizer can be found here: [<a class="reference external" href="https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py">https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py</a>)</p></li>
<li><p>The BPE tokenizer from OpenAI’s open-source [tiktoken](<a class="reference external" href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>) library, which implements its core algorithms in Rust to improve computational performance</p></li>
</ul>
</section>
<section id="data-sampling-with-a-sliding-window">
<h3>2.6 Data sampling with a sliding window<a class="headerlink" href="#data-sampling-with-a-sliding-window" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id25">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/bCiR3G.png" src="https://img.zhaoweiguo.com/uPic/2024/11/bCiR3G.png" />
<figcaption>
<p><span class="caption-text">Figure 2.12 Given a text sample, extract input blocks as subsamples that serve as input to the LLM, and the LLM’s prediction task during training is to predict the next word that follows the input block. During training, we mask out all words that are past the target. Note that the text shown in this figure would undergo tokenization before the LLM can process it; however, this figure omits the tokenization step for clarity.</span><a class="headerlink" href="#id25" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>For each text chunk, we want the inputs and targets</p></li>
<li><p>Since we want the model to predict the next word, the targets are the inputs shifted by one position to the right</p></li>
</ul>
<p>The prediction would look like as follows:(input-target pairs):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ow">and</span> <span class="o">----&gt;</span>  <span class="n">established</span>
<span class="ow">and</span> <span class="n">established</span> <span class="o">----&gt;</span>  <span class="n">himself</span>
<span class="ow">and</span> <span class="n">established</span> <span class="n">himself</span> <span class="o">----&gt;</span>  <span class="ow">in</span>
<span class="ow">and</span> <span class="n">established</span> <span class="n">himself</span> <span class="ow">in</span> <span class="o">----&gt;</span>  <span class="n">a</span>
</pre></div>
</div>
<figure class="align-default" id="id26">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/abgbDL.png" src="https://img.zhaoweiguo.com/uPic/2024/11/abgbDL.png" />
<figcaption>
<p><span class="caption-text">Figure 2.13 To implement efficient data loaders, we collect the inputs in a tensor, x, where each row represents one input context. A second tensor, y, contains the corresponding prediction targets (next words), which are created by shifting the input by one position.</span><a class="headerlink" href="#id26" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id27">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/khbYvv.png" src="https://img.zhaoweiguo.com/uPic/2024/11/khbYvv.png" />
<figcaption>
<p><span class="caption-text">Figure 2.14 When creating multiple batches from the input dataset, we slide an input window across the text. If the stride is set to 1, we shift the input window by 1 position when creating the next batch. If we set the stride equal to the input window size, we can prevent overlaps between the batches.</span><a class="headerlink" href="#id27" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul>
<li><p>Small batch sizes require less memory during training but lead to more noisy model updates. The batch size is a trade-off and hyperparameter to experiment with when training LLMs.</p></li>
<li><p>示例(batch_size=1, max_length=4, stride=1):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">tensor</span><span class="p">([[</span>  <span class="mi">40</span><span class="p">,</span>  <span class="mi">367</span><span class="p">,</span> <span class="mi">2885</span><span class="p">,</span> <span class="mi">1464</span><span class="p">]]),</span> <span class="n">tensor</span><span class="p">([[</span> <span class="mi">367</span><span class="p">,</span> <span class="mi">2885</span><span class="p">,</span> <span class="mi">1464</span><span class="p">,</span> <span class="mi">1807</span><span class="p">]])]</span>
</pre></div>
</div>
</li>
<li><p>示例(batch_size=8, max_length=4, stride=4):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Inputs</span><span class="p">:</span>
 <span class="n">tensor</span><span class="p">([[</span>   <span class="mi">40</span><span class="p">,</span>   <span class="mi">367</span><span class="p">,</span>  <span class="mi">2885</span><span class="p">,</span>  <span class="mi">1464</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1807</span><span class="p">,</span>  <span class="mi">3619</span><span class="p">,</span>   <span class="mi">402</span><span class="p">,</span>   <span class="mi">271</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10899</span><span class="p">,</span>  <span class="mi">2138</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">7026</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">15632</span><span class="p">,</span>   <span class="mi">438</span><span class="p">,</span>  <span class="mi">2016</span><span class="p">,</span>   <span class="mi">257</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">922</span><span class="p">,</span>  <span class="mi">5891</span><span class="p">,</span>  <span class="mi">1576</span><span class="p">,</span>   <span class="mi">438</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">568</span><span class="p">,</span>   <span class="mi">340</span><span class="p">,</span>   <span class="mi">373</span><span class="p">,</span>   <span class="mi">645</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">1049</span><span class="p">,</span>  <span class="mi">5975</span><span class="p">,</span>   <span class="mi">284</span><span class="p">,</span>   <span class="mi">502</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">284</span><span class="p">,</span>  <span class="mi">3285</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span>    <span class="mi">11</span><span class="p">]])</span>

<span class="n">Targets</span><span class="p">:</span>
 <span class="n">tensor</span><span class="p">([[</span>  <span class="mi">367</span><span class="p">,</span>  <span class="mi">2885</span><span class="p">,</span>  <span class="mi">1464</span><span class="p">,</span>  <span class="mi">1807</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3619</span><span class="p">,</span>   <span class="mi">402</span><span class="p">,</span>   <span class="mi">271</span><span class="p">,</span> <span class="mi">10899</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">2138</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">7026</span><span class="p">,</span> <span class="mi">15632</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">438</span><span class="p">,</span>  <span class="mi">2016</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>   <span class="mi">922</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">5891</span><span class="p">,</span>  <span class="mi">1576</span><span class="p">,</span>   <span class="mi">438</span><span class="p">,</span>   <span class="mi">568</span><span class="p">],</span>
        <span class="p">[</span>  <span class="mi">340</span><span class="p">,</span>   <span class="mi">373</span><span class="p">,</span>   <span class="mi">645</span><span class="p">,</span>  <span class="mi">1049</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">5975</span><span class="p">,</span>   <span class="mi">284</span><span class="p">,</span>   <span class="mi">502</span><span class="p">,</span>   <span class="mi">284</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">3285</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span>    <span class="mi">11</span><span class="p">,</span>   <span class="mi">287</span><span class="p">]])</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="creating-token-embeddings">
<h3>2.7 Creating token embeddings<a class="headerlink" href="#creating-token-embeddings" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>we converted the token IDs into a continuous vector representation, the so-called token embeddings.</p></li>
</ul>
<figure class="align-default" id="id28">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/NGr1lp.png" src="https://img.zhaoweiguo.com/uPic/2024/11/NGr1lp.png" />
<figcaption>
<p><span class="caption-text">Figure 2.15 Preparing the input text for an LLM involves tokenizing text, converting text tokens to token IDs, and converting token IDs into vector embedding vectors. In this section, we consider the token IDs created in previous sections to create the token embedding vectors.</span><a class="headerlink" href="#id28" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id29">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/OrtaX4.png" src="https://img.zhaoweiguo.com/uPic/2024/11/OrtaX4.png" />
<figcaption>
<p><span class="caption-text">Figure 2.16 Embedding layers perform a look-up operation, retrieving the embedding vector corresponding to the token ID from the embedding layer’s weight matrix. input text =&gt; token ID =&gt; embedded token ID</span><a class="headerlink" href="#id29" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="encoding-word-positions">
<h3>2.8 Encoding word positions<a class="headerlink" href="#encoding-word-positions" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id30">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/35sgO7.png" src="https://img.zhaoweiguo.com/uPic/2024/11/35sgO7.png" />
<figcaption>
<p><span class="caption-text">Figure 2.17 The embedding layer converts a token ID into the same vector representation regardless of where it is located in the input sequence. For example, the token ID 2, whether it’s in the first or last position in the token ID input vector, will result in the same embedding vector.</span><a class="headerlink" href="#id30" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility purposes.</p></li>
<li><p>However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM.</p></li>
</ul>
<p>two broad categories of position-aware embeddings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">1.</span> <span class="n">relative</span> <span class="n">positional</span> <span class="n">embeddings</span>
<span class="mf">2.</span> <span class="n">absolute</span> <span class="n">positional</span> <span class="n">embeddings</span>
</pre></div>
</div>
<figure class="align-default" id="id31">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/q49SEA.png" src="https://img.zhaoweiguo.com/uPic/2024/11/q49SEA.png" />
<figcaption>
<p><span class="caption-text">Figure 2.18 Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM. The positional vectors have the same dimension as the original token embeddings. The token embeddings are shown with value 1 for simplicity.</span><a class="headerlink" href="#id31" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Instead of focusing on the absolute position of a token, the emphasis of relative positional embeddings is on the relative position or distance between tokens. This means the model learns the relationships in terms of “how far apart” rather than “at which exact position.” The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn’t seen such lengths during training.</p></li>
<li><p>OpenAI’s GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original Transformer model. This optimization process is part of the model training itself, which we will implement later in this book. For now, let’s create the initial positional embeddings to create the LLM inputs for the upcoming chapters.</p></li>
</ul>
<figure class="align-default" id="id32">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/zMvBpf.png" src="https://img.zhaoweiguo.com/uPic/2024/11/zMvBpf.png" />
<figcaption>
<p><span class="caption-text">Figure 2.19 As part of the input processing pipeline, input text is first broken up into individual tokens. These tokens are then converted into token IDs using a vocabulary. The token IDs are converted into embedding vectors to which positional embeddings of a similar size are added, resulting in input embeddings that are used as input for the main LLM layers.</span><a class="headerlink" href="#id32" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">context_length</span></code> is a variable that represents the supported input size of the LLM.</p></li>
</ul>
</section>
<section id="summary">
<h3>2.9 Summary<a class="headerlink" href="#summary" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>LLMs require textual data to be converted into numerical vectors, known as embeddings since they can’t process raw text. Embeddings transform discrete data (like words or images) into continuous vector spaces, making them compatible with neural network operations.</p></li>
<li><p>As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs.</p></li>
<li><p>Special tokens, such as <code class="docutils literal notranslate"><span class="pre">&lt;|unk|&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;|endoftext|&gt;</span></code>, can be added to enhance the model’s understanding and handle various contexts, such as unknown words or marking the boundary between unrelated texts.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">byte</span> <span class="pre">pair</span> <span class="pre">encoding</span> <span class="pre">(BPE)</span></code> tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle unknown words by breaking them down into subword units or individual characters.</p></li>
<li><p>We use a sliding window approach on tokenized data to generate <code class="docutils literal notranslate"><span class="pre">input-target</span> <span class="pre">pairs</span></code> for LLM training.</p></li>
<li><p>Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial for training deep learning models like LLMs.</p></li>
<li><p>While token embeddings provide consistent vector representations for each token, they lack a sense of the token’s position in a sequence. To rectify this, two main types of positional embeddings exist: absolute and relative. OpenAI’s GPT models utilize absolute positional embeddings that are added to the token embedding vectors and are optimized during the model training.</p></li>
</ul>
</section>
</section>
<section id="coding-attention-mechanisms">
<h2>3. Coding Attention Mechanisms<a class="headerlink" href="#coding-attention-mechanisms" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>Exploring the reasons for using attention mechanisms in neural networks</p></li>
<li><p>Introducing a basic self-attention framework and progressing to an enhanced self-attention mechanism</p></li>
<li><p>Implementing a causal attention module that allows LLMs to generate one token at a time</p></li>
<li><p>Masking randomly selected attention weights with dropout to reduce overfitting</p></li>
<li><p>Stacking multiple causal attention modules into a multi-head attention module</p></li>
</ul>
<figure class="align-default" id="id33">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Z0VzmK.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Z0VzmK.png" />
<figcaption>
<p><span class="caption-text">Figure 3.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a general text dataset, and finetuning it on a labeled dataset. This chapter focuses on attention mechanisms, which are an integral part of an LLM architecture.</span><a class="headerlink" href="#id33" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id34">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/XwmbT3.png" src="https://img.zhaoweiguo.com/uPic/2024/11/XwmbT3.png" />
<figcaption>
<p><span class="caption-text">Figure 3.2 The figure depicts different attention mechanisms we will code in this chapter, starting with a simplified version of self-attention before adding the trainable weights. The causal attention mechanism adds a mask to self-attention that allows the LLM to generate one word at a time. Finally, multi-head attention organizes the attention mechanism into multiple heads, allowing the model to capture various aspects of the input data in parallel.</span><a class="headerlink" href="#id34" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="the-problem-with-modeling-long-sequences">
<h3>3.1 The problem with modeling long sequences<a class="headerlink" href="#the-problem-with-modeling-long-sequences" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id35">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/fHcmYY.png" src="https://img.zhaoweiguo.com/uPic/2024/11/fHcmYY.png" />
<figcaption>
<p><span class="caption-text">Figure 3.3 When translating text from one language to another, such as German to English, it’s not possible to merely translate word by word. Instead, the translation process requires contextual understanding and grammar alignment.</span><a class="headerlink" href="#id35" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>To address the issue that we cannot translate text word by word, it is common to use a deep neural network with two submodules, a so-called <code class="docutils literal notranslate"><span class="pre">encoder</span></code> and <code class="docutils literal notranslate"><span class="pre">decoder</span></code>. The job of the encoder is to first read in and process the entire text, and the decoder then produces the translated text. e.g. <code class="docutils literal notranslate"><span class="pre">encoder-decoder</span> <span class="pre">RNNs</span></code></p></li>
</ul>
<figure class="align-default" id="id36">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/9nYx0o.png" src="https://img.zhaoweiguo.com/uPic/2024/11/9nYx0o.png" />
<figcaption>
<p><span class="caption-text">Figure 3.4 Before the advent of transformer models, <code class="docutils literal notranslate"><span class="pre">encoder-decoder</span> <span class="pre">RNNs</span></code> were a popular choice for machine translation. The encoder takes a sequence of tokens from the source language as input, where a hidden state (an intermediate neural network layer) of the encoder encodes a compressed representation of the entire input sequence. Then, the decoder uses its current hidden state to begin the translation, token by token.</span><a class="headerlink" href="#id36" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>The big issue and limitation of encoder-decoder RNNs is that the RNN can’t directly access earlier hidden states from the encoder during the decoding phase. Consequently, it relies solely on the current hidden state, which encapsulates all relevant information. This can lead to a loss of context, especially in complex sentences where dependencies might span long distances.</p></li>
</ul>
</section>
<section id="capturing-data-dependencies-with-attention-mechanisms">
<h3>3.2 Capturing data dependencies with attention mechanisms<a class="headerlink" href="#capturing-data-dependencies-with-attention-mechanisms" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id37">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/yidBLJ.png" src="https://img.zhaoweiguo.com/uPic/2024/11/yidBLJ.png" />
<figcaption>
<p><span class="caption-text">Figure 3.5 Using an attention mechanism, the text-generating decoder part of the network can access all input tokens selectively. This means that some input tokens are more important than others for generating a given output token. The importance is determined by the so-called <code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">weights</span></code>, which we will compute later. Note that this figure shows the general idea behind attention and does not depict(描述) the exact implementation of the <code class="docutils literal notranslate"><span class="pre">Bahdanau</span> <span class="pre">mechanism</span></code>, which is an RNN method outside this book’s scope.</span><a class="headerlink" href="#id37" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Self-attention is a mechanism that allows each position in the input sequence to attend to(注意) all positions in the same sequence when computing the representation of a sequence. Self-attention is a key component of contemporary(当代) LLMs based on the transformer architecture, such as the GPT series.</p></li>
</ul>
<figure class="align-default" id="id38">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/KTZwGT.png" src="https://img.zhaoweiguo.com/uPic/2024/11/KTZwGT.png" />
<figcaption>
<p><span class="caption-text">Figure 3.6 Self-attention is a mechanism in transformers that is used to compute more efficient input representations by allowing each position in a sequence to interact with and weigh the importance of all other positions within the same sequence. In this chapter, we will code this self-attention mechanism from the ground up before we code the remaining parts of the GPT-like LLM in the following chapter.</span><a class="headerlink" href="#id38" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="attending-to-different-parts-of-the-input-with-self-attention">
<h3>3.3 Attending to different parts of the input with self-attention<a class="headerlink" href="#attending-to-different-parts-of-the-input-with-self-attention" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>The “self” in self-attention</p></li>
<li><p>In self-attention, the “self” refers to the mechanism’s ability to compute attention weights by relating different positions within a single input sequence. It assesses and learns the relationships and dependencies between various parts of the input itself, such as words in a sentence or pixels in an image.</p></li>
<li><p>This is in contrast to traditional attention mechanisms, where the focus is on the relationships between elements of two different sequences, such as in sequence-to-sequence models where the attention might be between an input sequence and an output sequence, such as the example depicted in Figure 3.5.</p></li>
</ul>
<section id="a-simple-self-attention-mechanism-without-trainable-weights">
<h4>3.3.1 A simple self-attention mechanism without trainable weights<a class="headerlink" href="#a-simple-self-attention-mechanism-without-trainable-weights" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id39">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/NuE983.png" src="https://img.zhaoweiguo.com/uPic/2024/11/NuE983.png" />
<figcaption>
<p><span class="caption-text">Figure 3.7 The goal of self-attention is to compute a context vector, for each input element, that combines information from all other input elements. In the example depicted in this figure, we compute the context vector <span class="math notranslate nohighlight">\(z^{(2)}\)</span> . The importance or contribution of each input element for computing <span class="math notranslate nohighlight">\(z^{(2)}\)</span> is determined by the attention weights <span class="math notranslate nohighlight">\(α_{21}\)</span> to <span class="math notranslate nohighlight">\(α_{2T}\)</span> . When computing <span class="math notranslate nohighlight">\(z^{(2)}\)</span> , the attention weights are calculated with respect to input element <span class="math notranslate nohighlight">\(x^{(2)}\)</span> and all other inputs. The exact computation of these attention weights is discussed later in this section.</span><a class="headerlink" href="#id39" title="此图像的永久链接">¶</a></p>
<div class="legend">
</div>
</figcaption>
</figure>
<ul class="simple">
<li><p>For example, consider an input text like <code class="docutils literal notranslate"><span class="pre">&quot;Your</span> <span class="pre">journey</span> <span class="pre">starts</span> <span class="pre">with</span> <span class="pre">one</span> <span class="pre">step.&quot;</span></code> In this case, each element of the sequence, such as <span class="math notranslate nohighlight">\(x^{(1)}\)</span>, corresponds to a <code class="docutils literal notranslate"><span class="pre">d-dimensional</span></code> embedding vector representing a specific token, like “Your.” These input vectors are shown as <code class="docutils literal notranslate"><span class="pre">3-dimensional</span></code> embeddings.</p></li>
<li><p>Let’s focus on the <code class="docutils literal notranslate"><span class="pre">embedding</span> <span class="pre">vector</span></code> of the second input element, <span class="math notranslate nohighlight">\(x^{(2)}\)</span> (which corresponds to the token “journey”), and the corresponding <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vector</span></code>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>. This enhanced <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vector</span></code>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>, is an embedding that contains information about <span class="math notranslate nohighlight">\(x^{(2)}\)</span> and all other input elements <span class="math notranslate nohighlight">\(x^{(1)}\)</span> to <span class="math notranslate nohighlight">\(x^{(T)}\)</span>.</p></li>
<li><p>In self-attention, <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vectors</span></code> play a crucial role. Their purpose is to create enriched representations of each element in an input sequence (like a sentence) by incorporating information from all other elements in the sequence. This is essential in LLMs, which need to understand the relationship and relevance of words in a sentence to each other. Later, we will add trainable weights that help an LLM learn to construct these context vectors so that they are relevant for the LLM to generate the next token.</p></li>
</ul>
<figure class="align-default" id="id40">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/1ArQGZ.png" src="https://img.zhaoweiguo.com/uPic/2024/11/1ArQGZ.png" />
<figcaption>
<p><span class="caption-text">Figure 3.8 The overall goal of this section is to illustrate the computation of the context vector <span class="math notranslate nohighlight">\(z^{(2)}\)</span> using the second input sequence, <span class="math notranslate nohighlight">\(x^{(2)}\)</span> as a query. This figure shows the first intermediate step, computing the attention scores <code class="docutils literal notranslate"><span class="pre">ω</span></code> between the query <span class="math notranslate nohighlight">\(x^{(2)}\)</span> and all other input elements as a dot product.</span><a class="headerlink" href="#id40" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
  <span class="p">[[</span><span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span> <span class="c1"># Your     (x^1)</span>
    <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">],</span> <span class="c1"># journey  (x^2)</span>
    <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.64</span><span class="p">],</span> <span class="c1"># starts    (x^3)</span>
    <span class="p">[</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span> <span class="c1"># with      (x^4)</span>
    <span class="p">[</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="c1"># one       (x^5)</span>
    <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]]</span> <span class="c1"># step      (x^6)</span>
<span class="p">)</span>


<span class="n">query</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1">#A</span>
<span class="n">attn_scores_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">attn_scores_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_scores_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>除了将点积运算视为组合两个向量以产生标量值的数学工具之外，点积还是相似性的度量，因为它量化了两个向量的对齐程度：更高的点积表示更大程度的对齐或相似性向量之间。在自注意力机制的背景下，点积决定了序列中元素相互关注的程度：点积越高，两个元素之间的相似度和注意力分数就越高。</p></li>
</ul>
<figure class="align-default" id="id41">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Gzln85.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Gzln85.png" />
<figcaption>
<p><span class="caption-text">Figure 3.9 After computing the attention scores <span class="math notranslate nohighlight">\(ω_{21}\)</span> to <span class="math notranslate nohighlight">\(ω_{2T}\)</span> with respect to the input query <span class="math notranslate nohighlight">\(x^{(2)}\)</span>, the next step is to obtain the attention weights <span class="math notranslate nohighlight">\(α_{21}\)</span> to <span class="math notranslate nohighlight">\(α_{2T}\)</span> by normalizing the attention scores.</span><a class="headerlink" href="#id41" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>标准化背后的主要目标是获得总和为 1 的注意力权重。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn_weights_2_tmp</span> <span class="o">=</span> <span class="n">attn_scores_2</span> <span class="o">/</span> <span class="n">attn_scores_2</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention weights:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2_tmp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2_tmp</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="c1"># 输出</span>
<span class="c1"># Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])</span>
<span class="c1"># Sum: tensor(1.0000)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In practice, it’s more common and advisable to use the softmax function for normalization.</p></li>
<li><p>In addition, the softmax function ensures that the attention weights are always positive.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">softmax_naive</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">attn_weights_2_naive</span> <span class="o">=</span> <span class="n">softmax_naive</span><span class="p">(</span><span class="n">attn_scores_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention weights:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2_naive</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2_naive</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="c1"># 输出</span>
<span class="c1"># Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])</span>
<span class="c1"># Sum: tensor(1.)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note that this naive softmax implementation (softmax_naive) may encounter numerical instability problems, such as overflow and underflow, when dealing with large or small input values.</p></li>
<li><p>PyTorch implementation of softmax</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn_weights_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores_2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Attention weights:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sum:&quot;</span><span class="p">,</span> <span class="n">attn_weights_2</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
<span class="c1"># 输出</span>
<span class="c1"># Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])</span>
<span class="c1"># Sum: tensor(1.)</span>
</pre></div>
</div>
<figure class="align-default" id="id42">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/iVEcj6.png" src="https://img.zhaoweiguo.com/uPic/2024/11/iVEcj6.png" />
<figcaption>
<p><span class="caption-text">Figure 3.10 The final step, after calculating and normalizing the attention scores to obtain the attention weights for query <span class="math notranslate nohighlight">\(x^{(2)}\)</span>, is to compute the context vector <span class="math notranslate nohighlight">\(z^{(2)}\)</span>. This context vector is a combination of all input vectors <span class="math notranslate nohighlight">\(x^{(1)}\)</span> to <span class="math notranslate nohighlight">\(x^{(T)}\)</span> weighted by the attention weights.</span><a class="headerlink" href="#id42" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 2nd input token is the query</span>
<span class="n">context_vec_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">query</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="n">context_vec_2</span> <span class="o">+=</span> <span class="n">attn_weights_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x_i</span>
<span class="nb">print</span><span class="p">(</span><span class="n">context_vec_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([0.4419, 0.6515, 0.5683])</span>
</pre></div>
</div>
</section>
<section id="computing-attention-weights-for-all-input-tokens">
<h4>3.3.2 Computing attention weights for all input tokens<a class="headerlink" href="#computing-attention-weights-for-all-input-tokens" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id43">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/sZt4xS.png" src="https://img.zhaoweiguo.com/uPic/2024/11/sZt4xS.png" />
<figcaption>
<p><span class="caption-text">Figure 3.11 The highlighted row shows the attention weights for the second input element as a query, as we computed in the previous section. This section generalizes the computation to obtain all other attention weights.(这是上面用softmax计算出的 attention weights <span class="math notranslate nohighlight">\(α_{21}\)</span> to <span class="math notranslate nohighlight">\(α_{2T}\)</span> )。用同样的方法，可以计算出其他的 attention weights</span><a class="headerlink" href="#id43" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id44">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Itah8v.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Itah8v.png" />
<figcaption>
<p><span class="caption-text">Figure 3.12 计算的3步骤</span><a class="headerlink" href="#id44" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
  <span class="p">[[</span><span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span> <span class="c1"># Your     (x^1)</span>
    <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">],</span> <span class="c1"># journey  (x^2)</span>
    <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.64</span><span class="p">],</span> <span class="c1"># starts    (x^3)</span>
    <span class="p">[</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span> <span class="c1"># with      (x^4)</span>
    <span class="p">[</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="c1"># one       (x^5)</span>
    <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]]</span> <span class="c1"># step      (x^6)</span>
<span class="p">)</span>

<span class="n">attn_scores</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">@</span> <span class="n">inputs</span><span class="o">.</span><span class="n">T</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9995</span><span class="p">,</span> <span class="mf">0.9544</span><span class="p">,</span> <span class="mf">0.9422</span><span class="p">,</span> <span class="mf">0.4753</span><span class="p">,</span> <span class="mf">0.4576</span><span class="p">,</span> <span class="mf">0.6310</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.9544</span><span class="p">,</span> <span class="mf">1.4950</span><span class="p">,</span> <span class="mf">1.4754</span><span class="p">,</span> <span class="mf">0.8434</span><span class="p">,</span> <span class="mf">0.7070</span><span class="p">,</span> <span class="mf">1.0865</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.9422</span><span class="p">,</span> <span class="mf">1.4754</span><span class="p">,</span> <span class="mf">1.4570</span><span class="p">,</span> <span class="mf">0.8296</span><span class="p">,</span> <span class="mf">0.7154</span><span class="p">,</span> <span class="mf">1.0605</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4753</span><span class="p">,</span> <span class="mf">0.8434</span><span class="p">,</span> <span class="mf">0.8296</span><span class="p">,</span> <span class="mf">0.4937</span><span class="p">,</span> <span class="mf">0.3474</span><span class="p">,</span> <span class="mf">0.6565</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4576</span><span class="p">,</span> <span class="mf">0.7070</span><span class="p">,</span> <span class="mf">0.7154</span><span class="p">,</span> <span class="mf">0.3474</span><span class="p">,</span> <span class="mf">0.6654</span><span class="p">,</span> <span class="mf">0.2935</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.6310</span><span class="p">,</span> <span class="mf">1.0865</span><span class="p">,</span> <span class="mf">1.0605</span><span class="p">,</span> <span class="mf">0.6565</span><span class="p">,</span> <span class="mf">0.2935</span><span class="p">,</span> <span class="mf">0.9450</span><span class="p">]])</span>

<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2098</span><span class="p">,</span> <span class="mf">0.2006</span><span class="p">,</span> <span class="mf">0.1981</span><span class="p">,</span> <span class="mf">0.1242</span><span class="p">,</span> <span class="mf">0.1220</span><span class="p">,</span> <span class="mf">0.1452</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1385</span><span class="p">,</span> <span class="mf">0.2379</span><span class="p">,</span> <span class="mf">0.2333</span><span class="p">,</span> <span class="mf">0.1240</span><span class="p">,</span> <span class="mf">0.1082</span><span class="p">,</span> <span class="mf">0.1581</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1390</span><span class="p">,</span> <span class="mf">0.2369</span><span class="p">,</span> <span class="mf">0.2326</span><span class="p">,</span> <span class="mf">0.1242</span><span class="p">,</span> <span class="mf">0.1108</span><span class="p">,</span> <span class="mf">0.1565</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1435</span><span class="p">,</span> <span class="mf">0.2074</span><span class="p">,</span> <span class="mf">0.2046</span><span class="p">,</span> <span class="mf">0.1462</span><span class="p">,</span> <span class="mf">0.1263</span><span class="p">,</span> <span class="mf">0.1720</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1526</span><span class="p">,</span> <span class="mf">0.1958</span><span class="p">,</span> <span class="mf">0.1975</span><span class="p">,</span> <span class="mf">0.1367</span><span class="p">,</span> <span class="mf">0.1879</span><span class="p">,</span> <span class="mf">0.1295</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.1385</span><span class="p">,</span> <span class="mf">0.2184</span><span class="p">,</span> <span class="mf">0.2128</span><span class="p">,</span> <span class="mf">0.1420</span><span class="p">,</span> <span class="mf">0.0988</span><span class="p">,</span> <span class="mf">0.1896</span><span class="p">]])</span>

<span class="n">all_context_vecs</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">inputs</span>
<span class="nb">print</span><span class="p">(</span><span class="n">all_context_vecs</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4421</span><span class="p">,</span> <span class="mf">0.5931</span><span class="p">,</span> <span class="mf">0.5790</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4419</span><span class="p">,</span> <span class="mf">0.6515</span><span class="p">,</span> <span class="mf">0.5683</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4431</span><span class="p">,</span> <span class="mf">0.6496</span><span class="p">,</span> <span class="mf">0.5671</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4304</span><span class="p">,</span> <span class="mf">0.6298</span><span class="p">,</span> <span class="mf">0.5510</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4671</span><span class="p">,</span> <span class="mf">0.5910</span><span class="p">,</span> <span class="mf">0.5266</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">0.4177</span><span class="p">,</span> <span class="mf">0.6503</span><span class="p">,</span> <span class="mf">0.5645</span><span class="p">]])</span>
</pre></div>
</div>
</section>
</section>
<section id="implementing-self-attention-with-trainable-weights">
<h3>3.4 Implementing self-attention with trainable weights<a class="headerlink" href="#implementing-self-attention-with-trainable-weights" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Self-attention mechanism is also called <code class="docutils literal notranslate"><span class="pre">scaled</span> <span class="pre">dot-product</span> <span class="pre">attention</span></code>.</p></li>
</ul>
<figure class="align-default" id="id45">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/PXVtSP.png" src="https://img.zhaoweiguo.com/uPic/2024/11/PXVtSP.png" />
<figcaption>
<p><span class="caption-text">Figure 3.13 A mental model illustrating how the self-attention mechanism we code in this section fits into the broader context of this book and chapter. In the previous section, we coded a simplified attention mechanism to understand the basic mechanism behind attention mechanisms. In this section, we add trainable weights to this attention mechanism. In the upcoming sections, we will then extend this self-attention mechanism by adding a causal mask and multiple heads.</span><a class="headerlink" href="#id45" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>本节主要工作是在前面的基础上增加 <strong>训练时更新权重</strong> 的功能，以实现在训练时学习以得到更好的权重。</p></li>
</ul>
<section id="computing-the-attention-weights-step-by-step">
<h4>3.4.1 Computing the attention weights step by step<a class="headerlink" href="#computing-the-attention-weights-step-by-step" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>three trainable weight matrices <span class="math notranslate nohighlight">\(W_q\)</span> , <span class="math notranslate nohighlight">\(W_k\)</span> , and <span class="math notranslate nohighlight">\(W_v\)</span> .</p></li>
<li><p>These three matrices are used to project the embedded input tokens, <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, into query, key, and value vectors</p></li>
</ul>
<figure class="align-default" id="id46">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/5i96Wp.png" src="https://img.zhaoweiguo.com/uPic/2024/11/5i96Wp.png" />
<figcaption>
<p><span class="caption-text">Figure 3.14 In the first step of the self-attention mechanism with trainable weight matrices, we compute query (q), key (k), and value (v) vectors for input elements x. Similar to previous sections, we designate the second input, <span class="math notranslate nohighlight">\(x^{(2)}\)</span>, as the query input. The query vector <span class="math notranslate nohighlight">\(q^{(2)}\)</span> is obtained via matrix multiplication between the input <span class="math notranslate nohighlight">\(x^{(2)}\)</span> and the weight matrix Wq. Similarly, we obtain the key and value vectors(即: <span class="math notranslate nohighlight">\(k^{(T)}\)</span> and <span class="math notranslate nohighlight">\(v^{(T)}\)</span> ) via matrix multiplication involving the weight matrices Wk and Wv.</span><a class="headerlink" href="#id46" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Note that in GPT-like models, the input and output dimensions are usually the same, but for illustration purposes, to better follow the computation, we choose different input (d_in=3) and output (d_out=2) dimensions here.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
  <span class="p">[[</span><span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span> <span class="c1"># Your     (x^1)</span>
    <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">],</span> <span class="c1"># journey  (x^2)</span>
    <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.64</span><span class="p">],</span> <span class="c1"># starts    (x^3)</span>
    <span class="p">[</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span> <span class="c1"># with      (x^4)</span>
    <span class="p">[</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="c1"># one       (x^5)</span>
    <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]]</span> <span class="c1"># step      (x^6)</span>
<span class="p">)</span>

<span class="n">x_2</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#A</span>
<span class="n">d_in</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#B  = 3</span>
<span class="n">d_out</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1">#C</span>

<span class="c1"># Initialize the three weight matrices Wq, Wk, and Wv</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">W_query</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_key</span>   <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">W_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># 示例结构: shape=(3,2)</span>
<span class="c1"># tensor([[0.3821, 0.6605],</span>
<span class="c1">#         [0.8536, 0.5932],</span>
<span class="c1">#         [0.6367, 0.9826]])</span>
<span class="c1"># 说明</span>
<span class="c1"># setting requires_grad=False to reduce clutter in the outputs for illustration purposes</span>
<span class="c1"># 正式使用时需要设置 requires_grad=True</span>

<span class="c1"># compute the query, key, and value vectors</span>
<span class="n">query_2</span> <span class="o">=</span> <span class="n">x_2</span> <span class="o">@</span> <span class="n">W_query</span>
<span class="n">key_2</span> <span class="o">=</span> <span class="n">x_2</span> <span class="o">@</span> <span class="n">W_key</span>
<span class="n">value_2</span> <span class="o">=</span> <span class="n">x_2</span> <span class="o">@</span> <span class="n">W_value</span>
<span class="nb">print</span><span class="p">(</span><span class="n">query_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([0.4306, 1.4551])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>【注意】不要将权重参数(weight parameters)与注意力权重(attention weights)混淆。</p></li>
<li><p>权重参数(weight parameters): 指的是神经网络在训练过程中被调优的权重，有时会缩写为weight。(“weight” is short for “weight parameters,” the values of a neural network that are optimized during training. )</p></li>
<li><p>注意力权重(attention weights): 决定了上下文向量依赖输入不同部分的程度，即网络关注输入不同部分的程度。(attention weights determine the extent to which a context vector depends on the different parts of the input, i.e., to what extent the network focuses on different parts of the input.)</p></li>
<li><p>总之，权重参数是定义网络连接的基本学习系数，而注意力权重是动态的、特定于上下文的值。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 虽然这儿只计算context vector`Z^(2)`，但仍然需要所有输入元素的键和值向量</span>
<span class="c1"># obtain all keys and values via matrix multiplication</span>
<span class="n">keys</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">@</span> <span class="n">W_key</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">inputs</span> <span class="o">@</span> <span class="n">W_value</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;keys.shape:&quot;</span><span class="p">,</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># keys.shape: torch.Size([6, 2])</span>
</pre></div>
</div>
<figure class="align-default" id="id47">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/xpgGR5.png" src="https://img.zhaoweiguo.com/uPic/2024/11/xpgGR5.png" />
<figcaption>
<p><span class="caption-text">Figure 3.15 The attention score computation is a dot-product computation similar to what we have used in the simplified self-attention mechanism in section 3.3. The new aspect here is that we are not directly computing the dot-product between the input elements but using the query and key obtained by transforming the inputs via the respective weight matrices.</span><a class="headerlink" href="#id47" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>compute the <code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">score</span></code> <span class="math notranslate nohighlight">\(ω_{22}\)</span></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">keys_2</span> <span class="o">=</span> <span class="n">keys</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1">#A</span>
<span class="n">attn_score_22</span> <span class="o">=</span> <span class="n">query_2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">keys_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_score_22</span><span class="p">)</span>
<span class="c1"># tensor(1.8524)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>同样的道理计算出所有的 <span class="math notranslate nohighlight">\(ω_2\)</span> (即 <span class="math notranslate nohighlight">\(ω_{21}\)</span> 到 <span class="math notranslate nohighlight">\(ω_{2T}\)</span> )</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">attn_scores_2</span> <span class="o">=</span> <span class="n">query_2</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span>    <span class="c1"># All attention scores for given query</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_scores_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])</span>
</pre></div>
</div>
<figure class="align-default" id="id48">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/ZOhueo.png" src="https://img.zhaoweiguo.com/uPic/2024/11/ZOhueo.png" />
<figcaption>
<p><span class="caption-text">Figure 3.16 After computing the attention scores ω, the next step is to normalize these scores using the softmax function to obtain the attention weights α.</span><a class="headerlink" href="#id48" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">d_k</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">attn_weights_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores_2</span> <span class="o">/</span> <span class="n">d_k</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_weights_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>通过将注意力分数除以嵌入的平方根(square root, 即d_k**0.5)来缩放注意力分数键的维度</p></li>
<li><p>when scaling up the embedding dimension, which is typically greater than thousand(比较gpt4等), large dot products can result in very small gradients during backpropagation due to the softmax function applied to them.</p></li>
<li><p>As dot products increase, the softmax function behaves more like a step function, resulting in gradients nearing zero.</p></li>
<li><p>These small gradients can drastically slow down learning or cause training to stagnate.</p></li>
<li><p>The scaling by the <code class="docutils literal notranslate"><span class="pre">square</span> <span class="pre">root</span></code> of the embedding dimension is the reason why this self-attention mechanism is also called <code class="docutils literal notranslate"><span class="pre">scaled-dot</span> <span class="pre">product</span> <span class="pre">attention</span></code> .</p></li>
<li><p>【关键点】为什么在自注意力机制中会用嵌入维度的平方根来缩放点积</p></li>
<li><p>缩放的目的：用嵌入维度的平方根来缩放，是为了避免训练过程中出现过小的梯度。若不做缩放，训练时可能会遇到梯度非常小的情况，导致模型学习变慢，甚至陷入停滞。</p></li>
<li><p>出现梯度变小的原因：1、当嵌入维度（即向量的维度）增加时，两个向量的点积值会变大。在GPT等大型语言模型（LLM）中，嵌入维度往往很高，可能达到上千，因此点积也变得很大。2、在点积结果上应用 softmax 函数时，如果数值较大，softmax 输出的概率分布会变得很尖锐，近似于阶跃函数。此时，大部分概率集中在几个值上，导致其他部分的梯度几乎为零。这样就会导致模型训练时更新不充分。</p></li>
<li><p>缩放的效果：通过用嵌入维度的平方根缩放点积的大小，可以让点积的数值控制在合理范围，使得 softmax 函数的输出更加平滑，从而使得梯度较大，模型可以更有效地学习。这种缩放的自注意力机制因此被称为“缩放点积注意力” (scaled-dot product attention)。</p></li>
</ul>
<figure class="align-default" id="id49">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/hTqYgH.png" src="https://img.zhaoweiguo.com/uPic/2024/11/hTqYgH.png" />
<figcaption>
<p><span class="caption-text">Figure 3.17 In the final step of the self-attention computation, we compute the <code class="docutils literal notranslate"><span class="pre">context</span> <span class="pre">vector</span></code> by combining all <code class="docutils literal notranslate"><span class="pre">value</span> <span class="pre">vectors</span></code> via the <code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">weights</span></code>.</span><a class="headerlink" href="#id49" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context_vec_2</span> <span class="o">=</span> <span class="n">attn_weights_2</span> <span class="o">@</span> <span class="n">values</span>
<span class="nb">print</span><span class="p">(</span><span class="n">context_vec_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([0.3061, 0.8210])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>【为啥需要query, key, and value】这3个是借用信息检索和数据库领域的概念。接合起来理解这3个概念。</p>
</div>
<ul class="simple">
<li><p>query（查询）：”query” 代表模型当前关注的项(如一个句子中的单词或词元)。模型通过 “query” 来探测其他部分，判断需要关注的程度。</p></li>
<li><p>key（键）：输入序列中的每个项（如句子中的每个词）都有一个对应的 “key”，这些 “key” 会和 “query” 进行匹配。通过这种匹配，模型可以找出哪些 “key”（即哪些输入部分）和 “query” 更相关。</p></li>
<li><p>value（值）：一旦模型确定了与 “query” 最相关的 “key”，就会取出相应的 “value”。这些 “value” 包含输入项的实际内容或表示，通过提取这些 “value”，模型获得当前 “query” 应关注的具体内容。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>上节讲的是根据 <code class="docutils literal notranslate"><span class="pre">关系相近的向量点积更大</span></code> 与计算相似度的关系，算出每个单词的权重，然后用这个计算出的权重和输入向量点击，得到与位置相关的上下文向量。而本节是用3个可训练参数(Wq, Wk, Wv)，由Wq和Wk来计算出每个单词的权重，而Wv提供了该位置上的特征信息，模型通过注意力机制自动调整从不同位置的 value 中提取多少信息。</p>
</div>
<ul class="simple">
<li><p>注：为啥 Wv 不能直接用 input来表示，因为 Wv 直接决定了提取的内容和最终输出的特征信息。它能够捕捉更复杂的特征，尤其是在上下文依赖和长序列建模中，可以更好地识别出不同位置特征的细微差别。直接使用 input，模型会缺乏这种内容和注意力分布的区分，使得不同位置的特征难以有效加权合成，从而降低注意力机制的表达能力。</p></li>
</ul>
</section>
<section id="implementing-a-compact-self-attention-python-class">
<h4>3.4.2 Implementing a compact self-attention Python class<a class="headerlink" href="#implementing-a-compact-self-attention-python-class" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Listing 3.1 A compact self-attention class</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention_v1</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">))</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span>

        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span> <span class="c1"># omega</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span>
        <span class="k">return</span> <span class="n">context_vec</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">sa_v1</span> <span class="o">=</span> <span class="n">SelfAttention_v1</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sa_v1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2996</span><span class="p">,</span> <span class="mf">0.8053</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3061</span><span class="p">,</span> <span class="mf">0.8210</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3058</span><span class="p">,</span> <span class="mf">0.8203</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2948</span><span class="p">,</span> <span class="mf">0.7939</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2927</span><span class="p">,</span> <span class="mf">0.7891</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2990</span><span class="p">,</span> <span class="mf">0.8040</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id50">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/0d15vS.png" src="https://img.zhaoweiguo.com/uPic/2024/11/0d15vS.png" />
<figcaption>
<p><span class="caption-text">Figure 3.18 In self-attention, we transform the input vectors in the input matrix X with the three weight matrices, Wq, Wk, and Wv. Then, we compute the attention weight matrix based on the resulting queries (Q) and keys (K). Using the attention weights and values (V), we then compute the context vectors (Z). (For visual clarity, we focus on a single input text with n tokens in this figure, not a batch of multiple inputs. Consequently, the 3D input tensor is simplified to a 2D matrix in this context. This approach allows for a more straightforward visualization and understanding of the processes involved.)</span><a class="headerlink" href="#id50" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> instead of manually implementing <code class="docutils literal notranslate"><span class="pre">nn.Parameter(torch.rand(...))</span></code> is that <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> has an optimized weight initialization scheme, contributing to more stable and effective model training.</p></li>
<li><p>Listing 3.2 A self-attention class using PyTorch’s Linear layers</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SelfAttention_v2</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">T</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>

        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span>
        <span class="k">return</span> <span class="n">context_vec</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">789</span><span class="p">)</span>
<span class="n">sa_v2</span> <span class="o">=</span> <span class="n">SelfAttention_v2</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sa_v2</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.0739</span><span class="p">,</span>  <span class="mf">0.0713</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0748</span><span class="p">,</span>  <span class="mf">0.0703</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0749</span><span class="p">,</span>  <span class="mf">0.0702</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0760</span><span class="p">,</span>  <span class="mf">0.0685</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0763</span><span class="p">,</span>  <span class="mf">0.0679</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">0.0754</span><span class="p">,</span>  <span class="mf">0.0693</span><span class="p">]],</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MmBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note that SelfAttention_v1 and SelfAttention_v2 give different outputs because they use different initial weights for the weight matrices since nn.Linear uses a more sophisticated weight initialization scheme.</p></li>
</ul>
</section>
</section>
<section id="hiding-future-words-with-causal-attention">
<h3>3.5 Hiding future words with causal attention<a class="headerlink" href="#hiding-future-words-with-causal-attention" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Causal</span> <span class="pre">attention</span></code>, also known as <code class="docutils literal notranslate"><span class="pre">masked</span> <span class="pre">attention</span></code>, is a specialized form of self-attention. It restricts a model to only consider previous and current inputs in a sequence when processing any given token. This is in contrast to the standard self-attention mechanism, which allows access to the entire input sequence at once.</p></li>
</ul>
<figure class="align-default" id="id51">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/blTF9A.png" src="https://img.zhaoweiguo.com/uPic/2024/11/blTF9A.png" />
<figcaption>
<p><span class="caption-text">Figure 3.19 In causal attention, we mask out the attention weights above the diagonal such that for a given input, the LLM can’t access future tokens when computing the context vectors using the attention weights. For example, for the word “journey” in the second row, we only keep the attention weights for the words before (“Your”) and in the current position (“journey”).</span><a class="headerlink" href="#id51" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="applying-a-causal-attention-mask">
<h4>3.5.1 Applying a causal attention mask<a class="headerlink" href="#applying-a-causal-attention-mask" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id52">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/e795Ol.png" src="https://img.zhaoweiguo.com/uPic/2024/11/e795Ol.png" />
<figcaption>
<p><span class="caption-text">Figure 3.20 One way to obtain the masked attention weight matrix in causal attention is to apply the softmax function to the attention scores, zeroing out the elements above the diagonal and normalizing the resulting matrix.</span><a class="headerlink" href="#id52" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>【第一步】使用上面的方法得到 attention weights</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1921</span><span class="p">,</span> <span class="mf">0.1646</span><span class="p">,</span> <span class="mf">0.1652</span><span class="p">,</span> <span class="mf">0.1550</span><span class="p">,</span> <span class="mf">0.1721</span><span class="p">,</span> <span class="mf">0.1510</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2041</span><span class="p">,</span> <span class="mf">0.1659</span><span class="p">,</span> <span class="mf">0.1662</span><span class="p">,</span> <span class="mf">0.1496</span><span class="p">,</span> <span class="mf">0.1665</span><span class="p">,</span> <span class="mf">0.1477</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2036</span><span class="p">,</span> <span class="mf">0.1659</span><span class="p">,</span> <span class="mf">0.1662</span><span class="p">,</span> <span class="mf">0.1498</span><span class="p">,</span> <span class="mf">0.1664</span><span class="p">,</span> <span class="mf">0.1480</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1869</span><span class="p">,</span> <span class="mf">0.1667</span><span class="p">,</span> <span class="mf">0.1668</span><span class="p">,</span> <span class="mf">0.1571</span><span class="p">,</span> <span class="mf">0.1661</span><span class="p">,</span> <span class="mf">0.1564</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1830</span><span class="p">,</span> <span class="mf">0.1669</span><span class="p">,</span> <span class="mf">0.1670</span><span class="p">,</span> <span class="mf">0.1588</span><span class="p">,</span> <span class="mf">0.1658</span><span class="p">,</span> <span class="mf">0.1585</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1935</span><span class="p">,</span> <span class="mf">0.1663</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1542</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1529</span><span class="p">]],</span>
<span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SoftmaxBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>【第二步】对象线上面的设置为0</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">context_length</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">mask_simple</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mask_simple</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>

<span class="c1"># multiply this mask with the attention weights</span>
<span class="n">masked_simple</span> <span class="o">=</span> <span class="n">attn_weights</span><span class="o">*</span><span class="n">mask_simple</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masked_simple</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1921</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2041</span><span class="p">,</span> <span class="mf">0.1659</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2036</span><span class="p">,</span> <span class="mf">0.1659</span><span class="p">,</span> <span class="mf">0.1662</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1869</span><span class="p">,</span> <span class="mf">0.1667</span><span class="p">,</span> <span class="mf">0.1668</span><span class="p">,</span> <span class="mf">0.1571</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1830</span><span class="p">,</span> <span class="mf">0.1669</span><span class="p">,</span> <span class="mf">0.1670</span><span class="p">,</span> <span class="mf">0.1588</span><span class="p">,</span> <span class="mf">0.1658</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1935</span><span class="p">,</span> <span class="mf">0.1663</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1542</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1529</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>【第三步】重新规范化(renormalize) attention weights使每一行值的和为1</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">row_sums</span> <span class="o">=</span> <span class="n">masked_simple</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">masked_simple_norm</span> <span class="o">=</span> <span class="n">masked_simple</span> <span class="o">/</span> <span class="n">row_sums</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masked_simple_norm</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.5517</span><span class="p">,</span> <span class="mf">0.4483</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3800</span><span class="p">,</span> <span class="mf">0.3097</span><span class="p">,</span> <span class="mf">0.3103</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2758</span><span class="p">,</span> <span class="mf">0.2460</span><span class="p">,</span> <span class="mf">0.2462</span><span class="p">,</span> <span class="mf">0.2319</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2175</span><span class="p">,</span> <span class="mf">0.1983</span><span class="p">,</span> <span class="mf">0.1984</span><span class="p">,</span> <span class="mf">0.1888</span><span class="p">,</span> <span class="mf">0.1971</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1935</span><span class="p">,</span> <span class="mf">0.1663</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1542</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1529</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">DivBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>【注意-信息泄露】当我们应用掩码然后重新规范化注意力权重时，最初来自未来标记（现在已经掩码）的信息仍然可能影响当前标记，因为它们的值是 softmax 计算的一部分。然而，关键的洞察是，当我们在屏蔽后重新规范化注意力权重时，我们本质上所做的是在较小的子集上重新计算 softmax（因为屏蔽位置对 softmax 值没有贡献）。Softmax 的数学优雅之处在于，尽管最初在分母中包含了所有位置，但在屏蔽和重新归一化之后，屏蔽位置的影响被抵消了——它们不会以任何有意义的方式对 SoftMax 得分做出贡献。所以不会有信息泄露。</p></li>
</ul>
<figure class="align-default" id="id53">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/lqdqs2.png" src="https://img.zhaoweiguo.com/uPic/2024/11/lqdqs2.png" />
<figcaption>
<p><span class="caption-text">Figure 3.21 A more efficient way to obtain the masked attention weight matrix in causal attention is to mask the attention scores with negative infinity values before applying the softmax function.</span><a class="headerlink" href="#id53" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>这儿可以利用 softmax 函数的数学特性，以更少的步骤更有效地实现屏蔽注意力权重的计算</p></li>
<li><p>【关键特征】当负无穷大值 (-∞) 连续存在时，softmax 函数将它们视为零概率（从数学上讲，这是因为 <span class="math notranslate nohighlight">\(e^{-\infty}\)</span> 接近 0）。</p></li>
<li><p>这儿把上面的0改为 <code class="docutils literal notranslate"><span class="pre">-∞(即-inf)</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">masked</span> <span class="o">=</span> <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">(),</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">masked</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2899</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4656</span><span class="p">,</span> <span class="mf">0.1723</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4594</span><span class="p">,</span> <span class="mf">0.1703</span><span class="p">,</span> <span class="mf">0.1731</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.2642</span><span class="p">,</span> <span class="mf">0.1024</span><span class="p">,</span> <span class="mf">0.1036</span><span class="p">,</span> <span class="mf">0.0186</span><span class="p">,</span>   <span class="o">-</span><span class="n">inf</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.2183</span><span class="p">,</span> <span class="mf">0.0874</span><span class="p">,</span> <span class="mf">0.0882</span><span class="p">,</span> <span class="mf">0.0177</span><span class="p">,</span> <span class="mf">0.0786</span><span class="p">,</span>
        <span class="p">[</span><span class="mf">0.3408</span><span class="p">,</span> <span class="mf">0.1270</span><span class="p">,</span> <span class="mf">0.1290</span><span class="p">,</span> <span class="mf">0.0198</span><span class="p">,</span> <span class="mf">0.1290</span><span class="p">,</span> <span class="mf">0.0078</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MaskedFillBackward0</span><span class="o">&gt;</span><span class="p">)</span>


<span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">masked</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>
<span class="c1"># 输出(已经规范化，不用再额外操作了，节省了操作)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.5517</span><span class="p">,</span> <span class="mf">0.4483</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3800</span><span class="p">,</span> <span class="mf">0.3097</span><span class="p">,</span> <span class="mf">0.3103</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2758</span><span class="p">,</span> <span class="mf">0.2460</span><span class="p">,</span> <span class="mf">0.2462</span><span class="p">,</span> <span class="mf">0.2319</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.2175</span><span class="p">,</span> <span class="mf">0.1983</span><span class="p">,</span> <span class="mf">0.1984</span><span class="p">,</span> <span class="mf">0.1888</span><span class="p">,</span> <span class="mf">0.1971</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.1935</span><span class="p">,</span> <span class="mf">0.1663</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1542</span><span class="p">,</span> <span class="mf">0.1666</span><span class="p">,</span> <span class="mf">0.1529</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">SoftmaxBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="masking-additional-attention-weights-with-dropout">
<h4>3.5.2 Masking additional attention weights with dropout<a class="headerlink" href="#masking-additional-attention-weights-with-dropout" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Dropout in deep learning is a technique where randomly selected hidden layer units are ignored during training, effectively “dropping” them out. This method helps prevent overfitting by ensuring that a model does not become overly reliant on any specific set of hidden layer units. It’s important to emphasize that dropout is only used during training and is disabled afterward.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>In the transformer architecture, including models like GPT, dropout in the attention mechanism is typically applied in two specific areas: <code class="docutils literal notranslate"><span class="pre">after</span> <span class="pre">calculating</span> <span class="pre">the</span> <span class="pre">attention</span> <span class="pre">scores</span></code> or <code class="docutils literal notranslate"><span class="pre">after</span> <span class="pre">applying</span> <span class="pre">the</span> <span class="pre">attention</span> <span class="pre">weights</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">value</span> <span class="pre">vectors</span></code>.(我还没完全理解具体的位置)</p>
</div>
<ul>
<li><p>我的理解(GPT给的说法有矛盾，下面是我的理解)</p></li>
<li><p>【Dropout 的应用位置】1、after calculating the attention scores：在计算注意力分数后应用Dropout意味着在得到Q（查询）、K（键）和V（值）向量之间的点积之后，但在将这些分数传递给Softmax函数之前，进行Dropout操作。这一步骤中的Dropout可以帮助模型避免对某些特定特征的过度依赖，因为它可能会随机地使一些注意力分数失效，从而促使模型学习到更加健壮的注意力分布。</p></li>
<li><p>伪代码:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">function</span> <span class="n">scaledDotProductAttention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dropoutRate</span><span class="p">):</span>
    <span class="c1"># 计算注意力分数</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="n">K</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># d_k 是键向量的维度</span>

    <span class="c1"># 应用Dropout</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">applyDropout</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dropoutRate</span><span class="p">)</span>

    <span class="c1"># 应用Softmax函数</span>
    <span class="n">attentionWeights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="c1"># 将注意力权重应用到值向量上</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">attentionWeights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</li>
<li><p>【Dropout 的应用位置】2、after applying the attention weights to the value vectors：在将注意力权重应用到值向量之后应用Dropout意味着在Softmax后的注意力权重与值向量相乘得到加权值向量之后，再执行Dropout操作。这样做可以进一步帮助模型泛化，因为即使某些信息被Dropout随机丢弃了，模型仍然需要能够利用剩余的信息来做出准确的预测。</p></li>
<li><p>伪代码:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">function</span> <span class="n">scaledDotProductAttention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">dropoutRate</span><span class="p">):</span>
    <span class="c1"># 计算注意力分数</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">transpose</span><span class="p">(</span><span class="n">K</span><span class="p">))</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>  <span class="c1"># d_k 是键向量的维度</span>

    <span class="c1"># 应用Softmax函数</span>
    <span class="n">attentionWeights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="c1"># 应用Dropout</span>
    <span class="n">attentionWeights</span> <span class="o">=</span> <span class="n">applyDropout</span><span class="p">(</span><span class="n">attentionWeights</span><span class="p">,</span> <span class="n">dropoutRate</span><span class="p">)</span>

    <span class="c1"># 将注意力权重应用到值向量上</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">attentionWeights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>下面示例演示了 “apply the dropout mask after computing the attention weights”</p>
</div>
<figure class="align-default" id="id54">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/TVHyRq.png" src="https://img.zhaoweiguo.com/uPic/2024/11/TVHyRq.png" />
<figcaption>
<p><span class="caption-text">Figure 3.22 Using the causal attention mask (upper left), we apply an additional dropout mask (upper right) to zero out additional attention weights to reduce overfitting during training.</span><a class="headerlink" href="#id54" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>在下面的示例中我们dropout 50% 的比例，在后面训练gpt 模型时，我们使用 10%-20% 的 dropout 比例。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1">#A</span>
<span class="n">example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span> <span class="c1">#B</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dropout</span><span class="p">(</span><span class="n">example</span><span class="p">))</span>
<span class="c1"># 输出(有近一半是0)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
</pre></div>
</div>
<p>apply dropout to the attention weight matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.7599</span><span class="p">,</span> <span class="mf">0.6194</span><span class="p">,</span> <span class="mf">0.6206</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.4921</span><span class="p">,</span> <span class="mf">0.4925</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.3966</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.3775</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0000</span><span class="p">,</span> <span class="mf">0.3327</span><span class="p">,</span> <span class="mf">0.3331</span><span class="p">,</span> <span class="mf">0.3084</span><span class="p">,</span> <span class="mf">0.3331</span><span class="p">,</span> <span class="mf">0.0000</span><span class="p">]],</span>
       <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="implementing-a-compact-causal-attention-class">
<h4>3.5.3 Implementing a compact causal attention class<a class="headerlink" href="#implementing-a-compact-causal-attention-class" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Listing 3.3 A compact causal attention class</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">CausalAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span>
                 <span class="n">dropout</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span> <span class="c1"># A</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s1">&#39;mask&#39;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">),</span>
                <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>
        <span class="p">)</span> <span class="c1">#B</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>  <span class="c1">#C</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1">#C</span>
        <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span>  <span class="c1">#D</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">],</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span>
        <span class="k">return</span> <span class="n">context_vec</span>


<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
  <span class="p">[[</span><span class="mf">0.43</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">],</span> <span class="c1"># Your     (x^1)</span>
   <span class="p">[</span><span class="mf">0.55</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.66</span><span class="p">],</span> <span class="c1"># journey  (x^2)</span>
   <span class="p">[</span><span class="mf">0.57</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.64</span><span class="p">],</span> <span class="c1"># starts   (x^3)</span>
   <span class="p">[</span><span class="mf">0.22</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">,</span> <span class="mf">0.33</span><span class="p">],</span> <span class="c1"># with     (x^4)</span>
   <span class="p">[</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.10</span><span class="p">],</span> <span class="c1"># one      (x^5)</span>
   <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.55</span><span class="p">]]</span> <span class="c1"># step     (x^6)</span>
<span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">context_length</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">ca</span> <span class="o">=</span> <span class="n">CausalAttention</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="n">context_vecs</span> <span class="o">=</span> <span class="n">ca</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;context_vecs.shape:&quot;</span><span class="p">,</span> <span class="n">context_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># context_vecs.shape: torch.Size([2, 6, 2])</span>
</pre></div>
</div>
<figure class="align-default" id="id55">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/NpD3Za.png" src="https://img.zhaoweiguo.com/uPic/2024/11/NpD3Za.png" />
<figcaption>
<p><span class="caption-text">Figure 3.23 A mental model summarizing the four different attention modules we are coding in this chapter. We began with a simplified attention mechanism, added trainable weights, and then added a casual attention mask. In the remainder of this chapter, we will extend the causal attention mechanism and code multi-head attention, which is the final module we will use in the LLM implementation in the next chapter.</span><a class="headerlink" href="#id55" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="extending-single-head-attention-to-multi-head-attention">
<h3>3.6 Extending single-head attention to multi-head attention<a class="headerlink" href="#extending-single-head-attention-to-multi-head-attention" title="此标题的永久链接">¶</a></h3>
<section id="stacking-multiple-single-head-attention-layers">
<h4>3.6.1 Stacking multiple single-head attention layers<a class="headerlink" href="#stacking-multiple-single-head-attention-layers" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id56">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/FxjRwY.png" src="https://img.zhaoweiguo.com/uPic/2024/11/FxjRwY.png" />
<figcaption>
<p><span class="caption-text">Figure 3.24 The multi-head attention module in this figure depicts two single-head attention modules stacked on top of each other. So, instead of using a single matrix Wv for computing the value matrices, in a multi-head attention module with two heads, we now have two value weight matrices: <span class="math notranslate nohighlight">\(W_{v1}\)</span> and <span class="math notranslate nohighlight">\(W_{v2}\)</span>. The same applies to the other weight matrices, Wq and Wk. We obtain two sets of context vectors Z1 and Z2 that we can combine into a single context vector matrix Z.</span><a class="headerlink" href="#id56" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Listing 3.4 A wrapper class to implement multi-head attention</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttentionWrapper</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">CausalAttention</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">)]</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id57">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/cXgCA8.png" src="https://img.zhaoweiguo.com/uPic/2024/11/cXgCA8.png" />
<figcaption>
<p><span class="caption-text">Figure 3.25 Using the MultiHeadAttentionWrapper, we specified the number of attention heads (num_heads). If we set num_heads=2, as shown in this figure, we obtain a tensor with two sets of context vector matrices. In each context vector matrix, the rows represent the context vectors corresponding to the tokens, and the columns correspond to the embedding dimension specified via d_out=4. We concatenate these context vector matrices along the column dimension. Since we have 2 attention heads and an embedding dimension of 2, the final embedding dimension is 2 × 2 = 4.</span><a class="headerlink" href="#id57" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">context_length</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># This is the number of tokens</span>
<span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttentionWrapper</span><span class="p">(</span>
    <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>

<span class="n">context_vecs</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;context_vecs.shape:&quot;</span><span class="p">,</span> <span class="n">context_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1">#context_vecs.shape: torch.Size([2, 6, 4])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">context_vecs</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],</span>
<span class="c1">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span>
<span class="c1">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span>
<span class="c1">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span>
<span class="c1">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span>
<span class="c1">#          [-0.5299, -0.1081,  0.5077,  0.3493]],</span>

<span class="c1">#         [[-0.4519,  0.2216,  0.4772,  0.1063],</span>
<span class="c1">#          [-0.5874,  0.0058,  0.5891,  0.3257],</span>
<span class="c1">#          [-0.6300, -0.0632,  0.6202,  0.3860],</span>
<span class="c1">#          [-0.5675, -0.0843,  0.5478,  0.3589],</span>
<span class="c1">#          [-0.5526, -0.0981,  0.5321,  0.3428],</span>
<span class="c1">#          [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=&lt;CatBackward0&gt;)</span>

<span class="c1"># 说明</span>
<span class="c1"># first  dimension: context_vecs tensor is 2 since we have two input texts</span>
<span class="c1">#       (两个值完全一样是因为输入文本batch, 是完全一样的)</span>
<span class="c1"># second dimension: 6 tokens in each input</span>
<span class="c1"># third  dimension: 4-dimensional embedding of each token</span>
</pre></div>
</div>
</section>
<section id="implementing-multi-head-attention-with-weight-splits">
<h4>3.6.2 Implementing multi-head attention with weight splits<a class="headerlink" href="#implementing-multi-head-attention-with-weight-splits" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">d_out</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">),</span> \
            <span class="s2">&quot;d_out must be divisible by num_heads&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># Reduce the projection dim to match desired output dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>  <span class="c1"># Linear layer to combine head outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;mask&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">),</span>
                       <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>    <span class="c1"># Shape: (b, num_tokens, d_out) =&gt; [2, 6, 2]</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># We implicitly split the matrix by adding a `num_heads` dimension</span>
        <span class="c1"># Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>        <span class="o">=&gt;</span><span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">num_headers</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span>  <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>             <span class="o">=&gt;</span> <span class="p">[</span><span class="n">b</span><span class="p">,</span> <span class="n">num_headers</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span>
        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Dot product for each head   =&gt; [2, 2, 6, 1] @ [2, 2, 1, 6] =&gt; [2, 2, 6, 6] =&gt; [b, num_headers, num_tokens, num_tokens]</span>

        <span class="c1"># Original mask truncated to the number of tokens and converted to boolean</span>
        <span class="n">mask_bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span>

        <span class="c1"># Use the mask to fill attention scores</span>
        <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask_bool</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Shape: (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>       <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span> <span class="o">@</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">context_vec</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span><span class="p">)</span>   <span class="o">=&gt;</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_vec</span><span class="p">)</span> <span class="c1"># optional projection</span>

        <span class="k">return</span> <span class="n">context_vec</span>
</pre></div>
</div>
<figure class="align-default" id="id58">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/nUNeSc.png" src="https://img.zhaoweiguo.com/uPic/2024/11/nUNeSc.png" />
<figcaption>
<p><span class="caption-text">Figure 3.26 In the <code class="docutils literal notranslate"><span class="pre">MultiheadAttentionWrapper</span></code> class with two attention heads, we initialized two weight matrices <span class="math notranslate nohighlight">\(W_{q1}\)</span> and <span class="math notranslate nohighlight">\(W_{q2}\)</span> and computed two query matrices Q1 and Q2 as illustrated at the top of this figure. In the <code class="docutils literal notranslate"><span class="pre">MultiheadAttention</span></code> class, we initialize one larger weight matrix <span class="math notranslate nohighlight">\(W_q\)</span> , only perform one matrix multiplication with the inputs to obtain a query matrix Q, and then split the query matrix into Q1 and Q2 as shown at the bottom of this figure. We do the same for the keys and values, which are not shown to reduce visual clutter.</span><a class="headerlink" href="#id58" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>使用示例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">batch_size</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">shape</span>
<span class="n">d_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">context_vecs</span> <span class="o">=</span> <span class="n">mha</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;context_vecs.shape:&quot;</span><span class="p">,</span> <span class="n">context_vecs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># context_vecs.shape: torch.Size([2, 6, 2])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">context_vecs</span><span class="p">)</span>
<span class="c1"># tensor([[[0.3190, 0.4858],</span>
<span class="c1">#          [0.2943, 0.3897],</span>
<span class="c1">#          [0.2856, 0.3593],</span>
<span class="c1">#          [0.2693, 0.3873],</span>
<span class="c1">#          [0.2639, 0.3928],</span>
<span class="c1">#          [0.2575, 0.4028]],</span>
<span class="c1">#         [[0.3190, 0.4858],</span>
<span class="c1">#          [0.2943, 0.3897],</span>
<span class="c1">#          [0.2856, 0.3593],</span>
<span class="c1">#          [0.2693, 0.3873],</span>
<span class="c1">#          [0.2639, 0.3928],</span>
<span class="c1">#          [0.2575, 0.4028]]], grad_fn=&lt;ViewBackward0&gt;)</span>
</pre></div>
</div>
</section>
</section>
<section id="id2">
<h3>3.7 Summary<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Attention mechanisms transform input elements into enhanced context vector representations that incorporate(包含,使合并) information about all inputs.</p></li>
<li><p>A self-attention mechanism computes the context vector representation as a weighted sum over the inputs.</p></li>
<li><p>In a simplified attention mechanism, the attention weights are computed via dot products.</p></li>
<li><p>A dot product is just a concise way of multiplying two vectors element-wise and then summing the products.</p></li>
<li><p>Matrix multiplications, while not strictly required, help us to implement computations more efficiently and compactly by replacing nested for-loops.</p></li>
<li><p>In self-attention mechanisms that are used in LLMs, also called scaled-dot product attention, we include trainable weight matrices to compute intermediate transformations of the inputs: queries, values, and keys. When working with LLMs that read and generate text from left to right, we add a causal attention mask to prevent the LLM from accessing future tokens.</p></li>
<li><p>Next to causal attention masks to zero out attention weights, we can also add a dropout mask to reduce overfitting in LLMs.</p></li>
<li><p>The attention modules in transformer-based LLMs involve multiple instances of causal attention, which is called multi-head attention.</p></li>
<li><p>We can create a multi-head attention module by stacking(堆叠) multiple instances of causal attention modules.</p></li>
<li><p>A more efficient way of creating multi-head attention modules involves batched matrix multiplications.</p></li>
</ul>
</section>
</section>
<section id="implementing-a-gpt-model-from-scratch-to-generate-text">
<h2>4 Implementing a GPT model from Scratch To Generate Text<a class="headerlink" href="#implementing-a-gpt-model-from-scratch-to-generate-text" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>Coding a GPT-like large language model (LLM) that can be trained to generate human-like text</p></li>
<li><p>Normalizing layer activations to stabilize neural network training</p></li>
<li><p>Adding shortcut connections in deep neural networks to train models more effectively</p></li>
<li><p>Implementing transformer blocks to create GPT models of various sizes</p></li>
<li><p>Computing the number of parameters and storage requirements of GPT models</p></li>
</ul>
<figure class="align-default" id="id59">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/EHDcf5.png" src="https://img.zhaoweiguo.com/uPic/2024/11/EHDcf5.png" />
<figcaption>
<p><span class="caption-text">Figure 4.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a general text dataset, and finetuning it on a labeled dataset. This chapter focuses on implementing the LLM architecture, which we will train in the next chapter.</span><a class="headerlink" href="#id59" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="coding-an-llm-architecture">
<h3>4.1 Coding an LLM architecture<a class="headerlink" href="#coding-an-llm-architecture" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>In previous chapters, we used small embedding dimensions for token inputs and outputs for ease of illustration, ensuring they fit on a single page</p></li>
<li><p>In this chapter, we consider embedding and model sizes akin to a small GPT-2 model</p></li>
<li><p>We’ll specifically code the architecture of the smallest GPT-2 model (124 million parameters)</p></li>
</ul>
<figure class="align-default" id="id60">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/UuEOhL.png" src="https://img.zhaoweiguo.com/uPic/2024/11/UuEOhL.png" />
<figcaption>
<p><span class="caption-text">Figure 4.2 A mental model of a GPT model. Next to the embedding layers, it consists of one or more transformer blocks containing the masked multi-head attention module we implemented in the previous chapter.</span><a class="headerlink" href="#id60" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>configuration of the small GPT-2 model:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>        <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>     <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>             <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>              <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>             <span class="c1"># Number of layers</span>
    <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>           <span class="c1"># Dropout rate</span>
    <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>           <span class="c1"># Query-Key-Value bias</span>
<span class="p">}</span>
</pre></div>
</div>
<figure class="align-default" id="id61">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/xEM2A7.png" src="https://img.zhaoweiguo.com/uPic/2024/11/xEM2A7.png" />
<figcaption>
<p><span class="caption-text">Figure 4.3 A mental model outlining the order in which we code the GPT architecture. In this chapter, we will start with the GPT backbone, a placeholder architecture, before we get to the individual core pieces and eventually assemble them in a transformer block for the final GPT architecture.</span><a class="headerlink" href="#id61" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DummyGPTModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

        <span class="c1"># Use a placeholder for TransformerBlock</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">DummyTransformerBlock</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>

        <span class="c1"># Use a placeholder for LayerNorm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">DummyLayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">in_idx</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">tok_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">pos_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tok_embeds</span> <span class="o">+</span> <span class="n">pos_embeds</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DummyTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># A simple placeholder</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># This block does nothing and just returns its input.</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DummyLayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># The parameters here are just to mimic the LayerNorm interface.</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># This layer does nothing and just returns its input.</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<figure class="align-default" id="id62">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/phEjtL.png" src="https://img.zhaoweiguo.com/uPic/2024/11/phEjtL.png" />
<figcaption>
<p><span class="caption-text">Figure 4.4 A big-picture overview showing how the input data is tokenized, embedded, and fed to the GPT model. Note that in our DummyGPTClass coded earlier, the token embedding is handled inside the GPT model. In LLMs, the embedded input token dimension typically matches the output dimension. The output embeddings here represent the context vectors we discussed in chapter 3.</span><a class="headerlink" href="#id62" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>
<span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">txt1</span> <span class="o">=</span> <span class="s2">&quot;Every effort moves you&quot;</span>
<span class="n">txt2</span> <span class="o">=</span> <span class="s2">&quot;Every day holds a&quot;</span>
<span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">txt1</span><span class="p">)))</span>
<span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">txt2</span><span class="p">)))</span>
<span class="n">batch</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="c1"># tensor([[ 6109,  3626,  6100,   345], #A</span>
<span class="c1">#         [ 6109,  1110,  6622,   257]])</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DummyGPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output shape:&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="c1"># Output shape: torch.Size([2, 4, 50257])</span>
<span class="c1"># tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],</span>
<span class="c1">#          [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],</span>
<span class="c1">#          [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],</span>
<span class="c1">#          [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],</span>
<span class="c1">#         [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],</span>
<span class="c1">#          [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],</span>
<span class="c1">#          [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],</span>
<span class="c1">#          [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],</span>
<span class="c1">#        grad_fn=&lt;UnsafeViewBackward0&gt;)</span>
</pre></div>
</div>
</section>
<section id="normalizing-activations-with-layer-normalization">
<h3>4.2 Normalizing activations with layer normalization<a class="headerlink" href="#normalizing-activations-with-layer-normalization" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Training deep neural networks with many layers can sometimes prove challenging due to issues like vanishing or exploding gradients.</p></li>
<li><p>These issues lead to unstable training dynamics and make it difficult for the network to effectively adjust its weights, which means the learning process struggles to find a set of parameters (weights) for the neural network that minimizes the loss function.</p></li>
<li><p>In other words, the network has difficulty learning the underlying patterns in the data to a degree that would allow it to make accurate predictions or decisions.</p></li>
<li><p>The main idea behind layer normalization is to adjust the <code class="docutils literal notranslate"><span class="pre">activations</span> <span class="pre">(outputs)</span></code> of a neural network layer to have a mean of 0 and a variance of 1, also known as <code class="docutils literal notranslate"><span class="pre">unit</span> <span class="pre">variance</span></code>.</p></li>
</ul>
<figure class="align-default" id="id63">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/7J1nUv.png" src="https://img.zhaoweiguo.com/uPic/2024/11/7J1nUv.png" />
<figcaption>
<p><span class="caption-text">Figure 4.5 An illustration of layer normalization where the 5 layer outputs, also called activations, are normalized such that they have a zero mean and variance of 1.</span><a class="headerlink" href="#id63" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id64">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/uQxKaT.png" src="https://img.zhaoweiguo.com/uPic/2024/11/uQxKaT.png" />
<figcaption>
<p><span class="caption-text">Figure 4.6 An illustration of the dim parameter when calculating the mean of a tensor. For instance,if we have a 2D tensor(matrix) with dimensions[rows, columns], using dim=0 will perform the operation across rows (vertically, as shown at the bottom), resulting in an output that aggregates the data for each column. Using dim=1 or dim=-1 will perform the operation across columns (horizontally, as shown at the top), resulting in an output aggregating the data for each row.</span><a class="headerlink" href="#id64" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 归一化L2(欧几里德范数归一)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">norm_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">norm_x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift</span>
</pre></div>
</div>
<figure class="align-default" id="id65">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/jeZEJm.png" src="https://img.zhaoweiguo.com/uPic/2024/11/jeZEJm.png" />
<figcaption>
<p><span class="caption-text">Figure 4.7 A mental model listing the different building blocks we implement in this chapter to assemble the GPT architecture.</span><a class="headerlink" href="#id65" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="implementing-a-feed-forward-network-with-gelu-activations">
<h3>4.3 Implementing a feed forward network with GELU activations<a class="headerlink" href="#implementing-a-feed-forward-network-with-gelu-activations" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>主要讲了GELU的实现和特点</p></li>
</ul>
<figure class="align-default" id="id66">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/G5BwCr.png" src="https://img.zhaoweiguo.com/uPic/2024/11/G5BwCr.png" />
<figcaption>
<p><span class="caption-text">Figure 4.11 A mental model showing the topics we cover in this chapter, with the black checkmarks indicating those that we have already covered.</span><a class="headerlink" href="#id66" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="adding-shortcut-connections">
<h3>4.4 Adding shortcut connections<a class="headerlink" href="#adding-shortcut-connections" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">shortcut</span> <span class="pre">connections</span></code>, also known as <code class="docutils literal notranslate"><span class="pre">skip</span> <span class="pre">or</span> <span class="pre">residual</span> <span class="pre">connections</span></code>.</p></li>
<li><p>Originally, <code class="docutils literal notranslate"><span class="pre">shortcut</span> <span class="pre">connections</span></code> were proposed for deep networks in computer vision (specifically, in residual networks) to mitigate the challenge of vanishing gradients.</p></li>
<li><p>The vanishing gradient problem refers to the issue where gradients (which guide weight updates during training) become progressively smaller as they propagate backward through the layers, making it difficult to effectively train earlier layers</p></li>
</ul>
<figure class="align-default" id="id67">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/bGNkqW.png" src="https://img.zhaoweiguo.com/uPic/2024/11/bGNkqW.png" />
<figcaption>
<p><span class="caption-text">Figure 4.12 A comparison between a deep neural network consisting of 5 layers without (on the left) and with shortcut connections (on the right). Shortcut connections involve adding the inputs of a layer to its outputs, effectively creating an alternate path that bypasses certain layers. The gradient illustrated in Figure 1.1 denotes the mean absolute gradient at each layer, which we will compute in the code example that follows.</span><a class="headerlink" href="#id67" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">ExampleDeepNeuralNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer_sizes</span><span class="p">,</span> <span class="n">use_shortcut</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_shortcut</span> <span class="o">=</span> <span class="n">use_shortcut</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">GELU</span><span class="p">()),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">GELU</span><span class="p">()),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">3</span><span class="p">]),</span> <span class="n">GELU</span><span class="p">()),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">4</span><span class="p">]),</span> <span class="n">GELU</span><span class="p">()),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">layer_sizes</span><span class="p">[</span><span class="mi">5</span><span class="p">]),</span> <span class="n">GELU</span><span class="p">())</span>
        <span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="c1"># Compute the output of the current layer</span>
            <span class="n">layer_output</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="c1"># Check if shortcut can be applied</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_shortcut</span> <span class="ow">and</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">layer_output</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">layer_output</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">layer_output</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">print_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># Forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.</span><span class="p">]])</span>

    <span class="c1"># Calculate loss based on how close the target</span>
    <span class="c1"># and output are</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># Backward pass to calculate the gradients</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="s1">&#39;weight&#39;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="c1"># Print the mean absolute gradient of the weights</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> has gradient mean of </span><span class="si">{</span><span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>print the gradient values <strong>without</strong> shortcut connections:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">sample_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]])</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model_without_shortcut</span> <span class="o">=</span> <span class="n">ExampleDeepNeuralNetwork</span><span class="p">(</span>
    <span class="n">layer_sizes</span><span class="p">,</span> <span class="n">use_shortcut</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
<span class="n">print_gradients</span><span class="p">(</span><span class="n">model_without_shortcut</span><span class="p">,</span> <span class="n">sample_input</span><span class="p">)</span>

<span class="c1"># 输出 ==&gt;</span>
<span class="c1"># layers.0.0.weight has gradient mean of 0.00020173587836325169</span>
<span class="c1"># layers.1.0.weight has gradient mean of 0.0001201116101583466</span>
<span class="c1"># layers.2.0.weight has gradient mean of 0.0007152041653171182</span>
<span class="c1"># layers.3.0.weight has gradient mean of 0.001398873864673078</span>
<span class="c1"># layers.4.0.weight has gradient mean of 0.005049646366387606</span>
</pre></div>
</div>
<p>print the gradient values <strong>with</strong> shortcut connections:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model_with_shortcut</span> <span class="o">=</span> <span class="n">ExampleDeepNeuralNetwork</span><span class="p">(</span>
    <span class="n">layer_sizes</span><span class="p">,</span> <span class="n">use_shortcut</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">print_gradients</span><span class="p">(</span><span class="n">model_with_shortcut</span><span class="p">,</span> <span class="n">sample_input</span><span class="p">)</span>
<span class="c1"># 输出 ==&gt;</span>
<span class="c1"># layers.0.0.weight has gradient mean of 0.22169792652130127</span>
<span class="c1"># layers.1.0.weight has gradient mean of 0.20694105327129364</span>
<span class="c1"># layers.2.0.weight has gradient mean of 0.32896995544433594</span>
<span class="c1"># layers.3.0.weight has gradient mean of 0.2665732502937317</span>
<span class="c1"># layers.4.0.weight has gradient mean of 1.3258541822433472</span>
</pre></div>
</div>
</section>
<section id="connecting-attention-and-linear-layers-in-a-transformer-block">
<h3>4.5 Connecting attention and linear layers in a transformer block<a class="headerlink" href="#connecting-attention-and-linear-layers-in-a-transformer-block" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id68">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/d9Qu7J.png" src="https://img.zhaoweiguo.com/uPic/2024/11/d9Qu7J.png" />
<figcaption>
<p><span class="caption-text">Figure 4.13 An illustration of a transformer block. The bottom of the diagram shows input tokens that have been embedded into 768-dimensional vectors. Each row corresponds to one token’s vector representation. The outputs of the transformer block are vectors of the same dimension as the input, which can then be fed into subsequent layers in an LLM.</span><a class="headerlink" href="#id68" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>The idea is that the self-attention mechanism in the multi-head attention block identifies and analyzes relationships between elements in the input sequence.</p></li>
<li><p>In contrast, the feed forward network modifies the data individually at each position.</p></li>
<li><p>This combination not only enables a more nuanced(细微差别) understanding and processing of the input but also enhances the model’s overall capacity for handling complex data patterns.</p></li>
<li><p>【组合的意义 fromGPT】自注意力机制提供全局信息：通过捕捉序列中元素之间的关系，让模型理解上下文和结构的复杂性。前馈网络强化局部信息：对每个位置进行特定的非线性变换，提升其独立特征表达。协同效果：这种组合让模型既能捕捉全局模式，又能处理局部细节，从而在面对复杂数据模式时表现出更强的处理能力。【例】句子翻译任务：自注意力机制帮助模型理解句子结构和词语之间的关系；前馈网络对每个单词的特定信息进行调整和优化，从而生成更准确的翻译。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">previous_chapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">MultiHeadAttention</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_in</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">d_out</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">context_length</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_heads&quot;</span><span class="p">],</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">],</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;qkv_bias&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Shortcut connection for attention block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="c1"># Shortcut connection for feed forward block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<ul class="simple">
<li><p>输入&amp;输出形状相同，是一个关键设计</p></li>
<li><p>The preservation of shape throughout the transformer block architecture is not incidental but a crucial aspect of its design.</p></li>
<li><p>This design enables its effective application across a wide range of sequence-to-sequence tasks, where each output vector directly corresponds to an input vector, maintaining a one-to-one relationship.</p></li>
<li><p>However, the output is a context vector that encapsulates information from the entire input sequence, as we learned in chapter 3.</p></li>
<li><p>This means that while the physical dimensions of the sequence (length and feature size) remain unchanged as it passes through the transformer block, the content of each output vector is re-encoded to integrate contextual information from across the entire input sequence.</p></li>
</ul>
<figure class="align-default" id="id69">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/REg2ra.png" src="https://img.zhaoweiguo.com/uPic/2024/11/REg2ra.png" />
<figcaption>
<p><span class="caption-text">Figure 4.14 A mental model of the different concepts we have implemented in this chapter so far.</span><a class="headerlink" href="#id69" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="coding-the-gpt-model">
<h3>4.6 Coding the GPT model<a class="headerlink" href="#coding-the-gpt-model" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id70">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/kbWfLF.png" src="https://img.zhaoweiguo.com/uPic/2024/11/kbWfLF.png" />
<figcaption>
<p><span class="caption-text">Figure 4.15 An overview of the GPT model architecture. This figure illustrates the flow of data through the GPT model. Starting from the bottom, tokenized text is first converted into token embeddings, which are then augmented with positional embeddings. This combined information forms a tensor that is passed through a series of transformer blocks shown in the center (each containing multi-head attention and feed forward neural network layers with dropout and layer normalization), which are stacked on top of each other and repeated 12 times.</span><a class="headerlink" href="#id70" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>the transformer block is repeated many times throughout a GPT model architecture.</p></li>
<li><p>In the case of the 124 million parameter GPT-2 model, it’s repeated 12 times</p></li>
<li><p>In the case of the largest GPT-2 model with 1,542 million parameters, this transformer block is repeated 36 times.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">GPTModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">in_idx</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">tok_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">pos_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tok_embeds</span> <span class="o">+</span> <span class="n">pos_embeds</span>  <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
<p>请求:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input batch:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Output shape:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="c1"># 输出</span>
<span class="c1"># Input batch:</span>
<span class="c1">#  tensor([[6109, 3626, 6100,  345],</span>
<span class="c1">#         [6109, 1110, 6622,  257]])</span>

<span class="c1"># Output shape: torch.Size([2, 4, 50257])</span>
<span class="c1"># tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],</span>
<span class="c1">#          [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],</span>
<span class="c1">#          [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],</span>
<span class="c1">#          [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],</span>

<span class="c1">#         [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],</span>
<span class="c1">#          [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],</span>
<span class="c1">#          [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],</span>
<span class="c1">#          [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],</span>
<span class="c1">#        grad_fn=&lt;UnsafeViewBackward0&gt;)</span>
</pre></div>
</div>
<p>参看参数总数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>total_params = sum(p.numel() for p in model.parameters())
print(f&quot;Total number of parameters: {total_params:,}&quot;)

# 输出
Total number of parameters: 163,009,536
# 疑问
为啥不是 GPT2 论文说的 124M 呢？
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>In the original GPT-2 paper, the researchers applied <code class="docutils literal notranslate"><span class="pre">weight</span> <span class="pre">tying</span></code> , which means that they reused the token embedding layer (<cite>tok_emb</cite>) as the output layer, which means setting <code class="docutils literal notranslate"><span class="pre">self.out_head.weight</span> <span class="pre">=</span> <span class="pre">self.tok_emb.weight</span></code> . The token embedding and output layers are very large due to the number of rows for the 50,257 in the tokenizer’s vocabulary. Weight tying reduces the overall memory footprint and computational complexity of the model. 具体参见 <code class="docutils literal notranslate"><span class="pre">WeightTying</span></code></p>
</div>
<p>去除GPT2重用的参数:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">total_params_gpt2</span> <span class="o">=</span>  <span class="n">total_params</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">out_head</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of trainable parameters considering weight tying: </span><span class="si">{</span><span class="n">total_params_gpt2</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 输出</span>
<span class="n">Number</span> <span class="n">of</span> <span class="n">trainable</span> <span class="n">parameters</span> <span class="n">considering</span> <span class="n">weight</span> <span class="n">tying</span><span class="p">:</span> <span class="mi">124</span><span class="p">,</span><span class="mi">412</span><span class="p">,</span><span class="mi">160</span>
</pre></div>
</div>
<p>内存占用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the total size in bytes (assuming float32, 4 bytes per parameter)</span>
<span class="n">total_size_bytes</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mi">4</span>

<span class="c1"># Convert to megabytes</span>
<span class="n">total_size_mb</span> <span class="o">=</span> <span class="n">total_size_bytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size of the model: </span><span class="si">{</span><span class="n">total_size_mb</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">Total</span> <span class="n">size</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span><span class="p">:</span> <span class="mf">621.83</span> <span class="n">MB</span>
</pre></div>
</div>
<p>Exercise:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="o">**</span><span class="n">GPT2</span><span class="o">-</span><span class="n">small</span><span class="o">**</span> <span class="p">(</span><span class="n">the</span> <span class="mi">124</span><span class="n">M</span> <span class="n">configuration</span> <span class="n">we</span> <span class="n">already</span> <span class="n">implemented</span><span class="p">):</span>
    <span class="o">-</span> <span class="s2">&quot;emb_dim&quot;</span> <span class="o">=</span> <span class="mi">768</span>
    <span class="o">-</span> <span class="s2">&quot;n_layers&quot;</span> <span class="o">=</span> <span class="mi">12</span>
    <span class="o">-</span> <span class="s2">&quot;n_heads&quot;</span> <span class="o">=</span> <span class="mi">12</span>

<span class="o">-</span> <span class="o">**</span><span class="n">GPT2</span><span class="o">-</span><span class="n">medium</span><span class="p">:</span><span class="o">**</span>
    <span class="o">-</span> <span class="s2">&quot;emb_dim&quot;</span> <span class="o">=</span> <span class="mi">1024</span>
    <span class="o">-</span> <span class="s2">&quot;n_layers&quot;</span> <span class="o">=</span> <span class="mi">24</span>
    <span class="o">-</span> <span class="s2">&quot;n_heads&quot;</span> <span class="o">=</span> <span class="mi">16</span>

<span class="o">-</span> <span class="o">**</span><span class="n">GPT2</span><span class="o">-</span><span class="n">large</span><span class="p">:</span><span class="o">**</span>
    <span class="o">-</span> <span class="s2">&quot;emb_dim&quot;</span> <span class="o">=</span> <span class="mi">1280</span>
    <span class="o">-</span> <span class="s2">&quot;n_layers&quot;</span> <span class="o">=</span> <span class="mi">36</span>
    <span class="o">-</span> <span class="s2">&quot;n_heads&quot;</span> <span class="o">=</span> <span class="mi">20</span>

<span class="o">-</span> <span class="o">**</span><span class="n">GPT2</span><span class="o">-</span><span class="n">XL</span><span class="p">:</span><span class="o">**</span>
    <span class="o">-</span> <span class="s2">&quot;emb_dim&quot;</span> <span class="o">=</span> <span class="mi">1600</span>
    <span class="o">-</span> <span class="s2">&quot;n_layers&quot;</span> <span class="o">=</span> <span class="mi">48</span>
    <span class="o">-</span> <span class="s2">&quot;n_heads&quot;</span> <span class="o">=</span> <span class="mi">25</span>
</pre></div>
</div>
</section>
<section id="generating-text">
<h3>4.7 Generating text<a class="headerlink" href="#generating-text" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id71">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/ZfRLSw.png" src="https://img.zhaoweiguo.com/uPic/2024/11/ZfRLSw.png" />
<figcaption>
<p><span class="caption-text">Figure 4.16 This diagram illustrates the step-by-step process by which an LLM generates text, one token at a time. Starting with an initial input context (“Hello, I am”), the model predicts a subsequent token during each iteration, appending it to the input context for the next round of prediction. As shown, the first iteration adds “a”, the second “model”, and the third “ready”, progressively building the sentence.</span><a class="headerlink" href="#id71" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id72">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/MaELd0.png" src="https://img.zhaoweiguo.com/uPic/2024/11/MaELd0.png" />
<figcaption>
<p><span class="caption-text">Figure 4.17 details the mechanics of text generation in a GPT model by showing a single iteration in the token generation process. The process begins by encoding the input text into token IDs, which are then fed into the GPT model. The outputs of the model are then converted back into text and appended to the original input text.</span><a class="headerlink" href="#id72" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_text_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
    <span class="c1"># idx is (batch, n_tokens) array of indices in the current context</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>

        <span class="c1"># Crop current context if it exceeds the supported context size</span>
        <span class="c1"># E.g., if LLM supports only 5 tokens, and the context size is 10</span>
        <span class="c1"># then only the last 5 tokens are used as context</span>
        <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>

        <span class="c1"># Get the predictions</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>

        <span class="c1"># Focus only on the last time step</span>
        <span class="c1"># (batch, n_tokens, vocab_size) becomes (batch, vocab_size)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Apply softmax to get probabilities</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, vocab_size)</span>

        <span class="c1"># Get the idx of the vocab entry with the highest probability value</span>
        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch, 1)</span>

        <span class="c1"># Append sampled index to the running sequence</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens+1)</span>

    <span class="k">return</span> <span class="n">idx</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>其实这儿不用执行 <code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code> 直接执行 <code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code> 可以得到同样的结果。we coded the conversion to illustrate the full process of transforming logits to probabilities, which can add additional intuition, such as that the model generates the most likely next token, which is known as <code class="docutils literal notranslate"><span class="pre">greedy</span> <span class="pre">decoding</span></code>. In the next chapter, when we will implement the GPT training code, we will also introduce additional <code class="docutils literal notranslate"><span class="pre">sampling</span> <span class="pre">techniques</span></code> where we modify the softmax outputs such that the model doesn’t always select the most likely token, which introduces variability and creativity in the generated text.</p>
</div>
<figure class="align-default" id="id73">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/7DkeWM.png" src="https://img.zhaoweiguo.com/uPic/2024/11/7DkeWM.png" />
<figcaption>
<p><span class="caption-text">Figure 4.18 An illustration showing six iterations of a token prediction cycle, where the model takes a sequence of initial token IDs as input, predicts the next token, and appends this token to the input sequence for the next iteration. (The token IDs are also translated into their corresponding text for better understanding.)</span><a class="headerlink" href="#id73" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>准备数据:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">start_context</span> <span class="o">=</span> <span class="s2">&quot;Hello, I am&quot;</span>

<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">start_context</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;encoded:&quot;</span><span class="p">,</span> <span class="n">encoded</span><span class="p">)</span>

<span class="n">encoded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;encoded_tensor.shape:&quot;</span><span class="p">,</span> <span class="n">encoded_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># encoded: [15496, 11, 314, 716]</span>
<span class="c1"># encoded_tensor.shape: torch.Size([1, 4])</span>
</pre></div>
</div>
<p>put the model into <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> mode, which disables random components like dropout, which are only used during training:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># disable dropout</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">encoded_tensor</span><span class="p">,</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output:&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output length:&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># 输出</span>
<span class="c1"># Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])</span>
<span class="c1"># Output length: 10</span>
</pre></div>
</div>
<p>Remove batch dimension and convert back into text:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="p">)</span>
<span class="c1"># 输出(未训练的)</span>
<span class="c1"># Hello, I am Featureiman Byeswickattribute argue</span>
</pre></div>
</div>
</section>
<section id="id3">
<h3>4.8 Summary<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Layer normalization stabilizes training by ensuring that each layer’s outputs have a consistent mean and variance.</p></li>
<li><p>Shortcut connections are connections that skip one or more layers by feeding the output of one layer directly to a deeper layer, which helps mitigate the vanishing gradient problem when training deep neural networks, such as LLMs.</p></li>
<li><p>Transformer blocks are a core structural component of GPT models, combining <code class="docutils literal notranslate"><span class="pre">masked</span> <span class="pre">multi-head</span> <span class="pre">attention</span> <span class="pre">modules</span></code> with <code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">connected</span> <span class="pre">feed-forward</span> <span class="pre">networks</span></code> that use the GELU activation function.</p></li>
<li><p>GPT models are LLMs with many repeated transformer blocks that have millions to billions of parameters.</p></li>
<li><p>GPT models come in various sizes, for example, 124 million, and 1542 million parameters, which we can implement with the same GPTModel Python class.</p></li>
<li><p>The text generation capability of a GPT-like LLM involves decoding output tensors into human-readable text by sequentially predicting one token at a time based on a given input context.</p></li>
<li><p>Without training, a GPT model generates incoherent text, which underscores the importance of model training for coherent text generation, which is the topic of subsequent chapters.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LayerNorm</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emb_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-5</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shift</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">emb_dim</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">var</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">norm_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">var</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">*</span> <span class="n">norm_x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GELU</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span> <span class="o">*</span> 
            <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="p">))</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">]),</span>
            <span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">]),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>



<span class="k">class</span><span class="w"> </span><span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">context_length</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">d_out</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;d_out must be divisible by num_heads&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span> <span class="o">=</span> <span class="n">d_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_out</span> <span class="o">//</span> <span class="n">num_heads</span>  <span class="c1"># Reduce the projection dim to match desired output dim</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_out</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_out</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>  <span class="c1"># Linear layer to combine head outputs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">context_length</span><span class="p">,</span> <span class="n">context_length</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">d_in</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_key</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape: (b, num_tokens, d_out)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_query</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W_value</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># We implicitly split the matrix by adding a `num_heads` dimension</span>
        <span class="c1"># Unroll last dim: (b, num_tokens, d_out) -&gt; (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Transpose: (b, num_tokens, num_heads, head_dim) -&gt; (b, num_heads, num_tokens, head_dim)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">queries</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute scaled dot-product attention (aka self-attention) with a causal mask</span>
        <span class="n">attn_scores</span> <span class="o">=</span> <span class="n">queries</span> <span class="o">@</span> <span class="n">keys</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># Dot product for each head</span>

        <span class="c1"># Original mask truncated to the number of tokens and converted to boolean</span>
        <span class="n">mask_bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mask</span><span class="o">.</span><span class="n">bool</span><span class="p">()[:</span><span class="n">num_tokens</span><span class="p">,</span> <span class="p">:</span><span class="n">num_tokens</span><span class="p">]</span>

        <span class="c1"># Use the mask to fill attention scores</span>
        <span class="n">attn_scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask_bool</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span><span class="p">)</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span> <span class="o">/</span> <span class="n">keys</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Shape: (b, num_tokens, num_heads, head_dim)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_weights</span> <span class="o">@</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Combine heads, where self.d_out = self.num_heads * self.head_dim</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="n">context_vec</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_out</span><span class="p">)</span>
        <span class="n">context_vec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">context_vec</span><span class="p">)</span>  <span class="c1"># optional projection</span>

        <span class="k">return</span> <span class="n">context_vec</span>



<span class="k">class</span><span class="w"> </span><span class="nc">TransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_in</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">d_out</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span>
            <span class="n">context_length</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_heads&quot;</span><span class="p">],</span> 
            <span class="n">dropout</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">],</span>
            <span class="n">qkv_bias</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;qkv_bias&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ff</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Shortcut connection for attention block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="c1"># Shortcut connection for feed forward block</span>
        <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_shortcut</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">shortcut</span>  <span class="c1"># Add the original input back</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">generate_text_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
    <span class="c1"># idx is (batch, n_tokens) array of indices in the current context</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        
        <span class="c1"># Crop current context if it exceeds the supported context size</span>
        <span class="c1"># E.g., if LLM supports only 5 tokens, and the context size is 10</span>
        <span class="c1"># then only the last 5 tokens are used as context</span>
        <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>
        
        <span class="c1"># Get the predictions</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
        
        <span class="c1"># Focus only on the last time step</span>
        <span class="c1"># (batch, n_tokens, vocab_size) becomes (batch, vocab_size)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  

        <span class="c1"># Apply softmax to get probabilities</span>
        <span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, vocab_size)</span>

        <span class="c1"># Get the idx of the vocab entry with the highest probability value</span>
        <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch, 1)</span>

        <span class="c1"># Append sampled index to the running sequence</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch, n_tokens+1)</span>

    <span class="k">return</span> <span class="n">idx</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GPTModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;drop_rate&quot;</span><span class="p">])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="o">*</span><span class="p">[</span><span class="n">TransformerBlock</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;n_layers&quot;</span><span class="p">])])</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">LayerNorm</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;vocab_size&quot;</span><span class="p">],</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_idx</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">in_idx</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">tok_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tok_emb</span><span class="p">(</span><span class="n">in_idx</span><span class="p">)</span>
        <span class="n">pos_embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_emb</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">in_idx</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">tok_embeds</span> <span class="o">+</span> <span class="n">pos_embeds</span>  <span class="c1"># Shape [batch_size, num_tokens, emb_size]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop_emb</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

<span class="n">GPT_CONFIG_124M</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">50257</span><span class="p">,</span>    <span class="c1"># Vocabulary size</span>
    <span class="s2">&quot;context_length&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span> <span class="c1"># Context length</span>
    <span class="s2">&quot;emb_dim&quot;</span><span class="p">:</span> <span class="mi">768</span><span class="p">,</span>         <span class="c1"># Embedding dimension</span>
    <span class="s2">&quot;n_heads&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>          <span class="c1"># Number of attention heads</span>
    <span class="s2">&quot;n_layers&quot;</span><span class="p">:</span> <span class="mi">12</span><span class="p">,</span>         <span class="c1"># Number of layers</span>
    <span class="s2">&quot;drop_rate&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>       <span class="c1"># Dropout rate</span>
    <span class="s2">&quot;qkv_bias&quot;</span><span class="p">:</span> <span class="kc">False</span>       <span class="c1"># Query-Key-Value bias</span>
<span class="p">}</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">start_context</span> <span class="o">=</span> <span class="s2">&quot;Hello, I am&quot;</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">start_context</span><span class="p">)</span>
<span class="n">encoded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span> <span class="c1"># disable dropout</span>

<span class="n">out</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">encoded_tensor</span><span class="p">,</span> 
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="n">decoded_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="p">)</span>






</pre></div>
</div>
</section>
</section>
<section id="pretraining-on-unlabeled-data">
<h2>5 Pretraining on Unlabeled Data<a class="headerlink" href="#pretraining-on-unlabeled-data" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>Computing the training and validation set losses to assess the quality of LLM-generated text during training</p></li>
<li><p>Implementing a training function and pretraining the LLM</p></li>
<li><p>Saving and loading model weights to continue training an LLM Loading pretrained weights from OpenAI</p></li>
</ul>
<figure class="align-default" id="id74">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/fk4Mc8.png" src="https://img.zhaoweiguo.com/uPic/2024/11/fk4Mc8.png" />
<figcaption>
<p><span class="caption-text">Figure 5.1 A mental model of the three main stages of coding an LLM, pretraining the LLM on a general text dataset and finetuning it on a labeled dataset. This chapter focuses on pretraining the LLM, which includes implementing the training code, evaluating the performance, and saving and loading model weights.</span><a class="headerlink" href="#id74" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="evaluating-generative-text-models">
<h3>5.1 Evaluating generative text models<a class="headerlink" href="#evaluating-generative-text-models" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id75">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/jKVWDh.png" src="https://img.zhaoweiguo.com/uPic/2024/11/jKVWDh.png" />
<figcaption>
<p><span class="caption-text">Figure 5.2 An overview of the topics covered in this chapter. We begin by recapping the text generation from the previous chapter and implementing basic model evaluation techniques that we can use during the pretraining stage.</span><a class="headerlink" href="#id75" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="using-gpt-to-generate-text">
<h4>5.1.1 Using GPT to generate text<a class="headerlink" href="#using-gpt-to-generate-text" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id76">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/L2KVis.png" src="https://img.zhaoweiguo.com/uPic/2024/11/L2KVis.png" />
<figcaption>
<p><span class="caption-text">Figure 5.3 Generating text involves encoding text into token IDs that the LLM processes into logit vectors. The logit vectors are then converted back into token IDs, detokenized into a text representation.</span><a class="headerlink" href="#id76" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Utility functions for text to token ID conversion</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tiktoken</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">previous_chapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">generate_text_simple</span>

<span class="k">def</span><span class="w"> </span><span class="nf">text_to_token_ids</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;&lt;|endoftext|&gt;&#39;</span><span class="p">})</span>
    <span class="n">encoded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># add batch dimension</span>
    <span class="k">return</span> <span class="n">encoded_tensor</span>

<span class="k">def</span><span class="w"> </span><span class="nf">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="n">flat</span> <span class="o">=</span> <span class="n">token_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># remove batch dimension</span>
    <span class="k">return</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">flat</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>

<span class="n">start_context</span> <span class="o">=</span> <span class="s2">&quot;Every effort moves you&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">get_encoding</span><span class="p">(</span><span class="s2">&quot;gpt2&quot;</span><span class="p">)</span>

<span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="n">GPT_CONFIG_124M</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">]</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output text:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">))</span>

<span class="c1"># 输出</span>
<span class="c1"># Output text:</span>
<span class="c1">#  Every effort moves you rentingetic wasnم refres RexMeCHicular stren</span>
</pre></div>
</div>
</section>
<section id="calculating-the-text-generation-loss">
<h4>5.1.2 Calculating the text generation loss<a class="headerlink" href="#calculating-the-text-generation-loss" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>先回顾一下前面 how the data is loaded from chapter 2 and how the text is generated via the generate_text_simple function from chapter 4.</p></li>
</ul>
<figure class="align-default" id="id77">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/S5XB0w.png" src="https://img.zhaoweiguo.com/uPic/2024/11/S5XB0w.png" />
<figcaption>
<p><span class="caption-text">Figure 5.4 For each of the 3 input tokens, shown on the left, we compute a vector containing probability scores corresponding to each token in the vocabulary. The index position of the highest probability score in each vector represents the most likely next token ID. These token IDs associated with the highest probability scores are selected and mapped back into a text that represents the text generated by the model.</span><a class="headerlink" href="#id77" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id78">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/sdWuJt.png" src="https://img.zhaoweiguo.com/uPic/2024/11/sdWuJt.png" />
<figcaption>
<p><span class="caption-text">Figure 5.5 We now implement the text evaluation function in the remainder of this section. In the next section, we apply this evaluation function to the entire dataset we use for model training</span><a class="headerlink" href="#id78" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>输入&amp;期望输出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">16833</span><span class="p">,</span> <span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">],</span>   <span class="c1"># [&quot;every effort moves&quot;,</span>
                       <span class="p">[</span><span class="mi">40</span><span class="p">,</span>    <span class="mi">1107</span><span class="p">,</span> <span class="mi">588</span><span class="p">]])</span>   <span class="c1">#  &quot;I really like&quot;]</span>

<span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">3626</span><span class="p">,</span> <span class="mi">6100</span><span class="p">,</span> <span class="mi">345</span>  <span class="p">],</span>  <span class="c1"># [&quot; effort moves you&quot;,</span>
                        <span class="p">[</span><span class="mi">1107</span><span class="p">,</span>  <span class="mi">588</span><span class="p">,</span> <span class="mi">11311</span><span class="p">]])</span> <span class="c1">#  &quot; really like chocolate&quot;]</span>
</pre></div>
</div>
<p>We feed the inputs into the model to calculate logit vectors for the two input examples, each comprising three tokens, and apply the softmax function to transform these logit values into probability scores:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Probability of each token in vocabulary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">probas</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># Shape: (batch_size, num_tokens, vocab_size)</span>
<span class="c1"># 输出</span>
<span class="c1"># torch.Size([2, 3, 50257])</span>
</pre></div>
</div>
<p>by applying the argmax function to the probability scores to obtain the corresponding token IDs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 这儿得到的是模型的最终输出和token ID(当然现在还没训练，结果当然乱七八糟)</span>
<span class="n">token_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Token IDs:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># Token IDs:</span>
<span class="c1"># tensor([[[36397],</span>
<span class="c1">#         [39619],</span>
<span class="c1">#         [20610]],</span>
<span class="c1">#        [[ 8615],</span>
<span class="c1">#         [49289],</span>
<span class="c1">#         [47105]]])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Part of the text evaluation process is to measure “how far” the generated tokens are from the correct predictions (targets).</p></li>
<li><p>The training function will use this information to adjust the model weights to generate text that is more similar to (or ideally matches) the target text.</p></li>
</ul>
<figure class="align-default" id="id79">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/yLleo1.png" src="https://img.zhaoweiguo.com/uPic/2024/11/yLleo1.png" />
<figcaption>
<p><span class="caption-text">Figure 5.6 Before training, the model produces random next-token probability vectors. The goal of model training is to ensure that the probability values corresponding to the highlighted target token IDs are maximized.</span><a class="headerlink" href="#id79" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>The token probabilities corresponding to the target indices are as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 看看现在期望的输出的概率(probabilities)</span>
<span class="n">text_idx</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">target_probas_1</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[</span><span class="n">text_idx</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">targets</span><span class="p">[</span><span class="n">text_idx</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Text 1:&quot;</span><span class="p">,</span> <span class="n">target_probas_1</span><span class="p">)</span>

<span class="n">text_idx</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">target_probas_2</span> <span class="o">=</span> <span class="n">probas</span><span class="p">[</span><span class="n">text_idx</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">targets</span><span class="p">[</span><span class="n">text_idx</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Text 2:&quot;</span><span class="p">,</span> <span class="n">target_probas_2</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># Text 1: tensor([2.3466e-05, 2.0531e-05, 1.1733e-05])</span>
<span class="c1"># Text 2: tensor([4.2794e-05, 1.6248e-05, 1.1586e-05])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>The goal of training an LLM is to maximize these values, aiming to get them as close to a probability of 1. This way, we ensure the LLM consistently picks the target token(essentially the next word in the sentence)as the next token it generates.</p>
</div>
<section id="backpropagation">
<h5>Backpropagation<a class="headerlink" href="#backpropagation" title="此标题的永久链接">¶</a></h5>
<figure class="align-default" id="id80">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/pZeCF1.png" src="https://img.zhaoweiguo.com/uPic/2024/11/pZeCF1.png" />
<figcaption>
<p><span class="caption-text">Figure 5.7 Calculating the loss involves several steps. Steps 1 to 3 calculate the token probabilities corresponding to the target tensors. These probabilities are then transformed via a logarithm and averaged in steps 4-6.</span><a class="headerlink" href="#id80" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute logarithm of all token probabilities</span>
<span class="n">log_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">target_probas_1</span><span class="p">,</span> <span class="n">target_probas_2</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">log_probas</span><span class="p">)</span>
<span class="c1"># tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])</span>

<span class="c1"># Calculate the average probability for each token</span>
<span class="n">avg_log_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">log_probas</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">avg_log_probas</span><span class="p">)</span>
<span class="c1"># tensor(-10.7940)</span>

<span class="c1"># In deep learning, the common practice is to bring the negative average log probability down to 0.</span>
<span class="n">neg_avg_log_probas</span> <span class="o">=</span> <span class="n">avg_log_probas</span> <span class="o">*</span> <span class="o">-</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">neg_avg_log_probas</span><span class="p">)</span>
<span class="c1"># tensor(10.7940)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>The term for this negative value, -10.7722 turning into 10.7722, is known as the cross entropy loss in deep learning.</p>
</div>
</section>
</section>
<section id="cross-entropy-loss">
<h4>Cross entropy loss<a class="headerlink" href="#cross-entropy-loss" title="此标题的永久链接">¶</a></h4>
<p>the shape of the logits and target tensors:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Logits have shape (batch_size, num_tokens, vocab_size)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Logits shape:&quot;</span><span class="p">,</span> <span class="n">logits</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Targets have shape (batch_size, num_tokens)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Targets shape:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>For the <cite>cross_entropy</cite> function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logits_flat</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">targets_flat</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened logits:&quot;</span><span class="p">,</span> <span class="n">logits_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Flattened targets:&quot;</span><span class="p">,</span> <span class="n">targets_flat</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># Flattened logits: torch.Size([6, 50257])</span>
<span class="c1"># Flattened targets: torch.Size([6])</span>
</pre></div>
</div>
<p>PyTorch’s cross_entropy function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_flat</span><span class="p">,</span> <span class="n">targets_flat</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>详见定义 <code class="docutils literal notranslate"><span class="pre">_cross_entropy:</span></code></p>
</div>
</section>
<section id="perplexity">
<h4>Perplexity<a class="headerlink" href="#perplexity" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Perplexity is a measure often used alongside cross entropy loss to evaluate the performance of models in tasks like language modeling. It can provide a more interpretable way to understand the uncertainty of a model in predicting the next token in a sequence.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>详见定义 <code class="docutils literal notranslate"><span class="pre">Perplexity</span> <span class="pre">困惑度</span></code></p>
</div>
</section>
<section id="calculating-the-training-and-validation-set-losses">
<h4>5.1.3 Calculating the training and validation set losses<a class="headerlink" href="#calculating-the-training-and-validation-set-losses" title="此标题的永久链接">¶</a></h4>
<figure class="align-default" id="id81">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/10h7zN.png" src="https://img.zhaoweiguo.com/uPic/2024/11/10h7zN.png" />
<figcaption>
<p><span class="caption-text">Figure 5.8 After computing the cross entropy loss in the previous section, we now apply this loss computation to the entire text dataset that we will use for model training.</span><a class="headerlink" href="#id81" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="the-cost-of-pretraining-llms">
<h5>The cost of pretraining LLMs<a class="headerlink" href="#the-cost-of-pretraining-llms" title="此标题的永久链接">¶</a></h5>
<figure class="align-default" id="id82">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/Uid7fP.png" src="https://img.zhaoweiguo.com/uPic/2024/11/Uid7fP.png" />
<figcaption>
<p><span class="caption-text">Figure 5.9 When preparing the data loaders, we split the input text into training and validation set portions. Then, we tokenize the text (only shown for the training set portion for simplicity) and divide the tokenized text into chunks of a user-specified length (here 6). Finally, we shuffle the rows and organize the chunked text into batches (here, batch size 2), which we can use for model training.</span><a class="headerlink" href="#id82" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>For visualization purposes, Figure 5.9 uses a max_length=6 due to spatial constraints. However, for the actual data loaders we are implementing, we set the max_length equal to the 256-token context length that the LLM supports so that the LLM sees longer texts during training.</p></li>
</ul>
</section>
<section id="training-with-variable-lengths">
<h5>Training with variable lengths<a class="headerlink" href="#training-with-variable-lengths" title="此标题的永久链接">¶</a></h5>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="k">def</span><span class="w"> </span><span class="nf">calc_loss_loader</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;nan&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">num_batches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Reduce the number of batches to match the total number of batches in the data loader</span>
        <span class="c1"># if num_batches exceeds the number of batches in the data loader</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_batches</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_batches</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="n">num_batches</span>




<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient tracking for efficiency because we are not training, yet</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="n">val_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training loss:&quot;</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Validation loss:&quot;</span><span class="p">,</span> <span class="n">val_loss</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id83">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/u7CMTh.png" src="https://img.zhaoweiguo.com/uPic/2024/11/u7CMTh.png" />
<figcaption>
<p><span class="caption-text">Figure 5.10 We have recapped the text generation process and implemented basic model evaluation techniques to compute the training and validation set losses. Next, we will go to the training functions and pretrain the LLM.</span><a class="headerlink" href="#id83" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
</section>
</section>
<section id="training-an-llm">
<h3>5.2 Training an LLM<a class="headerlink" href="#training-an-llm" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id84">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/G8oVmR.png" src="https://img.zhaoweiguo.com/uPic/2024/11/G8oVmR.png" />
<figcaption>
<p><span class="caption-text">Figure 5.11 A typical training loop for training deep neural networks in PyTorch consists of several steps, iterating over the batches in the training set for several epochs. In each loop, we calculate the loss for each training set batch to determine loss gradients, which we use to update the model weights so that the training set loss is minimized.</span><a class="headerlink" href="#id84" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_model_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span>
                       <span class="n">eval_freq</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">,</span> <span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="c1"># Initialize lists to track losses and tokens seen</span>
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># Main training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set model to training mode</span>

        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># Reset loss gradients from previous batch iteration</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Calculate loss gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Update model weights using loss gradients</span>
            <span class="n">tokens_seen</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Optional evaluation step</span>
            <span class="k">if</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">)</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
                <span class="n">track_tokens_seen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_seen</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ep </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> (Step </span><span class="si">{</span><span class="n">global_step</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">): &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Train loss </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Val loss </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Print a sample text after each epoch</span>
        <span class="n">generate_and_print_sample</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">start_context</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span>


<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span>


<span class="k">def</span><span class="w"> </span><span class="nf">generate_and_print_sample</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">start_context</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">context_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">pos_emb</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">text_to_token_ids</span><span class="p">(</span><span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate_text_simple</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="n">encoded</span><span class="p">,</span>
            <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">context_size</span><span class="o">=</span><span class="n">context_size</span>
        <span class="p">)</span>
    <span class="n">decoded_text</span> <span class="o">=</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">decoded_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">))</span>  <span class="c1"># Compact print format</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
</pre></div>
</div>
<section id="adamw">
<h4>AdamW<a class="headerlink" href="#adamw" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Adam optimizers are a popular choice for training deep neural networks.</p></li>
<li><p>However, in our training loop, we opt for the AdamW optimizer.</p></li>
<li><p>AdamW is a variant of Adam that improves the weight decay approach, which aims to minimize model complexity and prevent overfitting by penalizing(处罚) larger weights.</p></li>
<li><p>This adjustment allows AdamW to achieve more effective regularization and better generalization and is thus frequently used in the training of LLMs.</p></li>
<li><p>运行</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0004</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">tokens_seen</span> <span class="o">=</span> <span class="n">train_model_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">eval_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">start_context</span><span class="o">=</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>输出:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933
Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339
Every effort moves you,,,,,,,,,,,,.
Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048
Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616
Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,
... ...
Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293
Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393
Every effort moves you?&quot;  &quot;Yes--quite insensible to the irony. She wanted him vindicated--and by me!&quot;  He laughed again, and threw back the window-curtains, I had the donkey. &quot;There were days when I
Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452
Every effort moves you know,&quot; was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis
</pre></div>
</div>
</li>
</ul>
</section>
<section id="simple-plot">
<h4>simple plot<a class="headerlink" href="#simple-plot" title="此标题的永久链接">¶</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib.ticker</span><span class="w"> </span><span class="kn">import</span> <span class="n">MaxNLocator</span>


<span class="k">def</span><span class="w"> </span><span class="nf">plot_losses</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

    <span class="c1"># Plot training and validation loss against epochs</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">epochs_seen</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;-.&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Epochs&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper right&quot;</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_major_locator</span><span class="p">(</span><span class="n">MaxNLocator</span><span class="p">(</span><span class="n">integer</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>  <span class="c1"># only show integer labels on x-axis</span>

    <span class="c1"># Create a second x-axis for tokens seen</span>
    <span class="n">ax2</span> <span class="o">=</span> <span class="n">ax1</span><span class="o">.</span><span class="n">twiny</span><span class="p">()</span>  <span class="c1"># Create a second x-axis that shares the same y-axis</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Invisible plot for aligning ticks</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Tokens seen&quot;</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>  <span class="c1"># Adjust layout to make room</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s2">&quot;loss-plot.pdf&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">epochs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_losses</span><span class="p">))</span>
<span class="n">plot_losses</span><span class="p">(</span><span class="n">epochs_tensor</span><span class="p">,</span> <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id85">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/qdFTHI.png" src="https://img.zhaoweiguo.com/uPic/2024/11/qdFTHI.png" />
<figcaption>
<p><span class="caption-text">Figure 5.12 At the beginning of the training, we observe that both the training and validation set losses sharply decrease, which is a sign that the model is learning. However, the training set loss continues to decrease past the second epoch, whereas the validation loss stagnates. This is a sign that the model is still learning, but it’s overfitting to the training set past epoch 2.</span><a class="headerlink" href="#id85" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="id4">
<h4>小结<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Looking at the results above, we can see that the model starts out generating incomprehensible strings of words, whereas towards the end, it’s able to produce grammatically more or less correct sentences</p></li>
<li><p>However, based on the training and validation set losses, we can see that the model starts overfitting(最终训练集损失是0.541，而验证集损失是6.393)</p></li>
<li><p>If we were to check a few passages it writes towards the end, we would find that they are contained in the training set verbatim(逐字翻译) – it simply memorizes the training data</p></li>
<li><p>Later, we will cover decoding strategies that can mitigate this memorization by a certain degree</p></li>
<li><p>Note that the overfitting here occurs because we have a very, very small training set, and we iterate over it so many times</p></li>
</ul>
<figure class="align-default" id="id86">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/u3Vqxc.png" src="https://img.zhaoweiguo.com/uPic/2024/11/u3Vqxc.png" />
<figcaption>
<p><span class="caption-text">Figure 5.13 Our model can generate coherent text after implementing the training function. However, it often memorizes passages from the training set verbatim. The following section covers strategies to generate more diverse output texts.</span><a class="headerlink" href="#id86" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="decoding-strategies-to-control-randomness">
<h3>5.3 Decoding strategies to control randomness<a class="headerlink" href="#decoding-strategies-to-control-randomness" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>本节将重新实现 <code class="docutils literal notranslate"><span class="pre">generate_text_simple()</span> <span class="pre">in</span> <span class="pre">chapter</span> <span class="pre">4.7</span></code> 但 we will cover two techniques, temperature scaling, and top-k sampling, to improve this function.</p></li>
<li><p>之前实现的版本使用的贪心解码(greedy decoding)，每一次请求的返回结果都是选择概率最高的那个，导致每一次请求的结果都是相同的，这样对生成式的任务而言，效果单一。</p></li>
</ul>
<section id="temperature-scaling">
<h4>5.3.1 Temperature scaling<a class="headerlink" href="#temperature-scaling" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>This section introduces <code class="docutils literal notranslate"><span class="pre">temperature</span> <span class="pre">scaling</span></code> , a technique that adds a probabilistic selection process to the next-token generation task.</p></li>
<li><p>Previously, inside the <code class="docutils literal notranslate"><span class="pre">generate_text_simple</span></code> function, we always sampled the token with the highest probability as the next token using <code class="docutils literal notranslate"><span class="pre">torch.argmax</span></code>, also known as <code class="docutils literal notranslate"><span class="pre">greedy</span> <span class="pre">decoding</span></code>. To generate text with more variety, we can replace the argmax with a function that <code class="docutils literal notranslate"><span class="pre">samples</span> <span class="pre">from</span> <span class="pre">a</span> <span class="pre">probability</span> <span class="pre">distribution</span></code> (here, the probability scores the LLM generates for each vocabulary entry at each token generation step).</p></li>
</ul>
<p>small vocabulary for illustration purposes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;closer&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;every&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;effort&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;inches&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
    <span class="s2">&quot;moves&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
    <span class="s2">&quot;pizza&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">,</span>
    <span class="s2">&quot;toward&quot;</span><span class="p">:</span> <span class="mi">7</span><span class="p">,</span>
    <span class="s2">&quot;you&quot;</span><span class="p">:</span> <span class="mi">8</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">inverse_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">vocab</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</pre></div>
</div>
<p>上节讲的 <code class="docutils literal notranslate"><span class="pre">greedy</span> <span class="pre">decoding</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[</span><span class="mf">4.51</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.90</span><span class="p">,</span> <span class="mf">6.75</span><span class="p">,</span> <span class="mf">1.63</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.62</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.89</span><span class="p">,</span> <span class="mf">6.28</span><span class="p">,</span> <span class="mf">1.79</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">probas</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">next_token_id</span><span class="p">])</span>
<span class="c1"># &quot;forward&quot;</span>
<span class="c1"># 讲解:</span>
<span class="c1">#   通过对 next_token_logits 分析知道</span>
<span class="c1">#   最大概率是 6.75, 对应的softmax的最大值</span>
<span class="c1">#   所以next token的index是3，得到最终结果: forward</span>
</pre></div>
</div>
<p>对应的 probabilistic <code class="docutils literal notranslate"><span class="pre">sampling</span> <span class="pre">process</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># replace the argmax with the multinomial function in PyTorch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probas</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="n">next_token_id</span><span class="p">])</span>
<span class="c1"># 说明： 不管概率高低，都有被选中的可能</span>
</pre></div>
</div>
<p>temperature scaling 的作用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Temperatures greater than 1 result in more uniformly distributed token probabilities</span>
<span class="c1"># Temperatures smaller than 1 will result in more confident (sharper or more peaky) distributions.</span>
<span class="k">def</span><span class="w"> </span><span class="nf">softmax_with_temperature</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">temperature</span><span class="p">):</span>
    <span class="n">scaled_logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scaled_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id87">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/peGC1C.png" src="https://img.zhaoweiguo.com/uPic/2024/11/peGC1C.png" />
<figcaption>
<p><span class="caption-text">Figure 5.14 A temperature of 1 represents the unscaled probability scores for each token in the vocabulary. Decreasing the temperature to 0.1 sharpens the distribution, so the most likely token (here “forward”) will have an even higher probability score. Vice versa, increasing the temperature to 5 makes the distribution more uniform.</span><a class="headerlink" href="#id87" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="top-k-sampling">
<h4>5.3.2 Top-k sampling<a class="headerlink" href="#top-k-sampling" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>上节的方法allows for exploring less likely but potentially more interesting and creative paths in the generation process. However, One downside of this approach is that it sometimes leads to grammatically incorrect or completely nonsensicaloutputs(就是超低概率被命中，正常来说这种nexttoken是不太合适的，也就是无意义的语句生成)</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>In top-k sampling, we can restrict the sampled tokens to the top-k most likely tokens and exclude all other tokens from the selection process by masking their probability scores</p>
</div>
<figure class="align-default" id="id88">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/m6S6OJ.png" src="https://img.zhaoweiguo.com/uPic/2024/12/m6S6OJ.png" />
<figcaption>
<p><span class="caption-text">Figure 5.15 Using top-k sampling with k=3, we focus on the 3 tokens associated with the highest logits and mask out all other tokens with negative infinity (-inf) before applying the softmax function. This results in a probability distribution with a probability value 0 assigned to all non- top-k tokens.</span><a class="headerlink" href="#id88" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">top_k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">top_logits</span><span class="p">,</span> <span class="n">top_pos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
<span class="n">new_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
    <span class="n">condition</span><span class="o">=</span><span class="n">next_token_logits</span> <span class="o">&lt;</span> <span class="n">top_logits</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="nb">input</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">)),</span>
    <span class="n">other</span><span class="o">=</span><span class="n">next_token_logits</span>
<span class="p">)</span>
<span class="n">topk_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># 更高效实现</span>
<span class="o">&gt;</span> <span class="n">new_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span> <span class="c1"># create tensor containing -inf values</span>
<span class="o">&gt;</span>    <span class="n">next_token_logits</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">inf</span>
<span class="o">&gt;</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="n">new_logits</span><span class="p">[</span><span class="n">top_pos</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_token_logits</span><span class="p">[</span><span class="n">top_pos</span><span class="p">]</span> <span class="c1"># copy top k values into the -inf tensor</span>
<span class="o">&gt;</span> <span class="n">topk_probas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">new_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>top_p（也称为核采样或nucleus sampling）: 算法会首先将所有词按其概率排序，然后累积这些概率直到总和首次达到或超过 p 值。随后，只从这个累积概率达到了 p 的词集中随机抽样选择下一个词，而忽略其余概率较低的词。例如，如果设置 top_p=0.9，则会选择最小的词集，使得它们的累积概率至少为 90%，并从这个集合中随机抽取下一个词。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>当同时设置 top_k 和 top_p 时：先执行 top_k：首先从模型预测的概率分布中选择概率最高的前 k 个词。然后应用 top_p：在通过 top_k 筛选后的词汇集合中，进一步筛选出累积概率达到或超过 p 的最小词集。这意味着即使某个词在 top_k 范围内，如果它的累积概率超过了设定的 p 值，它也可能不会被考虑。</p>
</div>
</section>
<section id="modifying-the-text-generation-function">
<h4>5.3.3 Modifying the text generation function<a class="headerlink" href="#modifying-the-text-generation-function" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>本节把前面两节的 temperature scale 和 top-k sampling 合并到generator函数中</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="p">,</span> <span class="n">context_size</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eos_id</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># For-loop is the same as before: Get logits, and only focus on last time step</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_new_tokens</span><span class="p">):</span>
        <span class="n">idx_cond</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[:,</span> <span class="o">-</span><span class="n">context_size</span><span class="p">:]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">idx_cond</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># New: Filter logits with top_k sampling</span>
        <span class="k">if</span> <span class="n">top_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Keep only top_k values</span>
            <span class="n">top_logits</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">top_k</span><span class="p">)</span>
            <span class="n">min_val</span> <span class="o">=</span> <span class="n">top_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">logits</span> <span class="o">&lt;</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;-inf&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">logits</span><span class="p">)</span>

        <span class="c1"># New: Apply temperature scaling</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>

            <span class="c1"># Apply softmax to get probabilities</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, context_len)</span>

            <span class="c1"># Sample from the distribution</span>
            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, 1)</span>

        <span class="c1"># Otherwise same as before: get idx of the vocab entry with the highest logits value</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">idx_next</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># (batch_size, 1)</span>

        <span class="k">if</span> <span class="n">idx_next</span> <span class="o">==</span> <span class="n">eos_id</span><span class="p">:</span>  <span class="c1"># Stop generating early if end-of-sequence token is encountered and eos_id is specified</span>
            <span class="k">break</span>

        <span class="c1"># Same as before: append sampled index to the running sequence</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx_next</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (batch_size, num_tokens+1)</span>
</pre></div>
</div>
</section>
</section>
<section id="loading-and-saving-model-weights-in-pytorch">
<h3>5.4 Loading and saving model weights in PyTorch<a class="headerlink" href="#loading-and-saving-model-weights-in-pytorch" title="此标题的永久链接">¶</a></h3>
<ul>
<li><p>It’s common to train LLMs with adaptive optimizers like Adam or AdamW instead of regular SGD</p></li>
<li><p>These adaptive optimizers store additional parameters for each model weight, so it makes sense to save them as well in case we plan to continue the pretraining later:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
    <span class="s2">&quot;model_state_dict&quot;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">:</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="p">},</span>
    <span class="s2">&quot;model_and_optimizer.pth&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>loading:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model_and_optimizer.pth&quot;</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">GPT_CONFIG_124M</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;model_state_dict&quot;</span><span class="p">])</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;optimizer_state_dict&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="loading-pretrained-weights-from-openai">
<h3>5.5 Loading pretrained weights from OpenAI<a class="headerlink" href="#loading-pretrained-weights-from-openai" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>本节主要讲了，如何从gpt2中加载权重</p></li>
<li><p>因为gpt2和我们这儿定义的数据结构名称不相同，所以需要专门进行定制加载</p></li>
</ul>
</section>
<section id="id5">
<h3>5.6 Summary<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>When LLMs generate text, they output one token at a time.</p></li>
<li><p>By default, the next token is generated by converting the model outputs into probability scores and selecting the token from the vocabulary that corresponds to the highest probability score, which is known as “greedy decoding.”</p></li>
<li><p>Using probabilistic sampling and temperature scaling, we can influence the diversity and coherence of the generated text.</p></li>
<li><p>Training and validation set losses can be used to gauge(测量) the quality of text generated by LLM during training.</p></li>
<li><p>Pretraining an LLM involves changing its weights to minimize the training loss.</p></li>
<li><p>The training loop for LLMs itself is a standard procedure in deep learning, using a conventional <code class="docutils literal notranslate"><span class="pre">cross</span> <span class="pre">entropy</span> <span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">AdamW</span> <span class="pre">optimizer</span></code> . Pretraining an LLM on a large text corpus is time- and resource- intensive so we can load openly available weights from OpenAI as an alternative to pretraining the model on a large dataset ourselves.</p></li>
</ul>
</section>
</section>
<section id="fine-tuning-for-classification">
<h2>6 Fine-tuning for classification<a class="headerlink" href="#fine-tuning-for-classification" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>Introducing different LLM fine-tuning approaches</p></li>
<li><p>Preparing a dataset for text classification</p></li>
<li><p>Modifying a pretrained LLM for fine-tuning</p></li>
<li><p>Fine-tuning an LLM to identify spam messages</p></li>
<li><p>Evaluating the accuracy of a fine-tuned LLM classifier</p></li>
<li><p>Using a fine-tuned LLM to classify new data</p></li>
</ul>
<figure class="align-default" id="id89">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/R05xTV.png" src="https://img.zhaoweiguo.com/uPic/2024/12/R05xTV.png" />
<figcaption>
<p><span class="caption-text">Figure 6.1 The three main stages of coding an LLM. This chapter focus on stage 3 (step 8): fine-tuning a pretrained LLM as a classifier.</span><a class="headerlink" href="#id89" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="different-categories-of-fine-tuning">
<h3>6.1 Different categories of fine-tuning<a class="headerlink" href="#different-categories-of-fine-tuning" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>The most common ways to fine-tune language models are <code class="docutils literal notranslate"><span class="pre">instruction</span> <span class="pre">fine-tuning</span></code> and <code class="docutils literal notranslate"><span class="pre">classification</span> <span class="pre">fine-tuning</span></code> .</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Instruction</span> <span class="pre">fine-tuning</span></code> involves training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts</p></li>
</ul>
<figure class="align-default" id="id90">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/oVXiJM.png" src="https://img.zhaoweiguo.com/uPic/2024/12/oVXiJM.png" />
<figcaption>
<p><span class="caption-text">Figure 6.2 Two different instruction fine-tuning scenarios. At the top, the model is tasked with determining whether a given text is spam. At the bottom, the model is given an instruction on how to translate an English sentence into German.</span><a class="headerlink" href="#id90" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Instruction-finetuning, depicted below, is the topic of the next chapter. 本节主要讲 text classification.</p>
</div>
<figure class="align-default" id="id91">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/8T0XhD.png" src="https://img.zhaoweiguo.com/uPic/2024/12/8T0XhD.png" />
<figcaption>
<p><span class="caption-text">Figure 6.3 A text classification scenario using an LLM. A model fine-tuned for spam classification does not require further instruction alongside the input. In contrast to an instruction fine-tuned model, it can only respond with “spam” or “not spam.”</span><a class="headerlink" href="#id91" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Classification finetuning, the topic of this chapter, is a procedure you may already be familiar with if you have a background in machine learning – it’s similar to training a convolutional network to classify handwritten digits, for example</p></li>
<li><p>In classification finetuning, we have a specific number of class labels (for example, “spam” and “not spam”) that the model can output</p></li>
<li><p>A classification finetuned model can only predict classes it has seen during training (for example, “spam” or “not spam”), whereas an instruction-finetuned model can usually perform many tasks</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>We can think of a classification-finetuned model as a very specialized model; in practice, it is much easier to create a specialized model than a generalist model that performs well on many different tasks</p>
</div>
<section id="choosing-the-right-approach">
<h4>CHOOSING THE RIGHT APPROACH<a class="headerlink" href="#choosing-the-right-approach" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Instruction fine-tuning improves a model’s ability to understand and generate responses based on specific user instructions. Instruction fine-tuning is best suited for models that need to handle a variety of tasks based on complex user instructions, improving flexibility and interaction quality. Classification fine-tuning is ideal for projects requiring precise categorization of data into predefined classes, such as sentiment analysis or spam detection.</p></li>
<li><p>While instruction fine-tuning is more versatile, it demands larger datasets and greater computational resources to develop models proficient in various tasks. In contrast, classification fine-tuning requires less data and compute power, but its use is confined to the specific classes on which the model has been trained.</p></li>
</ul>
</section>
</section>
<section id="preparing-the-dataset">
<h3>6.2 Preparing the dataset<a class="headerlink" href="#preparing-the-dataset" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id92">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/1gOJPM.png" src="https://img.zhaoweiguo.com/uPic/2024/12/1gOJPM.png" />
<figcaption>
<p><span class="caption-text">Figure 6.4 The three-stage process for classification fine- tuning an LLM. Stage 1 involves dataset preparation. Stage 2 focuses on model setup. Stage 3 covers fine-tuning and evaluating the model.</span><a class="headerlink" href="#id92" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip&quot;</span>
<span class="o">=&gt;</span>
<span class="n">提取出3个数据集</span><span class="p">:</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;train.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">validation_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;validation.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">test_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;test.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="creating-data-loaders">
<h3>6.3 Creating data loaders<a class="headerlink" href="#creating-data-loaders" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id93">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/VjK9bp.png" src="https://img.zhaoweiguo.com/uPic/2024/12/VjK9bp.png" />
<figcaption>
<p><span class="caption-text">we use <cite>&lt;|endoftext|&gt;</cite> as a padding token</span><a class="headerlink" href="#id93" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SpamDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">csv_file</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">csv_file</span><span class="p">)</span>

        <span class="c1"># Pre-tokenize texts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;Text&quot;</span><span class="p">]</span>
        <span class="p">]</span>

        <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_longest_encoded_length</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
            <span class="c1"># Truncate sequences if they are longer than max_length</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">encoded_text</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
                <span class="k">for</span> <span class="n">encoded_text</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span>
            <span class="p">]</span>

        <span class="c1"># Pad sequences to the longest sequence</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">encoded_text</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">encoded_text</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span>
        <span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index</span><span class="p">][</span><span class="s2">&quot;Label&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_longest_encoded_length</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">encoded_text</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span><span class="p">:</span>
            <span class="n">encoded_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_text</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">encoded_length</span> <span class="o">&gt;</span> <span class="n">max_length</span><span class="p">:</span>
                <span class="n">max_length</span> <span class="o">=</span> <span class="n">encoded_length</span>
        <span class="k">return</span> <span class="n">max_length</span>
</pre></div>
</div>
<p>使用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">SpamDataset</span><span class="p">(</span>
    <span class="n">csv_file</span><span class="o">=</span><span class="s2">&quot;train.csv&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>
</div>
<p>验证集和测试集可以设定max_length(也可以和训练集一样设置max_length=None):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">val_dataset</span> <span class="o">=</span> <span class="n">SpamDataset</span><span class="p">(</span>
    <span class="n">csv_file</span><span class="o">=</span><span class="s2">&quot;validation.csv&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">SpamDataset</span><span class="p">(</span>
    <span class="n">csv_file</span><span class="o">=</span><span class="s2">&quot;test.csv&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id94">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/IXm5MP.png" src="https://img.zhaoweiguo.com/uPic/2024/12/IXm5MP.png" />
<figcaption>
<p><span class="caption-text">Figure 6.7 A single training batch consisting of eight text messages represented as token IDs. Each text message consists of 120 token IDs. A class label array stores the eight class labels corresponding to the text messages, which can be either 0 (“not spam”) or 1 (“spam”).</span><a class="headerlink" href="#id94" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>data loaders(以训练集示例):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="initializing-a-model-with-pretrained-weights">
<h3>6.4 Initializing a model with pretrained weights<a class="headerlink" href="#initializing-a-model-with-pretrained-weights" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id95">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/eeyyFq.png" src="https://img.zhaoweiguo.com/uPic/2024/12/eeyyFq.png" />
<figcaption>
<p><span class="caption-text">Figure 6.8 The three-stage process for classification fine- tuning the LLM. Having completed stage 1, preparing the dataset, we now must initialize the LLM, which we will then fine-tune to classify spam messages.</span><a class="headerlink" href="#id95" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul>
<li><p>用前面章节实现模型的初始化和加载gpt2的权重:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPTModel</span><span class="p">(</span><span class="n">BASE_CONFIG</span><span class="p">)</span>
<span class="n">load_weights_into_gpt</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">();</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="adding-a-classification-head">
<h3>6.5 Adding a classification head<a class="headerlink" href="#adding-a-classification-head" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id96">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/uGQchL.png" src="https://img.zhaoweiguo.com/uPic/2024/12/uGQchL.png" />
<figcaption>
<p><span class="caption-text">Figure 6.9 Adapting a GPT model for spam classification by altering its architecture. Initially, the model’s linear output layer mapped 768 hidden units to a vocabulary of 50,257 tokens. To detect spam, we replace this layer with a new output layer that maps the same 768 hidden units to just two classes, representing “spam” and “not spam.”</span><a class="headerlink" href="#id96" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="output-layer-nodes">
<h4>OUTPUT LAYER NODES<a class="headerlink" href="#output-layer-nodes" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>print the model architecture via print(model)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GPTModel</span><span class="p">(</span>
  <span class="p">(</span><span class="n">tok_emb</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">50257</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
  <span class="p">(</span><span class="n">pos_emb</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
  <span class="p">(</span><span class="n">drop_emb</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">trf_blocks</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">TransformerBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">att</span><span class="p">):</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
        <span class="p">(</span><span class="n">W_query</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">W_key</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">W_value</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">out_proj</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">ff</span><span class="p">):</span> <span class="n">FeedForward</span><span class="p">(</span>
        <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">GELU</span><span class="p">()</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">norm1</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
      <span class="p">(</span><span class="n">norm2</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
      <span class="p">(</span><span class="n">drop_resid</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">)</span>
   <span class="o">...</span> <span class="o">...</span> <span class="c1"># 省略11个</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">final_norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
  <span class="p">(</span><span class="n">out_head</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">50257</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fine-tuning-selected-layers-vs-all-layers">
<h4>FINE-TUNING SELECTED LAYERS VS. ALL LAYERS<a class="headerlink" href="#fine-tuning-selected-layers-vs-all-layers" title="此标题的永久链接">¶</a></h4>
<ul>
<li><p>The goal is to replace and finetune the output layer</p></li>
<li><p>To achieve this, we first freeze the model, meaning that we make all layers non-trainable:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</li>
</ul>
<p>Then, we replace the output layer (<cite>model.out_head</cite>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># finetune the model for binary classification (predicting 2 classes, &quot;spam&quot; and &quot;not spam&quot;)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">model</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="n">BASE_CONFIG</span><span class="p">[</span><span class="s2">&quot;emb_dim&quot;</span><span class="p">],</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id97">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/g6zEJP.png" src="https://img.zhaoweiguo.com/uPic/2024/12/g6zEJP.png" />
<figcaption>
<p><span class="caption-text">Figure 6.10 The GPT model includes 12 repeated transformer blocks. Alongside the output layer, we set the final LayerNorm and the last transformer block as trainable. The remaining 11 transformer blocks and the embedding layers are kept nontrainable.</span><a class="headerlink" href="#id97" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Technically, it’s sufficient to only train the output layer</p></li>
<li><p>However, as I found in <a class="reference external" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">Finetuning Large Language Models</a> , experiments show that finetuning additional layers can noticeably improve the performance</p></li>
<li><p>So, we are also making the last transformer block and the final <cite>LayerNorm</cite> module connecting the last transformer block to the output layer trainable</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">trf_blocks</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>

<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">final_norm</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<figure class="align-default" id="id98">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/JjmDiL.png" src="https://img.zhaoweiguo.com/uPic/2024/12/JjmDiL.png" />
<figcaption>
<p><span class="caption-text">Figure 6.11 The GPT model with a four-token example input and output. The output tensor consists of two columns due to the modified output layer. We are only interested in the last row corresponding to the last token when fine-tuning the model for spam classification.</span><a class="headerlink" href="#id98" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>In chapter 3, we discussed the attention mechanism, which connects each input token to each other input token</p></li>
<li><p>In chapter 3, we then also introduced the causal attention mask that is used in GPT-like models; this causal mask lets a current token only attend to the current and previous token positions</p></li>
<li><p>Based on this causal attention mechanism, the 4th (last) token contains the most information among all tokens because it’s the only token that includes information about all other tokens</p></li>
<li><p>Hence, we are particularly interested in this last token, which we will finetune for the spam classification task</p></li>
</ul>
<figure class="align-default" id="id99">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/yPYEBT.png" src="https://img.zhaoweiguo.com/uPic/2024/12/yPYEBT.png" />
<figcaption>
<p><span class="caption-text">Figure 6.12 The causal attention mechanism, where the attention scores between input tokens are displayed in a matrix format. The empty cells indicate masked positions due to the causal attention mask, preventing tokens from attending to future tokens. The values in the cells represent attention scores; the last token, time , is the only one that computes attention scores for all preceding tokens.</span><a class="headerlink" href="#id99" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="calculating-the-classification-loss-and-accuracy">
<h3>6.6 Calculating the classification loss and accuracy<a class="headerlink" href="#calculating-the-classification-loss-and-accuracy" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id100">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/h2tpK5.png" src="https://img.zhaoweiguo.com/uPic/2024/12/h2tpK5.png" />
<figcaption>
<p><span class="caption-text">Figure 6.13 The three-stage process for classification fine- tuning the LLM. We’ve completed the first six steps. We are now ready to undertake the last step of stage 2: implementing the functions to evaluate the model’s performance to classify spam messages before, during, and after the fine-tuning.</span><a class="headerlink" href="#id100" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>We previously computed the token ID of the next token generated by the LLM by converting the 50,257 outputs into probabilities via the softmax function and then returning the position of the highest probability via the argmax function.</p></li>
<li><p>We take the same approach here to calculate whether the model outputs a “spam” or “not spam” prediction for a given input, as shown in figure 6.14.</p></li>
<li><p>The only difference is that we work with 2-dimensional instead of 50,257-dimensional outputs.</p></li>
</ul>
<figure class="align-default" id="id101">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/Haass5.png" src="https://img.zhaoweiguo.com/uPic/2024/12/Haass5.png" />
<figcaption>
<p><span class="caption-text">Figure 6.14 The model outputs corresponding to the last token are converted into probability scores for each input text. The class labels are obtained by looking up the index position of the highest probability score. The model predicts the spam labels incorrectly because it has not yet been trained.</span><a class="headerlink" href="#id101" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>Note that the softmax function is optional here:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Class label:&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Compute the classification accuracy, which measures the percentage of correct predictions across a dataset.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calc_accuracy_loader</span><span class="p">(</span><span class="n">data_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct_predictions</span><span class="p">,</span> <span class="n">num_examples</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">if</span> <span class="n">num_batches</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_batches</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data_loader</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">num_batches</span><span class="p">:</span>
            <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Logits of last output token</span>
            <span class="n">predicted_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">num_examples</span> <span class="o">+=</span> <span class="n">predicted_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">correct_predictions</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted_labels</span> <span class="o">==</span> <span class="n">target_batch</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">correct_predictions</span> <span class="o">/</span> <span class="n">num_examples</span>
</pre></div>
</div>
<p>use the function to determine the classification accuracies across various datasets:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">calc_accuracy_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="o">...</span>  <span class="c1"># 验证和测试集相同做法</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Because classification accuracy is not a differentiable function, we use cross-entropy loss as a proxy to maximize accuracy.</p></li>
<li><p>Accordingly, the <code class="docutils literal notranslate"><span class="pre">calc_loss_batch</span></code> function remains the same, with one adjustment: we focus on optimizing only the last token, <code class="docutils literal notranslate"><span class="pre">model(input_batch)[:,</span> <span class="pre">-1,</span> <span class="pre">:]</span></code> , rather than all tokens, <code class="docutils literal notranslate"><span class="pre">model(input_batch)</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="o">=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">target_batch</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_batch</span><span class="p">)[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># Logits of last output token</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">calc_loss_loader</span></code> is exactly the same as in chapter 5</p></li>
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">calc_closs_loader</span></code>, we compute the initial training, validation, and test set losses before we start training</p>
<blockquote>
<div><dl class="simple">
<dt>with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet</dt><dd><p>train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)
val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)
test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)</p>
</dd>
</dl>
<p>print(f”Training loss: {train_loss:.3f}”)
print(f”Validation loss: {val_loss:.3f}”)
print(f”Test loss: {test_loss:.3f}”)</p>
</div></blockquote>
</li>
</ul>
</section>
<section id="finetuning-the-model-on-supervised-data">
<h3>6.7 Finetuning the model on supervised data<a class="headerlink" href="#finetuning-the-model-on-supervised-data" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>We must define and use the training function to fine-tune the pretrained LLM and improve its spam classification accuracy.</p></li>
</ul>
<figure class="align-default" id="id102">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/U7Ie4K.png" src="https://img.zhaoweiguo.com/uPic/2024/12/U7Ie4K.png" />
<figcaption>
<p><span class="caption-text">Figure 6.15 A typical training loop for training deep neural networks in PyTorch consists of several steps, iterating over the batches in the training set for several epochs. In each loop, we calculate the loss for each training set batch to determine loss gradients, which we use to update the model weights to minimize the training set loss.</span><a class="headerlink" href="#id102" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Overall the same as <code class="docutils literal notranslate"><span class="pre">train_model_simple</span></code> in chapter 5</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_classifier_simple</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span>
                            <span class="n">eval_freq</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">):</span>
    <span class="c1"># Initialize lists to track losses and examples seen</span>
    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">examples_seen</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># Main training loop</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>  <span class="c1"># Set model to training mode</span>

        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1"># Reset loss gradients from previous batch iteration</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># Calculate loss gradients</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># Update model weights using loss gradients</span>
            <span class="n">examples_seen</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># New: track examples instead of tokens</span>
            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Optional evaluation step</span>
            <span class="k">if</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">)</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ep </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> (Step </span><span class="si">{</span><span class="n">global_step</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">): &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Train loss </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, Val loss </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># Calculate accuracy after each epoch</span>
        <span class="n">train_accuracy</span> <span class="o">=</span> <span class="n">calc_accuracy_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
        <span class="n">val_accuracy</span> <span class="o">=</span> <span class="n">calc_accuracy_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training accuracy: </span><span class="si">{</span><span class="n">train_accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">% | &quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Validation accuracy: </span><span class="si">{</span><span class="n">val_accuracy</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="n">train_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_accuracy</span><span class="p">)</span>
        <span class="n">val_accs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_accuracy</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">,</span> <span class="n">examples_seen</span>
</pre></div>
</div>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">evaluate_model</span></code> function used in the <code class="docutils literal notranslate"><span class="pre">train_classifier_simple</span></code> is the same as the one we used in chapter 5:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Same as chapter 5</span>
<span class="k">def</span><span class="w"> </span><span class="nf">evaluate_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="n">calc_loss_loader</span><span class="p">(</span><span class="n">val_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_batches</span><span class="o">=</span><span class="n">eval_iter</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span>
</pre></div>
</div>
</li>
<li><p>The training takes about 5 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">,</span> <span class="n">examples_seen</span> <span class="o">=</span> <span class="n">train_classifier_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">eval_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The resulting accuracy values are:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">97.21</span><span class="o">%</span>
<span class="n">Validation</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">97.32</span><span class="o">%</span>
<span class="n">Test</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">95.67</span><span class="o">%</span>
</pre></div>
</div>
<section id="choosing-the-number-of-epochs">
<h4>CHOOSING THE NUMBER OF EPOCHS<a class="headerlink" href="#choosing-the-number-of-epochs" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>The number of epochs depends on the dataset and the task’s difficulty, and there is no universal solution or recommendation, although an epoch number of five is usually a good starting point.</p></li>
<li><p>If the model overfits after the first few epochs as a loss plot, you may need to reduce the number of epochs.</p></li>
<li><p>If the trendline suggests that the validation loss could improve with further training, you should increase the number of epochs.</p></li>
</ul>
</section>
</section>
<section id="using-the-llm-as-a-spam-classifier">
<h3>6.8 Using the LLM as a spam classifier<a class="headerlink" href="#using-the-llm-as-a-spam-classifier" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id103">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/L03Oqz.png" src="https://img.zhaoweiguo.com/uPic/2024/12/L03Oqz.png" />
<figcaption>
<p><span class="caption-text">Figure 6.18 The three-stage process for classification fine- tuning our LLM. Step 10 is the final step of stage 3—using the fine-tuned model to classify new spam messages.</span><a class="headerlink" href="#id103" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="id6">
<h3>Summary<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>There are different strategies for fine-tuning LLMs, including classification fine-tuning and instruction fine-tuning.</p></li>
<li><p>Classification fine-tuning involves replacing the output layer of an LLM via a small classification layer.</p></li>
<li><p>In the case of classifying text messages as “spam” or “not spam,” the new classification layer consists of only two output nodes.</p></li>
<li><p>Previously, we used the number of output nodes equal to the number of unique tokens in the vocabulary (i.e., 50,256).</p></li>
<li><p>Instead of predicting the next token in the text as in pretraining, classification fine-tuning trains the model to output a correct class label—for example, “spam” or “not spam.”</p></li>
<li><p>The model input for fine-tuning is text converted into token IDs, similar to pretraining.</p></li>
<li><p>Before fine-tuning an LLM, we load the pretrained model as a base model.</p></li>
<li><p>Evaluating a classification model involves calculating the classification accuracy (the fraction or percentage of correct predictions).</p></li>
<li><p>Fine-tuning a classification model uses the same cross entropy loss function as when pretraining the LLM.</p></li>
</ul>
</section>
</section>
<section id="fine-tuning-to-follow-instructions">
<h2>7 Fine-tuning to follow instructions<a class="headerlink" href="#fine-tuning-to-follow-instructions" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>This chapter covers</p></li>
<li><p>The instruction fine-tuning process of LLMs</p></li>
<li><p>Preparing a dataset for supervised instruction fine-tuning Organizing instruction data in training batches</p></li>
<li><p>Loading a pretrained LLM and fine-tuning it to follow human instructions</p></li>
<li><p>Extracting LLM-generated instruction responses for evaluation</p></li>
<li><p>Evaluating an instruction-fine-tuned LLM</p></li>
</ul>
<figure class="align-default" id="id104">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/q1uzZK.png" src="https://img.zhaoweiguo.com/uPic/2024/12/q1uzZK.png" />
<figcaption>
<p><span class="caption-text">Figure 7.1 The three main stages of coding an LLM. This chapter focuses on step 9 of stage 3: fine-tuning a pretrained LLM to follow human instructions.</span><a class="headerlink" href="#id104" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="introduction-to-instruction-fine-tuning">
<h3>7.1 Introduction to instruction fine-tuning<a class="headerlink" href="#introduction-to-instruction-fine-tuning" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>instruction fine-tuning, also known as supervised instruction fine-tuning. Because it involves training a model on a dataset where the input-output pairs are explicitly provided</p></li>
</ul>
<figure class="align-default" id="id105">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/AlLHgv.png" src="https://img.zhaoweiguo.com/uPic/2024/12/AlLHgv.png" />
<figcaption>
<p><span class="caption-text">Figure 7.2 Examples of instructions that are processed by an LLM to generate desired responses</span><a class="headerlink" href="#id105" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id106">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/7t7CNK.png" src="https://img.zhaoweiguo.com/uPic/2024/12/7t7CNK.png" />
<figcaption>
<p><span class="caption-text">Figure 7.3 The three-stage process for instruction fine-tuning an LLM. Stage 1 involves dataset preparation, stage 2 focuses on model setup and fine-tuning, and stage 3 covers the evaluation of the model. We will begin with step 1 of stage 1: downloading and formatting the dataset.</span><a class="headerlink" href="#id106" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="preparing-a-dataset-for-supervised-instruction-fine-tuning">
<h3>7.2 Preparing a dataset for supervised instruction fine-tuning<a class="headerlink" href="#preparing-a-dataset-for-supervised-instruction-fine-tuning" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>训练文件: <a class="reference external" href="https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json">https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json</a></p></li>
<li><p>There are different ways to format the entries as inputs to the LLM; the figure below illustrates two example formats that were used for training the Alpaca (<a class="reference external" href="https://crfm.stanford.edu/2023/03/13/alpaca.html">https://crfm.stanford.edu/2023/03/13/alpaca.html</a>) and Phi-3 (<a class="reference external" href="https://arxiv.org/abs/2404.14219">https://arxiv.org/abs/2404.14219</a>) LLMs, respectively</p></li>
</ul>
<figure class="align-default" id="id107">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/Fk7PjD.png" src="https://img.zhaoweiguo.com/uPic/2024/12/Fk7PjD.png" />
<figcaption>
<p><span class="caption-text">Figure 7.4 Comparison of prompt styles for instruction fine-tuning in LLMs. The Alpaca style (left) uses a structured format with defined sections for instruction, input, and response, while the Phi-3 style (right) employs a simpler format with designated <code class="docutils literal notranslate"><span class="pre">&lt;|user|&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;|assistant|&gt;</span></code> tokens.</span><a class="headerlink" href="#id107" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>prompt styles</p></li>
<li><p>Alpaca was one of the early LLMs to publicly detail its instruction fine-tuning process.</p></li>
<li><p>Phi-3, developed by Microsoft, is included to demonstrate the diversity in prompt styles.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Fine-tuning the model with the Phi-3 template is approximately 17% faster than Alpaca template, since it results in shorter model inputs. The score is similar.</p>
</div>
<ul class="simple">
<li><p>这儿用 Alpaca 格式的 prompt styles</p></li>
<li><p>下面代码是把数据转为 Alpaca 格式的代码</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
    <span class="n">instruction_text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Below is an instruction that describes a task. &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Write a response that appropriately completes the request.&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">### Input:</span><span class="se">\n</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>

    <span class="k">return</span> <span class="n">instruction_text</span> <span class="o">+</span> <span class="n">input_text</span>
</pre></div>
</div>
</section>
<section id="organizing-data-into-training-batches">
<h3>7.3 Organizing data into training batches<a class="headerlink" href="#organizing-data-into-training-batches" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id108">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/5Wm3vr.png" src="https://img.zhaoweiguo.com/uPic/2024/12/5Wm3vr.png" />
<figcaption>
<p><span class="caption-text">Figure 7.5 The three-stage process for instruction fine-tuning an LLM. Next, we look at step 2 of stage 1: assembling the training batches.</span><a class="headerlink" href="#id108" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id109">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/tf6fob.png" src="https://img.zhaoweiguo.com/uPic/2024/12/tf6fob.png" />
<figcaption>
<p><span class="caption-text">Figure 7.6 The five substeps involved in implementing the batching process: (2.1) applying the prompt template, (2.2) using tokenization from previous chapters, (2.3) adding padding tokens, (2.4) creating target token IDs, and (2.5) replacing -100 placeholder tokens to mask padding tokens in the loss function.</span><a class="headerlink" href="#id109" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id110">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/9x94Hb.png" src="https://img.zhaoweiguo.com/uPic/2024/12/9x94Hb.png" />
<figcaption>
<p><span class="caption-text">Figure 7.7 The first two steps involved in implementing the batching process. Entries are first formatted using a specific prompt template (2.1) and then tokenized (2.2), resulting in a sequence of token IDs that the model can process.</span><a class="headerlink" href="#id110" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dataset</span>


<span class="k">class</span><span class="w"> </span><span class="nc">InstructionDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>

        <span class="c1"># Pre-tokenize texts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">instruction_plus_input</span> <span class="o">=</span> <span class="n">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>
            <span class="n">response_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">### Response:</span><span class="se">\n</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="n">full_text</span> <span class="o">=</span> <span class="n">instruction_plus_input</span> <span class="o">+</span> <span class="n">response_text</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">full_text</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoded_texts</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id111">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/IVo1Vb.png" src="https://img.zhaoweiguo.com/uPic/2024/12/IVo1Vb.png" />
<figcaption>
<p><span class="caption-text">Figure 7.8 The padding of training examples in batches using token ID 50256 to ensure uniform length within each batch. Each batch may have different lengths, as shown by the first and second.</span><a class="headerlink" href="#id111" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Here, we take a more sophisticated(优雅) approach and develop a custom “collate” function that we can pass to the data loader</p></li>
<li><p>This custom <code class="docutils literal notranslate"><span class="pre">collate</span></code> function pads the training examples in each batch to have the same length (but different batches can have different lengths)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_collate_draft_1</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span>
<span class="p">):</span>
    <span class="c1"># Find the longest sequence in the batch</span>
    <span class="c1"># and increase the max length by +1, which will add one extra</span>
    <span class="c1"># padding token below</span>
    <span class="n">batch_max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Pad and prepare inputs</span>
    <span class="n">inputs_lst</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">new_item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Add an &lt;|endoftext|&gt; token</span>
        <span class="n">new_item</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span>
        <span class="c1"># Pad sequences to batch_max_length</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">new_item</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">batch_max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_item</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="c1"># Via padded[:-1], we remove the extra padded token</span>
        <span class="c1"># that has been added via the +1 setting in batch_max_length</span>
        <span class="c1"># (the extra padding token will be relevant in later codes)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">padded</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">inputs_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Convert list of inputs to tensor and transfer to target device</span>
    <span class="n">inputs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs_lst</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs_tensor</span>
</pre></div>
</div>
<p>使用示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs_1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">inputs_2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>
<span class="n">inputs_3</span> <span class="o">=</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>

<span class="n">batch</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">inputs_1</span><span class="p">,</span>
    <span class="n">inputs_2</span><span class="p">,</span>
    <span class="n">inputs_3</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">custom_collate_draft_1</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="c1"># tensor([[    0,     1,     2,     3,     4],</span>
<span class="c1">#         [    5,     6, 50256, 50256, 50256],</span>
<span class="c1">#         [    7,     8,     9, 50256, 50256]])</span>
</pre></div>
</div>
<figure class="align-default" id="id112">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/YPxzMN.png" src="https://img.zhaoweiguo.com/uPic/2024/12/YPxzMN.png" />
<figcaption>
<p><span class="caption-text">Figure 7.9 The five substeps involved in implementing the batching process. We are now focusing on step 2.4, the creation of target token IDs. This step is essential as it</span><a class="headerlink" href="#id112" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id113">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/bMJEBH.png" src="https://img.zhaoweiguo.com/uPic/2024/12/bMJEBH.png" />
<figcaption>
<p><span class="caption-text">Figure 7.10 The input and target token alignment used in the instruction fine-tuning process of an LLM. For each input sequence, the corresponding target sequence is created by shifting the token IDs one position to the right, omitting the first token of the input, and appending an end-of-text token.</span><a class="headerlink" href="#id113" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_collate_draft_2</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span>
<span class="p">):</span>
    <span class="c1"># Find the longest sequence in the batch</span>
    <span class="n">batch_max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Pad and prepare inputs</span>
    <span class="n">inputs_lst</span><span class="p">,</span> <span class="n">targets_lst</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">new_item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Add an &lt;|endoftext|&gt; token</span>
        <span class="n">new_item</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span>
        <span class="c1"># Pad sequences to max_length</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">new_item</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span>
            <span class="p">(</span><span class="n">batch_max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_item</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">padded</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Truncate the last token for inputs</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">padded</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># Shift +1 to the right for targets⭕️</span>
        <span class="n">inputs_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">targets_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Convert list of inputs to tensor and transfer to target device</span>
    <span class="n">inputs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs_lst</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">targets_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">targets_lst</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">targets_tensor</span>
</pre></div>
</div>
<p>使用示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">custom_collate_draft_2</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>
<span class="c1"># tensor([[    0,     1,     2,     3,     4],</span>
<span class="c1">#         [    5,     6, 50256, 50256, 50256],</span>
<span class="c1">#         [    7,     8,     9, 50256, 50256]])</span>
<span class="c1"># tensor([[    1,     2,     3,     4, 50256],</span>
<span class="c1">#         [    6, 50256, 50256, 50256, 50256],</span>
<span class="c1">#         [    8,     9, 50256, 50256, 50256]])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Next, we introduce an <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> value to replace all padding token IDs with a new value; the purpose of this <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code> is that we can ignore padding values in the loss function (more on that later)</p></li>
<li><p>This special value allows us to exclude these padding tokens from contributing to the training loss calculation, ensuring that only meaningful data influences model learning.</p></li>
<li><p>We will discuss this process in more detail after we implement this modification. (When fine-tuning for classification, we did not have to worry about this since we only trained the model based on the last output token.)</p></li>
</ul>
<figure class="align-default" id="id114">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/9FhReL.png" src="https://img.zhaoweiguo.com/uPic/2024/12/9FhReL.png" />
<figcaption>
<p><span class="caption-text">Figure 7.11 The five substeps involved in implementing the batching process. After creating the target sequence by shifting token IDs one position to the right and appending an end-of-text token, in step 2.5, we replace the end-of-text padding tokens with a placeholder value ( -100 ).</span><a class="headerlink" href="#id114" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id115">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/TJbha7.png" src="https://img.zhaoweiguo.com/uPic/2024/12/TJbha7.png" />
<figcaption>
<p><span class="caption-text">Figure 7.12 Step 2.4 in the token replacement process in the target batch for the training data preparation. We replace all but the first instance of the end-of-text token, which we use as padding, with the placeholder value -100 , while keeping the initial end-of-text token in each target sequence.</span><a class="headerlink" href="#id115" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">custom_collate_fn</span><span class="p">(</span>
    <span class="n">batch</span><span class="p">,</span>
    <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">50256</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">allowed_max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span>
<span class="p">):</span>
    <span class="c1"># Find the longest sequence in the batch</span>
    <span class="n">batch_max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">item</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>

    <span class="c1"># Pad and prepare inputs and targets</span>
    <span class="n">inputs_lst</span><span class="p">,</span> <span class="n">targets_lst</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">:</span>
        <span class="n">new_item</span> <span class="o">=</span> <span class="n">item</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="c1"># Add an &lt;|endoftext|&gt; token</span>
        <span class="n">new_item</span> <span class="o">+=</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span>
        <span class="c1"># Pad sequences to max_length</span>
        <span class="n">padded</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">new_item</span> <span class="o">+</span> <span class="p">[</span><span class="n">pad_token_id</span><span class="p">]</span> <span class="o">*</span>
            <span class="p">(</span><span class="n">batch_max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_item</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">padded</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Truncate the last token for inputs</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">padded</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># Shift +1 to the right for targets</span>

        <span class="c1"># New: Replace all but the first padding tokens in targets by ignore_index</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">targets</span> <span class="o">==</span> <span class="n">pad_token_id</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">targets</span><span class="p">[</span><span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]]</span> <span class="o">=</span> <span class="n">ignore_index</span>

        <span class="c1"># New: Optionally truncate to maximum sequence length</span>
        <span class="k">if</span> <span class="n">allowed_max_length</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:</span><span class="n">allowed_max_length</span><span class="p">]</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[:</span><span class="n">allowed_max_length</span><span class="p">]</span>

        <span class="n">inputs_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">targets_lst</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

    <span class="c1"># Convert list of inputs and targets to tensors and transfer to target device</span>
    <span class="n">inputs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs_lst</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">targets_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">targets_lst</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_tensor</span><span class="p">,</span> <span class="n">targets_tensor</span>
</pre></div>
</div>
<p>使用示例:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="n">custom_collate_fn</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">targets</span><span class="p">)</span>

<span class="c1"># tensor([[    0,     1,     2,     3,     4],</span>
<span class="c1">#         [    5,     6, 50256, 50256, 50256],</span>
<span class="c1">#         [    7,     8,     9, 50256, 50256]])</span>
<span class="c1"># tensor([[    1,     2,     3,     4, 50256],</span>
<span class="c1">#         [    6, 50256,  -100,  -100,  -100],</span>
<span class="c1">#         [    8,     9, 50256,  -100,  -100]])</span>
</pre></div>
</div>
<section id="why-replacement-by-100">
<h4>why replacement by -100<a class="headerlink" href="#why-replacement-by-100" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>Let’s see what this replacement by -100 accomplishes</p></li>
<li><p>For illustration purposes, let’s assume we have a small classification task with 2 class labels, 0 and 1, similar to chapter 6</p></li>
<li><p>If we have the following logits values (outputs of the last layer of the model), we calculate the following loss</p></li>
</ul>
<p>示例1:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logits_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>  <span class="c1"># 1st training example</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]]</span>  <span class="c1"># 2nd training example</span>
<span class="p">)</span>
<span class="n">targets_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>


<span class="n">loss_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_1</span><span class="p">,</span> <span class="n">targets_1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_1</span><span class="p">)</span>
<span class="c1"># tensor(1.1269)</span>
</pre></div>
</div>
<p>Now, adding one more training example will, as expected, influence the loss:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">logits_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
    <span class="p">[[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">],</span>
     <span class="p">[</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]]</span>  <span class="c1"># New 3rd training example</span>
<span class="p">)</span>
<span class="n">targets_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">loss_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_2</span><span class="p">,</span> <span class="n">targets_2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_2</span><span class="p">)</span>
<span class="c1"># tensor(0.7936)</span>
</pre></div>
</div>
<p>Let’s see what happens if we replace the class label of one of the examples with -100:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">targets_3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">])</span>

<span class="n">loss_3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits_2</span><span class="p">,</span> <span class="n">targets_3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss_3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss_1 == loss_3:&quot;</span><span class="p">,</span> <span class="n">loss_1</span> <span class="o">==</span> <span class="n">loss_3</span><span class="p">)</span>
<span class="c1"># tensor(1.1269)</span>
<span class="c1"># loss_1 == loss_3: tensor(True)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>As we can see, the resulting loss on these 3 training examples is the same as the loss we calculated from the 2 training examples, which means that the cross-entropy loss function ignored the training example with the -100 label</p></li>
<li><p>By default, PyTorch has the <code class="docutils literal notranslate"><span class="pre">cross_entropy(...,</span> <span class="pre">ignore_index=-100)</span></code> setting to ignore examples corresponding to the label -100</p></li>
<li><p>Using this -100 <code class="docutils literal notranslate"><span class="pre">ignore_index</span></code>, we can ignore the additional end-of-text (padding) tokens in the batches that we used to pad the training examples to equal length</p></li>
<li><p>However, we don’t want to ignore the first instance of the end-of-text (padding) token (50256) because it can help signal to the LLM when the response is complete</p></li>
</ul>
<figure class="align-default" id="id116">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/nMnhUg.png" src="https://img.zhaoweiguo.com/uPic/2024/12/nMnhUg.png" />
<figcaption>
<p><span class="caption-text">Figure 7.13 Left: The formatted input text we tokenize and then feed to the LLM during training. Right: The target text we prepare for the LLM where we can optionally mask out the instruction section, which means replacing the corresponding token IDs with <code class="docutils literal notranslate"><span class="pre">-100</span> <span class="pre">ignore_index</span></code> the value.</span><a class="headerlink" href="#id116" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>As of this writing, researchers are divided on whether masking the instructions is universally beneficial during instruction fine- tuning. For instance, the 2024 paper by Shi et al., “Instruction Tuning With Loss Over Instructions” (<a class="reference external" href="https://arxiv.org/abs/2405.14394">https://arxiv.org/abs/2405.14394</a>), demonstrated that not masking the instructions benefits the LLM performance (see appendix B for more details).</p>
</div>
</section>
</section>
<section id="creating-data-loaders-for-an-instruction-dataset">
<h3>7.4 Creating data loaders for an instruction dataset<a class="headerlink" href="#creating-data-loaders-for-an-instruction-dataset" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id117">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/IUxCm9.png" src="https://img.zhaoweiguo.com/uPic/2024/12/IUxCm9.png" />
<figcaption>
<p><span class="caption-text">Figure 7.14 The three-stage process for instruction fine-tuning an LLM. Thus far, we have prepared the dataset and implemented a custom collate function to batch the instruction dataset. Now, we can create and apply the data loaders to the training, validation, and test sets needed for the LLM instruction fine-tuning and evaluation.</span><a class="headerlink" href="#id117" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Instantiate the data loaders similar to previous chapters, except that we now provide our own collate function for the batching process</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>

<span class="n">customized_collate_fn</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span>
    <span class="n">custom_collate_fn</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
    <span class="n">allowed_max_length</span><span class="o">=</span><span class="mi">1024</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>


<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">InstructionDataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">customized_collate_fn</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span>
<span class="p">)</span>
<span class="o">...</span> <span class="o">...</span> <span class="c1"># 验证集、测试集相同</span>
</pre></div>
</div>
<p>应用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train loader:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">Train</span> <span class="n">loader</span><span class="p">:</span>
<span class="c1">#  8 represents the batch size and 61 is the number of tokens in each training example in this batch.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">61</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">61</span><span class="p">])</span>   <span class="c1"># 批次: 8, 本批次最长:61</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">76</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">76</span><span class="p">])</span>   <span class="c1"># 批次: 8, 本批次最长:76</span>
<span class="o">...</span> <span class="o">...</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">66</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">66</span><span class="p">])</span>   <span class="c1"># ...</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">74</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">74</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">69</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">69</span><span class="p">])</span>
</pre></div>
</div>
<p>print(inputs[0]):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span><span class="mi">21106</span><span class="p">,</span>   <span class="mi">318</span><span class="p">,</span>   <span class="mi">281</span><span class="p">,</span> <span class="mi">12064</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span>  <span class="mi">8477</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">4876</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span> <span class="mi">19430</span><span class="p">,</span>
          <span class="mi">257</span><span class="p">,</span>  <span class="mi">2882</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span> <span class="mi">20431</span><span class="p">,</span> <span class="mi">32543</span><span class="p">,</span>   <span class="mi">262</span><span class="p">,</span>  <span class="mi">2581</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>
        <span class="mi">21017</span><span class="p">,</span> <span class="mi">46486</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">30003</span><span class="p">,</span>  <span class="mi">6525</span><span class="p">,</span>   <span class="mi">262</span><span class="p">,</span>  <span class="mi">6827</span><span class="p">,</span>  <span class="mi">1262</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>
          <span class="mi">985</span><span class="p">,</span>   <span class="mi">576</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">21017</span><span class="p">,</span> <span class="mi">23412</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">464</span><span class="p">,</span>
         <span class="mi">5156</span><span class="p">,</span>   <span class="mi">318</span><span class="p">,</span>   <span class="mi">845</span><span class="p">,</span> <span class="mi">13779</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">21017</span><span class="p">,</span> <span class="mi">18261</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>
          <span class="mi">198</span><span class="p">,</span>   <span class="mi">464</span><span class="p">,</span>  <span class="mi">5156</span><span class="p">,</span>   <span class="mi">318</span><span class="p">,</span>   <span class="mi">355</span><span class="p">,</span> <span class="mi">13779</span><span class="p">,</span>   <span class="mi">355</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">4936</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>
        <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span> <span class="mi">50256</span><span class="p">],</span>
       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>print(targets[0]):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([</span>  <span class="mi">318</span><span class="p">,</span>   <span class="mi">281</span><span class="p">,</span> <span class="mi">12064</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span>  <span class="mi">8477</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">4876</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span> <span class="mi">19430</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>
         <span class="mi">2882</span><span class="p">,</span>   <span class="mi">326</span><span class="p">,</span> <span class="mi">20431</span><span class="p">,</span> <span class="mi">32543</span><span class="p">,</span>   <span class="mi">262</span><span class="p">,</span>  <span class="mi">2581</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">21017</span><span class="p">,</span>
        <span class="mi">46486</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">30003</span><span class="p">,</span>  <span class="mi">6525</span><span class="p">,</span>   <span class="mi">262</span><span class="p">,</span>  <span class="mi">6827</span><span class="p">,</span>  <span class="mi">1262</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>   <span class="mi">985</span><span class="p">,</span>
          <span class="mi">576</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">21017</span><span class="p">,</span> <span class="mi">23412</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">464</span><span class="p">,</span>  <span class="mi">5156</span><span class="p">,</span>
          <span class="mi">318</span><span class="p">,</span>   <span class="mi">845</span><span class="p">,</span> <span class="mi">13779</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span> <span class="mi">21017</span><span class="p">,</span> <span class="mi">18261</span><span class="p">,</span>    <span class="mi">25</span><span class="p">,</span>   <span class="mi">198</span><span class="p">,</span>
          <span class="mi">464</span><span class="p">,</span>  <span class="mi">5156</span><span class="p">,</span>   <span class="mi">318</span><span class="p">,</span>   <span class="mi">355</span><span class="p">,</span> <span class="mi">13779</span><span class="p">,</span>   <span class="mi">355</span><span class="p">,</span>   <span class="mi">257</span><span class="p">,</span>  <span class="mi">4936</span><span class="p">,</span>    <span class="mi">13</span><span class="p">,</span> <span class="mi">50256</span><span class="p">,</span>
         <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">,</span>  <span class="o">-</span><span class="mi">100</span><span class="p">],</span>
       <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="loading-a-pretrained-llm">
<h3>7.5 Loading a pretrained LLM<a class="headerlink" href="#loading-a-pretrained-llm" title="此标题的永久链接">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Instead of using the smallest 124-million-parameter model as before, we load the medium-sized model with 355 million parameters. The reason for this choice is that the 124-million- parameter model is too limited in capacity to achieve satisfactory results via instruction fine-tuning. Specifically, smaller models lack the necessary capacity to learn and retain the intricate patterns and nuanced behaviors required for high- quality instruction-following tasks.</p>
</div>
<figure class="align-default" id="id118">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/5YS09q.png" src="https://img.zhaoweiguo.com/uPic/2024/12/5YS09q.png" />
<figcaption>
<p><span class="caption-text">Figure 7.15 The three-stage process for instruction fine- tuning an LLM. After the dataset preparation, the process of fine-tuning an LLM for instruction-following begins with loading a pretrained LLM, which serves as the foundation for subsequent training.</span><a class="headerlink" href="#id118" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="fine-tuning-the-llm-on-instruction-data">
<h3>7.6 Fine-tuning the LLM on instruction data<a class="headerlink" href="#fine-tuning-the-llm-on-instruction-data" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id119">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/DGgrmy.png" src="https://img.zhaoweiguo.com/uPic/2024/12/DGgrmy.png" />
<figcaption>
<p><span class="caption-text">Figure 7.16 The three-stage process for instruction fine- tuning an LLM. In step 5, we train the pretrained model we previously loaded on the instruction dataset we prepared earlier.</span><a class="headerlink" href="#id119" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
</section>
<section id="extracting-and-saving-responses">
<h3>7.7 Extracting and saving responses<a class="headerlink" href="#extracting-and-saving-responses" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id120">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/5ibVm3.png" src="https://img.zhaoweiguo.com/uPic/2024/12/5ibVm3.png" />
<figcaption>
<p><span class="caption-text">Figure 7.18 The three-stage process for instruction fine- tuning the LLM. In the first two steps of stage 3, we extract and collect the model responses on the held-out test dataset for further analysis and then evaluate the model to quantify the performance of the instruction-fine-tuned LLM.</span><a class="headerlink" href="#id120" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><dl class="simple">
<dt>In practice, instruction-fine-tuned LLMs such as chatbots are evaluated via multiple approaches:</dt><dd><ul>
<li><p>Short-answer and multiple-choice benchmarks, such as Measuring Massive Multitask Language Understanding (MMLU; <a class="reference external" href="https://arxiv.org/abs/2009.03300">https://arxiv.org/abs/2009.03300</a>), which test the general knowledge of a model.</p></li>
<li><p>Human preference comparison to other LLMs, such as LMSYS chatbot arena (<a class="reference external" href="https://arena.lmsys.org">https://arena.lmsys.org</a>).</p></li>
<li><p>Automated conversational benchmarks, where another LLM like GPT-4 is used to evaluate the responses, such as AlpacaEval (<a class="reference external" href="https://tatsu-lab.github.io/alpaca_eval/">https://tatsu-lab.github.io/alpaca_eval/</a>).</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>本文将使用第3种方法(AlpacaEval)</p></li>
<li><p>先把输入、期望输出和模型输出数据整理出来</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">test_data</span><span class="p">),</span> <span class="n">total</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">test_data</span><span class="p">)):</span>

    <span class="n">input_text</span> <span class="o">=</span> <span class="n">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span>

    <span class="n">token_ids</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">idx</span><span class="o">=</span><span class="n">text_to_token_ids</span><span class="p">(</span><span class="n">input_text</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span>
        <span class="n">context_size</span><span class="o">=</span><span class="n">BASE_CONFIG</span><span class="p">[</span><span class="s2">&quot;context_length&quot;</span><span class="p">],</span>
        <span class="n">eos_id</span><span class="o">=</span><span class="mi">50256</span>
    <span class="p">)</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">token_ids_to_text</span><span class="p">(</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>
    <span class="n">response_text</span> <span class="o">=</span> <span class="n">generated_text</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">input_text</span><span class="p">):]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;### Response:&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

    <span class="n">test_data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s2">&quot;model_response&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response_text</span>


<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;instruction-data-with-response.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">file</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>  <span class="c1"># &quot;indent&quot; for pretty-printing</span>
</pre></div>
</div>
</section>
<section id="evaluating-the-fine-tuned-llm">
<h3>7.8 Evaluating the fine-tuned LLM<a class="headerlink" href="#evaluating-the-fine-tuned-llm" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id121">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/nOTHsV.png" src="https://img.zhaoweiguo.com/uPic/2024/12/nOTHsV.png" />
<figcaption>
<p><span class="caption-text">Figure 7.19 The three-stage process for instruction fine-tuning the LLM. In this last step of the instruction-fine-tuning pipeline, we implement a method to quantify the performance of the fine-tuned model by scoring the responses it generated for the test.</span><a class="headerlink" href="#id121" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
    <span class="n">instruction_text</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Below is an instruction that describes a task. &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;Write a response that appropriately completes the request.&quot;</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">### Instruction:</span><span class="se">\n</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;instruction&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="n">input_text</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">### Input:</span><span class="se">\n</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;input&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>

    <span class="k">return</span> <span class="n">instruction_text</span> <span class="o">+</span> <span class="n">input_text</span>

<span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">[:</span><span class="mi">3</span><span class="p">]:</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Given the input `</span><span class="si">{</span><span class="n">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span><span class="si">}</span><span class="s2">` &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;and correct output `</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">`, &quot;</span>
        <span class="sa">f</span><span class="s2">&quot;score the model response `</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;model_response&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">`&quot;</span>
        <span class="sa">f</span><span class="s2">&quot; on a scale from 0 to 100, where 100 is the best score. &quot;</span>
    <span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Dataset response:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&quot;</span><span class="p">,</span> <span class="n">entry</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Model response:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&quot;</span><span class="p">,</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;model_response&quot;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Score:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&gt;&gt;&quot;</span><span class="p">,</span> <span class="n">query_model</span><span class="p">(</span><span class="n">prompt</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">-------------------------&quot;</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">Dataset</span> <span class="n">response</span><span class="p">:</span>
<span class="o">&gt;&gt;</span> <span class="n">The</span> <span class="n">car</span> <span class="ow">is</span> <span class="k">as</span> <span class="n">fast</span> <span class="k">as</span> <span class="n">lightning</span><span class="o">.</span>

<span class="n">Model</span> <span class="n">response</span><span class="p">:</span>
<span class="o">&gt;&gt;</span> <span class="n">The</span> <span class="n">car</span> <span class="ow">is</span> <span class="k">as</span> <span class="n">fast</span> <span class="k">as</span> <span class="n">a</span> <span class="n">bullet</span><span class="o">.</span>

<span class="n">Score</span><span class="p">:</span>
<span class="o">&gt;&gt;</span> <span class="n">I</span><span class="s1">&#39;d rate the model response &quot;The car is as fast as a bullet.&quot; an 85 out of 100.</span>
</pre></div>
</div>
<ul>
<li><p>只显示百分比数字的prompt:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Given the input `</span><span class="si">{</span><span class="n">format_input</span><span class="p">(</span><span class="n">entry</span><span class="p">)</span><span class="si">}</span><span class="s2">` &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;and correct output `</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;output&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">`, &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;score the model response `</span><span class="si">{</span><span class="n">entry</span><span class="p">[</span><span class="n">json_key</span><span class="p">]</span><span class="si">}</span><span class="s2">`&quot;</span>
    <span class="sa">f</span><span class="s2">&quot; on a scale from 0 to 100, where 100 is the best score. &quot;</span>
    <span class="sa">f</span><span class="s2">&quot;Respond with the integer number only.&quot;</span>        <span class="c1"># added ❇️</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="conclusions">
<h3>7.9 Conclusions<a class="headerlink" href="#conclusions" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id122">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/qNEEIh.png" src="https://img.zhaoweiguo.com/uPic/2024/12/qNEEIh.png" />
<figcaption>
<p><span class="caption-text">Figure 7.21 The three main stages of coding an LLM.</span><a class="headerlink" href="#id122" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>While we covered the most essential steps, there is an optional step that can be performed after instruction fine-tuning: <code class="docutils literal notranslate"><span class="pre">preference</span> <span class="pre">fine-tuning</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Preference</span> <span class="pre">fine-tuning</span></code> is particularly useful for customizing a model to better align with specific user preferences.</p></li>
</ul>
</section>
<section id="id7">
<h3>Summary<a class="headerlink" href="#id7" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>The instruction-fine-tuning process adapts a pretrained LLM to follow human instructions and generate desired responses.</p></li>
<li><p>Preparing the dataset involves downloading an instruction-response dataset, formatting the entries, and splitting it into train, validation, and test sets.</p></li>
<li><p>Training batches are constructed using a custom collate function that pads sequences, creates target token IDs, and masks padding tokens.</p></li>
<li><p>We load a pretrained GPT-2 medium model with 355 million parameters to serve as the starting point for instruction fine-tuning.</p></li>
<li><p>The pretrained model is fine-tuned on the instruction dataset using a training loop similar to pretraining.</p></li>
<li><p>Evaluation involves extracting model responses on a test set and scoring them (for example, using another LLM).</p></li>
<li><p>The Ollama application with an 8-billion-parameter Llama model can be used to automatically score the fine-tuned model’s responses on the test set, providing an average score to quantify performance.</p></li>
</ul>
</section>
</section>
<section id="appendix-a-introduction-to-pytorch">
<h2>Appendix A. Introduction to PyTorch<a class="headerlink" href="#appendix-a-introduction-to-pytorch" title="此标题的永久链接">¶</a></h2>
<section id="a-1-what-is-pytorch">
<h3>A.1 What is PyTorch<a class="headerlink" href="#a-1-what-is-pytorch" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id123">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/LiTYmO.png" src="https://img.zhaoweiguo.com/uPic/2024/11/LiTYmO.png" />
<figcaption>
<p><span class="caption-text">Figure A.1 PyTorch’s three main components include a tensor library as a fundamental building block for computing, automatic differentiation for model optimization, and deep learning utility functions, making it easier to implement and train deep neural network models.</span><a class="headerlink" href="#id123" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>Firstly, PyTorch is a tensor library that extends the concept of array-oriented programming library NumPy with the additional feature of accelerated computation on GPUs, thus providing a seamless switch between CPUs and GPUs.</p></li>
<li><p>Secondly, PyTorch is an automatic differentiation engine, also known as autograd, which enables the automatic computation of gradients for tensor operations, simplifying backpropagation and model optimization.</p></li>
<li><p>Finally, PyTorch is a deep learning library, meaning that it offers modular, flexible, and efficient building blocks (including pre-trained models, loss functions, and optimizers) for designing and training a wide range of deep learning models, catering to both researchers and developers.</p></li>
</ul>
</section>
<section id="a-2-understanding-tensors">
<h3>A.2 Understanding tensors<a class="headerlink" href="#a-2-understanding-tensors" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id124">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/fXXxjF.png" src="https://img.zhaoweiguo.com/uPic/2024/11/fXXxjF.png" />
<figcaption>
<p><span class="caption-text">Figure A.6 An illustration of tensors with different ranks. Here 0D corresponds to rank 0, 1D to rank 1, and 2D to rank 2. Note that a 3D vector, which consists of 3 elements, is still a rank 1 tensor.</span><a class="headerlink" href="#id124" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>PyTorch’s has a NumPy-like API</p></li>
</ul>
</section>
<section id="a-3-seeing-models-as-computation-graphs">
<h3>A.3 Seeing models as computation graphs<a class="headerlink" href="#a-3-seeing-models-as-computation-graphs" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Automatic differentiation engine, also known as autograd. PyTorch’s autograd system provides functions to compute gradients in dynamic computational graphs automatically.</p></li>
<li><p>Computation graph is a directed graph that allows us to express and visualize mathematical expressions.</p></li>
<li><p>In the context of deep learning, a computation graph lays out the sequence of calculations needed to compute the output of a neural network – we will need this later to compute the required gradients for backpropagation, which is the main training algorithm for neural networks.</p></li>
</ul>
<figure class="align-default" id="id125">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/3m8Ye0.png" src="https://img.zhaoweiguo.com/uPic/2024/11/3m8Ye0.png" />
<figcaption>
<p><span class="caption-text">Figure A.7 A logistic regression forward pass as a computation graph. The input feature x1 is multiplied by a model weight w1 and passed through an activation function σ after adding the bias. The loss is computed by comparing the model output a with a given label y.</span><a class="headerlink" href="#id125" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>In fact, PyTorch builds such a computation graph in the background, and we can use this to calculate gradients of a loss function with respect to the model parameters (here w1 and b) to train the model</p></li>
</ul>
</section>
<section id="a-4-automatic-differentiation-made-easy">
<h3>A.4 Automatic differentiation made easy<a class="headerlink" href="#a-4-automatic-differentiation-made-easy" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id126">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/6lneDt.png" src="https://img.zhaoweiguo.com/uPic/2024/11/6lneDt.png" />
<figcaption>
<p><span class="caption-text">Figure A.8 The most common way of computing the loss gradients in a computation graph involves applying the chain rule from right to left, which is also called <code class="docutils literal notranslate"><span class="pre">reverse-model</span> <span class="pre">automatic</span> <span class="pre">differentiation</span></code> or <code class="docutils literal notranslate"><span class="pre">backpropagation</span></code>. It means we start from the output layer (or the loss itself) and work backward through the network to the input layer. This is done to compute the gradient of the loss with respect to each parameter (weights and biases) in the network, which informs how we update these parameters during training.</span><a class="headerlink" href="#id126" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>Gradients are required when training neural networks via the popular backpropagation algorithm, which can be thought of as an implementation of the chain rule from calculus for neural networks</p>
<section id="partial-derivatives-and-gradients">
<h4>Partial derivatives and gradients<a class="headerlink" href="#partial-derivatives-and-gradients" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>A partial derivatives, which measure the rate at which a function changes with respect to one of its variables.</p></li>
<li><p>A gradient is a vector containing all of the partial derivatives of a multivariate function, a function with more than one variable as input.</p></li>
<li><p>This provides the information needed to update each parameter in a way that minimizes the loss function, which serves as a proxy for measuring the model’s performance, using a method such as gradient descent.</p></li>
<li><p>Listing A.3 Computing gradients via autograd</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.autograd</span><span class="w"> </span><span class="kn">import</span> <span class="n">grad</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">2.2</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">*</span> <span class="n">w1</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">grad_L_w1</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1">#A</span>
<span class="n">grad_L_b</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>PyTorch provides even more high-level tools to automate this process:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="a-5-implementing-multilayer-neural-networks">
<h3>A.5 Implementing multilayer neural networks<a class="headerlink" href="#a-5-implementing-multilayer-neural-networks" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id127">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/WVZ7B9.png" src="https://img.zhaoweiguo.com/uPic/2024/11/WVZ7B9.png" />
<figcaption>
<p><span class="caption-text">Figure A.9 An illustration of a multilayer perceptron with 2 hidden layers. Each node represents a unit in the respective layer. Each layer has only a very small number of nodes for illustration purposes.</span><a class="headerlink" href="#id127" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<p>Listing A.4 A multilayer perceptron with two hidden layers</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">NeuralNetwork</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">):</span>  <span class="c1">#A</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># 1st hidden layer</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span>  <span class="c1">#B</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>  <span class="c1">#C</span>
            <span class="c1"># 2nd hidden layer</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>  <span class="c1">#D</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="c1"># output layer</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>   <span class="c1">#E</span>
</pre></div>
</div>
<p>instantiate a new neural network object:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">NeuralNetwork(</span>
<span class="go">  (layers): Sequential(</span>
<span class="go">    (0): Linear(in_features=50, out_features=30, bias=True)</span>
<span class="go">    (1): ReLU()</span>
<span class="go">    (2): Linear(in_features=30, out_features=20, bias=True)</span>
<span class="go">    (3): ReLU()</span>
<span class="go">    (4): Linear(in_features=20, out_features=3, bias=True)</span>
<span class="go">  )</span>
<span class="go">)</span>
</pre></div>
</div>
<p>check the total number of trainable parameters:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="go">2213</span>

<span class="go">手动计算:</span>
<span class="go">第一个隐藏层：50 个输入乘以 30 个隐藏单元加上 30 个偏置单元。</span>
<span class="go">    50*30+30</span>
<span class="go">第二个隐藏层：30 个输入单元乘以 20 个节点加上 20 个偏置单元。</span>
<span class="go">    30*20+20</span>
<span class="go">输出层：20 个输入节点乘以 3 个输出节点加上 3 个偏置单元。</span>
<span class="go">    20*3+3</span>
<span class="go">总共等于：1530+620+63=2213</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A linear layer multiplies the inputs with a weight matrix and adds a bias vector( <span class="math notranslate nohighlight">\(wx+b\)</span> ). This is sometimes also referred to as a <code class="docutils literal notranslate"><span class="pre">feedforward</span></code> or <code class="docutils literal notranslate"><span class="pre">fully</span> <span class="pre">connected</span> <span class="pre">layer</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># weight parameter matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

<span class="c1"># bias parameter matrix</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
</pre></div>
</div>
<p>we can make the random number initialization reproducible by seeding PyTorch’s random number generator:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
<p>When using for inference rather than training, it is a best practice to use <code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> context manager:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># This tells PyTorch that it doesn&#39;t need to keep track of the gradients,</span>
<span class="c1">#   which can result in significant savings in memory and computation.</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>In PyTorch, it’s common practice to code models such that they return the outputs of the last layer (logits) without passing them to a nonlinear activation function.</p></li>
<li><p>That’s because PyTorch’s commonly used loss functions combine the softmax (or sigmoid for binary classification) operation with the negative log-likelihood loss in a single class.</p></li>
<li><p>The reason for this is numerical efficiency and stability.</p></li>
<li><p>So, if we want to compute class-membership probabilities for our predictions, we have to call the softmax function explicitly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="a-6-setting-up-efficient-data-loaders">
<h3>A.6 Setting up efficient data loaders<a class="headerlink" href="#a-6-setting-up-efficient-data-loaders" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id128">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/9fcMmn.png" src="https://img.zhaoweiguo.com/uPic/2024/11/9fcMmn.png" />
<figcaption>
<p><span class="caption-text">Figure A.10 PyTorch implements a Dataset and a DataLoader class. The Dataset class is used to instantiate objects that define how each data record is loaded. The DataLoader handles how the data is shuffled and assembled into batches.</span><a class="headerlink" href="#id128" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span><span class="o">=</span><span class="n">train_ds</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>      <span class="c1"># crucial for parallelizing data loading and preprocessing.</span>
    <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span>      <span class="c1"># drop the last batch in each epoch(因为最后一个批次的数量可能不够)</span>
<span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id129">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/KvFx0K.png" src="https://img.zhaoweiguo.com/uPic/2024/11/KvFx0K.png" />
<figcaption>
<p><span class="caption-text">Figure A.11 参数 <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> 用于控制数据loading的工作线程，当num_workers=0时（像如图左边一样），计算损失值和加载数据一次只能做一个（计算完损失后，gpu会空闲等待数据载入）；而 <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> 启动后，载入数据和计算损失值可以并发执行（像如图右边一样）。Loading data without multiple workers (setting num_workers=0) will create a data loading bottleneck where the model sits idle until the next batch is loaded as illustrated in the left subpanel. If multiple workers are enabled, the data loader can already queue up the next batch in the background as shown in the right subpanel.</span><a class="headerlink" href="#id129" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="admonition warning">
<p class="admonition-title">警告</p>
<p>or Jupyter notebooks, setting num_workers to greater than 0 can sometimes lead to issues related to the sharing of resources between different processes, resulting in errors or notebook crashes.</p>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>根据经验，设置 num_workers=4 通常会在许多实际数据集上带来最佳性能，但最佳设置取决于您的硬件以及用于加载 Dataset 类中定义的训练示例的代码。</p>
</div>
</section>
<section id="a-7-a-typical-training-loop">
<h3>A.7 A typical training loop<a class="headerlink" href="#a-7-a-typical-training-loop" title="此标题的永久链接">¶</a></h3>
<p>Listing A.9 Neural network training in PyTorch</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_outputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1">#A</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1">#B</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">3</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1">#C</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1">#D</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1">#E</span>
        <span class="c1">### LOGGING</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">:</span><span class="s2">03d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_epochs</span><span class="si">:</span><span class="s2">03d</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; | Batch </span><span class="si">{</span><span class="n">batch_idx</span><span class="si">:</span><span class="s2">03d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span><span class="si">:</span><span class="s2">03d</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot; | Train Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="c1"># Optional model evaluation</span>
</pre></div>
</div>
<p>Listing A.10 A function to compute the prediction accuracy</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compute_accuracy</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">total_examples</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">compare</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="n">predictions</span> <span class="c1">#A</span>
        <span class="n">correct</span> <span class="o">+=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">compare</span><span class="p">)</span> <span class="c1">#B</span>
        <span class="n">total_examples</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">compare</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="n">total_examples</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1">#C</span>
</pre></div>
</div>
</section>
<section id="a-8-saving-and-loading-models">
<h3>A.8 Saving and loading models<a class="headerlink" href="#a-8-saving-and-loading-models" title="此标题的永久链接">¶</a></h3>
<p>模型保存:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;model.pth&quot;</span><span class="p">)</span>

<span class="c1"># model.state_dict 是一个 Python 字典对象，它将模型中的每一层映射到其可训练参数（权重和偏差）</span>
</pre></div>
</div>
<p>模型载入:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNetwork</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 架构需要与原始保存的模型完全匹配</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;model.pth&quot;</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="a-9-optimizing-training-performance-with-gpus">
<h3>A.9 Optimizing training performance with GPUs<a class="headerlink" href="#a-9-optimizing-training-performance-with-gpus" title="此标题的永久链接">¶</a></h3>
<section id="a-9-1-pytorch-computations-on-gpu-devices">
<h4>A.9.1 PyTorch computations on GPU devices<a class="headerlink" href="#a-9-1-pytorch-computations-on-gpu-devices" title="此标题的永久链接">¶</a></h4>
<p>CPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">tensor_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor_1</span> <span class="o">+</span> <span class="n">tensor_2</span><span class="p">)</span>

<span class="c1"># 输出</span>
<span class="c1"># tensor([5., 7., 9.])</span>
</pre></div>
</div>
<p>GPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor_1</span> <span class="o">=</span> <span class="n">tensor_1</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">tensor_2</span> <span class="o">=</span> <span class="n">tensor_2</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tensor_1</span> <span class="o">+</span> <span class="n">tensor_2</span><span class="p">)</span>

<span class="c1"># 输出</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="a-9-2-single-gpu-training">
<h4>A.9.2 Single-GPU training<a class="headerlink" href="#a-9-2-single-gpu-training" title="此标题的永久链接">¶</a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Nvidia GPU</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># Apple Silicon 芯片</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;mps&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>计算CPU&amp;GPU的计算速度:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CPU</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">a</span><span class="nd">@b</span>

<span class="c1"># GPU</span>
<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">),</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">a</span> <span class="o">@</span> <span class="n">b</span>
</pre></div>
</div>
</section>
<section id="a-9-3-training-with-multiple-gpus">
<h4>A.9.3 Training with multiple GPUs<a class="headerlink" href="#a-9-3-training-with-multiple-gpus" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>分布式训练是将模型训练划分到多个 GPU 和机器上的概念。</p></li>
</ul>
<figure class="align-default" id="id130">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/f1XfZm.png" src="https://img.zhaoweiguo.com/uPic/2024/11/f1XfZm.png" />
<figcaption>
<p><span class="caption-text">Figure A.12 The model and data transfer in DDP involves two key steps. First, we create a copy of the model on each of the GPUs. Then we divide the input data into unique minibatches that we pass on to each model copy.</span><a class="headerlink" href="#id130" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id131">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/0NwzaD.png" src="https://img.zhaoweiguo.com/uPic/2024/11/0NwzaD.png" />
<figcaption>
<p><span class="caption-text">Figure A.13 The forward and backward pass in DDP are executed independently on each GPU with its corresponding data subset. Once the forward and backward passes are completed, gradients from each model replica (on each GPU) are synchronized across all GPUs. This ensures that every model replica has the same updated weights.</span><a class="headerlink" href="#id131" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>DDP does not function properly within interactive Python environments like Jupyter notebooks, which don’t handle multiprocessing in the same way a standalone Python script does.</p>
</div>
<p>如果您的机器有四个 GPU 并且您只想使用第一个和第三个 GPU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span> <span class="n">python</span> <span class="n">some_script</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</section>
</section>
<section id="a-10-summary">
<h3>A.10 Summary<a class="headerlink" href="#a-10-summary" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>PyTorch is an open-source library that consists of three core components: a <code class="docutils literal notranslate"><span class="pre">tensor</span> <span class="pre">library</span></code>, <code class="docutils literal notranslate"><span class="pre">automatic</span> <span class="pre">differentiation</span> <span class="pre">functions</span></code>, and <code class="docutils literal notranslate"><span class="pre">deep</span> <span class="pre">learning</span> <span class="pre">utilities</span></code>.</p></li>
<li><p>PyTorch’s tensor library is similar to array libraries like NumPy</p></li>
<li><p>In the context of PyTorch, tensors are array-like data structures to represent <code class="docutils literal notranslate"><span class="pre">scalars</span></code>, <code class="docutils literal notranslate"><span class="pre">vectors</span></code>, <code class="docutils literal notranslate"><span class="pre">matrices</span></code>, and <code class="docutils literal notranslate"><span class="pre">higher-dimensional</span> <span class="pre">arrays</span></code>. PyTorch tensors can be executed on the CPU, but one major advantage of PyTorch’s tensor format is its GPU support to accelerate computations.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">automatic</span> <span class="pre">differentiation</span> <span class="pre">(autograd)</span></code> capabilities in PyTorch allow us to conveniently train neural networks using backpropagation without manually deriving gradients.</p></li>
<li><p>The deep learning utilities in PyTorch provide building blocks for creating custom deep neural networks.</p></li>
<li><p>PyTorch includes <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> classes to set up efficient data loading pipelines.</p></li>
<li><p>It’s easiest to train models on a CPU or single GPU.</p></li>
<li><p>Using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> is the simplest way in PyTorch to accelerate the training if multiple GPUs are available.</p></li>
</ul>
</section>
</section>
<section id="appendix-b-references-and-further-reading">
<h2>Appendix B. References and Further Reading<a class="headerlink" href="#appendix-b-references-and-further-reading" title="此标题的永久链接">¶</a></h2>
<section id="chapter-1-understanding-llm">
<h3>Chapter 1: Understanding LLM<a class="headerlink" href="#chapter-1-understanding-llm" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>“Attention Is All You Need” (2017) by Vaswani et al., <a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p></li>
<li><p>“BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” (2018) by Devlin et al., <a class="reference external" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p></li>
<li><p>The paper describing the decoder-style GPT-3 model, which inspired modern LLMs and will be used as a template for implementing an LLM from scratch in this book, is “Language Models are Few-Shot Learners” (2020) by Brown et al., <a class="reference external" href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a></p></li>
<li><p>The following covers the original vision transformer for classifying images, which illustrates that transformer architectures are not only restricted to text inputs: “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale” (2020) by Dosovitskiy et al., <a class="reference external" href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p></li>
<li><p>Meta AI’s model is a popular implementation of a GPT-like model that is openly available in contrast to GPT-3 and ChatGPT: “Llama 2: Open Foundation and Fine-Tuned Chat Models” (2023) by Touvron et al., <a class="reference external" href="https://arxiv.org/abs/2307.092881">https://arxiv.org/abs/2307.092881</a></p></li>
<li><p>The following paper provides the reference for InstructGPT for fine-tuning GPT-3: “Training Language Models to Follow Instructions with Human Feedback” (2022) by Ouyang et al., <a class="reference external" href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a></p></li>
<li><p>For readers interested in additional details about the dataset references in section 1.5, this paper describes the publicly available The Pile dataset curated by Eleuther AI: “The Pile: An 800GB Dataset of Diverse Text for Language Modeling” (2020) by Gao et al., <a class="reference external" href="https://arxiv.org/abs/2101.00027">https://arxiv.org/abs/2101.00027</a></p></li>
</ul>
</section>
<section id="chapter-2-working-with-text-data">
<h3>Chapter 2: Working with Text Data<a class="headerlink" href="#chapter-2-working-with-text-data" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>The code for the byte pair encoding tokenizer used to train GPT- 2 was open-sourced by OpenAI: <a class="reference external" href="https://github.com/openai/gpt-2/blob/master/src/encoder.py">https://github.com/openai/gpt-2/blob/master/src/encoder.py</a></p></li>
<li><p>OpenAI provides an interactive web UI to illustrate how the byte pair tokenizer in GPT models works: <a class="reference external" href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a></p></li>
<li><p>“A Minimal Implementation of a BPE Tokenizer,” <a class="reference external" href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p></li>
</ul>
</section>
<section id="chapter-3-coding-attention-mechanisms">
<h3>Chapter 3: Coding Attention Mechanisms<a class="headerlink" href="#chapter-3-coding-attention-mechanisms" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>A Minimal Implementation of a BPE Tokenizer,” <a class="reference external" href="https://github.com/karpathy/minbpe">https://github.com/karpathy/minbpe</a></p></li>
<li><p>The concept of self-attention as scaled dot-product attention was introduced in the original transformer paper: “Attention Is All You Need” (2017) by Vaswani et al., <a class="reference external" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p></li>
<li><dl class="simple">
<dt>FlashAttention is a highly efficient implementation of a self- attention mechanism, which accelerates the computation process by optimizing memory access patterns. FlashAttention is mathematically the same as the standard self-attention mechanism but optimizes the computational process for efficiency:</dt><dd><ul>
<li><p>“FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness” (2022) by Dao et al.,  <a class="reference external" href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a></p></li>
<li><p>“FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning” (2023) by Dao, <a class="reference external" href="https://arxiv.org/abs/2307.08691">https://arxiv.org/abs/2307.08691</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>“Dropout: A Simple Way to Prevent Neural Networks from Overfitting” (2014) by Srivastava et al., <a class="reference external" href="https://jmlr.org/papers/v15/srivastava14a.html">https://jmlr.org/papers/v15/srivastava14a.html</a></p></li>
</ul>
</section>
<section id="chapter-4-implementing-a-gpt-model">
<h3>Chapter 4: Implementing a GPT model<a class="headerlink" href="#chapter-4-implementing-a-gpt-model" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>“Layer Normalization” (2016) by Ba, Kiros, and Hinton, <a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a></p></li>
<li><p>“On Layer Normalization in the Transformer Architecture” (2020) by Xiong et al., <a class="reference external" href="https://arxiv.org/abs/2002.04745">https://arxiv.org/abs/2002.04745</a></p></li>
<li><p>“Gaussian Error Linear Units (GELUs)” (2016) by Hendricks and Gimpel, <a class="reference external" href="https://arxiv.org/abs/1606.08415">https://arxiv.org/abs/1606.08415</a></p></li>
<li><p>NanoGPT is a code repository with a minimalist yet efficient implementation of a GPT-2 model, similar to the model implemented in this book. “NanoGPT, a Repository for Training Medium-Sized GPTs, <a class="reference external" href="https://github.com/karpathy/nanoGPT">https://github.com/karpathy/nanoGPT</a></p></li>
</ul>
</section>
<section id="chapter-5-pretraining-on-unlabeled-data">
<h3>Chapter 5: Pretraining on Unlabeled Data<a class="headerlink" href="#chapter-5-pretraining-on-unlabeled-data" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>L8.2 Logistic Regression Loss Function, <a class="reference external" href="https://www.youtube.com/watch?v=GxJe0DZvydM">https://www.youtube.com/watch?v=GxJe0DZvydM</a></p></li>
<li><p>L8.7.1 OneHot Encoding and Multi-category Cross Entropy, <a class="reference external" href="https://www.youtube.com/watch?v=4n71-tZ94yk">https://www.youtube.com/watch?v=4n71-tZ94yk</a></p></li>
<li><p>Understanding Onehot Encoding and Cross Entropy in PyTorch, <a class="reference external" href="https://mng.bz/o05v">https://mng.bz/o05v</a></p></li>
<li><dl class="simple">
<dt>The following two papers detail the dataset, hyperparameter, and architecture details used for pretraining LLMs:</dt><dd><ul>
<li><p>“Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling” (2023) by Biderman et al., <a class="reference external" href="https://arxiv.org/abs/2304.01373">https://arxiv.org/abs/2304.01373</a></p></li>
<li><p>“OLMo: Accelerating the Science of Language Models” (2024) by Groeneveld et al., <a class="reference external" href="https://arxiv.org/abs/2402.00838">https://arxiv.org/abs/2402.00838</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>The paper that originally introduced top-k sampling is “Hierarchical Neural Story Generation” (2018) by Fan et al., <a class="reference external" href="https://arxiv.org/abs/1805.04833">https://arxiv.org/abs/1805.04833</a></p></li>
<li><p>Top-p sampling, <a class="reference external" href="https://en.wikipedia.org/wiki/Top-p_sampling">https://en.wikipedia.org/wiki/Top-p_sampling</a></p></li>
<li><p>“Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models” (2016) by Vijayakumar et al., <a class="reference external" href="https://arxiv.org/abs/1610.02424">https://arxiv.org/abs/1610.02424</a></p></li>
</ul>
</section>
<section id="chapter-6-fine-tuning-for-classification">
<h3>Chapter 6: Fine-tuning for classification<a class="headerlink" href="#chapter-6-fine-tuning-for-classification" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt>different types of fine- tuning are</dt><dd><ul>
<li><p>“Using and Finetuning Pretrained Transformers,” <a class="reference external" href="https://mng.bz/VxJG">https://mng.bz/VxJG</a></p></li>
<li><p>“Finetuning Large Language Models,” <a class="reference external" href="https://mng.bz/x28X">https://mng.bz/x28X</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Additional spam classification experiments(fine-tuning the first output token versus the last output token), <a class="reference external" href="https://mng.bz/AdJx">https://mng.bz/AdJx</a></p></li>
</ul>
</section>
<section id="chapter-7-fine-tuning-to-follow-instructions">
<h3>Chapter 7: Fine-tuning to follow instructions<a class="headerlink" href="#chapter-7-fine-tuning-to-follow-instructions" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt>datasets for instruction fine-tuning:</dt><dd><ul>
<li><p>“Stanford Alpaca: An Instruction-Following Llama Model,” <a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca">https://github.com/tatsu-lab/stanford_alpaca</a></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Preference fine-tuning is an optional step after instruction fine- tuning to align the LLM more closely with human preferences. The following articles by the author provide more information about this process:</dt><dd><ul>
<li><p>“LLM Training: RLHF and Its Alternatives,” <a class="reference external" href="https://mng.bz/ZVPm">https://mng.bz/ZVPm</a></p></li>
<li><p>“Tips for LLM Pretraining and Evaluating Reward Models,” <a class="reference external" href="https://mng.bz/RNXj">https://mng.bz/RNXj</a></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="appendix-a-pytorch">
<h3>Appendix A: PyTorch<a class="headerlink" href="#appendix-a-pytorch" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>If you want to learn more about model evaluation in machine learning, I recommend my article “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning” (2018) by Sebastian Raschka, <a class="reference external" href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</a></p></li>
<li><p>For readers who are interested in a refresher or gentle introduction to calculus, I’ve written a chapter on calculus that is freely available on my website: “Introduction to Calculus,” by Sebastian Raschka, <a class="reference external" href="https://mng.bz/WEyW">https://mng.bz/WEyW</a></p></li>
<li><p>If you want to learn more about gradient accumulation, please see the following article: “Finetuning Large Language Models on a Single GPU Using Gradient Accumulation” by Sebastian Raschka, <a class="reference external" href="https://mng.bz/8wPD">https://mng.bz/8wPD</a></p></li>
<li><p>DDP, which is a popular approach for training deep learning models across multiple GPUs. “Introducing PyTorch Fully Sharded Data Parallel (FSDP) API,” <a class="reference external" href="https://mng.bz/EZJR">https://mng.bz/EZJR</a></p></li>
</ul>
</section>
</section>
<section id="appendix-c-exercise-solutions">
<h2>Appendix C. Exercise Solutions<a class="headerlink" href="#appendix-c-exercise-solutions" title="此标题的永久链接">¶</a></h2>
<p>ignore</p>
</section>
<section id="appendix-d-adding-bells-and-whistles-to-the-training-loop">
<h2>Appendix D. Adding Bells and Whistles to the Training Loop<a class="headerlink" href="#appendix-d-adding-bells-and-whistles-to-the-training-loop" title="此标题的永久链接">¶</a></h2>
<ul>
<li><p>In this appendix, we enhance the training function for the pretraining and fine-tuning processes covered in chapters 5 to 7.</p></li>
<li><p>it covers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">learning</span> <span class="n">rate</span> <span class="n">warmup</span><span class="p">,</span>
<span class="n">cosine</span> <span class="n">decay</span><span class="p">,</span>
<span class="n">gradient</span> <span class="n">clipping</span><span class="o">.</span>
</pre></div>
</div>
</li>
</ul>
<section id="d-1-learning-rate-warmup">
<h3>D.1 Learning rate warmup<a class="headerlink" href="#d-1-learning-rate-warmup" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>When training complex models like LLMs, implementing learning rate warmup can help stabilize the training</p></li>
<li><p>In learning rate warmup, we gradually increase the learning rate from a very low value (<code class="docutils literal notranslate"><span class="pre">initial_lr</span></code>) to a user-specified maximum (<code class="docutils literal notranslate"><span class="pre">peak_lr</span></code> 这个是原本用户指定的学习率)</p></li>
<li><p>This way, the model will start the training with small weight updates, which helps decrease the risk of large destabilizing updates during the training</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_epochs</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">initial_lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">peak_lr</span> <span class="o">=</span> <span class="mf">0.01</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Typically, the number of warmup steps is between 0.1% to 20% of the total number of steps</p></li>
<li><p>We can compute the increment as the difference between the <code class="docutils literal notranslate"><span class="pre">peak_lr</span></code> and <code class="docutils literal notranslate"><span class="pre">initial_lr</span></code> divided by the number of warmup steps</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_epochs</span>
<span class="n">warmup_steps</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">total_steps</span><span class="p">)</span>   <span class="c1"># 20% warmup</span>
<span class="nb">print</span><span class="p">(</span><span class="n">warmup_steps</span><span class="p">)</span>


<span class="n">lr_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">peak_lr</span> <span class="o">-</span> <span class="n">initial_lr</span><span class="p">)</span> <span class="o">/</span> <span class="n">warmup_steps</span>

<span class="n">global_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">track_lrs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="n">global_step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="o">+</span> <span class="n">global_step</span> <span class="o">*</span> <span class="n">lr_increment</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">peak_lr</span>

        <span class="c1"># Apply the calculated learning rate to the optimizer</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="n">track_lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>

        <span class="c1"># Calculate loss and update weights</span>
        <span class="c1"># ...</span>
</pre></div>
</div>
</section>
<section id="d-2-cosine-decay">
<h3>D.2 Cosine decay<a class="headerlink" href="#d-2-cosine-decay" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Another popular technique for training complex deep neural networks is <code class="docutils literal notranslate"><span class="pre">cosine</span> <span class="pre">decay</span></code>, which also adjusts the learning rate across training epochs</p></li>
<li><p>In cosine decay, the learning rate follows a <code class="docutils literal notranslate"><span class="pre">cosine</span> <span class="pre">curve</span></code> , decreasing from its initial value to near zero following a half-cosine cycle</p></li>
<li><p>This gradual reduction is designed to slow the pace of learning as the model begins to improve its weights; it reduces the risk of overshooting minima as the training progresses, which is crucial for stabilizing the training in its later stages</p></li>
<li><p>Cosine decay is often preferred over linear decay for its smoother transition in learning rate adjustments, but linear decay is also used in practice (for example, [OLMo: Accelerating the Science of Language Models](<a class="reference external" href="https://arxiv.org/abs/2402.00838">https://arxiv.org/abs/2402.00838</a>))</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="n">min_lr</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">initial_lr</span>
<span class="n">track_lrs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">lr_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">peak_lr</span> <span class="o">-</span> <span class="n">initial_lr</span><span class="p">)</span> <span class="o">/</span> <span class="n">warmup_steps</span>
<span class="n">total_training_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_epochs</span>

<span class="n">global_step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="c1"># Adjust the learning rate based on the current phase (warmup or cosine annealing)</span>
        <span class="k">if</span> <span class="n">global_step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
            <span class="c1"># Linear warmup</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="o">+</span> <span class="n">global_step</span> <span class="o">*</span> <span class="n">lr_increment</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Cosine annealing after warmup</span>
            <span class="n">progress</span> <span class="o">=</span> <span class="p">((</span><span class="n">global_step</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span>
                        <span class="p">(</span><span class="n">total_training_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">))</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">min_lr</span> <span class="o">+</span> <span class="p">(</span><span class="n">peak_lr</span> <span class="o">-</span> <span class="n">min_lr</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

        <span class="c1"># Apply the calculated learning rate to the optimizer</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="n">track_lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>

        <span class="c1"># Calculate loss and update weights</span>
</pre></div>
</div>
</section>
<section id="d-3-gradient-clipping">
<h3>D.3 Gradient clipping<a class="headerlink" href="#d-3-gradient-clipping" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Gradient clipping is yet another technique used to stabilize the training when training LLMs</p></li>
<li><p>By setting a threshold, gradients exceeding this limit are scaled down to a maximum magnitude to ensure that the updates to the model’s parameters during backpropagation remain within a manageable range</p></li>
<li><p>For instance, using the <code class="docutils literal notranslate"><span class="pre">max_norm=1.0</span></code> setting in PyTorch’s <code class="docutils literal notranslate"><span class="pre">clip_grad_norm_</span></code> method means that the norm of the gradients is clipped such that their maximum norm does not exceed 1.0</p></li>
<li><p>the “norm” refers to a measure of the gradient vector’s length (or magnitude) in the parameter space of the model</p></li>
<li><p>Specifically, it’s the L2 norm, also known as the Euclidean norm</p></li>
<li><p>Mathematically, for a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> with components <span class="math notranslate nohighlight">\(\mathbf{v} = [v_1, v_2, ..., v_n]\)</span></p></li>
<li><p>the L2 norm is defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\| \mathbf{v} \|_2 = \sqrt{v_1^2 + v_2^2 + ... + v_n^2}\]</div>
<ul class="simple">
<li><p>The L2 norm is calculated similarly for matrices.</p></li>
<li><p>Let’s assume our gradient matrix is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}G = \begin{bmatrix}
1 &amp; 2 \\
2 &amp; 4
\end{bmatrix}\end{split}\]</div>
<ul class="simple">
<li><p>And we want to clip these gradients with a <cite>max_norm</cite> of 1.</p></li>
<li><p>First, we calculate the L2 norm of these gradients:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\|G\|_2 = \sqrt{1^2 + 2^2 + 2^2 + 4^2} = \sqrt{25} = 5\]</div>
<ul class="simple">
<li><p>Since <span class="math notranslate nohighlight">\(\|G\|_2 = 5\)</span> is greater than our <code class="docutils literal notranslate"><span class="pre">max_norm</span></code> of 1, we need to scale down the gradients so that their norm is exactly 1. The scaling factor is calculated as <span class="math notranslate nohighlight">\(\frac{max\_norm}{\|G\|_2} = \frac{1}{5}\)</span> .</p></li>
<li><p>Therefore, the scaled gradient matrix <code class="docutils literal notranslate"><span class="pre">G'</span></code> will be as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}G' = \frac{1}{5} \times G = \begin{bmatrix}
\frac{1}{5} &amp; \frac{2}{5} \\
\frac{2}{5} &amp; \frac{4}{5}
\end{bmatrix}\end{split}\]</div>
<ul>
<li><p>If we call <cite>.backward()</cite>, PyTorch will calculate the gradients and store them in a <cite>.grad</cite> attribute for each weight (parameter) matrix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</li>
<li><p>Let’s define a utility function to calculate the highest gradient based on all model weights</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">find_highest_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">max_grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">grad_values</span> <span class="o">=</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">max_grad_param</span> <span class="o">=</span> <span class="n">grad_values</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">max_grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_grad_param</span> <span class="o">&gt;</span> <span class="n">max_grad</span><span class="p">:</span>
                <span class="n">max_grad</span> <span class="o">=</span> <span class="n">max_grad_param</span>
    <span class="k">return</span> <span class="n">max_grad</span>

<span class="nb">print</span><span class="p">(</span><span class="n">find_highest_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor(0.0411)</span>
</pre></div>
</div>
<ul>
<li><p>Applying gradient clipping, we can see that the largest gradient is now substantially smaller:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">find_highest_gradient</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
<span class="c1"># 输出</span>
<span class="c1"># tensor(0.0185)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="d-4-the-modified-training-function">
<h3>D.4 The modified training function<a class="headerlink" href="#d-4-the-modified-training-function" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>Now let’s add the three concepts covered above (<code class="docutils literal notranslate"><span class="pre">learning</span> <span class="pre">rate</span> <span class="pre">warmup</span></code>, <code class="docutils literal notranslate"><span class="pre">cosine</span> <span class="pre">decay</span></code>, and <cite>gradient clipping</cite>) to the <cite>train_model_simple</cite> function covered in chapter 5 to create the more sophisticated <cite>train_model</cite> function below:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">previous_chapters</span><span class="w"> </span><span class="kn">import</span> <span class="n">evaluate_model</span><span class="p">,</span> <span class="n">generate_and_print_sample</span>

<span class="n">BOOK_VERSION</span> <span class="o">=</span> <span class="kc">True</span>


<span class="k">def</span><span class="w"> </span><span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
                <span class="n">n_epochs</span><span class="p">,</span> <span class="n">eval_freq</span><span class="p">,</span> <span class="n">eval_iter</span><span class="p">,</span> <span class="n">start_context</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span>
                <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">initial_lr</span><span class="o">=</span><span class="mf">3e-05</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>

    <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span><span class="p">,</span> <span class="n">track_lrs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">global_step</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span>

    <span class="c1"># Retrieve the maximum learning rate from the optimizer</span>
    <span class="n">peak_lr</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>

    <span class="c1"># Calculate the total number of iterations in the training process</span>
    <span class="n">total_training_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span> <span class="o">*</span> <span class="n">n_epochs</span>

    <span class="c1"># Calculate the learning rate increment during the warmup phase</span>
    <span class="n">lr_increment</span> <span class="o">=</span> <span class="p">(</span><span class="n">peak_lr</span> <span class="o">-</span> <span class="n">initial_lr</span><span class="p">)</span> <span class="o">/</span> <span class="n">warmup_steps</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">:</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">global_step</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># Adjust the learning rate based on the current phase (warmup or cosine annealing)</span>
            <span class="k">if</span> <span class="n">global_step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
                <span class="c1"># Linear warmup</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">initial_lr</span> <span class="o">+</span> <span class="n">global_step</span> <span class="o">*</span> <span class="n">lr_increment</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># Cosine annealing after warmup</span>
                <span class="n">progress</span> <span class="o">=</span> <span class="p">((</span><span class="n">global_step</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">)</span> <span class="o">/</span>
                            <span class="p">(</span><span class="n">total_training_steps</span> <span class="o">-</span> <span class="n">warmup_steps</span><span class="p">))</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">min_lr</span> <span class="o">+</span> <span class="p">(</span><span class="n">peak_lr</span> <span class="o">-</span> <span class="n">min_lr</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">progress</span><span class="p">))</span>

            <span class="c1"># Apply the calculated learning rate to the optimizer</span>
            <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>
            <span class="n">track_lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>  <span class="c1"># Store the current learning rate</span>

            <span class="c1"># Calculate and backpropagate the loss</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">calc_loss_batch</span><span class="p">(</span><span class="n">input_batch</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

            <span class="c1"># Apply gradient clipping after the warmup phase to avoid exploding gradients</span>

            <span class="k">if</span> <span class="n">BOOK_VERSION</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;</span> <span class="n">warmup_steps</span><span class="p">:</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">global_step</span> <span class="o">&gt;=</span> <span class="n">warmup_steps</span><span class="p">:</span>  <span class="c1"># the book originally used global_step &gt; warmup_steps, which lead to a skipped clipping step after warmup</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>

            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            <span class="n">tokens_seen</span> <span class="o">+=</span> <span class="n">input_batch</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

            <span class="c1"># Periodically evaluate the model on the training and validation sets</span>
            <span class="k">if</span> <span class="n">global_step</span> <span class="o">%</span> <span class="n">eval_freq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">train_loss</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate_model</span><span class="p">(</span>
                    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span>
                    <span class="n">device</span><span class="p">,</span> <span class="n">eval_iter</span>
                <span class="p">)</span>
                <span class="n">train_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
                <span class="n">val_losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">val_loss</span><span class="p">)</span>
                <span class="n">track_tokens_seen</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tokens_seen</span><span class="p">)</span>
                <span class="c1"># Print the current losses</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ep </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> (Iter </span><span class="si">{</span><span class="n">global_step</span><span class="si">:</span><span class="s2">06d</span><span class="si">}</span><span class="s2">): &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Train loss </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, &quot;</span>
                      <span class="sa">f</span><span class="s2">&quot;Val loss </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

        <span class="c1"># Generate and print a sample from the model to monitor progress</span>
        <span class="n">generate_and_print_sample</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">start_context</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">track_tokens_seen</span><span class="p">,</span> <span class="n">track_lrs</span>
</pre></div>
</div>
<p>使用:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">peak_lr</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># this was originally set to 5e-4 in the book by mistake</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">peak_lr</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># the book accidentally omitted the lr assignment</span>

<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">tokens_seen</span><span class="p">,</span> <span class="n">lrs</span> <span class="o">=</span> <span class="n">train_model</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">n_epochs</span><span class="o">=</span><span class="n">n_epochs</span><span class="p">,</span>
    <span class="n">eval_freq</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">eval_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">start_context</span><span class="o">=</span><span class="s2">&quot;Every effort moves you&quot;</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">warmup_steps</span><span class="o">=</span><span class="n">warmup_steps</span><span class="p">,</span>
    <span class="n">initial_lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-5</span>
<span class="p">)</span>
<span class="c1"># 输出</span>
<span class="n">Ep</span> <span class="mi">1</span> <span class="p">(</span><span class="n">Iter</span> <span class="mi">000000</span><span class="p">):</span> <span class="n">Train</span> <span class="n">loss</span> <span class="mf">10.934</span><span class="p">,</span> <span class="n">Val</span> <span class="n">loss</span> <span class="mf">10.939</span>
<span class="n">Ep</span> <span class="mi">1</span> <span class="p">(</span><span class="n">Iter</span> <span class="mi">000005</span><span class="p">):</span> <span class="n">Train</span> <span class="n">loss</span> <span class="mf">9.151</span><span class="p">,</span> <span class="n">Val</span> <span class="n">loss</span> <span class="mf">9.461</span>
<span class="o">...</span>
<span class="n">Ep</span> <span class="mi">14</span> <span class="p">(</span><span class="n">Iter</span> <span class="mi">000120</span><span class="p">):</span> <span class="n">Train</span> <span class="n">loss</span> <span class="mf">0.038</span><span class="p">,</span> <span class="n">Val</span> <span class="n">loss</span> <span class="mf">6.907</span>
<span class="n">Ep</span> <span class="mi">14</span> <span class="p">(</span><span class="n">Iter</span> <span class="mi">000125</span><span class="p">):</span> <span class="n">Train</span> <span class="n">loss</span> <span class="mf">0.040</span><span class="p">,</span> <span class="n">Val</span> <span class="n">loss</span> <span class="mf">6.912</span>
<span class="n">Ep</span> <span class="mi">15</span> <span class="p">(</span><span class="n">Iter</span> <span class="mi">000130</span><span class="p">):</span> <span class="n">Train</span> <span class="n">loss</span> <span class="mf">0.041</span><span class="p">,</span> <span class="n">Val</span> <span class="n">loss</span> <span class="mf">6.915</span>
</pre></div>
</div>
</section>
</section>
<section id="appendix-e-parameter-efficient-fine-tuning-with-lora">
<h2>appendix E Parameter-efficient fine- tuning with LoRA<a class="headerlink" href="#appendix-e-parameter-efficient-fine-tuning-with-lora" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>Low-rank adaptation (LoRA) is one of the most widely used techniques for parameter-efficient fine-tuning.</p></li>
</ul>
<section id="e-1-introduction-to-lora">
<h3>E.1 Introduction to LoRA<a class="headerlink" href="#e-1-introduction-to-lora" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>LoRA is a technique that adapts a pretrained model to better suit a specific, often smaller dataset by adjusting only a small subset of the model’s weight parameters.</p></li>
<li><p>The LoRA method is useful and popular because it enables efficient fine-tuning of large models on task-specific data, significantly cutting down on the computational costs and resources usually required for fine-tuning.</p></li>
</ul>
<ul class="simple">
<li><p>Suppose we have a large weight matrix <code class="docutils literal notranslate"><span class="pre">W</span></code> for a given layer</p></li>
<li><p>During backpropagation, we learn a <span class="math notranslate nohighlight">\(\Delta W\)</span> matrix, which contains information on how much we want to update the original weights to minimize the loss function during training</p></li>
<li><p>In regular training and finetuning, the weight update is defined as follows:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_{\text{updated}} = W + \Delta W\]</div>
<ul class="simple">
<li><p>The <a class="reference external" href="https://arxiv.org/abs/2106.09685">LoRA</a> method offers a more efficient alternative to computing the weight updates <span class="math notranslate nohighlight">\(\Delta W\)</span> by learning an approximation of it, <span class="math notranslate nohighlight">\(\Delta W \approx AB\)</span>.</p></li>
<li><p>In other words, in LoRA, we have the following, where <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code> are two small weight matrices:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[W_{\text{updated}} = W + AB\]</div>
<figure class="align-default" id="id132">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/0Euhs5.png" src="https://img.zhaoweiguo.com/uPic/2024/12/0Euhs5.png" />
<figcaption>
<p><span class="caption-text">Figure E.1 A comparison between weight update methods: regular fine-tuning and LoRA. Regular fine-tuning involves updating the pretrained weight matrix W directly with DW (left). LoRA uses two smaller matrices, A and B, to approximate DW, where the product AB is added to W, and r denotes the inner dimension, a tunable hyperparameter (right).</span><a class="headerlink" href="#id132" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>If you paid close attention, the full finetuning and LoRA depictions in the figure above look slightly different from the formulas I have shown earlier</p></li>
<li><p>That’s due to the distributive law of matrix multiplication: we don’t have to add the weights with the updated weights but can keep them separate</p></li>
<li><p>For instance, if <code class="docutils literal notranslate"><span class="pre">x</span></code> is the input data, then we can write the following for regular finetuning:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}x (W+\Delta W) = x W + x \Delta W\\=&gt; for LoRA:
x (W+A B) = x W + x A B\end{aligned}\end{align} \]</div>
<ul class="simple">
<li><p>The fact that we can keep the LoRA weight matrices separate makes LoRA especially attractive</p></li>
<li><p>In practice, this means that we don’t have to modify the weights of the pretrained model at all, as we can apply the LoRA matrices on the fly</p></li>
</ul>
</section>
<section id="e-2-preparing-the-dataset">
<h3>E.2 Preparing the dataset<a class="headerlink" href="#e-2-preparing-the-dataset" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>数据集: url = “<a class="reference external" href="https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip">https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip</a>”</p></li>
</ul>
</section>
<section id="e-3-initializing-the-model">
<h3>E.3 Initializing the model<a class="headerlink" href="#e-3-initializing-the-model" title="此标题的永久链接">¶</a></h3>
<ul>
<li><p>像 chapter 6 一样替换 output layer:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">model</span><span class="o">.</span><span class="n">out_head</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="e-4-parameter-efficient-finetuning-with-lora">
<h3>E.4 Parameter-efficient finetuning with LoRA<a class="headerlink" href="#e-4-parameter-efficient-finetuning-with-lora" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>We begin by initializing a LoRALayer that creates the matrices <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code>, along with the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> scaling hyperparameter and the <code class="docutils literal notranslate"><span class="pre">rank</span></code> (<code class="docutils literal notranslate"><span class="pre">r</span></code>) hyperparameters</p></li>
</ul>
<figure class="align-default" id="id133">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/fE1Kho.png" src="https://img.zhaoweiguo.com/uPic/2024/12/fE1Kho.png" />
<figcaption>
<p><span class="caption-text">Figure E.2 The LoRA matrices A and B are applied to the layer inputs and are involved in computing the model outputs. The inner dimension <code class="docutils literal notranslate"><span class="pre">r</span></code> of these matrices serves as a setting that adjusts the number of trainable parameters by varying the sizes of A and B.</span><a class="headerlink" href="#id133" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LoRALayer</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">rank</span><span class="p">))</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>  <span class="c1"># similar to standard weight initialization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">))</span>  <span class="c1"># 由于初始时，self.B为0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
<ul class="simple">
<li><p>In the code above, <code class="docutils literal notranslate"><span class="pre">rank</span></code> is a hyperparameter that controls the inner dimension of the matrices <code class="docutils literal notranslate"><span class="pre">A</span></code> and <code class="docutils literal notranslate"><span class="pre">B</span></code></p></li>
<li><p>In other words, this parameter controls the number of additional parameters introduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency</p></li>
<li><p>The second hyperparameter, <code class="docutils literal notranslate"><span class="pre">alpha</span></code>, is a scaling hyperparameter applied to the output of the low-rank adaptation</p></li>
<li><p>It essentially controls the extent to which the adapted layer’s output is allowed to influence the original output of the layer being adapted</p></li>
<li><p>This can be seen as a way to regulate the impact of the low-rank adaptation on the layer’s output</p></li>
<li><p>So far, the <code class="docutils literal notranslate"><span class="pre">LoRALayer</span></code> class we implemented above allows us to transform the layer inputs <code class="docutils literal notranslate"><span class="pre">x</span></code></p></li>
<li><p>However, in LoRA, we are usually interested in replacing existing <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers so that the weight update is applied to the existing pretrained weights, as shown in the figure below</p></li>
</ul>
<figure class="align-default" id="id134">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/LgIwrf.png" src="https://img.zhaoweiguo.com/uPic/2024/12/LgIwrf.png" />
<figcaption>
<p><span class="caption-text">Figure E.3 The integration of LoRA into a model layer. The original pretrained weights (W) of a layer are combined with the outputs from LoRA matrices (A and B), which approximate the weight update matrix (DW). The final output is calculated by adding the output of the adapted layer (using LoRA weights) to the original output.</span><a class="headerlink" href="#id134" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>To incorporate the original <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layer weights as shown in the figure above, we implement a <code class="docutils literal notranslate"><span class="pre">LinearWithLoRA</span></code> layer below that uses the previously implemented LoRALayer and can be used to replace existing <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers in a neural network, for example, the self-attention module or feed forward modules in an LLM</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LinearWithLoRA</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">linear</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">linear</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lora</span> <span class="o">=</span> <span class="n">LoRALayer</span><span class="p">(</span>
            <span class="n">linear</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lora</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>To try LoRA on the GPT model we defined earlier, we define a <code class="docutils literal notranslate"><span class="pre">replace_linear_with_lora</span></code> function to <strong>replace all</strong> <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers in the model with the new <code class="docutils literal notranslate"><span class="pre">LinearWithLoRA</span></code> layers</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="c1"># Replace the Linear layer with LinearWithLoRA</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">LinearWithLoRA</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Recursively apply the same function to child modules</span>
            <span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default" id="id135">
<img alt="https://img.zhaoweiguo.com/uPic/2024/12/4Nov6o.png" src="https://img.zhaoweiguo.com/uPic/2024/12/4Nov6o.png" />
<figcaption>
<p><span class="caption-text">Figure E.4 The architecture of the GPT model. It highlights the parts of the model where <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers are upgraded to <code class="docutils literal notranslate"><span class="pre">LinearWithLoRA</span></code> layers for parameter-efficient fine-tuning.</span><a class="headerlink" href="#id135" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>We then <strong>freeze</strong> the original model parameter and use the <code class="docutils literal notranslate"><span class="pre">replace_linear_with_lora</span></code> to replace the said <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers using the code below</p></li>
<li><p>This will replace the <code class="docutils literal notranslate"><span class="pre">Linear</span></code> layers in the LLM with <code class="docutils literal notranslate"><span class="pre">LinearWithLoRA</span></code> layers</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable parameters before: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable parameters after: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># Total trainable parameters before: 124,441,346</span>
<span class="c1"># Total trainable parameters after: 0</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">replace_linear_with_lora</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">total_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total trainable LoRA parameters: </span><span class="si">{</span><span class="n">total_params</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1"># 输出</span>
<span class="c1"># Total trainable LoRA parameters: 2,666,528</span>
</pre></div>
</div>
<ul class="simple">
<li><p>打印看下现在模型的结构</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">GPTModel</span><span class="p">(</span>
  <span class="p">(</span><span class="n">tok_emb</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">50257</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
  <span class="p">(</span><span class="n">pos_emb</span><span class="p">):</span> <span class="n">Embedding</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="mi">768</span><span class="p">)</span>
  <span class="p">(</span><span class="n">drop_emb</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
  <span class="p">(</span><span class="n">trf_blocks</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
    <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">TransformerBlock</span><span class="p">(</span>
      <span class="p">(</span><span class="n">att</span><span class="p">):</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
        <span class="p">(</span><span class="n">W_query</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">W_key</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">W_value</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">out_proj</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
          <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">dropout</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">ff</span><span class="p">):</span> <span class="n">FeedForward</span><span class="p">(</span>
        <span class="p">(</span><span class="n">layers</span><span class="p">):</span> <span class="n">Sequential</span><span class="p">(</span>
          <span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
            <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
          <span class="p">)</span>
          <span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">GELU</span><span class="p">()</span>
          <span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
            <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">3072</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
          <span class="p">)</span>
        <span class="p">)</span>
      <span class="p">)</span>
      <span class="p">(</span><span class="n">norm1</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
      <span class="p">(</span><span class="n">norm2</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
      <span class="p">(</span><span class="n">drop_resid</span><span class="p">):</span> <span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="o">...</span> <span class="o">...</span> <span class="c1"># 省略11个</span>
  <span class="p">)</span>
  <span class="p">(</span><span class="n">final_norm</span><span class="p">):</span> <span class="n">LayerNorm</span><span class="p">()</span>
  <span class="p">(</span><span class="n">out_head</span><span class="p">):</span> <span class="n">LinearWithLoRA</span><span class="p">(</span>
    <span class="p">(</span><span class="n">linear</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">(</span><span class="n">lora</span><span class="p">):</span> <span class="n">LoRALayer</span><span class="p">()</span>
  <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<ul>
<li><p>同样的调优步骤:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">train_losses</span><span class="p">,</span> <span class="n">val_losses</span><span class="p">,</span> <span class="n">train_accs</span><span class="p">,</span> <span class="n">val_accs</span><span class="p">,</span> <span class="n">examples_seen</span> <span class="o">=</span> <span class="n">train_classifier_simple</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">num_epochs</span><span class="p">,</span> <span class="n">eval_freq</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">eval_iter</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>The resulting accuracy values are:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Training</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">Validation</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">96.64</span><span class="o">%</span>
<span class="n">Test</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">98.00</span><span class="o">%</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>结论：However, the slightly lower validation and test accuracies (96.64% and 97.33%, respectively) suggest a small degree of overfitting, as the model does not generalize quite as well on unseen data compared to the training set. Overall, the results are very impressive, considering we fine-tuned only a relatively small number of model weights (2.7 million LoRA weights instead of the original 124 million model weights).</p>
</div>
</section>
</section>
<section id="id8">
<h2>其他<a class="headerlink" href="#id8" title="此标题的永久链接">¶</a></h2>
<figure class="align-default" id="id136">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/transformer_decoding_1.gif" src="https://img.zhaoweiguo.com/uPic/2024/11/transformer_decoding_1.gif" />
<figcaption>
<p><span class="caption-text">The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of <code class="docutils literal notranslate"><span class="pre">attention</span> <span class="pre">vectors</span> <span class="pre">K</span> <span class="pre">and</span> <span class="pre">V</span></code>. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:</span><a class="headerlink" href="#id136" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id137">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/transformer_decoding_2.gif" src="https://img.zhaoweiguo.com/uPic/2024/11/transformer_decoding_2.gif" />
<figcaption>
<p><span class="caption-text">The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</span><a class="headerlink" href="#id137" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id138">
<img alt="https://img.zhaoweiguo.com/uPic/2024/11/1GKHaZ.png" src="https://img.zhaoweiguo.com/uPic/2024/11/1GKHaZ.png" />
<figcaption>
<p><span class="caption-text">The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. The <code class="docutils literal notranslate"><span class="pre">softmax</span></code> layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</span><a class="headerlink" href="#id138" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Dive_into_Deep_Learning.html" class="btn btn-neutral float-right" title="动手学深度学习(Dive into Deep Learning)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../scikit-learn.html" class="btn btn-neutral" title="scikit-learn 1.3.2" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'V2025.06',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="../../../None"></script>
      <script type="text/javascript" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>