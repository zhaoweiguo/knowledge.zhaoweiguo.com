

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>2312.14132_DUSt3R: Geometric 3D Vision Made Easy &mdash; 新溪-gordon V2025.07 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R" href="2406.09756_MASt3R.html" />
    <link rel="prev" title="2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish" href="2203.08586_VanishingPointEstimation.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../0normal.html">通用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../0normals/normal.html">通用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/normal.html#id3">如何看一个论文是不是重要</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../0normals/website.html">学术网站</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id2">整体分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id3">1. 学术搜索平台（核心功能：检索与发现文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#google-scholar">Google Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#semantic-scholar">Semantic Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#web-of-science">Web of Science</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#id4">百度学术</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id5">2. 资源共享平台（核心功能：免费获取付费文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#sci-hub">Sci-Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#library-genesis-libgen">Library Genesis (LibGen)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#unpaywall">Unpaywall</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id6">论文数据库（核心功能：存储与提供文献原文）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#acl-anthology">ACL Anthology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#arxiv">ArXiv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#cnki">知网 CNKI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#id10">万方数据库</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Benchmarking.html">评测基准</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id3">评测基准</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html">02xx.xxxxx_BLEU: a Method for Automatic Evaluation of Machine Translation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id8">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-baseline-bleu-metric">2.The Baseline BLEU Metric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-bleu-evaluation">3.The BLEU Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-human-evaluation">4.The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#bleu-vs-the-human-evaluation">5.BLEU vs The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html">0401.xxxxx_ROUGE: A Package for Automatic Evaluation of Summaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-n-n-gram-co-occurrence-statistics">2.ROUGE-N: N-gram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-l-longest-common-subsequence">3.ROUGE-L: Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-w-weighted-longest-common-subsequence">4 ROUGE-W: Weighted Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-s-skip-bigram-co-occurrence-statistics">5.ROUGE-S: Skip-Bigram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#evaluations-of-rouge">6 Evaluations of ROUGE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#conclusions">7 Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html">1803.01937_ROUGE2.0: Updated and Improved Measures for Evaluation of Summarization Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#problems-with-the-current-rouge-measures">1. Problems with the current ROUGE measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#rouge-2-0">2. ROUGE 2.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html">1804.08771_SacreBLEU: A Call for Clarity in Reporting BLEU Scores</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#bleu">BLEU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#id3">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#problem-description">2 Problem Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#a-way-forward">3 A way forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#summary">4 Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html">2306.05685_Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#mt-bench-and-chatbot-arena">2 MT-Bench and Chatbot Arena</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm-as-a-judge">3 LLM as a Judge</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#agreement-evaluation">4 Agreement Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#human-preference-benchmark-and-standardized-benchmark">5 Human Preference Benchmark and Standardized Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-a-prompt-templates">Appendix A Prompt templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-b-case-study">Appendix B Case Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-c-data-collection">Appendix C Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-d-additional-experimental-results">Appendix D Additional Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-e-training-details-of-vicuna-models">Appendix E Training Details of Vicuna Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-f-exploring-vicuna-as-a-judge">Appendix F Exploring Vicuna as a judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#agent">数据集-Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html">2312.14033_T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#t-eval">2 T-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-a-t-eval-benchmark-details">Appendix A T-Eval Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-c-detailed-evaluation-metrics">Appendix C Detailed Evaluation Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-d-api-documentation">Appendix D API Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html">2406.12045_τ-bench: A Benchmark for Tool-Agent-User</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#bench-a-benchmark-for-t-ool-a-gent-u-ser-interaction">3.τ-bench: A benchmark for T ool-A gent-U ser Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#benchmark-construction">4. Benchmark Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#experiments">5.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#disscussion">6.Disscussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html">2506.07982_𝜏²-Bench: Evaluating Conversational Agents in a Dual-Control Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#tau-2-bench-evaluating-agents-in-a-dual-control-environment">3 <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench: Evaluating Agents in a Dual-Control Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#broader-impact">Broader Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-a-telecom-domain">Appendix A Telecom Domain</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-b-verifying-original-tau-2-bench">Appendix B Verifying Original <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-d-domain-policies">Appendix D Domain Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-e-user-simulator-quality">Appendix E User Simulator Quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#qa">数据集-QA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html">1809.09600_HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#data-collection">2 Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#processing-and-benchmark-settings">3 Processing and Benchmark Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#dataset-analysis">4 Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#conclusions">7 Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-a-data-collection-details">Appendix A Data Collection Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#a">附录A 数据收集细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-b-further-data-analysis">Appendix B Further Data Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-c-full-wiki-setting-details">Appendix C Full Wiki Setting Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html">2109.07958_TruthfulQA: Measuring How Models Mimic Human Falsehoods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#the-truthfulqa-benchmark">2 The TruthfulQA Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#ethics-and-impact">8 Ethics and Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-a-additional-examples-from-truthfulqa">Appendix A Additional examples from TruthfulQA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-b-additional-results">Appendix B Additional results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-c-dataset-construction">Appendix C Dataset construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-d-human-evaluations">Appendix D Human evaluations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-e-prompts">Appendix E Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-f-checking-for-data-quality-and-disagreement">Appendix F Checking for data quality and disagreement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html">2311.12022_GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#data-collection">2.Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#dataset-analysis">3.Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#baseline">4.Baseline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#related-work">5.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html">2411.04368_SimpleQA: Measuring short-form factuality in large language models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#data-collection-and-verification">2.Data Collection and Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#measuring-calibration">4.Measuring calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#appendix-b-guessing-strategy-and-f-score">Appendix B Guessing strategy and F-score</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id4">数据集-编程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html">2107.03374_HumanEval: Evaluating Large Language Models Trained on Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#evaluation-framework">2.Evaluation Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#code-fine-tuning">3.Code Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#supervised-fine-tuning">4.Supervised Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#docstring-generation">5.Docstring Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#broader-impacts-and-hazard-analysis">7.Broader Impacts and Hazard Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#conclusions">9.Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html">2108.07732_MBPP: Program Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#datasets">2 Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#model-and-methods">3 Model and Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mbpp-synthesis-results">4 MBPP Synthesis Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#human-model-collaboration-results">5 Human-Model Collaboration Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#program-execution-results">6 Program Execution Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mathqa-results">7 MathQA Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#related-work">8 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#risks-and-limitations">9 Risks and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#conclusion">10 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html">2310.06770_SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id7">2 SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#swe-llama-fine-tuning-codellama-for-swe-bench">3 SWE-Llama: Fine-tuning CodeLlama for SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#experimental-setup">4 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#discussion">7 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#ethics-statement">8 Ethics Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#reproducibility-statement">9 Reproducibility Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-a-benchmark-details">Appendix A Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-b-additional-details-on-training-swe-llama">Appendix B Additional Details on Training SWE-Llama</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-c-additional-results">Appendix C Additional Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-d-additional-experimental-details">Appendix D Additional Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-e-societal-impact">Appendix E Societal Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-f-in-depth-analysis-of-swe-llama-generations">Appendix F In-depth Analysis of SWE-Llama Generations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html">2402.16694_HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#a-multilingual-code-generation-benchmark-for-cross-lingual-natural-language-generalization">A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#introduction">1.   Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#related-work">2.   Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#humaneval-xl">3.   HumanEval-XL</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#experiments">4.   Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#conclusion">5.   Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-a-experiment-settings">Appendix A Experiment Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-b-comprehensive-experiment-results">Appendix B Comprehensive Experiment Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html">2403.07974_LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#holistic-evaluation">2 Holistic Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#benchmark-curation">3 Benchmark Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#experiment-setup">4 Experiment Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-b-ui">Appendix B UI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-d-results">Appendix D Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-e-qualitative-examples">Appendix E Qualitative Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html">2407.10499_CIBench: Evaluating Your LLMs with a Code Interpreter Plugin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#cibench">3 CIBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-b-construction-prompts-and-rules">Appendix B Construction Prompts and Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-c-experiment-example-demo">Appendix C Experiment Example Demo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-d-subjective-visualization-evaluation">Appendix D Subjective Visualization Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-e-dataset-error-analysis">Appendix E Dataset Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-f-human-annotator">Appendix F Human Annotator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-g-ethical-consideration">Appendix G Ethical Consideration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html">2410.03859_SWE-bench-Multimodal: Do AI Systems Generalize to Visual Software Domains?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#swe-bench-multimodal">2 SWE-bench Multimodal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#evaluating-on-swe-bench-m">3 Evaluating on SWE-bench M</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-b-collection">Appendix B Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-c-experiments">Appendix C Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-d-human-validation">Appendix D Human Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-e-limitations">Appendix E Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html">2410.06992_SWE-Bench+: Enhanced Coding Benchmark for LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-analysis-of-swe-bench">2 Robustness Analysis of SWE-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#building-swe-bench">3 Building SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-of-swe-bench">4 Robustness of SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#effectiveness-aware-evaluation">5 Effectiveness-aware Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#conclusion">7 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html">2501.01257_CodeForces: Benchmarking Competition-level Code Generation of LLMs on CodeForces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#codeforces-benchmark">3 CodeForces Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#evaluation-on-existing-llms">4 Evaluation on Existing LLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#analysis-experiments">5 Analysis Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#ethical-statement">8 Ethical Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-a-model-cards">Appendix A Model Cards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-b-decoding-hyperparameters">Appendix B Decoding Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-c-analysis-of-our-elo-rating-calculation-system">Appendix C Analysis of Our Elo Rating Calculation System</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-d-human-comparable-elo-rating">Appendix D Human-comparable Elo Rating</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-e-problem-demonstration">Appendix E Problem Demonstration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-f-special-judge">Appendix F Special Judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id5">数据集-长文本</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html">2402.05136_LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#lv-eval-benchmark">3 LV-Eval Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-c-detailed-evaluation-results">Appendix C Detailed Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-d-detailed-ablation-results">Appendix D Detailed Ablation Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html">2402.17753_LoCoMo: Evaluating Very Long-Term Conversational Memory of LLM Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#generative-pipeline-for-locomo">3 Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#locomo-evaluation-benchmark">4 LoCoMo Evaluation Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-setup">5 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-results">6 Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#broader-impacts">9 Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-overview">Appendix Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-a-generative-pipeline-for-locomo">Appendix A Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-b-dataset">Appendix B Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-d-results">Appendix D Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html">2404.06654_RULER: What’s the Real Context Size of Your Long-Context Language Models?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#the-ruler-benchmark">3 The Ruler Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#experiments-results">4 Experiments &amp; Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#task-error-analysis">5 Task Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#model-analysis">6 Model Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-a-models">Appendix A Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-b-task-configurations">Appendix B Task Configurations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-c-task-correlation-analysis">Appendix C Task Correlation Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-d-prompt-templates">Appendix D Prompt Templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-e-passkey-retrieval-and-vanilla-niah-results">Appendix E Passkey Retrieval and Vanilla NIAH Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-f-additional-results">Appendix F Additional Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html">2407.11963_NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#tasks-and-datasets">3 Tasks and Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#impact-of-language-which-model-performs-better-under-the-bilingual-scenario">4.1.5 Impact of Language_ Which Model Performs Better under the Bilingual Scenario_</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#conclusion-and-future-work">5 Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-a-evaluated-models">Appendix A Evaluated Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-b-needlebench-prompt-examples">Appendix B NeedleBench Prompt Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-c-error-analysis-examples">Appendix C Error Analysis Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id6">数据集-数学</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2103.03874_MATH.html">2103.03874_MATH: Measuring Mathematical Problem Solving With the MATH Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html">2110.14168_GSM8K: Training Verifiers to Solve Math Word Problems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#dataset">2 Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#related-work">3 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#methods">4 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#additional-experiments">5 Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-b-hyperparameters">Appendix B Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-c-calculator-annotations">Appendix C Calculator Annotations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-d-example-model-solutions">Appendix D Example Model Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-e-verifier-details">Appendix E Verifier Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-f-verifier-visualization">Appendix F Verifier Visualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html">2405.12209_MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#experiments-and-analysis">3 Experiments and Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#ethical-considerations">8 Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-a-mathbench-statistics">Appendix A MathBench Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-b-detailed-experimental-results">Appendix B Detailed Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-c-extra-analysis">Appendix C Extra Analysis</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id7">数据集-图片</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html">2306.13394_MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#mme-evaluation-suite">2 MME Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#analysis">4 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html">2307.06281_MMBench: Is Your Multi-modal Model an All-around Player?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#the-construction-of-mmbench">3 The construction of MMBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-strategy">4 Evaluation Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-results">5 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-a-more-details-about-the-data">Appendix A More Details about the Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-b-more-details-on-mmbench-construction">Appendix B More Details on MMBench Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-c-more-details-on-llm-based-choice-extraction">Appendix C More Details on LLM-based Choice Extraction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-d-evaluation-settings-and-results">Appendix D Evaluation Settings and Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html">2307.16125_SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id7">3 SEED-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#evaluation-results">4 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html">2311.12793_ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-dataset">3 ShareGPT4V Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-7b-model">4 ShareGPT4V-7B Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id11"><strong>4.1 模型架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id12"><strong>4.2 预训练</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sft"><strong>4.3 监督微调（SFT）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-a-data-sources">Appendix A Data Sources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-b-caption-analysis">Appendix B Caption Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-d-examples">Appendix D Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html">2506.18095_ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#sharegpt-4o-image">2 ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#janus-4o-fine-tuning-with-sharegpt-4o-image">3 Janus-4o: Fine-Tuning with ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#conclusion">5 conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-b-image-generation-categories">Appendix B Image Generation Categories</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-c-prompts-for-generation">Appendix C Prompts for Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-d-document-pipeline">Appendix D Document Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-e-ethical-considerations-and-societal-impact">Appendix E Ethical Considerations and Societal Impact</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id8">数据集</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#id2">评测标准</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#accuracy">准确率(Accuracy)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#precision">精确率(Precision, 精准率)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#recall">召回率(Recall)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#f1-score">F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#id3">可视化精度和召回率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html">2009.03300_MMLU: Measuring Massive Multitask Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#a-multitask-test">3.A Multitask Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html">2305.08322_C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#id2">C-Eval_ A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#the-c-eval-evaluation-suite">2 The C-Eval Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#experiment">3 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-a-author-contributions">Appendix A Author Contributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-b-detailed-stats-of-c-eval">Appendix B Detailed Stats of C-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-c-explanation-data-generation">Appendix C Explanation Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-d-evaluation-prompts">Appendix D Evaluation Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-e-details-of-the-models-being-evaluated">Appendix E Details of the models being evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-f-breakdown-of-model-performance">Appendix F Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-g-option-bias">Appendix G Option Bias</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-h-compute-and-resources-used-for-evaluation">Appendix H Compute and Resources Used for Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html">2306.09212_CMMLU: Measuring massive multitask language understanding in Chinese</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#cmmlu">3 CMMLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#impact-of-model-size-on-performance">Impact of model size on performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-a-comparison-to-concurrent-benchmarks">Appendix A Comparison to concurrent benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-b-cmmlu-subjects">Appendix B CMMLU Subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-c-cmmlu-examples">Appendix C CMMLU Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-d-cmmlu-difficulty-distribution">Appendix D CMMLU Difficulty Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-e-emergent-ability-shown-in-cmmlu-subjects">Appendix E Emergent Ability shown in CMMLU subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-f-models-being-evaluated">Appendix F Models being Evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-g-strategies-for-estimating-model-choices">Appendix G Strategies for Estimating Model Choices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-h-regular-expressions-matching-algorithmsl">Appendix H Regular expressions matching algorithmsl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-i-correlation-to-other-benchmarks">Appendix I Correlation to other Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-j-breakdown-of-model-performance">Appendix J Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#j-3-the-effect-of-chain-of-thought-prompt">J.3 The effect of chain-of-thought prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html">2307.15020_SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#superclue-benchmark">3 SuperCLUE Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#additional-analysis">5 Additional Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-a-evaluation-process">Appendix A Evaluation Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-b-capability-categories">Appendix B Capability Categories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html">2311.12983_GAIA: a benchmark for General AI Assistants</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#related-work">2.Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#id4">3.GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#llms-results-on-gaia">4.LLMs results on GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-c-extended-description-of-gaia">Appendix C Extended description of GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-d-extended-description-of-our-question-design-framework">Appendix D Extended description of our question design framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html">2404.07972_OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-environment">2. OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-benchmark">3. OSWORLD Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#benchmarking-llm-and-vlm-agent-baselines">4. Benchmarking LLM and VLM Agent Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#analysis">5. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#a-details-of-osworld-environment">A. Details of OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#c-details-of-baseline-methods">C. Details of Baseline Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#d-examples-of-qualitative-analysis">D. Examples of Qualitative Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html">2501.14249_HLE: Humanity’s Last Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#dataset">3.Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#evaluation">4.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#discussion">5.Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">LLM 模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#nlp">NLP 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html">1810.04805_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#bert">3 BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#appendix-a-additional-details-for-bert">Appendix A Additional Details for BERT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html">18xx_GPT1: Improving Language Understanding by Generative Pre-Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#framework">3. Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#analysis">5 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#id3">引文口碑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#id4">要点解读</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html">19xx_GPT2: Language Models are Unsupervised Multitask Learners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html#the-illustrated-gpt-2">The Illustrated GPT-2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2012.00413_CPM.html">2012.00413_CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2302.13971_LLaMA.html">2302.13971_LLaMA: Open and Efficient Foundation Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2307.09288_Llama2.html">2307.09288_Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html">2309.16609_Qwen Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#pretraining">2. Pretraining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#alignment">3. Alignment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#code-qwen-specialized-model-for-coding">4. CODE-QWEN: SPECIALIZED MODEL FOR CODING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#math-qwen-specialized-model-for-mathematics-reasoning">5. MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-1-more-training-details">A.1 MORE TRAINING DETAILS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-2-evaluation">A.2 EVALUATION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html">2310.19341_Skywork: A More Open Bilingual Foundation Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#limitation">6 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-a-details-on-gpt-7b-vs-llama-7b-experiment">Appendix A Details on GPT-7B vs. LLaMA-7B Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-b-preliminary-experiments-on-distributed-training">Appendix B Preliminary Experiments on Distributed Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-c-more-benchmark-results">Appendix C More Benchmark Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-d-details-on-lm-test-sets">Appendix D Details on LM Test Sets</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2401.14196_DeepSeek-Coder.html">2401.14196_DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html">2404.06395_MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#two-stage-pre-training-strategy">5. Two Stage Pre-training Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#model">6. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#minicpm-family">7 MiniCPM Family</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2405.04434_DeepSeek-V2.html">2405.04434_DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2406.12793_ChatGLM.html">2406.12793_ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html">2407.10671_Qwen2 Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#tokenizer-model">2. Tokenizer &amp; Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html">2412.15115_Qwen2.5</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#architecture-and-tokenizer">2. Architecture and Tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html">2505.09388_Qwen3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id2">多模态模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html">2112.15093_CTR: Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#datasets">3. Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#baselines">4. Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#an-empirical-study">5. An Empirical Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-a-details-of-prab">Appendix A Details of PRAB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-c-visualization-of-failure-cases">Appendix C Visualization of Failure Cases.</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html">2304.08485_LLaVA: Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#gpt-assisted-visual-instruction-data-generation">3. GPT-assisted Visual Instruction Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#visual-instruction-tuning">4. Visual Instruction Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html">2308.12966_Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#methodology">Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#evaluation">Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#b-data-format-details-of-training">B. Data Format Details of Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html">2310.03744_LLaVA2: Improved Baselines with Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#empirical-evaluation">4. Empirical Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#open-problems-in-lmms">5. Open Problems in LMMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#a-implementation-details">A. Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#b-qualitative-results">B. Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html">2312.07533_VILA: On Pre-training for Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#on-pre-training-for-visual-language-models">3. On Pre-training for Visual Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html">2403.05525_DeepSeek-VL: Towards Real-World Vision-Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html">2408.01800_MiniCPM-V: A GPT-4V Level MLLM on Your Phone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#model-architecture">3. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#end-side-deployment">5. End-side Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#experiments">6. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html">2409.17146_Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#data">3. Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#ablations">6. Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-a-model-details">Appendix A: Model Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-b-training-details">Appendix B: Training Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-c-evaluation-results">Appendix C: Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-d-result-details">Appendix D: Result Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-e-ablations-details">Appendix E Ablations Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-f-data-details">Appendix F Data Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-g-dataset-examples">Appendix G Dataset Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-h-related-work">Appendix H Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html">2410.13848_Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#janus-a-simple-unified-and-flexible-multimodal-framework">3 Janus: A Simple, Unified and Flexible Multimodal Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-a-details-of-semantic-tokenizer-mentioned-in-ablation-study">Appendix A Details of Semantic Tokenizer Mentioned in Ablation Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-b-additional-qualitative-results">Appendix B Additional Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html">2411.00774_Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#model">2. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#experience">3. Experience</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#conclusion-and-future-work">4. Conclusion and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html">2412.04468_NVILA: Efficient Frontier Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#more-capabilities">4. More Capabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html">2502.13923_Qwen2.5-VL</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html">2503.20215_Qwen2.5-Omni Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#archtecture">2. Archtecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#id2">3 预训练</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#post-training">4 后训练（Post-training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#id4">3. Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#results-and-analyses">5. Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#id9">3 Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#data-construction">3.2.1 Data Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#results-and-analyses">5 Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-c-case-study">Appendix C Case Study</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id3">LLM 音频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html">2005.08100_Conformer: Convolution-augmented Transformer for Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#conformer-encoder">2 Conformer Encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#conclusion">4 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html">2106.07447_HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#i-introduction">I Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#ii-method">II Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#iii-related-work">III Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#iv-experimental-details">IV Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#v-results">V Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#vi-conclusion">VI Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html">2112.02418_YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#id1">关键概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#yourtts-model">2. YourTTS Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#results-and-discussion">4. Results and Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#zero-shot-voice-conversion">5. Zero-Shot Voice Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#speaker-adaptation">6. Speaker Adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#conclusions-limitations-and-future-work">7. Conclusions, limitations and future work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html">2212.04356_whisper: Robust Speech Recognition via Large-Scale Weak Supervision</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#analysis-and-ablations">4. Analysis and Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#limitations-and-future-work">6. Limitations and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#conclusions">7. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#a-evaluation-datasets">A. Evaluation Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#b-compared-models">B Compared Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#c-text-standardization">C. Text Standardization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html">2301.02111_Vall-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#background-speech-quantization">3. Background: Speech Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#id9">4. VALL-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#conclusion-limitations-and-future-work">6. Conclusion, Limitations, and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html">2303.03926_VALL-E_X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#cross-lingual-codec-language-model">3 Cross-Lingual Codec Language Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#vall-e-x-application">4. VALL-E X Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#a-appendix">A. Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html">2406.05370_VALL-E2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#id5">3. VALL-E 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html">2407.05407_CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#cosyvoice-a-scalable-tts-model-using-supervised-semantic-tokens">2. CosyVoice: A Scalable TTS model using Supervised Semantic Tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#dataset">3. Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#experimental-settings">4. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html">2407.10759_Qwen2-Audio Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html">2410.00037_Moshi: a speech-text foundation model for real-time dialogue</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#model">3.Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#datasets-and-training">4. Datasets and Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#safety">6.Safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html">2412.10117_CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#instroduction">1. Instroduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#id5">2. CosyVoice 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-settings">3. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-results">4. Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html">2501.06282_MinMo: A Multimodal Large Language Model for Seamless Voice Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#instruction">1.Instruction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#id9">3.MinMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#conclusion">5.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#a-prompts-for-voice-understanding-tasks">A. Prompts for Voice Understanding Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html">2505.02707_Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#voila-voice-language-foundation-models">3. Voila: Voice-Language Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html">2505.17589_CosyVoice3: Towards In-the-wild Speech Generation via Scaling-up and Post-training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#id3">2.CosyVoice 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#the-multilingual-data-pipeline">3.The Multilingual Data Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-settings">4.Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-results">5.Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#conclusion">6.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#limitations">7.Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id4">LLM 视频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html">2301.12597_BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#method">3 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#experiment">4 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#limitation">5 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#conclusion">6 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html">2308.01390_OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#id1">OpenFlamingo_ An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#related-work">2 Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#approach">3 Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-a-extended-results">Appendix A Extended results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-b-additional-notes-on-filtering-mmc4">Appendix B Additional notes on filtering MMC4</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-c-synthetic-data-prompt">Appendix C Synthetic data prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-d-image-credits">Appendix D Image credits</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#llm-moe">LLM MoE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMoEs/2408.15664_AUXILIARY-LOSS-FREE_LB.html">2408.15664_AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMoEs/2410.07490_MoDEM.html">2410.07490_MoDEM: Mixture of Domain Expert Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id5">商业模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2303.08774_GPT4.html">2303.08774_GPT-4 Technical Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html">2312.11805_Gemini: A Family of Highly Capable Multimodal Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#model-architecture">2. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#training-infrastructure">3. Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#post-training-models">6. Post-Training Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#responsible-deployment">7. Responsible Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#discussion-and-conclusion">8. Discussion and Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2403.05530_Gemini1.5.html">2403.05530_Gemini1.5: Unlocking multimodal understanding across millions of tokens of context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html">2406.02430_Seed-TTS: A Family of High-Quality Versatile Speech Generation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#method">2 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-extensions">4 Model extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-applications-limitations-and-safety">5 Model applications, limitations, and safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#authors-alphabetical-order">6 Authors (alphabetical order)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#acknowledgement">7 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html">2407.04675_Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#motivation">2 Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#methods">3 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#model-and-evaluation">4 Model and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2503.20020_Gemini2.html">2503.20020_Gemini2: Gemini Robotics: Bringing AI into the Physical World</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2504.xxxxx_Seed-Thinking-v1.5.html">2504.xxxxx_Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html">2505.07062_Seed1.5-VL Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#id1">Seed1.5-VL Technical Report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#architecture">2 Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-recipe">3.2 Training Recipe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#post-training">4 Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#hybrid-reinforcement-learning">4.4 Hybrid Reinforcement Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-infrastructure">5 Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation">6 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#video-task-evaluation">6.1.3 Video Task Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#comparison-with-state-of-the-arts">6.3.2 Comparison with State-of-the-arts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#conclusion-and-next-steps">7 Conclusion and Next Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#contributions-and-acknowledgments">8 Contributions and Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#qualitative-examples">9 Qualitative examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#visual-reasoning-visual-pattern-recognition">9.7 Visual Reasoning_ Visual Pattern Recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#failure-cases-combinatorial-search-i">9.19 Failure Cases_ Combinatorial Search I</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation-details">10 Evaluation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#dream-1k">DREAM-1K</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM_tech.html">LLM 周边技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#framework">Framework</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html">1712.05889_Ray: A Distributed Framework for Emerging AI Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#motivation-and-requirements">2. Motivation and Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#programming-and-computation-model">3. Programming and Computation Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#architecture">4. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#discussion-and-experiences">7 Discussion and Experiences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1712.05889_Ray.html#conclusion">8. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html">1910.02054_DeepSpeed_ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#extended-introduction">1. Extended Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#where-did-all-the-memory-go">3 Where Did All the Memory Go?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#zero-insights-and-overview">4 ZeRO: Insights and Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-dp">5 Deep Dive into ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-r">6 Deep Dive into ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-dp">7 Communication Analysis of ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-r">8. Communication Analysis of ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#step-towards-1-trillion-parameters">9. Step Towards 1 Trillion Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#implementation-and-evaluation">10. Implementation and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#concluding-remarks">11. Concluding Remarks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/19XX_PyTorch.html">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/20XX_Transformers.html">Transformers: State-of-the-Art Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html">2210.XX_Ray v2 Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#architecture-overview">Architecture Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#object-management">Object Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#task-management">Task Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#resource-management-and-scheduling">Resource Management and Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#actor-management">Actor management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#global-control-service">Global Control Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#cluster-management">Cluster Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2210.XX_Ray_v2.html#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html">2309.06180_vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#memory-challenges-in-llm-serving">3. Memory Challenges in LLM Serving</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#method">4. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#implementation">5. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#evaluation">6. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#ablation-studies">7. Ablation Studies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Frameworks/2309.06180_vLLM.html#conclusion">10. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id2">大模型调优</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2101.00190_Prefix-Tuning.html">2101.00190_Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2103.10385_p-tuning.html">2103.10385_p-tuning: GPT Understands, Too</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2104.08691_Prompt_Tuning.html">2104.08691_Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2106.09685_LoRA.html">2106.09685_LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2401.01335_Self-Play.html">2401.01335_Self-Play: Fine-Tuning Converts Weak Language Models to Strong Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2402.09353_DoRA.html">2402.09353_DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2402.12354_LoRA%2B.html">2402.12354_LoRA+: Efficient Low Rank Adaptation of Large Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.03507_GaLore.html">2403.03507_GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.13372_LlamaFactory.html">2403.13372_LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#id2">竞争框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#efficient-fine-tuning-techniques">3. Efficient Fine-Tuning Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#llamafactory-framework">4 LlamaFactory Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#conclusion-and-future-work">6 Conclusion and Future Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id3">分布式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/1701.06538_MoE.html">1701.06538_MoE: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html">1806.03377_PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#background-related-work">2. Background &amp; Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#parallel-training-in-pipedream">3. Parallel Training in PipeDream</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#implementation">4. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1806.03377_PipeDream.html#evaluation">5. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html">1811.06965_GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#the-gpipe-library">2. The GPipe Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#performance-analyses">3. Performance Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#image-classification">4. Image Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#massive-massively-multilingual-machine-translation">5. Massive Massively Multilingual Machine Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1811.06965_GPipe.html#design-features-and-trade-offs">6. Design Features and Trade-Offs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html">1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#background-and-challenges">2. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#model-parallel-transformers">3. Model Parallel Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html">19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#background-and-related-work">2. BACKGROUND AND RELATED WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#pipeline-parallelism">3. 流水线并行(PIPELINE PARALLELISM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#id5">4. 实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/1910_PipeDream2.html#id6">6. 结论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2006.09503_PipeDream-2BW.html">2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2006.09503_PipeDream-2BW.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2006.15704DataParallel.html">2006.15704_PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2006.16668_GShard.html">2006.16668_GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2104.04473_Megatron-LM2.html">2104.04473_Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2104.04473_Megatron-LM2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html">2205.14135_FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#background">2 Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#flashattention-algorithm-analysis-and-extensions">3. FLASHATTENTION: Algorithm, Analysis, and Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#limitations-and-future-directions">5. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-b-algorithm-details">Appendix B Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-c-proofs">Appendix C Proofs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-d-extension-details">Appendix D Extension Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html">2307.08691_FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#flashattention-2-algorithm-parallelism-and-work-partitioning">3. FlashAttention-2: Algorithm, Parallelism, and Work Partitioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#empirical-validation">4. Empirical Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#discussion-and-future-directions">5. Discussion and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Parallelism/normal.html">通用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id4">LLM 量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/0normal.html#id2">混合精度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/0normal.html#id3">浮点数格式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/0normal.html#weight-only-quantization">weight-only quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html">2110.02861_bitsandbytes: 8-bit Optimizers via Block-wise Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#background">1. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#bit-optimizers">2. 8-bit Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#bit-vs-32-bit-optimizer-performance-for-common-benchmarks">3. 8-bit vs 32-bit Optimizer Performance for common Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#analysis">4. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#related-work">5. Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html">2206.01861_ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#relative-work">2. Relative Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#background-and-challenges">3. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#methodology">4. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#results">5. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#appendix-a-background">Appendix A Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#appendix-d-details-about-system-optimization">Appendix D Details about System Optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html">2206.09557_LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#design-methodology-of-lut-gemm">3. Design Methodology of LUT-GEMM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#accelerating-quantized-opt-175b">5. Accelerating Quantized OPT-175B</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#appendix-a-llm-inference-latency-breakdown">Appendix A LLM Inference Latency Breakdown</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#appendix-b-detailed-implementation">Appendix B Detailed Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html">2208.07339_LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#id1">相关参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#int8-matrix-multiplication-at-scale">3. Int8 Matrix Multiplication at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#emergent-large-magnitude-features-in-transformers-at-scale">4. Emergent Large Magnitude Features in Transformers at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#discussion-and-limitations">6. Discussion and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#broader-impacts">7. Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2208.07339_LLM.int8.html#id17">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html">2209.05433_FP8: FP8 Formats For Deep Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#aspects-of-fp8-usage-in-deep-learning">2. Aspects of FP8 Usage in Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#fp8-binary-interchange-format">3. FP8 Binary Interchange Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#id3">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#empirical-results">4. Empirical Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2209.05433_FP8.html#conclusions">5. Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html">2210.17323_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#background">3. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#the-gptq-algorithm">4. The GPTQ Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#experimental-validation">5. Experimental Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2210.17323_GPTQ.html#summary-and-limitations">6. Summary and Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html">2211.10438_SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#review-of-quantization-difficulty">3. Review of Quantization Difficulty</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#id9">4. SmoothQuant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#appendix-a-discussion-on-weight-only-quantization">Appendix A. Discussion on Weight-Only Quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html">2305.14314_QLoRA: Efficient Finetuning of Quantized LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#qlora-finetuning">3. QLoRA Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#qlora-vs-standard-finetuning">4. QLoRA vs. Standard Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#pushing-the-chatbot-state-of-the-art-with-qlora">5. Pushing the Chatbot State-of-the-art with QLoRA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#qualitative-analysis">6. Qualitative Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#related-work">7. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2305.14314_QLoRA.html#limitations-and-discussion">8. Limitations and Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html">2306.00978_AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#awq-activation-aware-weight-quantization">3. AWQ: Activation-aware Weight Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#tinychat-mapping-awq-onto-edge-platforms">4. TinyChat: Mapping AWQ onto Edge Platforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2306.00978_AWQ.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html">2309.05516_AutoRound: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#methodology">3. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/Quantizations/2309.05516_AutoRound.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id5">LLM 安全</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/Securitys/2312.06674_Llama_Guard.html">2312.06674_Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id6">LLM强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/RLs/1703.03864_EvolutionStrategies.html">1703.03864_Evolution Strategies: as a Scalable Alternative to Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html">2504.02495_DeepSeek-GRM: Inference-Time Scaling for Generalist Reward Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#self-principled-critique-tuning-spct">3. Self-Principled Critique Tuning (SPCT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#inference-time-scaling-with-spct">4. Inference-Time Scaling with SPCT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#results-on-reward-modeling-benchmarks">5. Results on Reward Modeling Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#a-additional-related-work">A. Additional Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#b-limitations-and-future-directions">B. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#g-prompt-templates">G. Prompt Templates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/RLs/2504.13958_ToolRL.html">2504.13958_ToolRL: Reward is All Tool Learning Needs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id7">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html">2203.02155_Training language models to follow instructions with human feedback(InstructGPT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#methods-and-experimental-details">3. Methods and experimental details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#discussion">5. Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#appendix-a-additional-prompt-data-details">Appendix A Additional prompt data details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#appendix-b-additional-human-data-collection-details">Appendix B Additional human data collection details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#appendix-c-additional-model-details">Appendix C Additional model details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2203.02155_InstructGPT.html#appendix-d-automatic-evaluation-details">Appendix D Automatic evaluation details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html">2305.20050_Let’s Verify Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id2">1. 研究背景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id3">2. 监督方法对比</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id4">3. 核心发现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html">2408.03314_Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#how-to-scale-test-time-computation-optimally">3. How to Scale Test-Time Computation Optimally</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#scaling-test-time-compute-via-verifiers">5. Scaling Test-Time Compute via Verifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#refining-the-proposal-distribution">6. Refining the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#id7">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html">2412.14135_Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#fromgpt">FromGPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id2">3. Policy Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id3">4. Reward Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id5">5. Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id8">6. Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#open-source-o1-project">7 Open-source o1 Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#future-directions">8. Future Directions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ML.html">机器学习</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ML.html#ml-vision">ML Vision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1506.02640_YOLO.html">1506.02640_You Only Look Once: Unified, Real-Time Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/1506.02640_YOLO.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1612.08242_YOLO9000.html">1612.08242_YOLO9000: Better, Faster, Stronger</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/1612.08242_YOLO9000.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1804.02767_YOLOv3.html">1804.02767_YOLOv3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2004.10934_YOLOv4.html">2004.10934_YOLOv4: Optimal Speed and Accuracy of Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2004.10934_YOLOv4.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html">2205.00159_SVTR: Scene Text Recognition with a Single Visual Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#method">2. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2207.02696_YOLOv7.html">2207.02696_YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2207.02696_YOLOv7.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2303.05499_GroundingDINO.html">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2304.08485_VisualInstructionTuning.html">2304.08485_Visual Instruction Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2402.13616_YOLOv9.html">2402.13616_YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2402.13616_YOLOv9.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2405.14458_YOLOv10.html">2405.14458_YOLOv10: Real-Time End-to-End Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2405.14458_YOLOv10.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html">2411.15858_SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#id1">定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#methods">3. Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#more-detail-of-real-world-datasets">8. More detail of real-world datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../ML.html#ml">ML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2112.09332_WebGPT.html">2112.09332_WebGPT: Browser-assisted question-answering with human feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2203.11147_GopherCite.html">2203.11147_GopherCite: Teaching language models to support answers with verified quotes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2304.09848_Generative_Search.html">2304.09848_Generative_Search: Evaluating Verifiability in Generative Search Engines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2305.14251_FActScore.html">2305.14251_FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html">2305.14627_ALCE: Enabling Large Language Models to Generate Text with Citations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html#nli">NLI 在引用质量评估中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html#prompt">论文中用的prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2307.02185_Citation.html">2307.02185_Citation: A Key to Building Responsible and Accountable Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2307.16883_HAGRID.html">2307.16883_HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Agent.html">AI Agent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agent">通用 Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2210.03629_ReAct.html">2210.03629_ReAct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html">2303.08268_Chat-with-the-Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html#id2">正文</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.11366_Reflexion.html">2303.11366_Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html">2303.16434_TaskMatrix.AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id2">大脑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id3">接口平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#api">API 选择器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2304.03442_Generative-Agents.html">2304.03442_Generative-Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2304.03442_Generative-Agents.html#generative-agent-architecture">Generative Agent Architecture</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2307.07924_ChatDev.html">2307.07924_ChatDev: Communicative Agents for Software Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.00352_MetaGPT.html">2308.00352_MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.04026_AgentSims.html">2308.04026_AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.08155_AutoGen.html">2308.08155_AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.10848_AgentVerse.html">2308.10848_AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2308.10848_AgentVerse.html#id2">理念</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2310.06117_Step-Back.html">2310.06117_Step-Back: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2402.18679_MetaGPT_DI.html">2402.18679_MetaGPT_DI: Data Interpreter: An LLM Agent For Data Science</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2402.18679_MetaGPT_DI.html#introduction">INTRODUCTION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html">2407.07061_IoA: Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#overview-of-ioa">2.1 OVERVIEW OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#architecture-of-ioa">2.2 ARCHITECTURE OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#key-mechanisms">2.3 KEY MECHANISMS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#putting-it-all-together">2.5 Putting It All Together</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2408.08435_ADAS.html">2408.08435_ADAS: Automated Design of Agentic Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2408.08435_ADAS.html#prompt">Prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html">2408.08435_ADAS: Automating Agentic Workflow Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html#introduce">Introduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html#preliminary">PRELIMINARY</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html">2410.17238_SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#method">3 Method</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.21012_FACT.html">2410.21012_FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.21012_FACT.html#introduce">Introduce</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2504.01990_foundation-agents.html">2504.01990_Advances and Challenges in Foundation Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html">2506.12508_AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#agentorchestra">3.AgentOrchestra</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#experiments">4.Experiments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agent-aios">视觉 Agent&amp;AIOS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html">2108.03353_ Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#dataset-creation">3. Dataset Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#model-design">4. Model Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#id3">其它</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html">2209.08199_ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#problem-setting-tasks-and-metrics">3. Problem Setting: Tasks and Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#data-annotation">4. Data Annotation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#dataset-analysis">5. Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#experiments-and-baselines">6. Experiments and Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#limitations">8. Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#ethical-considerations">9. Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#a-data-annotation-details">A. Data Annotation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#b-data-examples">B. Data Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html">2212.06817_RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#preliminaries">3. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#system-overview">4. System Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#rt-1-robotics-transformer">5. RT-1: ROBOTICS TRANSFORMER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#experiments">6. EXPERIMENTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#conclusions-limitations-and-future-work">7. CONCLUSIONS, LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#b-model-card">B. MODEL CARD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#c-model-and-data">C. MODEL AND DATA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#d-experiments">D. EXPERIMENTS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html">2312.13771_AppAgent: Multimodal Agents as Smartphone Users</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#environment-and-action-space">3.1 Environment and Action Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#exploration-phase">3.2 Exploration Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#deployment-phase">3.3 Deployment Phase</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html">2401.10935_SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#screenspot-a-grounding-benchmark">4. ScreenSpot: A Grounding Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#ethical-considerations">Ethical considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#a-details-of-seeclick-pre-training">A. Details of SeeClick Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#b-screenspot-annotation-evaluation">B ScreenSpot Annotation &amp; Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#c-downstream-agent-tasks">C. Downstream Agent Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html">2402.04615_ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#automatic-data-generation">3. Automatic data generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#data-mixtures">4. Data Mixtures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#experiments-and-results">5. Experiments and Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#a-definitions-of-metrics">A Definitions of Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#b-screen-schema-examples">B. Screen Schema Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#c-prompts-for-llm-generated-content">C. Prompts For LLM Generated Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#d-screen-navigation-generated-examples">D. Screen Navigation Generated Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#f-screenqa-short-answers-generation">F. ScreenQA Short Answers Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#g-complex-question-answering-datasets">G. Complex Question Answering Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#h-new-benchmarks-repositories">H. New Benchmarks Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html">2402.07939_UFO: A UI-Focused Agent for Windows OS Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#the-design-of-ufo">3.The Design of UFO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#experiment">4.Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#limitations-lessons-learned">5.Limitations &amp; Lessons Learned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html">2403.16971_AIOS: LLM Agent Operating System</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#the-architecture-of-aios">2. The Architecture of AIOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#aios-kernel">3. AIOS Kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#appendix-e-discussion">Appendix E Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2406.01014_Mobile-Agent-v2.html">2406.01014_Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2411.02059_TableGPT2.html">2411.02059_TableGPT2: A Large Multimodal Model with Tabular Data Integration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2411.02059_TableGPT2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html">2501.11733_Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#mobile-agent-e">2. Mobile-Agent-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#conclusion-and-future-work">6. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-a-full-trajectory-comparison-example-with-previous-sota">Appendix A Full Trajectory Comparison Example with Previous SOTA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-b-error-recovery-with-escalation-to-manager">Appendix B Error Recovery with Escalation to Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-c-remaining-limitations">Appendix C Remaining Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-d-all-tasks-in-mobile-eval-e-benchmark">Appendix D All Tasks in Mobile-Eval-E Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-e-atomic-operation-space">Appendix E Atomic Operation Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-f-full-list-of-self-evolved-shortcuts">Appendix F Full list of Self-Evolved Shortcuts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-g-full-list-of-self-evolved-tips">Appendix G Full list of Self-Evolved Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html">2501.12326_UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#evolution-path-of-gui-agents">2. Evolution Path of GUI Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#core-capabilities-of-native-agent-model">3. Core Capabilities of Native Agent Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#ui-tars">4. UI-TARS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#experiment">5. Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html">2502.14282_PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#pc-agent">2. PC-Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#related-work">4. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html">2504.14603_UFO2: The Desktop AgentOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#background">2.Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#system-design-of-ufo2">3.System Design of UFO2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#picture-in-picture-interface">4.Picture-in-Picture Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#implementation-and-specialized-engineering-design">5.Implementation and Specialized Engineering Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#evaluation">6.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#discussion-future-work">7.Discussion &amp; Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#conclusion">9.Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#id2">记忆</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html">2505.22101_MemOS: An Operating System for Memory-Augmented Generation (MAG) in LLM (Short Version)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memos">4 MemOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id2"><strong>4.1 MemOS 中的记忆类型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memcube"><strong>4.2 记忆立方体（MemCube）：核心资源</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id3"><strong>4.3 MemOS 架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id4"><strong>4.4 系统执行流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#tools">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2205.00445_MRKL.html">2205.00445_MRKL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2302.04761_Toolformer.html">2302.04761_Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2303.17580_HuggingGPT.html">2303.17580_HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html">2307.16789_ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#dataset-construction">2 Dataset Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#appendix-a-implementation-details">Appendix A Implementation Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agi">AGI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/AGIs/1905.10985_AI-GA.html">1905.10985_AI-GA: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/AGIs/2408.06292_AI-Scientist.html">2408.06292_The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../RAG.html">RAG</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2005.11401_RAG_for_KI_NLP_task.html">2005.11401_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html">2312.10997_Retrieval-Augmented Generation for Large Language Models: A Survey</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-overview-of-rag">II. Overview of RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-a-naive-rag">II-A Naive RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-b-advanced-rag">II-B Advanced RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-c-modular-rag">II-C Modular RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-d-rag-vs-fine-tuning">II-D RAG vs Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-retrieval">III. Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-a-retrieval-source">III-A Retrieval Source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-b-indexing-optimization">III-B Indexing Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-c-query-optimization">III-C Query Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-d-embedding">III-D Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-e-adapter">III-E Adapter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-generation">IV. Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-a-context-curation">IV-A Context Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-b-llm-fine-tuning">IV-B LLM Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-augmentation-process-in-rag">V. Augmentation process in RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-a-iterative-retrieval">V-A Iterative Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-b-recursive-retrieval">V-B Recursive Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-c-adaptive-retrieval">V-C Adaptive Retrieval</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-task-and-evaluation">VI. Task and Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-a-downstream-task">VI-A Downstream Task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-b-evaluation-target">VI-B Evaluation Target</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-c-evaluation-aspects">VI-C Evaluation Aspects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-d-evaluation-benchmarks-and-tools">VI-D Evaluation Benchmarks and Tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-discussion-and-future-prospects">VII. Discussion and Future Prospects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-a-rag-vs-long-context">VII-A RAG vs Long Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-b-rag-robustness">VII-B RAG Robustness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-c-hybrid-approaches">VII-C Hybrid Approaches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-d-scaling-laws-of-rag">VII-D Scaling laws of RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-e-production-ready-rag">VII-E Production-Ready RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-f-multi-modal-rag">VII-F Multi-modal RAG</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2401.15884_CRAG.html">2401.15884_CRAG: Corrective Retrieval Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2403.14403_Adaptive-RAG.html">2403.14403_Adaptive-RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html">2404.16130_GraphRAG: From Local to Global: A GraphRAG Approach to Query-Focused Summarization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#background">2 Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#rag">2.1 RAG方法与系统</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#llmrag">2.2 知识图谱在LLM与RAG中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id2">2.3 自适应基准测试</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id3">2.4 RAG评估标准</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#methods">3 Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#graphrag"><strong>3.1 GraphRAG 工作流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id4"><strong>3.2 全局理解问题生成</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id5"><strong>3.3 全局理解评估标准</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id6"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#analysis">4 Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id7">4.1 实验1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id11">4.2 实验2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id14">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#results">5 Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id15">5.1 实验一：不同方法在摘要任务中的表现比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id18">5.2 实验二：基于声明的指标评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id20">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#discussion">6 Discussion</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id21">6.1 评估方法的局限性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id22">6.2 未来工作</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id23">更广泛的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-a-entity-and-relationship-extraction-approach">Appendix A Entity and Relationship Extraction Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id24"><strong>1. 实体与关系抽取方法</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#self-reflection"><strong>2. 自我反思（Self-Reflection）技术</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id25"><strong>3. 分块大小与抽取效果的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id26"><strong>4. 实验结果（图3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-b-example-community-detection">Appendix B Example Community Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-c-context-window-selection">Appendix C Context Window Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-d-example-answer-comparison">Appendix D Example Answer Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-e-system-prompts">Appendix E System Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-1-element-instance-generation"><strong>E.1 实体实例生成（Element Instance Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-2-community-summary-generation"><strong>E.2 社区摘要生成（Community Summary Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-3-community-answer-generation"><strong>E.3 社区问题回答生成（Community Answer Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-4-global-answer-generation"><strong>E.4 全局问题回答生成（Global Answer Generation）</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-f-evaluation-prompts">Appendix F Evaluation Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#f-1-relative-assessment-prompt">F.1 Relative Assessment Prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#f-2-relative-assessment-metrics">F.2 Relative Assessment Metrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-g-statistical-analysis">Appendix G Statistical Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id38">统计方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id39">主要结果总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id40">总体趋势：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id41">重要结论：</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2406.13213_Multi-Meta-RAG.html">2406.13213_Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html">2410.05779_LightRAG: Simple and Fast Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#retrieval-augmented-generation">2 Retrieval-Augmented Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#the-lightrag-architecture">3 The LightRAG Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#lightrag">一、LightRAG架构概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#graph-based-text-indexing">二、基于图的文本索引（Graph-based Text Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#dual-level-retrieval-paradigm">三、双层检索范式（Dual-level Retrieval Paradigm）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#retrieval-augmented-answer-generation">四、检索增强的答案生成（Retrieval-Augmented Answer Generation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id2">五、复杂度分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id3">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#evaluation">4 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#experimental-settings"><strong>1. 实验设置（4.1 Experimental Settings）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#lightrag-rag-4-2-rq1"><strong>2. LightRAG 与现有 RAG 方法的对比（4.2 RQ1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq2"><strong>3. 消融实验（4.3 RQ2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id8"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#case-study-rq3">4.4 Case Study (RQ3)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq3">4.4 案例研究（RQ3）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq4">4.5 模型成本与适应性分析（RQ4）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id9">总体结论：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#related-work">5 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id10">第5章 相关工作（总结）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#appendix">7 Appendix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html">2410.10450_KBLaM: Knowledge Base augmented Language Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#related-work">2. Related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#background">3. Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#self-attention-layer">Self-attention layer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#augmenting-llm-with-the-kb">4. Augmenting LLM with the KB</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#knowledge-tokens">Knowledge tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#rectangular-attention-injecting-knowledge-token-into-prompt-tokens">Rectangular Attention: Injecting knowledge token into prompt tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#kb-length-generalization-through-attention-score-scaling">KB length generalization through attention score scaling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#kb-instruction-tuning">5. KB instruction tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiments">6. EXPERIMENTS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiment-setting">6.1 EXPERIMENT SETTING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiment-results">6.2 EXPERIMENT RESULTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#id2">总结亮点</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#conclusion">7. CONCLUSION</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#limitations-and-future-work">8. LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-b-ablation-study">Appendix B Ablation study</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-c-sample-kb">Appendix C Sample KB</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#sample-q-a">SAMPLE Q&amp;A</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt">PROMPT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-synthetic-kb-generation">PROMPT FOR SYNTHETIC KB GENERATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-open-ended-q-a-generation">Prompt for open-ended Q&amp;A generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-gpt-evaluation-of-open-ended-q-a">PROMPT FOR GPT EVALUATION OF OPEN-ENDED Q&amp;A</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-llama-evaluation">PROMPT FOR LLAMA EVALUATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#question-template">QUESTION TEMPLATE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#sample-output">SAMPLE OUTPUT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#synthetic-kb">SYNTHETIC KB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#enron">ENRON</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html">2504.03137_LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#related-work">Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#llm-prompt-engineering">LLM Prompt Engineering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#kg-based-llm-reasoning">KG-based LLM Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#preliminaries">Preliminaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#knowledge-graph-kg">1. Knowledge Graph (KG)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#anchor-entities">2. Anchor Entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#relation-link">3. Relation Link</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#reasoning-path">4. Reasoning Path</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#methodology">Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage1-reasoning-graph-retrieval">Stage1: Reasoning Graph Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage2-knowledge-embedding">Stage2: Knowledge Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage3-knowledge-prompts-mixed-reasoning">Stage3: Knowledge Prompts Mixed Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#experiments">Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/graphrag.html">GraphRAG 官方文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/graphrag.html#indexing">Indexing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#indexing-architecture">&gt; Indexing Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#indexing-dataflow">&gt; Indexing Dataflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#prompt-tuning">&gt; Prompt Tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/graphrag.html#query">Query</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../paper_pool.html">论文池</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html">2305.16300_Random-Access Infinite Context Length for Transformers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#llm">LLM 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id1"><strong>研究背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id2"><strong>核心问题</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id3"><strong>主要贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id4"><strong>关键技术点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id5"><strong>实验结果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id6"><strong>意义与应用前景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id7"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#related-work">2 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#methodology">3 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id8"><strong>总体思路</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id9"><strong>方法详解</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id11"><strong>位置编码处理</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id12"><strong>与其他方法的对比</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#memory-computation">3.3 Memory &amp; Computation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#experiments">4 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id14"><strong>4.1 语言建模实验</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id18"><strong>4.2 微调预训练模型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id22"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#future-work">5 Future Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#acknowledgment">Acknowledgment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-a-grouped-softmax-example">Appendix A Grouped Softmax Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-b-dataset-description">Appendix B Dataset Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-c-number-of-unique-retrieved-blocks">Appendix C Number of Unique Retrieved Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-d-context-miss-token">Appendix D Context Miss Token</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-e-positional-augmentation">Appendix E Positional Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-f-additional-extensions-and-details">Appendix F Additional Extensions and Details</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#masked-language-modeling">1. <strong>掩码语言建模（Masked Language Modeling）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#flash-attention">2. <strong>与 Flash Attention 的结合</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id23">3. <strong>检索块数量与块大小的权衡</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id24">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-g-offloading-kv-cache-to-cpu">Appendix G Offloading KV Cache to CPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html">2311.18743_AlignBench: Benchmarking Chinese Alignment of Large Language Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id1">主要内容总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id2">总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id3"><strong>1. 背景与挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#alignbench"><strong>2. AlignBench的设计目标</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id4"><strong>3. AlignBench的主要特点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id5"><strong>4. AlignBench的应用与成果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id6"><strong>5. 总体贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id7"><strong>6. 表格对比</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#dataset">2 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#methods">3 Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#human-evaluation-on-alignbench">4 Human Evaluation on AlignBench</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#agreement-evaluation">一、一致性评估（Agreement Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#quality-evaluation">二、解释质量评估（Quality Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id8">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#alignbench-benchmarking-results">5 AlignBench: Benchmarking Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-2-prompts-and-details-of-methods">A.2 Prompts and Details of Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-2">A.2 提示模板与方法细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-3">A.3 各维度表现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-4">A.4 案例分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id10">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id11">一、核心问题：参考材料缺失导致评估困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id12">二、数学积分问题对比分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id13">三、总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html">2401.15391_MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id1"><strong>背景与动机</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id2"><strong>贡献</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id3"><strong>方法概览</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id4"><strong>实验结果</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id6">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id7">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#rag-with-multi-hop-queries">2 RAG with multi-Hop queries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#rag">2.1 RAG（检索增强生成）概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#multi-hop-queries">2.2 多跳查询（Multi-Hop Queries）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id8">2.3 评估指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#a-benchmarking-dataset-multihop-rag">3 A Benchmarking Dataset: MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#multihop-rag">一、MultiHop-RAG 数据集构建流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id10">二、MultiHop-RAG 数据集统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id11">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#benchmarking-rag-system-using-multihop-rag">4 Benchmarking RAG system using MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#retrieval-related-task">一、检索相关任务（Retrieval-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#generation-related-task">二、生成相关任务（Generation-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#other-use-cases">三、其他潜在改进方向（Other Use Cases）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id12">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#related-work">5 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#appendix-a-appendix-a-gpt-4-prompts-used-for-data-generation">Appendix A Appendix A: GPT-4 Prompts Used for Data Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#appendix-b-appendix-b-dataset-examples">Appendix B Appendix B: Dataset Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#related-work">2 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#prompt-tuning">2.1 Prompt Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#llms">2.2 LLMs在图相关任务中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id1">2.3 图上的检索方法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#problem-formalization">3 Problem Formalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#methodology">4 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id2">概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id3">4.1 文本子图检索</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#indexing">文本子图索引（Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#ranking">文本子图排序（Ranking）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#soft-pruning">文本子图软剪枝（Soft Pruning）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id4">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#textual-graph-augmented-generation">4.2 Textual Graph Augmented Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#text-view-of-textual-graphs">1. 文本视图（Text View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#graph-view-of-textual-graphs">2. 图视图（Graph View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#generation-phase">3. 生成阶段（Generation Phase）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#experiments">5 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id6">总结：第五章 实验部分</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#limitations">7 Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#a"><strong>附录A 总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id12"><strong>总结</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html">2407.01178_Memory3: Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#language-modeling-with-explicit-memory">Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id1">研究背景与动机：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id2">主要内容与方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id3">实验与结果：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id4">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#abstract">Abstract</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id5">核心思想</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory3">Memory3 模型特点</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id6">实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id7">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#introduction">1 _ Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#retrieval-augmented-training">1.1.1 _ Retrieval-augmented Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id8">1.1.1 | 基于检索的训练（Retrieval-augmented Training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#sparse-computation">1.1.2 | 稀疏计算（Sparse Computation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#parameter-as-memory">1.1.3 | 参数即记忆（Parameter as Memory）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory-circuitry-theory">2 _ Memory Circuitry Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id10">核心概念总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id11">总体贡献：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#definition-2">Definition 2.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id12">1. <strong>定义与核心概念：计算图、同构与知识（电路）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id13">2. <strong>知识的实例</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id14">3. <strong>知识的外部化与记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id15">4. <strong>结论与断言</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id16">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#remark-1">Remark 1.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id17">1. <strong>电路构造的关键性质</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#llm">2. <strong>记忆增强 LLM 的形式化定义</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id18">3. <strong>写入代价与读取代价的权衡（记忆层次结构）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id19">4. <strong>知识使用频率与记忆分配</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id20">5. <strong>图示与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id21">小结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#design">3 _ Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id22"><strong>3 | Design</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id25"><strong>3.1 | 推理过程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id27"><strong>3.2 | 写入与读取记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id28"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory-sparsification-and-storage">3.3 _ Memory Sparsification and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id29">一、显式记忆的存储挑战</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id30">二、各维度的稀疏化策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id31">三、压缩效果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id32">四、部署方式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id33">五、补充说明与建议</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id36">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#model-shape">3.4 _ Model Shape</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id37">3.4 | 模型结构（Model Shape）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#training-designs">3.5 | 训练设计（Training Designs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id38">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#two-stage-pretrain">3.6 _ Two-stage Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id39">一、预训练的两个阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#continual-train">二、对 continual train 的优化</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id40">三、防止信息泄露</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id41">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#pretraining-data">4 _ Pretraining Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#data-collection"><strong>4.1 数据收集（Data Collection）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#filtering"><strong>4.2 数据过滤（Filtering）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#tokenizer"><strong>4.3 分词器（Tokenizer）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#knowledge-base"><strong>4.4 知识库（Knowledge Base）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id42"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#pretrain">5 _ Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id43">1. 预训练总体设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#set-up">2. 训练设置（Set-up）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#warmup-stage">3. 预热阶段（Warmup Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#continual-train-stage">4. 持续训练阶段（Continual Train Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id44">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#fine-tuning-and-alignment">6 _ Fine-tuning and Alignment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#supervised-finetuning-sft">6.1 监督微调（Supervised Finetuning, SFT）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#direct-preference-optimization-dpo">6.2 直接偏好优化（Direct Preference Optimization, DPO）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#evaluation">7 _ Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id45">7.1 通用能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id46">7.2 对话能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id47">7.3 幻觉与事实性评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id48">7.4 专业任务评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id49">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#inference-speed">7.5 _ Inference Speed</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id50">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id51">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#conclusion">8 _ Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-a-cost-estimation">Appendix A Cost Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id52">模型参数设定</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#implicit-memory">隐式记忆（Implicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#explicit-memory">显式记忆（Explicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#external-information-rag">外部信息（External Information，如 RAG）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id53">综合比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id54">拓展讨论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-b-vector-compression">Appendix B Vector Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-c-supplementary-evaluation-results">Appendix C Supplementary Evaluation Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html">2505.14683_Emerging Properties in Unified Multimodal Pretraining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id1">核心内容总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id2">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#model">2 Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id3">1. 模型架构概览</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id4">2. 生成策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id5">3. 模型细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#generalized-causal-attention">4. 广义因果注意力（Generalized Causal Attention）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#transformer">5. Transformer结构选择与实验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id6">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#data">3 Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id7">数据特点与目标</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id8">数据来源与统计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id9">数据构建方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id16">数据训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id17">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#training">4 Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id18">1. 多阶段训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id19">2. 关键超参数调整</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id22">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#evaluation">5 Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#emerging-properties">6 Emerging Properties</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id23">1. <strong>新兴属性的定义与研究背景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id24">2. <strong>任务表现与训练阶段的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id25">3. <strong>多模态特征的重要性</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id26">4. <strong>定性分析与生成质量提升</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id27">5. <strong>核心发现与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id28">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#main-results">7 Main Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id29">7.1 图像理解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id30">7.2 图像生成</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id31">7.3 图像编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id32">7.4 带有推理的生成/编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id33">7.5 世界建模</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id34">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#more-qualitative-results">7.6 More Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#acknowledgement">9 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html">MemOS: A Memory OS for AI System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#llm">LLM 总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id1"><strong>1. 背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id2"><strong>2. 现有方法的不足</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id3"><strong>3. 四大典型挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos"><strong>4. MemOS的提出与核心理念</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id4"><strong>5. 总结与意义</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id5">总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id6"><strong>一、记忆研究的四个阶段</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id7"><strong>二、第一阶段：记忆定义与探索</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id14"><strong>三、MemOS 的初步构想</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id15"><strong>四、总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-1"><strong>2.1 显式长期记忆的建立（Stage 1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-2"><strong>2.2 人脑式记忆机制的引入（Stage 2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-3"><strong>2.3 基于工具的记忆管理（Stage 3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-4"><strong>2.4 系统化记忆治理（Stage 4）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id16"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-3-1-vision-of-memos">一、MemOS 的愿景（3.1 Vision of MemOS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#from-computer-os-to-memory-os">二、从传统操作系统到记忆操作系统（3.2 From Computer OS to Memory OS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id17">三、总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-modeling-in-memos">4 Memory Modeling in MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id18"><strong>4.1 内存类型与语义演化路径</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-cube-memcube"><strong>4.2 Memory Cube（MemCube）：内存的核心资源单元</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id19"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#architecture-of-memos">5 Architecture of MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id20">总结：MemOS 架构与执行流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id24">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id25">5.5.1 MemGovernance（内存治理模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memvault">5.5.2 MemVault（内存存储与路由基础设施）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memloader-memdumper">5.5.3 MemLoader 与 MemDumper（内存加载与导出模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memstore">5.5.4 MemStore（内存存储与分发接口）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id26">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#evaluation">6 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#end-to-end-evaluation-on-locomo"><strong>1. 整体系统评估（End-to-End Evaluation on LOCOMO）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#evaluation-of-memory-retrieval"><strong>2. 内存检索评估（Evaluation of Memory Retrieval）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#kv-evaluation-of-kv-based-memory-acceleration"><strong>3. KV缓存加速评估（Evaluation of KV-Based Memory Acceleration）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-for-architecture-innovation-and-applications">7 MemOS for Architecture Innovation and Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id28">一、MemOS推动的架构创新</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id29">二、MemOS的应用场景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id30">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#conclusion">8 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../other.html">其他</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../other.html#id3">数据集&amp;数据蒸馏</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../DataSets/1811.10959v3_Dataset_Distillation.html">1811.10959v3_Dataset Distillation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/1811.10959v3_Dataset_Distillation.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/1811.10959v3_Dataset_Distillation.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/1811.10959v3_Dataset_Distillation.html#introduction">1. INTRODUCTION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/1811.10959v3_Dataset_Distillation.html#approach">3. APPROACH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../DataSets/2502.20653_Dataset_Distillation.html">2502.20653_Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/2502.20653_Dataset_Distillation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/2502.20653_Dataset_Distillation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/2502.20653_Dataset_Distillation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/2502.20653_Dataset_Distillation.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../DataSets/normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../DataSets/normal.html#dataset-distillation">Dataset distillation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../other.html#d">3D</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="2003.08934_NeRF.html">2003.08934_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#neural-radiance-field-scene-representation">3. Neural Radiance Field Scene Representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#volume-rendering-with-radiance-fields">4. Volume Rendering with Radiance Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#optimizing-a-neural-radiance-field">5. Optimizing a Neural Radiance Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#result">6. Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="2003.08934_NeRF.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html">2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#id1">概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#geometric-priors-for-vp-detection">3. Geometric priors for VP detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="2203.08586_VanishingPointEstimation.html#conclusion-and-limitations">5. Conclusion and limitations</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">2312.14132_DUSt3R: Geometric 3D Vision Made Easy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">相关概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="#experiments-with-dust3r">4. Experiments with DUSt3R</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-a">Appendix A <strong>附录概览</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-b-qualitative-results">Appendix B.  Qualitative results</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-c-extended-related-work">Appendix C. Extended Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-d-multi-view-pose-estimation">Appendix D. 多视角姿态估计（Multi-view Pose Estimation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-e-visual-localization">Appendix E. 视觉定位（Visual Localization）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#appendix-f-training-details">Appendix F. Training details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2406.09756_MASt3R.html">2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#id1">前言</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#id2">🧠 思维导图式总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#related-works">2. Related works</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#id3">🧠 总结思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#appendix-a-additional-qualitative-results">Appendix A Additional Qualitative Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#b-fast-reciprocal-matching">B. Fast Reciprocal Matching</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#c-coarse-to-fine">C. Coarse-to-Fine</a></li>
<li class="toctree-l4"><a class="reference internal" href="2406.09756_MASt3R.html#d-detailed-experimental-settings">D. Detailed experimental settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2412.09401_SLAM3R.html">2412.09401_SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#id1">术语</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#id14">6. 致谢</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#appendix-a-implementation-details">Appendix A Implementation details</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#appendix-b-details-for-experimental-settings">Appendix B Details for experimental settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#appendix-c-additional-comparisons-and-analyses">Appendix C Additional comparisons and analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.09401_SLAM3R.html#d-more-visual-results">D. More visual results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html">2412.12392_MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#gpt">GPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#id1">先验知识</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#limitations-and-future-work">5. Limitations and Future Work（局限与未来工作）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#conclusion">🧾 6. Conclusion（总结）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#id32">🧠 总结一句话版：</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#initialisation">8. Initialisation（初始化）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#runtime-breakdown">9. Runtime Breakdown（运行时分析）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#evaluation-setup">10. Evaluation Setup（评估设置）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2412.12392_MASt3R-SLAM.html#id35">11. EuRoC 结果总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2503.11651_VGGT.html">2503.11651_VGGT: Visual Geometry Grounded Transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#discussions">5. Discussions</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#appendix-a-formal-definitions">Appendix A Formal Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#appendix-c-additional-experiments">Appendix C Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#appendix-d-qualitative-examples">Appendix D Qualitative Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="2503.11651_VGGT.html#appendix-e-related-work">Appendix E Related Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html#id4">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html">A PAINLESS GUIDE TO CRC ERROR DETECTION ALGORITHMS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#the-basic-idea-behind-crc-algorithms">The Basic Idea Behind CRC Algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#polynomical-arithmetic">Polynomical Arithmetic</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#binary-arithmetic-with-no-carries">Binary Arithmetic with No Carries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id2">一个可用的实例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#choosing-a-poly">Choosing A Poly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-straightforward-crc-implementation">A Straightforward CRC Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-table-driven-implementation">A Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-slightly-mangled-table-driven-implementation">A Slightly Mangled Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/Distributed%20Representations%20of%20Sentences%20and%20Documents.html">Distributed Representations of Sentences and Documents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../other.html">其他</a> &raquo;</li>
        
      <li>2312.14132_DUSt3R: Geometric 3D Vision Made Easy</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/others/3D/2312.14132_DUSt3R.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">2312.14132_DUSt3R: Geometric 3D Vision Made Easy</a><ul>
<li><a class="reference internal" href="#id1">关键词</a></li>
<li><a class="reference internal" href="#id2">相关概念</a><ul>
<li><a class="reference internal" href="#intrinsic-parameters">📷 一、内参（Intrinsic Parameters）</a></li>
<li><a class="reference internal" href="#extrinsic-parameters">🌍 二、外参（Extrinsic Parameters）</a></li>
<li><a class="reference internal" href="#id3">🧮 三、三维点到图像点的映射公式</a></li>
<li><a class="reference internal" href="#id4">🎯 应用场景</a></li>
</ul>
</li>
<li><a class="reference internal" href="#abstract">Abstract</a></li>
<li><a class="reference internal" href="#introduction">1. Introduction</a><ul>
<li><a class="reference internal" href="#id5">💡 第一段：背景介绍</a></li>
<li><a class="reference internal" href="#id6">🔧 第二段：传统方法的不足</a></li>
<li><a class="reference internal" href="#dust3r">🚀 第三段：DUSt3R 的核心创新</a></li>
<li><a class="reference internal" href="#id7">📚 第四段：训练与建模思路</a></li>
<li><a class="reference internal" href="#id8">🧩 第五段：多视图融合方式的新设计</a></li>
<li><a class="reference internal" href="#id9">📝 总结：论文贡献（四个方面）</a></li>
</ul>
</li>
<li><a class="reference internal" href="#related-work">2. Related Work</a><ul>
<li><a class="reference internal" href="#structure-from-motion-sfm"><strong>1. Structure-from-Motion (SfM)</strong></a></li>
<li><a class="reference internal" href="#multi-view-stereo-mvs"><strong>2. Multi-View Stereo (MVS)</strong></a></li>
<li><a class="reference internal" href="#direct-rgb-to-3d"><strong>3. Direct RGB-to-3D</strong></a></li>
<li><a class="reference internal" href="#pointmaps"><strong>4. 本文的方法：Pointmaps 和双视角输入</strong></a></li>
<li><a class="reference internal" href="#id10"><strong>5. Pointmaps 的使用</strong></a></li>
<li><a class="reference internal" href="#id11">🧠 总结：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#method">3. Method</a><ul>
<li><a class="reference internal" href="#id12">概念定义</a><ul>
<li><a class="reference internal" href="#pointmap">🟡 <strong>Pointmap（点图）</strong></a></li>
<li><a class="reference internal" href="#cameras-and-scene">🟡 <strong>相机与场景建模（Cameras and Scene）</strong></a></li>
<li><a class="reference internal" href="#id13">🟡 <strong>跨相机坐标转换（多视角点图的转换）</strong></a></li>
<li><a class="reference internal" href="#id14">✍️ 总结：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#overview">3.1 Overview</a><ul>
<li><a class="reference internal" href="#id15">🧠 <strong>目标概述</strong></a></li>
<li><a class="reference internal" href="#inspired-by-croco">🏗️ <strong>网络架构（Inspired by CroCo）</strong></a></li>
<li><a class="reference internal" href="#discussion">💡 <strong>设计讨论（Discussion）</strong></a></li>
<li><a class="reference internal" href="#id16">✅ 总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#training-objective">3.2 Training Objective</a><ul>
<li><a class="reference internal" href="#d-3d-regression-loss">一、3D回归损失（3D Regression Loss）</a></li>
<li><a class="reference internal" href="#confidence-aware-loss">二、置信度感知损失（Confidence-aware Loss）</a></li>
<li><a class="reference internal" href="#id17">总结理解：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#downstream-applications">3.3 Downstream Applications</a><ul>
<li><a class="reference internal" href="#point-matching">✅ <strong>Point Matching（点匹配）</strong></a></li>
<li><a class="reference internal" href="#recovering-intrinsics">✅ <strong>Recovering Intrinsics（恢复相机内参）</strong></a></li>
<li><a class="reference internal" href="#relative-pose-estimation">✅ <strong>Relative Pose Estimation（相对姿态估计）</strong></a></li>
<li><a class="reference internal" href="#id18">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#global-alignment">3.4 Global Alignment</a><ul>
<li><a class="reference internal" href="#pairwise-graph">✦ 一、构建图结构: Pairwise Graph</a></li>
<li><a class="reference internal" href="#global-optimization">✦ 二、全局优化: Global optimization</a></li>
<li><a class="reference internal" href="#camera-parameter-recovery">✦ 三、相机参数恢复：Camera Parameter Recovery</a></li>
<li><a class="reference internal" href="#id19">✅ 总结重点</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#experiments-with-dust3r">4. Experiments with DUSt3R</a><ul>
<li><a class="reference internal" href="#id20">前言</a><ul>
<li><a class="reference internal" href="#training-data">🧠 <strong>1. 训练数据（Training data）</strong></a></li>
<li><a class="reference internal" href="#training-details">🏗️ <strong>2. 训练细节（Training details）</strong></a></li>
<li><a class="reference internal" href="#evaluation">📊 <strong>3. 评估（Evaluation）</strong></a></li>
<li><a class="reference internal" href="#qualitative-results">🎨 <strong>4. 定性结果（Qualitative results）</strong></a></li>
<li><a class="reference internal" href="#id21">✅ 总结一下关键点：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#visual-localization"><strong>4.1 视觉定位（Visual Localization）</strong></a></li>
<li><a class="reference internal" href="#multi-view-pose-estimation"><strong>4.2 多视角相对位姿估计（Multi-view Pose Estimation）</strong></a></li>
<li><a class="reference internal" href="#monocular-depth-estimation"><strong>4.3 单目深度估计（Monocular Depth Estimation）</strong></a></li>
<li><a class="reference internal" href="#multi-view-depth">4.4 多视角深度估计（Multi-view Depth）</a></li>
<li><a class="reference internal" href="#d-reconstruction">4.5 三维重建（3D Reconstruction）</a></li>
<li><a class="reference internal" href="#ablations">4.6 消融实验（Ablations）</a></li>
<li><a class="reference internal" href="#id22">🧩 总结一句话：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">5. Conclusion</a><ul>
<li><a class="reference internal" href="#id23">🔚 总结内容解析</a></li>
<li><a class="reference internal" href="#id24">✅ 关键词解释：</a></li>
<li><a class="reference internal" href="#id25">📌 总结关键词提炼</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-a">Appendix A <strong>附录概览</strong></a></li>
<li><a class="reference internal" href="#appendix-b-qualitative-results">Appendix B.  Qualitative results</a></li>
<li><a class="reference internal" href="#appendix-c-extended-related-work">Appendix C. Extended Related Work</a><ul>
<li><a class="reference internal" href="#implicit-camera-models">1. <strong>Implicit Camera Models（隐式相机模型）</strong></a></li>
<li><a class="reference internal" href="#dense-visual-slam-slam">2. <strong>Dense Visual SLAM（密集视觉SLAM）</strong></a></li>
<li><a class="reference internal" href="#implicit-3d-reconstruction-3d">3. <strong>Implicit 3D Reconstruction（隐式3D重建）</strong></a></li>
<li><a class="reference internal" href="#rgb-pairs-to-3d">4. <strong>RGB-pairs-to-3D</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-d-multi-view-pose-estimation">Appendix D. 多视角姿态估计（Multi-view Pose Estimation）</a><ul>
<li><a class="reference internal" href="#id26">🌟 核心结论：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-e-visual-localization">Appendix E. 视觉定位（Visual Localization）</a><ul>
<li><a class="reference internal" href="#id27">🌟 核心结论：</a></li>
<li><a class="reference internal" href="#id28">🧪 实验设定：</a></li>
<li><a class="reference internal" href="#tab-6">📊 表格解读（Tab. 6）：</a></li>
<li><a class="reference internal" href="#id29">📌 总结亮点：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-f-training-details">Appendix F. Training details</a><ul>
<li><a class="reference internal" href="#id30">一、训练数据来源和处理</a></li>
<li><a class="reference internal" href="#id31">二、数据增强策略</a></li>
<li><a class="reference internal" href="#id32">三、训练超参数（见表 7）</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
  <section class="tex2jax_ignore mathjax_ignore" id="dust3r-geometric-3d-vision-made-easy">
<h1>2312.14132_DUSt3R: Geometric 3D Vision Made Easy<a class="headerlink" href="#dust3r-geometric-3d-vision-made-easy" title="此标题的永久链接">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/html/2312.14132">https://arxiv.org/html/2312.14132</a></p></li>
<li><p>GitHub: <a class="reference external" href="https://github.com/naver/dust3r">https://github.com/naver/dust3r</a></p></li>
<li><p>组织: Aalto University, Naver Labs Europe</p></li>
</ul>
<section id="id1">
<h2>关键词<a class="headerlink" href="#id1" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>MVS: Multi-view stereo reconstruction</p></li>
<li><p>PnP: Perspective-n-Point</p></li>
</ul>
</section>
<section id="id2">
<h2>相关概念<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h2>
<section id="intrinsic-parameters">
<h3>📷 一、内参（Intrinsic Parameters）<a class="headerlink" href="#intrinsic-parameters" title="此标题的永久链接">¶</a></h3>
<p>内参描述的是<strong>摄像头自身的成像特性</strong>，即如何从摄像头坐标系中的三维点映射到二维图像平面上。常见的内参包括：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\( f_x, f_y \)</span></p></td>
<td><p>焦距在图像x轴和y轴的像素单位值（通常是焦距乘以像素密度）</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\( c_x, c_y \)</span></p></td>
<td><p>主点（principal point）的坐标，通常是图像中心</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\( s \)</span></p></td>
<td><p>偏轴系数（skew），描述x、y轴不垂直时的情况，通常为0</p></td>
</tr>
</tbody>
</table>
<p><strong>内参矩阵（相机矩阵）K 通常表示为：</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
K = \begin{bmatrix}
f_x &amp; s &amp; c_x \\
0 &amp; f_y &amp; c_y \\
0 &amp; 0 &amp; 1
\end{bmatrix}
\end{split}\]</div>
<p>此外，有时还包括<strong>畸变参数</strong>（distortion coefficients）：</p>
<ul class="simple">
<li><p>径向畸变：<span class="math notranslate nohighlight">\( k_1, k_2, k_3 \)</span></p></li>
<li><p>切向畸变：<span class="math notranslate nohighlight">\( p_1, p_2 \)</span></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="extrinsic-parameters">
<h3>🌍 二、外参（Extrinsic Parameters）<a class="headerlink" href="#extrinsic-parameters" title="此标题的永久链接">¶</a></h3>
<p>外参描述的是<strong>摄像头相对于世界坐标系的位置和朝向</strong>，即世界坐标如何变换到摄像头坐标。包含：</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>参数</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\( R \)</span></p></td>
<td><p>旋转矩阵，表示世界坐标系到摄像头坐标系的旋转</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\( t \)</span></p></td>
<td><p>平移向量，表示世界坐标系原点在摄像头坐标系下的位置</p></td>
</tr>
</tbody>
</table>
<p>可以组成一个变换矩阵：</p>
<div class="math notranslate nohighlight">
\[
\text{Extrinsic} = [R \ | \ t]
\]</div>
</section>
<hr class="docutils" />
<section id="id3">
<h3>🧮 三、三维点到图像点的映射公式<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h3>
<p>给定一个世界坐标系中的三维点 <span class="math notranslate nohighlight">\( X_{world} \)</span>，它投影到图像坐标系的公式如下：</p>
<div class="math notranslate nohighlight">
\[
x_{image} = K \cdot [R \ | \ t] \cdot X_{world}
\]</div>
<p>其中：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( X_{world} \)</span> 是齐次坐标表示的三维点：$<span class="math notranslate nohighlight">\(
[X, Y, Z, 1]^T
\)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\( [R|t] \)</span> 是 3x4 的外参矩阵</p></li>
<li><p><span class="math notranslate nohighlight">\( K \)</span> 是 3x3 的内参矩阵</p></li>
<li><p><span class="math notranslate nohighlight">\( x_{image} \)</span> 是图像中的点，通常需要再归一化处理（除以z）</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id4">
<h3>🎯 应用场景<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>相机标定（Camera Calibration）</strong>：获取内外参</p></li>
<li><p><strong>三维重建 / SLAM</strong>：利用外参追踪摄像头位置</p></li>
<li><p><strong>AR 增强现实</strong>：实现虚实对齐，必须准确知道内外参</p></li>
</ul>
</section>
</section>
<section id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>Multi-view stereo reconstruction (MVS) in the wild requires to first estimate the camera parameters e.g. intrinsic and extrinsic parameters. These are usually tedious and cumbersome to obtain, yet they are mandatory to triangulate corresponding pixels in 3D space, which is the core of all best performing MVS algorithms.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>在自然环境下进行多视图立体重建（MVS）时，首先需要估计相机的参数，例如内参（intrinsic）和外参（extrinsic）。但这些参数通常很难获取且过程繁琐。然而，它们是必须的，因为只有通过这些参数，才能将不同图像中对应像素点三角测量还原为3D空间中的点，这是目前表现最好的MVS算法的核心。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>In this work, we take an opposite stance and introduce DUSt3R, a radically novel paradigm for Dense and Unconstrained Stereo 3D Reconstruction of arbitrary image collections, i.e. operating without prior information about camera calibration nor viewpoint poses.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>本研究提出了一种完全不同的做法，名为 <strong>DUSt3R</strong>，它是一种<strong>密集且无约束的立体三维重建新范式</strong>。它能处理任意图像集合，<strong>不需要任何相机标定信息或相机位姿先验</strong>，彻底摆脱了传统MVS的限制。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>We cast the pairwise reconstruction problem as a regression of pointmaps, relaxing the hard constraints of usual projective camera models. We show that this formulation smoothly unifies the monocular and binocular reconstruction cases.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>我们将两张图像之间的重建问题<strong>转化为“点图（pointmaps）”的回归任务</strong>，从而放松了传统投影相机模型的严格约束。我们证明，这种表述方式能够<strong>自然地统一单目和双目重建任务</strong>。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>In the case where more than two images are provided, we further propose a simple yet effective global alignment strategy that expresses all pairwise pointmaps in a common reference frame.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>当输入的图像超过两张时，我们进一步提出了一种<strong>简单而有效的全局对齐策略</strong>，可以将所有两两图像之间的点图<strong>统一到一个公共参考坐标系中</strong>。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>We base our network architecture on standard Transformer encoders and decoders, allowing us to leverage powerful pretrained models.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>我们采用标准的 Transformer 编码器和解码器作为网络架构，<strong>可以充分利用已有的强大预训练模型</strong>。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>Our formulation directly provides a 3D model of the scene as well as depth information, but interestingly, we can seamlessly recover from it, pixel matches, relative and absolute cameras.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>我们的方法不仅能<strong>直接输出场景的3D模型和深度图信息</strong>，而且有趣的是，还能从中<strong>自然恢复像素匹配关系、相对和绝对的相机参数</strong>。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>Exhaustive experiments on all these tasks showcase that the proposed DUSt3R can unify various 3D vision tasks and set new SoTAs on monocular/multi-view depth estimation as well as relative pose estimation.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>大量实验表明，DUSt3R 能<strong>统一多种3D视觉任务</strong>，并在<strong>单目/多视图深度估计、相对位姿估计等任务上设立了新的SOTA（最佳结果）</strong>。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>原文：</p>
<ul>
<li><p><strong>In summary, DUSt3R makes many geometric 3D vision tasks easy.</strong></p></li>
</ul>
</li>
<li><p>中文解释：</p>
<ul>
<li><p>总之，DUSt3R <strong>让很多几何类3D视觉任务变得更简单</strong>了。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul class="simple">
<li><p>总结要点：</p></li>
</ul>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>关键点</p></th>
<th class="head"><p>内容</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>传统MVS问题</p></td>
<td><p>依赖繁琐的相机内外参估计</p></td>
</tr>
<tr class="row-odd"><td><p>DUSt3R创新点</p></td>
<td><p>不依赖相机参数，直接从图像构建3D模型</p></td>
</tr>
<tr class="row-even"><td><p>技术方法</p></td>
<td><p>点图回归 + Transformer架构</p></td>
</tr>
<tr class="row-odd"><td><p>适用范围</p></td>
<td><p>单目、双目、多视图都适用</p></td>
</tr>
<tr class="row-even"><td><p>能力输出</p></td>
<td><p>可获取3D模型、深度图、像素匹配、相机姿态</p></td>
</tr>
<tr class="row-odd"><td><p>成果表现</p></td>
<td><p>多个任务上刷新SOTA表现</p></td>
</tr>
</tbody>
</table>
</section>
<hr class="docutils" />
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="此标题的永久链接">¶</a></h2>
<p>DUSt3R：一种用于从多视图图像中进行稠密三维重建（Dense 3D Reconstruction）的新方法。</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/0xxYoP.png" /></p>
<p>Figure 1: Overview: Given an unconstrained image collection, i.e. a set of photographs with unknown camera poses and intrinsics, our proposed method DUSt3R outputs a set of corresponding pointmaps, from which we can straightforwardly recover a variety of geometric quantities normally difficult to estimate all at once, such as the camera parameters, pixel correspondences, depthmaps, and fully-consistent 3D reconstruction. Note that DUSt3R also works for a single input image (e.g. achieving in this case monocular reconstruction). We also show qualitative examples on the DTU, Tanks and Temples and ETH-3D datasets [108, 51, 1] obtained without known camera parameters. For each sample, from left to right: input image, colored point cloud, and rendered with shading for a better view of the underlying geometry.</p>
<hr class="docutils" />
<section id="id5">
<h3>💡 第一段：背景介绍<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>任务目标：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>从多张图像中，估计一个场景的三维几何结构和相机参数——这是计算机视觉长期追求的终极目标之一。</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>Unconstrained image-based dense 3D reconstruction from multiple views is one of a few long-researched end-goals of computer vision.</p>
</div>
<blockquote>
<div><p><strong>实际意义：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>它有很多应用，如地图制作、导航、考古、文化遗产保护、机器人等。同时，它也涵盖了几乎所有几何相关的 3D 视觉任务。</p></li>
</ul>
<blockquote>
<div><p><strong>技术演进：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>传统 3D 重建方法是多个子领域长期技术积累的成果，比如</p>
<ul>
<li><p>关键点检测(keypoint detection)</p></li>
<li><p>特征匹配(keypoint matching)</p></li>
<li><p>鲁棒估计(robust estimation)</p></li>
<li><p>Structure-from-Motion（SfM）、</p></li>
<li><p>Bundle Adjustment（BA）、</p></li>
<li><p>稠密多视图立体(dense Multi-View Stereo (MVS) )</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id6">
<h3>🔧 第二段：传统方法的不足<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>传统 SfM + MVS 流程：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>是一连串“最小子问题”（如特征匹配、三角化、相机估计、稀疏重建到稠密重建）组成的复杂流程。</p></li>
</ul>
<blockquote>
<div><p><strong>问题在于：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>这些子问题之间<strong>相互独立</strong>，彼此不协作，错误会级联传递，导致结果容易受到影响。此外，一些关键步骤（比如 SfM）在很多情况下不稳固，容易失败（比如图像数量太少、物体反光、相机运动太小等）。</p></li>
</ul>
<blockquote>
<div><p><strong>核心问题：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>如果相机参数估计不准，MVS 的结果质量也会跟着下降。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="dust3r">
<h3>🚀 第三段：DUSt3R 的核心创新<a class="headerlink" href="#dust3r" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>目标：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>提出一种新的、<strong>端到端</strong>的系统 DUSt3R，支持从 <strong>任意两张图像</strong> 中，重建三维结构，<strong>不需要任何相机参数（甚至连内参都不需要）</strong>。</p></li>
</ul>
<blockquote>
<div><p><strong>关键点：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>DUSt3R 输出的是一种新的结构叫 <strong>3D Pointmap</strong>，它具有以下能力：</p>
<ol class="arabic simple">
<li><p>表示场景几何；</p></li>
<li><p>关联图像像素和三维点；</p></li>
<li><p>表达两个视角之间的几何关系。</p></li>
</ol>
</li>
</ul>
<blockquote>
<div><p><strong>优势：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>只通过一个网络，就能联合处理输入图像和生成的 3D Pointmap，<strong>把过去多个子问题合并为一个问题来学习和解决</strong>，从而提高整体协同与准确性。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id7">
<h3>📚 第四段：训练与建模思路<a class="headerlink" href="#id7" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>训练方式：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>采用监督学习，使用合成数据、SfM 重建数据或真实传感器采集数据。</p></li>
</ul>
<blockquote>
<div><p><strong>架构选择：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>摒弃任务特定模块，使用<strong>通用 Transformer 架构</strong>，完全数据驱动，不强加几何先验。</p></li>
</ul>
<blockquote>
<div><p><strong>网络学到的东西：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>包括纹理、阴影、轮廓等形状先验，与传统 MVS 所依赖的几何 cue 类似，但是通过学习得到的。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id8">
<h3>🧩 第五段：多视图融合方式的新设计<a class="headerlink" href="#id8" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>传统 BA：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>通过最小化重投影误差对相机和点进行优化。</p></li>
</ul>
<blockquote>
<div><p><strong>DUSt3R 的做法：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>引入一种新的全局对齐方法，不再使用重投影误差，而是<strong>直接在三维空间中对齐相机和几何信息</strong>，更快、更稳定。</p></li>
</ul>
<blockquote>
<div><p><strong>效果：</strong></p>
</div></blockquote>
<ul class="simple">
<li><p>在真实场景和未知相机条件下也能得到一致的重建效果。并且可以用于单目和多视图的场景。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id9">
<h3>📝 总结：论文贡献（四个方面）<a class="headerlink" href="#id9" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>首次实现：</strong><br />
一个统一、端到端的 3D 重建方法，不需要相机参数，支持单目和双目输入。</p></li>
<li><p><strong>Pointmap 表示：</strong><br />
提出 Pointmap 的方式保留了像素与场景之间的关系，<strong>规避了透视投影中的约束</strong>。</p></li>
<li><p><strong>多视图融合新策略：</strong><br />
通过 3D 对齐来融合多对图像的重建结果，<strong>不依赖 SfM/MVS 的中间步骤</strong>，但能导出它们常见的输出（相机位姿、稀疏点云等）。</p></li>
<li><p><strong>优异性能表现：</strong><br />
在单目/多视图深度估计、相机位姿估计任务中取得 SOTA（state-of-the-art）结果。</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="related-work">
<h2>2. Related Work<a class="headerlink" href="#related-work" title="此标题的永久链接">¶</a></h2>
<section id="structure-from-motion-sfm">
<h3><strong>1. Structure-from-Motion (SfM)</strong><a class="headerlink" href="#structure-from-motion-sfm" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>🔍 是什么？</p>
<ul>
<li><p>SfM（运动恢复结构）是指从多张图像中估计相机参数，同时重建稀疏的 3D 点云地图。</p></li>
</ul>
</li>
<li><p>📌 传统流程：</p>
<ul>
<li><p>基于关键点匹配（keypoint matching）获得像素间的对应关系。</p></li>
<li><p>然后通过束束调整（bundle adjustment）优化相机姿态和 3D 点的位置。</p></li>
</ul>
</li>
<li><p>🚀 近年来的进展：</p>
<ul>
<li><p>引入了 <strong>学习方法</strong>，增强了如下模块：</p>
<ul>
<li><p>特征提取与描述（feature description）</p></li>
<li><p>图像匹配（image matching）</p></li>
<li><p>特征度量优化（featuremetric refinement）</p></li>
<li><p>神经束束调整（neural bundle adjustment）</p></li>
</ul>
</li>
</ul>
</li>
<li><p>⚠️ 主要问题：</p>
<ul>
<li><p>尽管有很多学习驱动的改进，SfM 依然是一个<strong>串行流程（sequential pipeline）</strong>，各模块之间强依赖，因此容易受到噪声传播的影响。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="multi-view-stereo-mvs">
<h3><strong>2. Multi-View Stereo (MVS)</strong><a class="headerlink" href="#multi-view-stereo-mvs" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>🔍 是什么？</p>
<ul>
<li><p>MVS 是用于从多视角图像中<strong>密集重建物体表面</strong>的任务，通常要求已知相机参数。</p></li>
</ul>
</li>
<li><p>📌 MVS方法分为三类：</p>
<ul>
<li><p>纯手工设计的方法</p></li>
<li><p>基于场景优化的方法</p></li>
<li><p>基于学习的方法</p></li>
</ul>
</li>
<li><p>⚠️ 主要问题：
这些方法都依赖准确的相机参数（如内参、位姿），但<strong>现实中获取准确相机参数并不容易</strong>，影响算法的鲁棒性。</p></li>
<li><p>✅ 本文的方法：</p>
<ul>
<li><p>直接预测几何结构（3D 表面），<strong>不依赖相机参数</strong>，减少实际使用难度。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="direct-rgb-to-3d">
<h3><strong>3. Direct RGB-to-3D</strong><a class="headerlink" href="#direct-rgb-to-3d" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>🔍 是什么？</p>
<ul>
<li><p>近年来有些方法尝试从<strong>单张 RGB 图像直接预测 3D 结构</strong>，但这个任务本质上是病态问题（ill-posed），因为缺乏深度信息。</p></li>
</ul>
</li>
<li><p>📌 两类方法：</p>
<ol class="arabic simple">
<li><p><strong>基于类别的形状先验</strong>：</p>
<ul>
<li><p>从大量图像中学习每类物体的通用外形（如人、车等），如 Pavllo 等人的方法。</p></li>
<li><p>缺点：泛化能力差，无法处理未知类别。</p></li>
</ul>
</li>
<li><p><strong>基于场景的深度估计</strong>（和本文更相关）：</p>
<ul>
<li><p>通常依赖**单目深度估计（MDE）**网络，结合相机内参将深度图转换为点云。</p></li>
<li><p>也有方法试图通过视频帧时序信息恢复相机内参，或直接预测相机内参。</p></li>
</ul>
</li>
</ol>
</li>
<li><p>⚠️ 主要问题：</p>
<ul>
<li><p>这些方法最终依然依赖于深度估计，而单目深度估计本身不可靠。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="pointmaps">
<h3><strong>4. 本文的方法：Pointmaps 和双视角输入</strong><a class="headerlink" href="#pointmaps" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>✅ 核心创新：</p>
<ul>
<li><p><strong>同时输入两张图像（两个视角）</strong>，输出两个点图（pointmaps），而不是传统深度图。</p></li>
<li><p>所有输出的 3D 点都统一在第一个视角的坐标系中。</p></li>
<li><p>理论上可以通过三角测量实现几何重建。</p></li>
</ul>
</li>
<li><p>🔧 与传统的 MVS/SfM 区别：</p>
<ul>
<li><p>不需要明确的相机参数输入。</p></li>
<li><p>网络结构支持 end-to-end 学习，不再是传统串行流程。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id10">
<h3><strong>5. Pointmaps 的使用</strong><a class="headerlink" href="#id10" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>🔍 什么是 Pointmap？</p>
<ul>
<li><p>Pointmap 是一个二维图像大小的数组，其中每个像素对应一个 3D 坐标点。</p></li>
<li><p>比深度图包含更丰富的信息，便于直接处理 3D 结构。</p></li>
</ul>
</li>
<li><p>📌 用途：</p>
<ul>
<li><p>在视觉定位、视图合成、单目重建等任务中越来越常用。</p></li>
<li><p>提供了一种可以“在图像空间处理 3D 形状”的方式。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id11">
<h3>🧠 总结：<a class="headerlink" href="#id11" title="此标题的永久链接">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模块</p></th>
<th class="head"><p>内容</p></th>
<th class="head"><p>问题/瓶颈</p></th>
<th class="head"><p>本文创新</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>SfM</p></td>
<td><p>稀疏重建，依赖关键点匹配和优化</p></td>
<td><p>串行结构，易被噪声破坏</p></td>
<td><p>End-to-end 结构避免错误累积</p></td>
</tr>
<tr class="row-odd"><td><p>MVS</p></td>
<td><p>密集重建，要求已知相机参数</p></td>
<td><p>相机参数估计不准确</p></td>
<td><p>无需相机参数，直接预测几何结构</p></td>
</tr>
<tr class="row-even"><td><p>单目 RGB-3D</p></td>
<td><p>单张图预测深度</p></td>
<td><p>深度估计不可靠</p></td>
<td><p>双图预测 pointmap，显式三角测量</p></td>
</tr>
<tr class="row-odd"><td><p>本文方法</p></td>
<td><p>同时处理两视角，输出 pointmap</p></td>
<td><p>-</p></td>
<td><p>对相机参数零假设、鲁棒泛化好</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="method">
<h2>3. Method<a class="headerlink" href="#method" title="此标题的永久链接">¶</a></h2>
<section id="id12">
<h3>概念定义<a class="headerlink" href="#id12" title="此标题的永久链接">¶</a></h3>
<section id="pointmap">
<h4>🟡 <strong>Pointmap（点图）</strong><a class="headerlink" href="#pointmap" title="此标题的永久链接">¶</a></h4>
<p>我们把一张图片上每一个像素点都对应一个三维空间的点，组织成一个二维的结构（宽 × 高 × 3），称为一个 <strong>pointmap（点图）</strong>，记作：</p>
<div class="math notranslate nohighlight">
\[X ∈ ℝ^{(W×H×3)}\]</div>
<p>这意味着：</p>
<ul class="simple">
<li><p>图像有 <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">×</span> <span class="pre">H</span></code> 个像素点</p></li>
<li><p>每个像素对应一个 <code class="docutils literal notranslate"><span class="pre">3D坐标</span> <span class="pre">(x,</span> <span class="pre">y,</span> <span class="pre">z)</span></code></p></li>
<li><p>所以一张图像对应了一个 <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">×</span> <span class="pre">H</span> <span class="pre">×</span> <span class="pre">3</span></code> 的三维张量</p></li>
</ul>
<p>并且，这个 pointmap 和原始的 RGB 图像是 <strong>一一对应</strong> 的：</p>
<div class="math notranslate nohighlight">
\[I[i, j] ↔ X[i, j]\]</div>
<p>即每个像素 <code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> 在图像中有颜色信息 <code class="docutils literal notranslate"><span class="pre">I[i,j]</span></code>，同时也有一个 3D 点 <code class="docutils literal notranslate"><span class="pre">X[i,j]</span></code>，表示这个像素点在现实三维空间中所对应的位置。</p>
<p>👉 可以理解为：<strong>点图是一个像素到三维世界坐标的映射。</strong></p>
<blockquote>
<div><p>📌 注意：这里假设每个像素射出去的光线只能打到一个唯一的三维点，即不考虑透明或半透明表面。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="cameras-and-scene">
<h4>🟡 <strong>相机与场景建模（Cameras and Scene）</strong><a class="headerlink" href="#cameras-and-scene" title="此标题的永久链接">¶</a></h4>
<p>我们使用相机的内参矩阵 <span class="math notranslate nohighlight">\(K ∈ ℝ^{(3×3)}\)</span> 和一张对应的 <strong>深度图 <span class="math notranslate nohighlight">\(D ∈ ℝ^{(W×H)}\)</span></strong> 来生成 pointmap <code class="docutils literal notranslate"><span class="pre">X</span></code>。</p>
<p>具体计算公式如下：</p>
<div class="math notranslate nohighlight">
\[X[i,j] = K⁻¹ * [i * D[i,j], j * D[i,j], D[i,j]]^T\]</div>
<p>意思是：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">D[i,j]</span></code> 是第 <code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> 个像素点的深度值（相机到物体的距离）</p></li>
<li><p>我们将像素坐标 <code class="docutils literal notranslate"><span class="pre">(i,</span> <span class="pre">j)</span></code> 乘以深度，再加上深度值本身，组成一个 <code class="docutils literal notranslate"><span class="pre">[x,</span> <span class="pre">y,</span> <span class="pre">z]</span></code> 向量</p></li>
<li><p>然后用相机内参矩阵的逆 <code class="docutils literal notranslate"><span class="pre">K⁻¹</span></code> 反投影回相机坐标系，得到 3D 点 <code class="docutils literal notranslate"><span class="pre">X[i,j]</span></code></p></li>
</ul>
<blockquote>
<div><p>✅ 这一步是从“图像平面 + 深度” → “三维坐标”的标准反投影操作。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="id13">
<h4>🟡 <strong>跨相机坐标转换（多视角点图的转换）</strong><a class="headerlink" href="#id13" title="此标题的永久链接">¶</a></h4>
<p>定义一个记号： <span class="math notranslate nohighlight">\(X^{n,m}\)</span> 表示第 <code class="docutils literal notranslate"><span class="pre">n</span></code> 个相机拍到的场景，通过转换，映射到第 <code class="docutils literal notranslate"><span class="pre">m</span></code> 个相机的坐标系中。</p>
<p>具体公式是：</p>
<div class="math notranslate nohighlight">
\[X^{n,m} = P_m * P_n⁻¹ * h(X^n)\]</div>
<p>解释如下：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X^n\)</span>：在相机 <code class="docutils literal notranslate"><span class="pre">n</span></code> 的坐标系下的点图</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">h(X)</span></code>：表示对 <code class="docutils literal notranslate"><span class="pre">X</span></code> 中的每个点做齐次变换 <code class="docutils literal notranslate"><span class="pre">(x,y,z)</span> <span class="pre">→</span> <span class="pre">(x,y,z,1)</span></code>，这是为了解决矩阵乘法需要齐次坐标</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P_n</span></code> 和 <code class="docutils literal notranslate"><span class="pre">P_m</span></code> 是两个相机的 <strong>世界到相机坐标的变换矩阵（pose）</strong>， <span class="math notranslate nohighlight">\(P ∈ ℝ^{(3×4)}\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P_n⁻¹</span></code>：把相机 <code class="docutils literal notranslate"><span class="pre">n</span></code> 坐标系的点变换到世界坐标系</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">P_m</span></code>：再从世界坐标系变换到相机 <code class="docutils literal notranslate"><span class="pre">m</span></code> 的坐标系</p></li>
</ul>
<blockquote>
<div><p>✅ 所以这一公式完成了从相机 <code class="docutils literal notranslate"><span class="pre">n</span></code> 拍到的点图，变换到相机 <code class="docutils literal notranslate"><span class="pre">m</span></code> 的坐标系下的点图。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="id14">
<h4>✍️ 总结：<a class="headerlink" href="#id14" title="此标题的永久链接">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>项目</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Pointmap (X)</strong></p></td>
<td><p>图像中每个像素点对应的 3D 点，维度为 <code class="docutils literal notranslate"><span class="pre">W</span> <span class="pre">×</span> <span class="pre">H</span> <span class="pre">×</span> <span class="pre">3</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>I[i,j] ↔ X[i,j]</strong></p></td>
<td><p>每个像素和其三维点一一对应</p></td>
</tr>
<tr class="row-even"><td><p><strong>X[i,j] 生成方式</strong></p></td>
<td><p>利用深度图和相机内参进行反投影</p></td>
</tr>
<tr class="row-odd"><td><p><strong>X^{n,m}</strong></p></td>
<td><p>相机 <code class="docutils literal notranslate"><span class="pre">n</span></code> 拍到的点图，转换到相机 <code class="docutils literal notranslate"><span class="pre">m</span></code> 的坐标系中</p></td>
</tr>
<tr class="row-even"><td><p><strong>变换公式</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">X^{n,m}</span> <span class="pre">=</span> <span class="pre">P_m</span> <span class="pre">*</span> <span class="pre">P_n⁻¹</span> <span class="pre">*</span> <span class="pre">h(X^n)</span></code></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="overview">
<h3>3.1 Overview<a class="headerlink" href="#overview" title="此标题的永久链接">¶</a></h3>
<section id="id15">
<h4>🧠 <strong>目标概述</strong><a class="headerlink" href="#id15" title="此标题的永久链接">¶</a></h4>
<p>训练一个网络来解决「<strong>广义立体视觉下的3D重建任务</strong>」，采用的方式是 <strong>直接回归（regression）</strong>。</p>
<p>输入：</p>
<ul class="simple">
<li><p>两张 RGB 图像 <span class="math notranslate nohighlight">\( I^1, I^2 \in \mathbb{R}^{W \times H \times 3} \)</span></p></li>
</ul>
<p>输出：</p>
<ul class="simple">
<li><p>两个与图像对应的 <strong>点图（pointmaps）</strong>，分别是 <span class="math notranslate nohighlight">\( X^{1,1}, X^{2,1} \in \mathbb{R}^{W \times H \times 3} \)</span>（每个像素都有一个三维坐标）</p></li>
<li><p>以及各自的 <strong>置信图（confidence maps）</strong> <span class="math notranslate nohighlight">\( C^{1,1}, C^{2,1} \in \mathbb{R}^{W \times H} \)</span></p></li>
</ul>
<p><strong>特别之处：</strong> 这两个点图都用第一个图像 <span class="math notranslate nohighlight">\( I^1 \)</span> 的坐标系来表达，这是一个非常规但很有优势的设计。</p>
</section>
<section id="inspired-by-croco">
<h4>🏗️ <strong>网络架构（Inspired by CroCo）</strong><a class="headerlink" href="#inspired-by-croco" title="此标题的永久链接">¶</a></h4>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/OXutWV.png" /></p>
<p>Figure 2: Architecture of the network <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> . Two views of a scene <span class="math notranslate nohighlight">\((I^{1},I^{2})\)</span> are first encoded in a Siamese manner with a shared ViT encoder. The resulting token representations <span class="math notranslate nohighlight">\(F^{1}\)</span> and <span class="math notranslate nohighlight">\(F^{2}\)</span> are then passed to two transformer decoders that constantly exchange information via cross-attention. Finally, two regression heads output the two corresponding pointmaps and associated confidence maps. Importantly, the two pointmaps are expressed in the same coordinate frame of the first image <span class="math notranslate nohighlight">\(I^{1}\)</span>. The network <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is trained using a simple regression loss</p>
<hr class="docutils" />
<p>整体结构是一个 <strong>Siamese 双支路网络</strong>，即：</p>
<ol class="arabic simple">
<li><p>编码阶段（Encoder）：
两个图像 <span class="math notranslate nohighlight">\( I^1, I^2 \)</span> 会被一个共享权重的 ViT（Vision Transformer）编码器编码为：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( F^1 = \text{Encoder}(I^1) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( F^2 = \text{Encoder}(I^2) \)</span></p></li>
</ul>
</li>
</ol>
<hr class="docutils" />
<ol class="arabic" start="2">
<li><p>解码阶段（Decoder）：
采用类似 CroCo 的 decoder，由多个 transformer block 组成，每个 block 有：</p>
<ul class="simple">
<li><p><strong>Self-Attention（自注意）</strong>：在每个视角内进行</p></li>
<li><p><strong>Cross-Attention（交叉注意）</strong>：跨两个图像之间的信息交互（比如 <span class="math notranslate nohighlight">\( G^1_i \)</span> 需要看 <span class="math notranslate nohighlight">\( G^2_{i-1} \)</span> 的 token）</p></li>
</ul>
<p>这个设计的关键点在于 <strong>每一层都在两个视角之间传递信息</strong>，确保生成的点图彼此对齐（aligned）。</p>
<p>初始化：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( G_0^1 := F^1 \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( G_0^2 := F^2 \)</span></p></li>
</ul>
<p>每一层的计算形如：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
G_i^1 = \text{DecoderBlock}_i^1(G_{i-1}^1, G_{i-1}^2) \\
    G_i^2 = \text{DecoderBlock}_i^2(G_{i-1}^2, G_{i-1}^1)
\end{split}\]</div>
</li>
</ol>
<hr class="docutils" />
<ol class="arabic" start="3">
<li><p>回归头（Regression Heads）：
将每个分支的 decoder token 送入各自的回归头，输出最终的：</p>
<ul class="simple">
<li><p>点图（pointmap）</p></li>
<li><p>置信图（confidence map）</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
X^{1,1}, C^{1,1} = \text{Head}^1(G_0^1, ..., G_B^1) \\
    X^{2,1}, C^{2,1} = \text{Head}^2(G_0^2, ..., G_B^2)
\end{split}\]</div>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="discussion">
<h4>💡 <strong>设计讨论（Discussion）</strong><a class="headerlink" href="#discussion" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>输出的点图只是「相对点云」，尺度是未知的（即 scale-ambiguous）。</p></li>
<li><p>网络本身<strong>不强制遵守几何约束</strong>，而是完全依赖数据中的几何一致性来「学出几何」。</p></li>
<li><p>虽然不是基于传统相机模型（如投影几何），但可以<strong>利用强大的预训练模型</strong>来获得更强泛化。</p></li>
<li><p>这让它相比于任务特定设计的网络，<strong>更通用、扩展性更强</strong>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id16">
<h4>✅ 总结<a class="headerlink" href="#id16" title="此标题的永久链接">¶</a></h4>
<blockquote>
<div><p>DUSt3R 用一个 ViT 编码器 + 跨视角 transformer 解码器 + 回归头，直接从两张 RGB 图中输出两个对齐的 3D 点图（不依赖几何建模），通过训练数据自动学习几何。</p>
</div></blockquote>
</section>
</section>
<hr class="docutils" />
<section id="training-objective">
<h3>3.2 Training Objective<a class="headerlink" href="#training-objective" title="此标题的永久链接">¶</a></h3>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/7KBo1p.png" /></p>
<p>Figure 3: shows the outcome of global alignment</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/3ROUVD.png" /></p>
<p>Figure 3: Reconstruction examples on two scenes never seen during training. From left to right: RGB, depth map, confidence map, reconstruction. The left scene shows the raw result output from <span class="math notranslate nohighlight">\(\mathcal{F}(I^{1},I^{2})\)</span> .</p>
<section id="d-3d-regression-loss">
<h4>一、3D回归损失（3D Regression Loss）<a class="headerlink" href="#d-3d-regression-loss" title="此标题的永久链接">¶</a></h4>
<p>这是网络的<strong>主要训练目标</strong>，核心思想是：</p>
<blockquote>
<div><p><strong>直接在三维空间上进行点的回归损失计算</strong>，即：预测的3D点图（pointmap）与真实的3D点图之间的误差（欧几里得距离）。</p>
</div></blockquote>
<ul>
<li><p>1.变量定义</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\bar{X}^{1,1}, \bar{X}^{2,1}\)</span>：来自 ground-truth（真实点图）的两幅图像对应的 3D 点图。</p></li>
<li><p><span class="math notranslate nohighlight">\(X^{1,1}, X^{2,1}\)</span>：对应预测的 3D 点图。</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}^{1}, \mathcal{D}^{2}\)</span>：分别是有效像素（即有 ground-truth 的像素）的位置集合。</p></li>
<li><p><span class="math notranslate nohighlight">\(i \in \mathcal{D}^v\)</span>：视角 <span class="math notranslate nohighlight">\(v\)</span> 中的一个有效像素点索引。</p></li>
<li><p><span class="math notranslate nohighlight">\(X_i^{v,1}\)</span>：在视角 <span class="math notranslate nohighlight">\(v\)</span> 下，像素 <span class="math notranslate nohighlight">\(i\)</span> 对应的预测 3D 点。</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{X}_i^{v,1}\)</span>：对应的 ground-truth 3D 点。</p></li>
</ul>
</li>
<li><p>2.损失函数定义
单个像素 <span class="math notranslate nohighlight">\(i\)</span> 的回归损失是预测点与真实点之间的 <strong>归一化欧几里得距离</strong>：</p>
<div class="math notranslate nohighlight">
\[
\ell_{\text{regr}}(v,i) = \left\|\frac{1}{z}X^{v,1}_{i}-\frac{1}{\bar{z}}\bar{X}^{v,1}_{i}\right\|
\]</div>
<ul class="simple">
<li><p>归一化的原因：
由于输入图像对的尺度/深度范围不确定，预测结果和 ground-truth 可能存在尺度差异。所以：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(z\)</span> 是对预测点图的整体尺度归一化因子</p></li>
<li><p><span class="math notranslate nohighlight">\(\bar{z}\)</span> 是对真实点图的整体归一化因子</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>归一化方式为：</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{norm}(X^1, X^2) = \frac{1}{|\mathcal{D}^1| + |\mathcal{D}^2|} \sum_{v \in \{1, 2\}} \sum_{i \in \mathcal{D}^v} \|X^v_i\|
\]</div>
<ul class="simple">
<li><p>意思是：<strong>所有有效像素的平均欧几里得距离（从原点出发）</strong>。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="confidence-aware-loss">
<h4>二、置信度感知损失（Confidence-aware Loss）<a class="headerlink" href="#confidence-aware-loss" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>现实中图像存在很多“不确定区域”：</p>
<ul>
<li><p>比如天空、透明物体、反射表面等，这些地方的 3D 重建不靠谱；</p></li>
<li><p>网络预测这些地方时信心也低。</p></li>
</ul>
</li>
<li><p>引入置信度的原因：
让网络不仅预测 3D 点，还预测对每个像素的 <strong>置信度分数 <span class="math notranslate nohighlight">\(C_i^{v,1}\)</span></strong>，用来调节该像素对损失函数的影响。</p></li>
<li><p>置信度加权后的最终损失函数：
$<span class="math notranslate nohighlight">\(
\mathcal{L}_{\text{conf}} = \sum_{v \in \{1,2\}} \sum_{i \in \mathcal{D}^v} C_i^{v,1} \cdot \ell_{\text{regr}}(v, i) - \alpha \log C_i^{v,1}
\)</span>$</p></li>
<li><p>含义如下：</p>
<ul>
<li><p><strong>第一项</strong>是置信度加权的 3D 回归损失；</p></li>
<li><p><strong>第二项</strong>是正则项，鼓励置信度不要无限大（否则模型就可以总是给高置信度来压低损失）；</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> 是权重超参数。</p></li>
</ul>
</li>
<li><p>为保证 <span class="math notranslate nohighlight">\(C_i &gt; 0\)</span>，定义方式为：
$<span class="math notranslate nohighlight">\(
C_i^{v,1} = 1 + \exp(\widetilde{C}_i^{v,1}) &gt; 1
\)</span>$</p>
<ul>
<li><p>即：预测的是 <span class="math notranslate nohighlight">\(\widetilde{C}\)</span>，然后通过 <code class="docutils literal notranslate"><span class="pre">exp</span></code> 变换加1，保证最终置信度始终为正。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id17">
<h4>总结理解：<a class="headerlink" href="#id17" title="此标题的永久链接">¶</a></h4>
<p>DUSt3R 网络训练目标设计非常简洁直观：</p>
<ul class="simple">
<li><p>核心思想：直接对<strong>三维点云进行回归</strong>，而非像很多方法那样预测深度或视差图；</p></li>
<li><p>为了应对现实图像中存在的不确定区域，引入<strong>每像素置信度预测机制</strong>，并将其用于加权损失；</p></li>
<li><p>整体损失具有<strong>尺度不变性</strong>（通过点图归一化）和<strong>不确定性鲁棒性</strong>（通过置信度权重调节）。</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="downstream-applications">
<h3>3.3 Downstream Applications<a class="headerlink" href="#downstream-applications" title="此标题的永久链接">¶</a></h3>
<section id="point-matching">
<h4>✅ <strong>Point Matching（点匹配）</strong><a class="headerlink" href="#point-matching" title="此标题的永久链接">¶</a></h4>
<p>这部分讲的是如何在两张图片之间建立像素级别的匹配关系：</p>
<blockquote>
<div><p>在两张图片的 pointmap（例如 <span class="math notranslate nohighlight">\(X^{1,1}\)</span> 和 <span class="math notranslate nohighlight">\(X^{1,2}\)</span>）中，每个像素对应一个三维点（x, y, z）。所以可以直接在三维空间中使用最近邻（Nearest Neighbor, NN）搜索来做点匹配。</p>
</div></blockquote>
<ul>
<li><p><strong>双向最近邻匹配</strong>：为了减少错误匹配，使用的是 <em>互为最近邻</em> 的点对：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{M}_{1,2} = \{(i, j)\ |\ i = \text{NN}_{1}^{1,2}(j)\ \text{and}\ j = \text{NN}_{1}^{2,1}(i)\}
\]</div>
<ul class="simple">
<li><p>也就是说，从图像 1 的第 <span class="math notranslate nohighlight">\( i \)</span> 个像素出发，在图像 2 中找到最近的三维点是 <span class="math notranslate nohighlight">\( j \)</span>，</p></li>
<li><p>同时，从图像 2 的 <span class="math notranslate nohighlight">\( j \)</span> 出发，最近的三维点回到了图像 1 的 <span class="math notranslate nohighlight">\( i \)</span>，则认为这是一个匹配对。</p></li>
</ul>
</li>
<li><p>最近邻的定义：</p>
<div class="math notranslate nohighlight">
\[
\text{NN}_{k}^{n,m}(i) = \arg\min_{j \in \{0,\ldots,WH\}} \|X^{n,k}_j - X^{m,k}_i\|
\]</div>
<p>表示在图像 <span class="math notranslate nohighlight">\( n \)</span> 和 <span class="math notranslate nohighlight">\( m \)</span> 的第 <span class="math notranslate nohighlight">\( k \)</span> 层之间，对 <span class="math notranslate nohighlight">\( i \)</span> 点寻找最小欧氏距离的 <span class="math notranslate nohighlight">\( j \)</span> 点。</p>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="recovering-intrinsics">
<h4>✅ <strong>Recovering Intrinsics（恢复相机内参）</strong><a class="headerlink" href="#recovering-intrinsics" title="此标题的永久链接">¶</a></h4>
<blockquote>
<div><p>Pointmap 是以图像坐标系表达的三维点，所以可以用这些点倒推出相机的内参，特别是焦距 <span class="math notranslate nohighlight">\( f \)</span>。</p>
</div></blockquote>
<ul>
<li><p>假设主点（principal point）在图像中心，像素是正方形，所以只需估计焦距 <span class="math notranslate nohighlight">\( f_1^* \)</span>。</p></li>
<li><p>优化目标是最小化 3D 点投影与实际像素位置之间的差异：</p>
<div class="math notranslate nohighlight">
\[
f_1^* = \arg\min_{f_1} \sum_{i=0}^{W} \sum_{j=0}^{H} C^{1,1}_{i,j} \left\|(i',j') - f_1 \cdot \frac{(X^{1,1}_{i,j,0}, X^{1,1}_{i,j,1})}{X^{1,1}_{i,j,2}} \right\|
\]</div>
<ul class="simple">
<li><p>其中：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( (i', j') = (i - W/2, j - H/2) \)</span>，将像素中心化；</p></li>
<li><p><span class="math notranslate nohighlight">\( X^{1,1}_{i,j,*} \)</span> 是三维点坐标；</p></li>
<li><p><span class="math notranslate nohighlight">\( C^{1,1}_{i,j} \)</span> 是权重（例如可见性、置信度）。</p></li>
</ul>
</li>
</ul>
</li>
<li><p>可用 Weiszfeld 算法等快速迭代方法求解。</p></li>
<li><p>第二张图像的焦距 <span class="math notranslate nohighlight">\( f_2^* \)</span> 同理，只需换用 <span class="math notranslate nohighlight">\( X^{2,2} \)</span>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="relative-pose-estimation">
<h4>✅ <strong>Relative Pose Estimation（相对姿态估计）</strong><a class="headerlink" href="#relative-pose-estimation" title="此标题的永久链接">¶</a></h4>
<p>相机之间的相对位置和方向可以通过两种方式估计：</p>
<ul>
<li><p>方法一：传统方式</p>
<ol class="arabic simple">
<li><p>用 pointmap 做点匹配；</p></li>
<li><p>恢复内参；</p></li>
<li><p>用匹配点对计算 <em>基础矩阵或本质矩阵（Epipolar Matrix）</em>；</p></li>
<li><p>从中恢复旋转 <span class="math notranslate nohighlight">\( R \)</span> 和位移 <span class="math notranslate nohighlight">\( t \)</span>。</p></li>
</ol>
</li>
<li><p>方法二：Procrustes 对齐（推荐）</p>
<ul class="simple">
<li><p>更直接的方法是对两个 pointmap 直接对齐：</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
R^*, t^* = \arg\min_{\sigma, R, t} \sum_i C^{1,1}_i C^{1,2}_i \left\| \sigma(R X^{1,1}_i + t) - X^{1,2}_i \right\|^2
\]</div>
<ul class="simple">
<li><p>这是经典的 <em>带缩放的 Procrustes 对齐</em> 问题；</p></li>
<li><p>目标是寻找最优的旋转 <span class="math notranslate nohighlight">\( R \)</span>、平移 <span class="math notranslate nohighlight">\( t \)</span> 和尺度因子 <span class="math notranslate nohighlight">\( \sigma \)</span>，使两个点云对齐；</p></li>
<li><p><span class="math notranslate nohighlight">\( C^{1,1}_i \)</span> 和 <span class="math notranslate nohighlight">\( C^{1,2}_i \)</span> 是每个点的权重或置信度。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id18">
<h4>总结<a class="headerlink" href="#id18" title="此标题的永久链接">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>任务</p></th>
<th class="head"><p>做法</p></th>
<th class="head"><p>特点</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>点匹配</p></td>
<td><p>最近邻搜索 pointmap，双向过滤</p></td>
<td><p>快速、鲁棒</p></td>
</tr>
<tr class="row-odd"><td><p>相机内参恢复</p></td>
<td><p>优化像素与 3D 投影的差异，求焦距</p></td>
<td><p>可直接从 pointmap 得到</p></td>
</tr>
<tr class="row-even"><td><p>相对姿态估计</p></td>
<td><p>可选：传统基础矩阵 or Procrustes 点云对齐</p></td>
<td><p>后者更直接、高效</p></td>
</tr>
</tbody>
</table>
<p>这个方法的关键优势是：<strong>它将像素映射到三维点（pointmap）后，可以跳过中间很多传统步骤，直接用 3D 几何做视觉任务。</strong></p>
</section>
</section>
<hr class="docutils" />
<section id="global-alignment">
<h3>3.4 Global Alignment<a class="headerlink" href="#global-alignment" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>核心思想是：通过构建一个图结构，将多个图像对中预测的点云进行<strong>全局对齐</strong>，从而将多个视角下的点图（pointmap）联合优化，统一到一个三维坐标系中。这种优化是 <strong>DUSt3R</strong> 这一类方法实现跨图像一致性的关键。我们可以从三个层面来理解它：</p></li>
</ul>
<hr class="docutils" />
<section id="pairwise-graph">
<h4>✦ 一、构建图结构: Pairwise Graph<a class="headerlink" href="#pairwise-graph" title="此标题的永久链接">¶</a></h4>
<p>给定一组图像 <span class="math notranslate nohighlight">\(\{I^1, I^2, \ldots, I^N\}\)</span>，我们构造一个连接图 <span class="math notranslate nohighlight">\(\mathcal{G}(\mathcal{V}, \mathcal{E})\)</span>：</p>
<ul class="simple">
<li><p><strong>节点 <span class="math notranslate nohighlight">\(\mathcal{V}\)</span>：</strong> 每个图像为一个节点。</p></li>
<li><p><strong>边 <span class="math notranslate nohighlight">\(\mathcal{E}\)</span>：</strong> 图像对之间存在视觉重叠（通过图像检索或网络 <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> 推理得到置信度高的图像对）才建立边。</p></li>
</ul>
<p>这样一来，我们就得到了图像之间“可以相互配准”的关系图。</p>
</section>
<hr class="docutils" />
<section id="global-optimization">
<h4>✦ 二、全局优化: Global optimization<a class="headerlink" href="#global-optimization" title="此标题的永久链接">¶</a></h4>
<p>目标是对每个图像 <span class="math notranslate nohighlight">\(n\)</span>，估计其在世界坐标系下的点图 <span class="math notranslate nohighlight">\(\chi^n \in \mathbb{R}^{W \times H \times 3}\)</span>，即将每幅图像的深度映射结果全局对齐。</p>
<p>步骤如下：</p>
<ol class="arabic">
<li><p><strong>预测每对图像的点图和置信度：</strong></p>
<p>对每条边 <span class="math notranslate nohighlight">\(e = (n, m)\)</span>，通过网络 <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> 预测出：</p>
<ul class="simple">
<li><p>图像 <span class="math notranslate nohighlight">\(n\)</span> 和 <span class="math notranslate nohighlight">\(m\)</span> 的点图 <span class="math notranslate nohighlight">\(X^{n,n}, X^{m,n}\)</span>（统一在图像 <span class="math notranslate nohighlight">\(n\)</span> 的坐标系中）</p></li>
<li><p>对应的置信度图 <span class="math notranslate nohighlight">\(C^{n,n}, C^{m,n}\)</span></p></li>
</ul>
<p>为统一记号，定义：
$<span class="math notranslate nohighlight">\(
X^{n,e} := X^{n,n},\quad X^{m,e} := X^{m,n}
\)</span>$</p>
</li>
<li><p><strong>引入刚性变换(rigid transformation)和尺度：</strong></p>
<p>每条图像对 <span class="math notranslate nohighlight">\(e\)</span> 关联一个刚性变换矩阵 <span class="math notranslate nohighlight">\(P_e \in \mathbb{R}^{3 \times 4}\)</span>（包含旋转和平移）和尺度 <span class="math notranslate nohighlight">\(\sigma_e &gt; 0\)</span></p>
</li>
<li><p><strong>构建优化目标：</strong></p>
<p>最终优化目标是最小化每个点图 <span class="math notranslate nohighlight">\(\chi^n\)</span> 与其预测的变换点图 <span class="math notranslate nohighlight">\(\sigma_e P_e X^{n,e}\)</span> 之间的加权 L2 距离，权重来自置信度图：</p>
<div class="math notranslate nohighlight">
\[
   \chi^* = \arg\min_{\chi, P, \sigma} \sum_{e \in \mathcal{E}} \sum_{v \in \{n, m\}} \sum_{i=1}^{HW} C^{v,e}_i \left\| \chi^v_i - \sigma_e P_e X^{v,e}_i \right\|
   \]</div>
<p>直观理解就是：让每张图像的预测点图，在刚性变换与尺度下，尽可能接近它在世界坐标中的估计值。</p>
</li>
<li><p><strong>防止退化（全零尺度）：</strong></p>
<p>为防止 <span class="math notranslate nohighlight">\(\sigma_e = 0\)</span> 导致的退化，增加约束：
$<span class="math notranslate nohighlight">\(
\prod_e \sigma_e = 1
\)</span>$</p>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="camera-parameter-recovery">
<h4>✦ 三、相机参数恢复：Camera Parameter Recovery<a class="headerlink" href="#camera-parameter-recovery" title="此标题的永久链接">¶</a></h4>
<p>前面我们是直接优化每个图像的点图 <span class="math notranslate nohighlight">\(\chi^n\)</span>，但实际上也可以从深度图 <span class="math notranslate nohighlight">\(D^n\)</span> 反推 <span class="math notranslate nohighlight">\(\chi^n\)</span>：</p>
<ul class="simple">
<li><p>使用标准针孔相机模型（Eq. 1）：
$<span class="math notranslate nohighlight">\(
\chi^n_{i,j} := P_n^{-1} \cdot h \left( K_n^{-1} \cdot 
\begin{bmatrix}
i D^n_{i,j} \\
j D^n_{i,j} \\
D^n_{i,j}
\end{bmatrix} \right)
\)</span>$</p></li>
<li><p>这样可以通过 <span class="math notranslate nohighlight">\(\chi^n\)</span> 反推出每个相机的位姿 <span class="math notranslate nohighlight">\(P_n\)</span> 和内参 <span class="math notranslate nohighlight">\(K_n\)</span>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id19">
<h4>✅ 总结重点<a class="headerlink" href="#id19" title="此标题的永久链接">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模块</p></th>
<th class="head"><p>作用</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> 网络</p></td>
<td><p>对图像对进行点图预测</p></td>
</tr>
<tr class="row-odd"><td><p>图结构 <span class="math notranslate nohighlight">\(\mathcal{G}\)</span></p></td>
<td><p>指示哪些图像需要对齐</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(P_e, \sigma_e\)</span></p></td>
<td><p>对每对图像的刚体变换与尺度进行建模</p></td>
</tr>
<tr class="row-odd"><td><p>优化目标</p></td>
<td><p>保证所有点图在统一世界坐标系中对齐，保持一致性</p></td>
</tr>
<tr class="row-even"><td><p>相机参数恢复</p></td>
<td><p>利用优化后的点图反推每个图像的位姿与内参</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<hr class="docutils" />
<section id="experiments-with-dust3r">
<h2>4. Experiments with DUSt3R<a class="headerlink" href="#experiments-with-dust3r" title="此标题的永久链接">¶</a></h2>
<section id="id20">
<h3>前言<a class="headerlink" href="#id20" title="此标题的永久链接">¶</a></h3>
<section id="training-data">
<h4>🧠 <strong>1. 训练数据（Training data）</strong><a class="headerlink" href="#training-data" title="此标题的永久链接">¶</a></h4>
<p>他们用了 <strong>8个数据集</strong> 来训练模型，涵盖各种场景：</p>
<ul class="simple">
<li><p><strong>室内/室外</strong>（例如 ScanNet++ 是室内，Waymo 是户外）</p></li>
<li><p><strong>真实/合成</strong>（例如 Blended MVS 是合成数据）</p></li>
<li><p><strong>面向物体/面向场景</strong>（例如 CO3D 是物体中心的）</p></li>
</ul>
<p>有些数据集没有自带图像对（image pairs），采用了一种方法（参考文献[149]）来自动构建图像对：</p>
<blockquote>
<div><p>使用现成的图像检索 + 特征匹配方法，筛选和验证图像对。</p>
</div></blockquote>
<p>最终一共收集了 <strong>850 万对图像（8.5M pairs）</strong> 作为训练数据。</p>
</section>
<hr class="docutils" />
<section id="training-details">
<h4>🏗️ <strong>2. 训练细节（Training details）</strong><a class="headerlink" href="#training-details" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><strong>数据平衡</strong>：每轮训练中，会均匀采样每个数据集中的图像对，防止某些数据集占比过大。</p></li>
<li><p><strong>图像分辨率策略</strong>：</p>
<ul>
<li><p>初期训练用较小尺寸 <code class="docutils literal notranslate"><span class="pre">224×224</span></code></p></li>
<li><p>后期再用更高的 <code class="docutils literal notranslate"><span class="pre">512px</span></code> 最大边尺寸图像训练（提高性能但成本更高）</p></li>
</ul>
</li>
<li><p><strong>图像形状处理</strong>：随机选择长宽比（如 16:9、4:3），裁剪后统一调整最大边到 512px，使模型对不同形状图像更鲁棒。</p></li>
<li><p><strong>数据增强</strong>：使用常见的数据增强方法（没有具体列出，但可理解为旋转、缩放、颜色扰动等）。</p></li>
<li><p><strong>网络结构</strong>：</p>
<ul>
<li><p><strong>Encoder：ViT-Large（大模型）</strong></p></li>
<li><p><strong>Decoder：ViT-Base（中模型）</strong></p></li>
<li><p><strong>Head：DPT Head（用于深度预测等任务）</strong></p></li>
</ul>
</li>
<li><p><strong>预训练初始化</strong>：
使用 CroCo 预训练模型权重作为初始化。CroCo 是一种 <strong>跨视图图像恢复任务的自监督预训练方法</strong>，非常适合 3D 视觉任务。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="evaluation">
<h4>📊 <strong>3. 评估（Evaluation）</strong><a class="headerlink" href="#evaluation" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>使用 <strong>同一个 DUSt3R 模型（DUSt3R 512）</strong> 来评估多个下游任务，从不对特定任务进行微调（zero-shot 评估风格）。</p></li>
<li><p>所有测试图像都会被等比例缩放到最大边 512px。</p></li>
<li><p>针对每个下游任务（如三维重建、配准、姿态估计等），采用不同输出方式，具体方法见第 3.3 和 3.4 节。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="qualitative-results">
<h4>🎨 <strong>4. 定性结果（Qualitative results）</strong><a class="headerlink" href="#qualitative-results" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>DUSt3R 即使在困难场景中也能生成高质量、稠密的 3D 重建结果。</p></li>
<li><p>附录中展示了很多 <strong>“非挑选”的可视化例子</strong>，包括图像对和多视图的重建效果。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id21">
<h4>✅ 总结一下关键点：<a class="headerlink" href="#id21" title="此标题的永久链接">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>部分</p></th>
<th class="head"><p>内容</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>训练数据</strong></p></td>
<td><p>多源数据集、850万图像对、多样性强</p></td>
</tr>
<tr class="row-odd"><td><p><strong>训练方法</strong></p></td>
<td><p>小分辨率预热→大分辨率主训，数据增强，随机长宽比</p></td>
</tr>
<tr class="row-even"><td><p><strong>模型结构</strong></p></td>
<td><p>ViT-Large 编码器 + ViT-Base 解码器 + DPT Head</p></td>
</tr>
<tr class="row-odd"><td><p><strong>预训练</strong></p></td>
<td><p>使用 CroCo 初始化，适合 3D 任务</p></td>
</tr>
<tr class="row-even"><td><p><strong>评估策略</strong></p></td>
<td><p>单模型，多任务评估，无微调，输入统一为 512px</p></td>
</tr>
<tr class="row-odd"><td><p><strong>结果展示</strong></p></td>
<td><p>可视化效果好，不挑图，鲁棒性强</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="visual-localization">
<h3><strong>4.1 视觉定位（Visual Localization）</strong><a class="headerlink" href="#visual-localization" title="此标题的永久链接">¶</a></h3>
<p><strong>实验目标：</strong>
评估 DUSt3R 在绝对位姿估计任务（absolute pose estimation）中的表现，测试集为两个经典数据集：</p>
<ul class="simple">
<li><p><strong>7Scenes</strong>：7个室内场景，RGB-D 图像+6自由度相机位姿。</p></li>
<li><p><strong>Cambridge Landmarks</strong>：6个室外场景，RGB 图像+SfM恢复的相机位姿。</p></li>
</ul>
<p><strong>评价指标：</strong></p>
<ul class="simple">
<li><p>中位数平移误差（translation error，单位 cm）</p></li>
<li><p>中位数旋转误差（rotation error，单位 °）</p></li>
</ul>
<p><strong>实验方法：</strong></p>
<ul class="simple">
<li><p>DUSt3R 作为一个 <strong>2D-2D像素匹配器（pixel matcher）</strong>，用于计算查询图像与数据库图像之间的像素点对应。</p></li>
<li><p>数据库图像通过图像检索方法 <strong>AP-GeM</strong> 检索获得。</p></li>
<li><p>对于 Cambridge 使用 top 20 图像，7Scenes 使用 top 1。</p></li>
<li><p>没有使用任何后处理或精化（例如 bundle adjustment），也没有使用 GT 内参会有单独说明。</p></li>
</ul>
<p><strong>关键点：</strong></p>
<ul class="simple">
<li><p>仅使用 DUSt3R 输出的点云匹配（pointmap）来做位姿估计，完全 <strong>零训练用于视觉定位的监督</strong>。</p></li>
<li><p>DUSt3R 训练时也没有见过测试集中的图像。</p></li>
<li><p>与特征匹配类方法（如 HLoc）和端到端学习方法比较，表现相当甚至超越部分强基线（如 HLoc）。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="multi-view-pose-estimation">
<h3><strong>4.2 多视角相对位姿估计（Multi-view Pose Estimation）</strong><a class="headerlink" href="#multi-view-pose-estimation" title="此标题的永久链接">¶</a></h3>
<p><strong>实验目标：</strong></p>
<ul class="simple">
<li><p>测试 DUSt3R 在多视角序列中的相对位姿估计效果。</p></li>
</ul>
<p><strong>数据集：</strong></p>
<ul class="simple">
<li><p><strong>CO3Dv2</strong>：6M 帧，来自 37K 视频，51类对象（来自 MS-COCO），使用 COLMAP 计算真值位姿。</p></li>
<li><p><strong>RealEstate10k</strong>：室内外场景，10M 帧，约 80K YouTube 视频片段，用 SLAM + BA 提取位姿。</p></li>
</ul>
<p><strong>实验设置：</strong></p>
<ul class="simple">
<li><p>对每段视频随机选 10 帧，构成 45 对图像输入 DUSt3R。</p></li>
<li><p>使用两种方式求解相对位姿：</p>
<ul>
<li><p>PnP-RANSAC</p></li>
<li><p>全局对齐（global alignment）</p></li>
</ul>
</li>
</ul>
<p><strong>评价指标：</strong></p>
<ul class="simple">
<li><p><strong>RRA&#64;15</strong>：相对旋转精度（角度误差 &lt; 15° 的比例）</p></li>
<li><p><strong>RTA&#64;15</strong>：相对平移精度（角度误差 &lt; 15° 的比例）</p></li>
<li><p><strong>mAA&#64;30</strong>：最小值 min(RRA&#64;30, RTA&#64;30) 下的平均准确率（AUC）</p></li>
</ul>
<p><strong>结果亮点：</strong></p>
<ul class="simple">
<li><p>DUSt3R 在两种数据集上都表现 <strong>优于目前最强的方法 PoseDiffusion</strong>。</p></li>
<li><p>即便只使用简单的 PnP 解法也超过现有学习与几何基线。</p></li>
<li><p>RealEstate10K 中 PoseDiffusion 使用 CO3Dv2 训练，但 DUSt3R 是纯零训练。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="monocular-depth-estimation">
<h3><strong>4.3 单目深度估计（Monocular Depth Estimation）</strong><a class="headerlink" href="#monocular-depth-estimation" title="此标题的永久链接">¶</a></h3>
<p><strong>方法简述：</strong></p>
<ul class="simple">
<li><p>直接用图像 <code class="docutils literal notranslate"><span class="pre">I</span></code> 输入 DUSt3R 自身作为查询图像和数据库图像，即 <code class="docutils literal notranslate"><span class="pre">ℱ(I,</span> <span class="pre">I)</span></code>。</p></li>
<li><p>输出点云中的 <strong>z 轴坐标即为预测深度</strong>。</p></li>
</ul>
<p><strong>数据集：</strong></p>
<ul class="simple">
<li><p><strong>户外：</strong> KITTI, DDAD</p></li>
<li><p><strong>室内：</strong> NYUv2, BONN, TUM</p></li>
</ul>
<p><strong>评价指标：</strong></p>
<ul class="simple">
<li><p><strong>AbsRel</strong>：绝对相对误差 = |y - ŷ| / y</p></li>
<li><p><strong>δ&lt;1.25</strong>：预测深度和真实深度之比在 1.25 倍以内的像素比例</p></li>
</ul>
<p><strong>结果：</strong></p>
<ul class="simple">
<li><p>DUSt3R 属于 <strong>zero-shot 模型</strong>，没有针对深度估计训练。</p></li>
<li><p>表现优于自监督方法，甚至<strong>接近或超越</strong>很多监督方法。</p></li>
<li><p>可与 SlowTv（一个融合多源数据训练的大模型）媲美。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="multi-view-depth">
<h3>4.4 多视角深度估计（Multi-view Depth）<a class="headerlink" href="#multi-view-depth" title="此标题的永久链接">¶</a></h3>
<p>📌 方法概述：</p>
<ul class="simple">
<li><p>DUSt3R 被用于多视图立体（Multi-view Stereo, MVS）深度估计任务，具体做法是：</p></li>
<li><p>利用它预测出的 <strong>点云图（pointmaps）中的 z 坐标作为深度图（depthmap）</strong>。</p></li>
<li><p>如果某张图像有多个角度的预测深度图，就会先统一缩放（rescale）到同一尺度，然后通过**置信度加权平均（weighted averaging by confidence）**的方法融合。</p></li>
</ul>
<p>📊 评估方式：</p>
<ul class="simple">
<li><p>使用了四个常见 MVS 数据集：DTU、ETH3D、Tanks and Temples、ScanNet。</p></li>
<li><p>指标包括：</p>
<ul>
<li><p><strong>Absolute Relative Error（绝对相对误差）</strong></p></li>
<li><p><strong>Inlier Ratio（内点比）</strong>：阈值为 1.03</p></li>
</ul>
</li>
<li><p>他们<strong>不使用 ground-truth 摄像机参数或深度范围</strong>，所以预测结果只有相对尺度。</p></li>
<li><p>为了做量化评估，会用 <strong>预测值和 GT 深度的中位数比值</strong>来做归一化（参考文献 [110] 的方法）。</p></li>
</ul>
<p>🏆 实验结果：</p>
<ul class="simple">
<li><p>DUSt3R 在 <strong>ETH3D 数据集上达到 SOTA 精度</strong>，在所有数据集上总体表现也超过了大部分使用 GT 摄像机位姿的 SOTA 方法。</p></li>
<li><p><strong>速度方面</strong>也远超传统的 COLMAP 重建管线。</p></li>
<li><p>这说明该方法在多种场景（室内/室外，小尺度/大尺度）下都很适用，尤其在没见过的测试集上也能泛化（除了 ScanNet，因为它的训练集是 Habitat 数据集一部分）。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="d-reconstruction">
<h3>4.5 三维重建（3D Reconstruction）<a class="headerlink" href="#d-reconstruction" title="此标题的永久链接">¶</a></h3>
<p>📌 方法概述：</p>
<ul class="simple">
<li><p>这部分评估的是整套流程（包括前面提到的 <strong>global alignment 对齐过程</strong>）之后得到的 3D 重建质量。</p></li>
<li><p>该方法特别之处在于：<strong>无监督、无先验（unconstrained MVS）</strong> —— 也就是说，<strong>完全不依赖于摄像头的内/外参信息</strong>。</p></li>
<li><p>评估时将预测结果对齐到 ground-truth 坐标系（用 Eq.5 的参数作为常数）。</p></li>
</ul>
<p>📊 数据集与评估指标：</p>
<ul class="simple">
<li><p>使用 <strong>DTU</strong> 数据集，<strong>零样本测试（zero-shot）</strong>：不在 DTU 上微调，直接用原始模型。</p></li>
<li><p>指标包括：</p>
<ul>
<li><p><strong>Accuracy（精度）</strong>：预测点到 GT 点云的最小距离</p></li>
<li><p><strong>Completeness（完整性）</strong>：GT 点到预测点云的最小距离</p></li>
<li><p><strong>Overall</strong>：两者的平均</p></li>
</ul>
</li>
</ul>
<p>⚖️ 实验结果分析：</p>
<ul class="simple">
<li><p><strong>精度上略低于 SOTA</strong> 方法。原因：</p>
<ul>
<li><p>其他方法用了 GT 摄像机位姿并在 DTU 上训练过。</p></li>
<li><p>最好的重建方法依赖<strong>亚像素级三角测量（sub-pixel triangulation）</strong>，这需要明确的相机参数。</p></li>
</ul>
</li>
<li><p>而 DUSt3R 使用的是<strong>回归</strong>方式（regression），精度本身就会受限。</p></li>
<li><p>尽管如此，在无任何相机信息前提下，<strong>仍达到 2.7mm 的平均精度，0.8mm 的完整性，总体误差 1.7mm</strong>，对于实用场景已经相当不错，尤其考虑到它是一个 plug-and-play 的方法。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="ablations">
<h3>4.6 消融实验（Ablations）<a class="headerlink" href="#ablations" title="此标题的永久链接">¶</a></h3>
<p>📌 消融点：</p>
<ul class="simple">
<li><p>分析了两个因素对模型性能的影响：</p>
<ol class="arabic simple">
<li><p><strong>CroCo 的预训练效果</strong></p></li>
<li><p><strong>图像分辨率的影响</strong></p></li>
</ol>
</li>
</ul>
<p>📊 实验方式：</p>
<ul class="simple">
<li><p>对比了前面任务（视觉定位、多视角位姿估计、多视角深度估计）在不同设置下的表现（见表 1、2、3）。</p></li>
</ul>
<p>🧠 结论：</p>
<ul class="simple">
<li><p>高分辨率和良好的预训练策略对最终表现有显著帮助。</p></li>
<li><p>和其他研究（如 [149] 和 [78]）的结论一致：现代数据驱动方法<strong>高度依赖预训练和输入质量</strong>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id22">
<h3>🧩 总结一句话：<a class="headerlink" href="#id22" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p>DUSt3R 是一个无需摄像机参数、可 plug-and-play 的端到端 3D 重建系统，在多种任务上取得了令人瞩目的性能，虽然在极限精度上不及传统方法，但大大简化了流程，并保持了良好的泛化能力。</p>
</div></blockquote>
</section>
</section>
<section id="conclusion">
<h2>5. Conclusion<a class="headerlink" href="#conclusion" title="此标题的永久链接">¶</a></h2>
<section id="id23">
<h3>🔚 总结内容解析<a class="headerlink" href="#id23" title="此标题的永久链接">¶</a></h3>
<blockquote>
<div><p><strong>原文摘要：</strong><br />
“我们提出了一种新范式，能够在没有任何关于场景或相机的先验信息的情况下完成3D重建，并且适用于各种3D视觉任务。”</p>
</div></blockquote>
</section>
<section id="id24">
<h3>✅ 关键词解释：<a class="headerlink" href="#id24" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>新范式（novel paradigm）</strong>：DUSt3R 引入的是一种新的思路，不像传统方法那样依赖相机参数或结构先验，而是通过训练好的模型从图像中直接恢复3D信息。</p></li>
<li><p><strong>in-the-wild</strong>：这意味着方法能在真实环境中工作，不局限于实验室或合成数据。</p></li>
<li><p><strong>无需先验信息</strong>：DUSt3R 不需要已知的相机内参（intrinsics）、外参（extrinsics），或场景的深度范围等信息。</p></li>
<li><p><strong>多种任务</strong>：不只是 3D重建，DUSt3R 还能处理视觉定位、多视图几何等任务。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id25">
<h3>📌 总结关键词提炼<a class="headerlink" href="#id25" title="此标题的永久链接">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>关键词</p></th>
<th class="head"><p>含义</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Raw output</p></td>
<td><p>未后处理的网络原始输出（深度、置信图、点云）</p></td>
</tr>
<tr class="row-odd"><td><p>Viewpoint/Focal change</p></td>
<td><p>摄像机角度或焦距变化（DUSt3R能处理）</p></td>
</tr>
<tr class="row-even"><td><p>Pointmap recovery</p></td>
<td><p>从图像恢复出3D点图+相机姿态</p></td>
</tr>
<tr class="row-odd"><td><p>Unseen scenes</p></td>
<td><p>测试图像没有出现在训练中，验证泛化能力</p></td>
</tr>
<tr class="row-even"><td><p>Plug-and-play</p></td>
<td><p>开箱即用，不需要先验信息或复杂配置</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="appendix-a">
<h2>Appendix A <strong>附录概览</strong><a class="headerlink" href="#appendix-a" title="此标题的永久链接">¶</a></h2>
<p>本附录提供了关于 <strong>DUSt3R</strong> 的补充信息与定性结果，具体内容如下：</p>
<ul class="simple">
<li><p><strong>B节</strong> 展示了该架构在具有挑战性的真实世界数据集上的成对预测的定性结果，同时还描述了与本文配套的视频资料。</p></li>
<li><p><strong>C节</strong> 扩展了相关工作的讨论，涵盖了更广泛的方法类别及几何视觉任务。</p></li>
<li><p><strong>D节</strong> 提供了多视图姿态估计任务的辅助消融实验结果，这部分由于篇幅限制未能包含在主文中。</p></li>
<li><p><strong>E节</strong> 报告了一个实验性的视觉定位任务的结果，在该任务中，相机内参是未知的。</p></li>
<li><p><strong>F节</strong> 详细介绍了训练流程与所采用的数据增强策略。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="appendix-b-qualitative-results">
<h2>Appendix B.  Qualitative results<a class="headerlink" href="#appendix-b-qualitative-results" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>相关视频: <a class="reference external" href="https://europe.naverlabs.com/research/publications/dust3r-geometric-3d-vision-made-easy/">视频链接</a></p></li>
</ul>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/XmXf41.png" /></p>
<p>Figure 4:Example of 3D reconstruction of an unseen MegaDepth scene from two images (top-left). Note this is the raw output of the network, i.e. we show the output depthmaps (top-center, see Eq. 8) and confidence maps (top-right), as well as two different viewpoints on the colored pointcloud (middle and bottom). Camera parameters are recovered from the raw pointmaps, see Sec. 3.3 in the main paper. DUSt3R handles strong viewpoint and focal changes without apparent problems</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/qBUzKv.png" /></p>
<p>Figure 5:Example of 3D reconstruction of an unseen MegaDepth [56] scene from two images only. Note this is the raw output of the network, i.e. we show the output depthmaps (top-center) and confidence maps (top-right), as well as different viewpoints on the colored pointcloud (middle and bottom). Camera parameters are recovered from the raw pointmaps, see Sec. 3.3 in the main paper. DUSt3R handles strong viewpoint and focal changes without apparent problems</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/n0UVQk.png" /></p>
<p>Figure 6:Example of 3D reconstruction from two images only of unseen scenes, namely KingsCollege(Top-Left), OldHospital (Top-Middle), StMarysChurch(Top-Right), ShopFacade(Bottom-Left), GreatCourt(Bottom-Right). Note this is the raw output of the network, i.e. we show new viewpoints on the colored pointclouds. Camera parameters are recovered from the raw pointmaps, see Sec. 3.3 in the main paper.</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/u5GnVI.png" /></p>
<p>Figure 7:Example of 3D reconstruction from two images only of unseen scenes, namely Chess, Fire, Heads, Office (Top-Row), Pumpkin, Kitchen, Stairs (Bottom-Row). Note this is the raw output of the network, i.e. we show new viewpoints on the colored pointclouds. Camera parameters are recovered from the raw pointmaps, see Sec. 3.3 in the main paper.</p>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/DCU5oG.png" /></p>
<p>Figure 8:Examples of 3D reconstructions from nearly opposite viewpoints. For each of the 4 cases (motorcycle, toaster, bench, stop sign), we show the two input images (top-left) and the raw output of the network: output depthmaps (top-center) and confidence maps (top-right), as well as two different views on the colored point-clouds (middle and bottom). Camera parameters are recovered from the raw pointmaps, see Sec. 3.3 in the main paper. DUSt3R handles drastic viewpoint changes without apparent issues, even when there is almost no overlapping visual content between images, e.g. for the stop sign and motorcycle. Note that these example cases are not cherry-picked; they are randomly chosen from the set of unseen CO3D_v2 sequences. Please refer to the video for animated visualizations.</p>
</section>
<section id="appendix-c-extended-related-work">
<h2>Appendix C. Extended Related Work<a class="headerlink" href="#appendix-c-extended-related-work" title="此标题的永久链接">¶</a></h2>
<section id="implicit-camera-models">
<h3>1. <strong>Implicit Camera Models（隐式相机模型）</strong><a class="headerlink" href="#implicit-camera-models" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>核心思想：不显式建模相机参数，而是将3D形状表示为某种规范空间中的隐式表示。</p></li>
<li><p>表示方式包括：</p>
<ul>
<li><p>占据网格（occupancy grids）</p></li>
<li><p>八叉树结构（octrees structures）</p></li>
<li><p>参数化曲面集合</p></li>
<li><p>点云编码器</p></li>
<li><p>模板网格自由形变</p></li>
<li><p>每视图深度图</p></li>
</ul>
</li>
<li><p>问题：</p>
<ul>
<li><p>多用于 ShapeNet 等人工数据集，泛化能力差。</p></li>
<li><p>难以处理复杂自然场景。</p></li>
</ul>
</li>
<li><p>DUSt3R 的创新点：</p>
<ul>
<li><p>使用点图（pointmaps）保持像素与3D空间之间的联系，具备更好的可解释性和一致性。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="dense-visual-slam-slam">
<h3>2. <strong>Dense Visual SLAM（密集视觉SLAM）</strong><a class="headerlink" href="#dense-visual-slam-slam" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>传统方法利用深度相机或 RGB 视频流进行：</p>
<ul>
<li><p>高质量深度图生成</p></li>
<li><p>相机轨迹估计</p></li>
</ul>
</li>
<li><p>存在的问题：</p>
<ul>
<li><p>噪声、漂移、像素对应误差较多</p></li>
<li><p>假设图像序列紧密相关（同一相机、相似视角、光照变化小）</p></li>
</ul>
</li>
<li><p>DUSt3R 的不同：</p>
<ul>
<li><p>不依赖图像顺序</p></li>
<li><p>可处理“完全非约束”的图像对集合</p></li>
<li><p>不要求已知相机内参/外参</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="implicit-3d-reconstruction-3d">
<h3>3. <strong>Implicit 3D Reconstruction（隐式3D重建）</strong><a class="headerlink" href="#implicit-3d-reconstruction-3d" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>方法如 NeRF、Volumetric Rendering 等，使用 MLP 表达连续的 5D 函数。</p></li>
<li><p>优势：新视角合成能力强</p></li>
<li><p>局限：</p>
<ul>
<li><p>训练和渲染时间长</p></li>
<li><p>几何结构不明确（隐式表示不利于下游任务）</p></li>
</ul>
</li>
<li><p>DUSt3R 采用 <strong>显式重建（explicit 3D reconstruction）</strong>，以点图形式输出，具备：</p>
<ul>
<li><p>明确的几何结构</p></li>
<li><p>更适合下游任务如配准、重定位、姿态估计等</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="rgb-pairs-to-3d">
<h3>4. <strong>RGB-pairs-to-3D</strong><a class="headerlink" href="#rgb-pairs-to-3d" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>两视图几何的经典问题：</p>
<ul>
<li><p>深度图 + 相对相机姿态估计</p></li>
</ul>
</li>
<li><p>学习方法两类：</p>
<ul>
<li><p>单目深度 + 姿态回归</p></li>
<li><p>立体匹配 + 姿态估计</p></li>
</ul>
</li>
<li><p>新趋势：</p>
<ul>
<li><p>用于无监督预训练（如 CroCo）从大量图像对中学习隐式几何表示</p></li>
</ul>
</li>
<li><p>DUSt3R 的策略：</p>
<ul>
<li><p>借鉴 CroCo，但目标是<strong>直接生成3D点图</strong>而非作为预训练任务</p></li>
<li><p>深度图和相机姿态仅是中间产物，而非最终目标</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p>📌 DUSt3R 方法的核心优势对比总结</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>类别</p></th>
<th class="head"><p>传统方法问题</p></th>
<th class="head"><p>DUSt3R 优势</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>相机建模</p></td>
<td><p>假设已知或共享内参</p></td>
<td><p>完全无需已知相机参数，支持异构相机</p></td>
</tr>
<tr class="row-odd"><td><p>数据约束</p></td>
<td><p>要求相邻视图、顺序输入</p></td>
<td><p>支持非顺序、任意图像对输入</p></td>
</tr>
<tr class="row-even"><td><p>表示方式</p></td>
<td><p>隐式表示难泛化、不可解释</p></td>
<td><p>显式点图易解释、易扩展</p></td>
</tr>
<tr class="row-odd"><td><p>下游任务</p></td>
<td><p>隐式建模结果不通用</p></td>
<td><p>明确几何结构适配多任务</p></td>
</tr>
<tr class="row-even"><td><p>训练依赖</p></td>
<td><p>多使用大规模数据+监督</p></td>
<td><p>支持少量图像，无需 GT 相机</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<hr class="docutils" />
<section id="appendix-d-multi-view-pose-estimation">
<h2>Appendix D. 多视角姿态估计（Multi-view Pose Estimation）<a class="headerlink" href="#appendix-d-multi-view-pose-estimation" title="此标题的永久链接">¶</a></h2>
<section id="id26">
<h3>🌟 核心结论：<a class="headerlink" href="#id26" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>DUSt3R 在 CO3Dv2 数据集上，即便只有 3 张输入图像，也大幅超过所有已有方法，包括当前最强的 PoseDiffusion。</p></li>
<li><p>[Fig. 9] 展示了 RealEstate10K 上的稠密点云恢复效果，即使首尾帧视角变化巨大。</p></li>
</ul>
<p><img alt="" src="https://img.zhaoweiguo.com/uPic/2025/04/ZPAZsZ.png" /></p>
<p>Figure 9:Reconstruction example from 4 random frames of a RealEstate10K indoor sequence, after global alignment. On the left-hand side, we show the 4 input frames, and on the right-hand side the resulting point-cloud and the recovered camera intrinsics and poses.</p>
</section>
</section>
<hr class="docutils" />
<section id="appendix-e-visual-localization">
<h2>Appendix E. 视觉定位（Visual Localization）<a class="headerlink" href="#appendix-e-visual-localization" title="此标题的永久链接">¶</a></h2>
<section id="id27">
<h3>🌟 核心结论：<a class="headerlink" href="#id27" title="此标题的永久链接">¶</a></h3>
<p>DUSt3R 在 <strong>7-Scenes</strong> 数据集（室内）上表现稳定，但在 <strong>Cambridge-Landmarks</strong> 数据集（室外）上效果较差，原因是后者点云稀疏，难以对重建结果做比例缩放。</p>
</section>
<section id="id28">
<h3>🧪 实验设定：<a class="headerlink" href="#id28" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>摄像机焦距未知（Realistic setting）</p></li>
<li><p>方法：从 DUSt3R 输出的相对姿态（Rel-Pose）+ 使用数据库图像 GT 点云进行比例缩放</p></li>
<li><p>最终从缩放后的点云提取绝对姿态</p></li>
</ul>
</section>
<section id="tab-6">
<h3>📊 表格解读（Tab. 6）：<a class="headerlink" href="#tab-6" title="此标题的永久链接">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>数据集</p></th>
<th class="head"><p>方法</p></th>
<th class="head"><p>平移误差(cm) / 旋转误差(°)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7Scenes（Chess）</p></td>
<td><p>DUSt3R（scaled rel-pose）</p></td>
<td><p>5 / 1.08</p></td>
</tr>
<tr class="row-odd"><td><p>Cambridge（S. Facade）</p></td>
<td><p>DUSt3R（scaled rel-pose）</p></td>
<td><p>64 / 0.97</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>在 7Scenes 上几乎都是 2~6cm 级别的误差，旋转也在 1° 左右，表现很强。</p></li>
<li><p>在 Cambridge 上平移误差非常大（几十cm），关键原因是“数据库点云稀疏，比例缩放不可靠”。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id29">
<h3>📌 总结亮点：<a class="headerlink" href="#id29" title="此标题的永久链接">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>模块</p></th>
<th class="head"><p>优势</p></th>
<th class="head"><p>挑战</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Multi-view Pose Estimation</p></td>
<td><p>极少视图（3帧）下表现优异，泛化强，无需目标数据集训练</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>Visual Localization</p></td>
<td><p>在密集 GT 点云下有较好定位精度</p></td>
<td><p>稀疏点云时比例缩放难以估计，影响定位</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="appendix-f-training-details">
<h2>Appendix F. Training details<a class="headerlink" href="#appendix-f-training-details" title="此标题的永久链接">¶</a></h2>
<section id="id30">
<h3>一、训练数据来源和处理<a class="headerlink" href="#id30" title="此标题的永久链接">¶</a></h3>
<p>✅ 点云（pointmaps）的生成</p>
<ul class="simple">
<li><p>对于图像 <span class="math notranslate nohighlight">\( I^1 \)</span> 和 <span class="math notranslate nohighlight">\( I^2 \)</span>，DUSt3R 通过相机内参 <span class="math notranslate nohighlight">\( K_1, K_2 \in \mathbb{R}^{3\times3} \)</span>、相机姿态 <span class="math notranslate nohighlight">\( P_1, P_2 \in \mathbb{R}^{3\times4} \)</span> 和深度图 <span class="math notranslate nohighlight">\( D_1, D_2 \in \mathbb{R}^{W\times H} \)</span> 构建参考帧下的点云：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( \bar{X}^{1,1} = K_1^{-1}([U; V; 1] \cdot D_1) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \bar{X}^{2,1} = P_1 P_2^{-1} h(K_2^{-1}([U; V; 1] \cdot D_2)) \)</span></p></li>
<li><p>其中 <span class="math notranslate nohighlight">\( U, V \in \mathbb{R}^{W \times H} \)</span> 是像素坐标网格，<span class="math notranslate nohighlight">\( h(\cdot) \)</span> 表示齐次坐标变换。</p></li>
</ul>
</li>
</ul>
<p>✅ 深度图的提取方式</p>
<ul class="simple">
<li><p>每个像素点的深度值 <span class="math notranslate nohighlight">\( D^1_{i,j} \)</span> 可由点云的第三个坐标维度直接获得：</p>
<ul>
<li><p><span class="math notranslate nohighlight">\( D^1_{i,j} = \bar{X}^{1,1}_{i,j,2} \)</span></p></li>
</ul>
</li>
<li><p>因此，主文和附录中的所有深度图均可由点云输出的第 2 维直接提取（例如：<span class="math notranslate nohighlight">\( X^{1,1}_{:,:,2} \)</span>）。</p></li>
</ul>
<p>✅ 训练集混合</p>
<ul class="simple">
<li><p>使用 8 个数据集混合训练，覆盖各种场景（室内、室外、真实、合成、物体中心等）：</p>
<ul>
<li><p>Habitat, ARKitScenes, MegaDepth, Static Scenes 3D, Blended MVS, ScanNet++, CO3Dv2, Waymo</p></li>
<li><p>总共提取了约 <strong>850 万对图像对</strong>。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id31">
<h3>二、数据增强策略<a class="headerlink" href="#id31" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>标准图像增强：随机颜色抖动 + 居中裁剪（焦距增强的一种形式）。</p></li>
<li><p>居中裁剪确保主点始终在中心，有助于生成更多变化的焦距组合。</p></li>
<li><p>训练时图像对双向输入：即同时训练 <span class="math notranslate nohighlight">\( (I^1, I^2) \)</span> 和 <span class="math notranslate nohighlight">\( (I^2, I^1) \)</span>，增强泛化能力。</p></li>
<li><p>注意：两个方向的 token 不发生交互。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id32">
<h3>三、训练超参数（见表 7）<a class="headerlink" href="#id32" title="此标题的永久链接">¶</a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>项目</p></th>
<th class="head"><p>低分辨率训练</p></th>
<th class="head"><p>高分辨率训练</p></th>
<th class="head"><p>DPT 训练</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Prediction Head</strong></p></td>
<td><p>Linear</p></td>
<td><p>Linear</p></td>
<td><p>DPT</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Optimizer</strong></p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
<td><p>AdamW</p></td>
</tr>
<tr class="row-even"><td><p><strong>学习率</strong></p></td>
<td><p>1e-4</p></td>
<td><p>1e-4</p></td>
<td><p>1e-4</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Weight Decay</strong></p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
<td><p>0.05</p></td>
</tr>
<tr class="row-even"><td><p><strong>Adam β 参数</strong></p></td>
<td><p>(0.9, 0.95)</p></td>
<td><p>(0.9, 0.95)</p></td>
<td><p>(0.9, 0.95)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>每轮样本对数</strong></p></td>
<td><p>70 万对</p></td>
<td><p>7 万对</p></td>
<td><p>7 万对</p></td>
</tr>
<tr class="row-even"><td><p><strong>Batch Size</strong></p></td>
<td><p>128</p></td>
<td><p>64</p></td>
<td><p>64</p></td>
</tr>
<tr class="row-odd"><td><p><strong>训练轮数</strong></p></td>
<td><p>50</p></td>
<td><p>100</p></td>
<td><p>90</p></td>
</tr>
<tr class="row-even"><td><p><strong>Warmup</strong></p></td>
<td><p>10 轮</p></td>
<td><p>20 轮</p></td>
<td><p>15 轮</p></td>
</tr>
<tr class="row-odd"><td><p><strong>学习率调度器</strong></p></td>
<td><p>Cosine Decay</p></td>
<td><p>Cosine Decay</p></td>
<td><p>Cosine Decay</p></td>
</tr>
<tr class="row-even"><td><p><strong>输入分辨率</strong></p></td>
<td><p>224×224</p></td>
<td><p>多种组合，包括 512×384, 512×336, 512×288, 512×256, 512×160 等</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="2406.09756_MASt3R.html" class="btn btn-neutral float-right" title="2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="2203.08586_VanishingPointEstimation.html" class="btn btn-neutral" title="2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="../../None"></script>
      <script type="text/javascript" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>