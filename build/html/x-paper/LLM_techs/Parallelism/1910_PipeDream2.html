

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training &mdash; 新溪-gordon V2025.07 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training" href="2006.09503_PipeDream-2BW.html" />
    <link rel="prev" title="1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism" href="1909.08053_Megatron-LM.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>
  <script src="../../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../0normal.html">通用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../0normals/normal.html">通用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/normal.html#id3">如何看一个论文是不是重要</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../0normals/website.html">学术网站</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id2">整体分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id3">1. 学术搜索平台（核心功能：检索与发现文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#google-scholar">Google Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#semantic-scholar">Semantic Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#web-of-science">Web of Science</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#id4">百度学术</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id5">2. 资源共享平台（核心功能：免费获取付费文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#sci-hub">Sci-Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#library-genesis-libgen">Library Genesis (LibGen)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#unpaywall">Unpaywall</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../0normals/website.html#id6">论文数据库（核心功能：存储与提供文献原文）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#acl-anthology">ACL Anthology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#arxiv">ArXiv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#cnki">知网 CNKI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../0normals/website.html#id10">万方数据库</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Benchmarking.html">评测基准</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id3">评测基准</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html">02xx.xxxxx_BLEU: a Method for Automatic Evaluation of Machine Translation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id8">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-baseline-bleu-metric">2.The Baseline BLEU Metric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-bleu-evaluation">3.The BLEU Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-human-evaluation">4.The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#bleu-vs-the-human-evaluation">5.BLEU vs The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html">0401.xxxxx_ROUGE: A Package for Automatic Evaluation of Summaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-n-n-gram-co-occurrence-statistics">2.ROUGE-N: N-gram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-l-longest-common-subsequence">3.ROUGE-L: Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-w-weighted-longest-common-subsequence">4 ROUGE-W: Weighted Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-s-skip-bigram-co-occurrence-statistics">5.ROUGE-S: Skip-Bigram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#evaluations-of-rouge">6 Evaluations of ROUGE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#conclusions">7 Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html">1803.01937_ROUGE2.0: Updated and Improved Measures for Evaluation of Summarization Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#problems-with-the-current-rouge-measures">1. Problems with the current ROUGE measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1803.01937_ROUGE2.html#rouge-2-0">2. ROUGE 2.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html">1804.08771_SacreBLEU: A Call for Clarity in Reporting BLEU Scores</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#bleu">BLEU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#id3">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#problem-description">2 Problem Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#a-way-forward">3 A way forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/1804.08771_SacreBLEU.html#summary">4 Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html">2306.05685_Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#mt-bench-and-chatbot-arena">2 MT-Bench and Chatbot Arena</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm-as-a-judge">3 LLM as a Judge</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#agreement-evaluation">4 Agreement Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#human-preference-benchmark-and-standardized-benchmark">5 Human Preference Benchmark and Standardized Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-a-prompt-templates">Appendix A Prompt templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-b-case-study">Appendix B Case Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-c-data-collection">Appendix C Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-d-additional-experimental-results">Appendix D Additional Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-e-training-details-of-vicuna-models">Appendix E Training Details of Vicuna Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-f-exploring-vicuna-as-a-judge">Appendix F Exploring Vicuna as a judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#agent">数据集-Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html">2312.14033_T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#t-eval">2 T-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-a-t-eval-benchmark-details">Appendix A T-Eval Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-c-detailed-evaluation-metrics">Appendix C Detailed Evaluation Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-d-api-documentation">Appendix D API Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html">2406.12045_τ-bench: A Benchmark for Tool-Agent-User</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#bench-a-benchmark-for-t-ool-a-gent-u-ser-interaction">3.τ-bench: A benchmark for T ool-A gent-U ser Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#benchmark-construction">4. Benchmark Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#experiments">5.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#disscussion">6.Disscussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html">2506.07982_𝜏²-Bench: Evaluating Conversational Agents in a Dual-Control Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#tau-2-bench-evaluating-agents-in-a-dual-control-environment">3 <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench: Evaluating Agents in a Dual-Control Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#broader-impact">Broader Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-a-telecom-domain">Appendix A Telecom Domain</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-b-verifying-original-tau-2-bench">Appendix B Verifying Original <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-d-domain-policies">Appendix D Domain Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-e-user-simulator-quality">Appendix E User Simulator Quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#qa">数据集-QA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html">1809.09600_HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#data-collection">2 Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#processing-and-benchmark-settings">3 Processing and Benchmark Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#dataset-analysis">4 Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#conclusions">7 Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-a-data-collection-details">Appendix A Data Collection Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#a">附录A 数据收集细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-b-further-data-analysis">Appendix B Further Data Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-c-full-wiki-setting-details">Appendix C Full Wiki Setting Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html">2109.07958_TruthfulQA: Measuring How Models Mimic Human Falsehoods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#the-truthfulqa-benchmark">2 The TruthfulQA Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#ethics-and-impact">8 Ethics and Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-a-additional-examples-from-truthfulqa">Appendix A Additional examples from TruthfulQA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-b-additional-results">Appendix B Additional results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-c-dataset-construction">Appendix C Dataset construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-d-human-evaluations">Appendix D Human evaluations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-e-prompts">Appendix E Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-f-checking-for-data-quality-and-disagreement">Appendix F Checking for data quality and disagreement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html">2311.12022_GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#data-collection">2.Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#dataset-analysis">3.Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#baseline">4.Baseline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#related-work">5.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2311.12022_GPQA.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html">2411.04368_SimpleQA: Measuring short-form factuality in large language models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#data-collection-and-verification">2.Data Collection and Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#measuring-calibration">4.Measuring calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#appendix-b-guessing-strategy-and-f-score">Appendix B Guessing strategy and F-score</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id4">数据集-编程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html">2107.03374_HumanEval: Evaluating Large Language Models Trained on Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#evaluation-framework">2.Evaluation Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#code-fine-tuning">3.Code Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#supervised-fine-tuning">4.Supervised Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#docstring-generation">5.Docstring Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#broader-impacts-and-hazard-analysis">7.Broader Impacts and Hazard Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#conclusions">9.Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html">2108.07732_MBPP: Program Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#datasets">2 Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#model-and-methods">3 Model and Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mbpp-synthesis-results">4 MBPP Synthesis Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#human-model-collaboration-results">5 Human-Model Collaboration Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#program-execution-results">6 Program Execution Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mathqa-results">7 MathQA Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#related-work">8 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#risks-and-limitations">9 Risks and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#conclusion">10 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2108.07732_MBPP.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html">2310.06770_SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id7">2 SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#swe-llama-fine-tuning-codellama-for-swe-bench">3 SWE-Llama: Fine-tuning CodeLlama for SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#experimental-setup">4 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#discussion">7 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#ethics-statement">8 Ethics Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#reproducibility-statement">9 Reproducibility Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-a-benchmark-details">Appendix A Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-b-additional-details-on-training-swe-llama">Appendix B Additional Details on Training SWE-Llama</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-c-additional-results">Appendix C Additional Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-d-additional-experimental-details">Appendix D Additional Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-e-societal-impact">Appendix E Societal Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-f-in-depth-analysis-of-swe-llama-generations">Appendix F In-depth Analysis of SWE-Llama Generations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html">2402.16694_HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#a-multilingual-code-generation-benchmark-for-cross-lingual-natural-language-generalization">A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#introduction">1.   Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#related-work">2.   Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#humaneval-xl">3.   HumanEval-XL</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#experiments">4.   Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#conclusion">5.   Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-a-experiment-settings">Appendix A Experiment Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-b-comprehensive-experiment-results">Appendix B Comprehensive Experiment Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html">2403.07974_LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#holistic-evaluation">2 Holistic Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#benchmark-curation">3 Benchmark Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#experiment-setup">4 Experiment Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-b-ui">Appendix B UI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-d-results">Appendix D Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-e-qualitative-examples">Appendix E Qualitative Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html">2407.10499_CIBench: Evaluating Your LLMs with a Code Interpreter Plugin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#cibench">3 CIBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-b-construction-prompts-and-rules">Appendix B Construction Prompts and Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-c-experiment-example-demo">Appendix C Experiment Example Demo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-d-subjective-visualization-evaluation">Appendix D Subjective Visualization Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-e-dataset-error-analysis">Appendix E Dataset Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-f-human-annotator">Appendix F Human Annotator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-g-ethical-consideration">Appendix G Ethical Consideration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html">2410.03859_SWE-bench-Multimodal: Do AI Systems Generalize to Visual Software Domains?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#swe-bench-multimodal">2 SWE-bench Multimodal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#evaluating-on-swe-bench-m">3 Evaluating on SWE-bench M</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-b-collection">Appendix B Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-c-experiments">Appendix C Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-d-human-validation">Appendix D Human Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-e-limitations">Appendix E Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html">2410.06992_SWE-Bench+: Enhanced Coding Benchmark for LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-analysis-of-swe-bench">2 Robustness Analysis of SWE-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#building-swe-bench">3 Building SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-of-swe-bench">4 Robustness of SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#effectiveness-aware-evaluation">5 Effectiveness-aware Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#conclusion">7 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html">2501.01257_CodeForces: Benchmarking Competition-level Code Generation of LLMs on CodeForces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#codeforces-benchmark">3 CodeForces Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#evaluation-on-existing-llms">4 Evaluation on Existing LLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#analysis-experiments">5 Analysis Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#ethical-statement">8 Ethical Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-a-model-cards">Appendix A Model Cards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-b-decoding-hyperparameters">Appendix B Decoding Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-c-analysis-of-our-elo-rating-calculation-system">Appendix C Analysis of Our Elo Rating Calculation System</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-d-human-comparable-elo-rating">Appendix D Human-comparable Elo Rating</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-e-problem-demonstration">Appendix E Problem Demonstration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-f-special-judge">Appendix F Special Judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id5">数据集-长文本</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html">2402.05136_LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#lv-eval-benchmark">3 LV-Eval Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-c-detailed-evaluation-results">Appendix C Detailed Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-d-detailed-ablation-results">Appendix D Detailed Ablation Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html">2402.17753_LoCoMo: Evaluating Very Long-Term Conversational Memory of LLM Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#generative-pipeline-for-locomo">3 Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#locomo-evaluation-benchmark">4 LoCoMo Evaluation Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-setup">5 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-results">6 Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#broader-impacts">9 Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-overview">Appendix Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-a-generative-pipeline-for-locomo">Appendix A Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-b-dataset">Appendix B Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-d-results">Appendix D Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html">2404.06654_RULER: What’s the Real Context Size of Your Long-Context Language Models?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#the-ruler-benchmark">3 The Ruler Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#experiments-results">4 Experiments &amp; Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#task-error-analysis">5 Task Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#model-analysis">6 Model Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-a-models">Appendix A Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-b-task-configurations">Appendix B Task Configurations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-c-task-correlation-analysis">Appendix C Task Correlation Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-d-prompt-templates">Appendix D Prompt Templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-e-passkey-retrieval-and-vanilla-niah-results">Appendix E Passkey Retrieval and Vanilla NIAH Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-f-additional-results">Appendix F Additional Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html">2407.11963_NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#tasks-and-datasets">3 Tasks and Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#impact-of-language-which-model-performs-better-under-the-bilingual-scenario">4.1.5 Impact of Language_ Which Model Performs Better under the Bilingual Scenario_</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#conclusion-and-future-work">5 Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-a-evaluated-models">Appendix A Evaluated Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-b-needlebench-prompt-examples">Appendix B NeedleBench Prompt Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-c-error-analysis-examples">Appendix C Error Analysis Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id6">数据集-数学</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2103.03874_MATH.html">2103.03874_MATH: Measuring Mathematical Problem Solving With the MATH Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html">2110.14168_GSM8K: Training Verifiers to Solve Math Word Problems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#dataset">2 Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#related-work">3 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#methods">4 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#additional-experiments">5 Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-b-hyperparameters">Appendix B Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-c-calculator-annotations">Appendix C Calculator Annotations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-d-example-model-solutions">Appendix D Example Model Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-e-verifier-details">Appendix E Verifier Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-f-verifier-visualization">Appendix F Verifier Visualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html">2405.12209_MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#experiments-and-analysis">3 Experiments and Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#ethical-considerations">8 Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-a-mathbench-statistics">Appendix A MathBench Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-b-detailed-experimental-results">Appendix B Detailed Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-c-extra-analysis">Appendix C Extra Analysis</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id7">数据集-图片</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html">2306.13394_MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#mme-evaluation-suite">2 MME Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#analysis">4 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2306.13394_MME.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html">2307.06281_MMBench: Is Your Multi-modal Model an All-around Player?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#the-construction-of-mmbench">3 The construction of MMBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-strategy">4 Evaluation Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-results">5 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-a-more-details-about-the-data">Appendix A More Details about the Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-b-more-details-on-mmbench-construction">Appendix B More Details on MMBench Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-c-more-details-on-llm-based-choice-extraction">Appendix C More Details on LLM-based Choice Extraction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-d-evaluation-settings-and-results">Appendix D Evaluation Settings and Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html">2307.16125_SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id7">3 SEED-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#evaluation-results">4 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html">2311.12793_ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-dataset">3 ShareGPT4V Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-7b-model">4 ShareGPT4V-7B Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id11"><strong>4.1 模型架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id12"><strong>4.2 预训练</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sft"><strong>4.3 监督微调（SFT）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-a-data-sources">Appendix A Data Sources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-b-caption-analysis">Appendix B Caption Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-d-examples">Appendix D Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html">2506.18095_ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#sharegpt-4o-image">2 ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#janus-4o-fine-tuning-with-sharegpt-4o-image">3 Janus-4o: Fine-Tuning with ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#conclusion">5 conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-b-image-generation-categories">Appendix B Image Generation Categories</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-c-prompts-for-generation">Appendix C Prompts for Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-d-document-pipeline">Appendix D Document Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-e-ethical-considerations-and-societal-impact">Appendix E Ethical Considerations and Societal Impact</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Benchmarking.html#id8">数据集</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#id2">评测标准</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#accuracy">准确率(Accuracy)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#precision">精确率(Precision, 精准率)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#recall">召回率(Recall)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#f1-score">F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/0normal.html#id3">可视化精度和召回率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html">2009.03300_MMLU: Measuring Massive Multitask Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#a-multitask-test">3.A Multitask Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2009.03300_MMLU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html">2305.08322_C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#id2">C-Eval_ A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#the-c-eval-evaluation-suite">2 The C-Eval Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#experiment">3 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-a-author-contributions">Appendix A Author Contributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-b-detailed-stats-of-c-eval">Appendix B Detailed Stats of C-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-c-explanation-data-generation">Appendix C Explanation Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-d-evaluation-prompts">Appendix D Evaluation Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-e-details-of-the-models-being-evaluated">Appendix E Details of the models being evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-f-breakdown-of-model-performance">Appendix F Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-g-option-bias">Appendix G Option Bias</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-h-compute-and-resources-used-for-evaluation">Appendix H Compute and Resources Used for Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html">2306.09212_CMMLU: Measuring massive multitask language understanding in Chinese</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#cmmlu">3 CMMLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#impact-of-model-size-on-performance">Impact of model size on performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-a-comparison-to-concurrent-benchmarks">Appendix A Comparison to concurrent benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-b-cmmlu-subjects">Appendix B CMMLU Subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-c-cmmlu-examples">Appendix C CMMLU Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-d-cmmlu-difficulty-distribution">Appendix D CMMLU Difficulty Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-e-emergent-ability-shown-in-cmmlu-subjects">Appendix E Emergent Ability shown in CMMLU subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-f-models-being-evaluated">Appendix F Models being Evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-g-strategies-for-estimating-model-choices">Appendix G Strategies for Estimating Model Choices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-h-regular-expressions-matching-algorithmsl">Appendix H Regular expressions matching algorithmsl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-i-correlation-to-other-benchmarks">Appendix I Correlation to other Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-j-breakdown-of-model-performance">Appendix J Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2306.09212_CMMLU.html#j-3-the-effect-of-chain-of-thought-prompt">J.3 The effect of chain-of-thought prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html">2307.15020_SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#superclue-benchmark">3 SuperCLUE Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#additional-analysis">5 Additional Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-a-evaluation-process">Appendix A Evaluation Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-b-capability-categories">Appendix B Capability Categories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html">2311.12983_GAIA: a benchmark for General AI Assistants</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#related-work">2.Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#id4">3.GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#llms-results-on-gaia">4.LLMs results on GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-c-extended-description-of-gaia">Appendix C Extended description of GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-d-extended-description-of-our-question-design-framework">Appendix D Extended description of our question design framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html">2404.07972_OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-environment">2. OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-benchmark">3. OSWORLD Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#benchmarking-llm-and-vlm-agent-baselines">4. Benchmarking LLM and VLM Agent Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#analysis">5. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#a-details-of-osworld-environment">A. Details of OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#c-details-of-baseline-methods">C. Details of Baseline Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2404.07972_OSWorld.html#d-examples-of-qualitative-analysis">D. Examples of Qualitative Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html">2501.14249_HLE: Humanity’s Last Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#dataset">3.Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#evaluation">4.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Benchmarkings/Datasets/2501.14249_HLE.html#discussion">5.Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../LLM.html">LLM 模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#nlp">NLP 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html">1810.04805_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#bert">3 BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/1810.04805_BERT.html#appendix-a-additional-details-for-bert">Appendix A Additional Details for BERT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html">18xx_GPT1: Improving Language Understanding by Generative Pre-Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#framework">3. Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#analysis">5 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#id3">引文口碑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/18_GPT1.html#id4">要点解读</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html">19xx_GPT2: Language Models are Unsupervised Multitask Learners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html#the-illustrated-gpt-2">The Illustrated GPT-2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/19_GPT2.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2012.00413_CPM.html">2012.00413_CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2302.13971_LLaMA.html">2302.13971_LLaMA: Open and Efficient Foundation Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2307.09288_Llama2.html">2307.09288_Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html">2309.16609_Qwen Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#pretraining">2. Pretraining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#alignment">3. Alignment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#code-qwen-specialized-model-for-coding">4. CODE-QWEN: SPECIALIZED MODEL FOR CODING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#math-qwen-specialized-model-for-mathematics-reasoning">5. MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-1-more-training-details">A.1 MORE TRAINING DETAILS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-2-evaluation">A.2 EVALUATION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html">2310.19341_Skywork: A More Open Bilingual Foundation Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#limitation">6 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-a-details-on-gpt-7b-vs-llama-7b-experiment">Appendix A Details on GPT-7B vs. LLaMA-7B Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-b-preliminary-experiments-on-distributed-training">Appendix B Preliminary Experiments on Distributed Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-c-more-benchmark-results">Appendix C More Benchmark Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-d-details-on-lm-test-sets">Appendix D Details on LM Test Sets</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2401.14196_DeepSeek-Coder.html">2401.14196_DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html">2404.06395_MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#two-stage-pre-training-strategy">5. Two Stage Pre-training Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#model">6. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#minicpm-family">7 MiniCPM Family</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2405.04434_DeepSeek-V2.html">2405.04434_DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2406.12793_ChatGLM.html">2406.12793_ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html">2407.10671_Qwen2 Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#tokenizer-model">2. Tokenizer &amp; Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2407.10671_Qwen2.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html">2412.15115_Qwen2.5</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#architecture-and-tokenizer">2. Architecture and Tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html">2505.09388_Qwen3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLM_NLPs/2505.09388_Qwen3.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id2">多模态模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html">2112.15093_CTR: Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#datasets">3. Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#baselines">4. Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#an-empirical-study">5. An Empirical Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-a-details-of-prab">Appendix A Details of PRAB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-c-visualization-of-failure-cases">Appendix C Visualization of Failure Cases.</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html">2304.08485_LLaVA: Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#gpt-assisted-visual-instruction-data-generation">3. GPT-assisted Visual Instruction Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#visual-instruction-tuning">4. Visual Instruction Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2304.08485_LLaVA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html">2308.12966_Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#methodology">Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#evaluation">Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#b-data-format-details-of-training">B. Data Format Details of Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html">2310.03744_LLaVA2: Improved Baselines with Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#empirical-evaluation">4. Empirical Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#open-problems-in-lmms">5. Open Problems in LMMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#a-implementation-details">A. Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#b-qualitative-results">B. Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html">2312.07533_VILA: On Pre-training for Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#on-pre-training-for-visual-language-models">3. On Pre-training for Visual Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2312.07533_VILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html">2403.05525_DeepSeek-VL: Towards Real-World Vision-Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html">2408.01800_MiniCPM-V: A GPT-4V Level MLLM on Your Phone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#model-architecture">3. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#end-side-deployment">5. End-side Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#experiments">6. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html">2409.17146_Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#data">3. Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#ablations">6. Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-a-model-details">Appendix A: Model Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-b-training-details">Appendix B: Training Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-c-evaluation-results">Appendix C: Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-d-result-details">Appendix D: Result Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-e-ablations-details">Appendix E Ablations Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-f-data-details">Appendix F Data Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-g-dataset-examples">Appendix G Dataset Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-h-related-work">Appendix H Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html">2410.13848_Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#janus-a-simple-unified-and-flexible-multimodal-framework">3 Janus: A Simple, Unified and Flexible Multimodal Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-a-details-of-semantic-tokenizer-mentioned-in-ablation-study">Appendix A Details of Semantic Tokenizer Mentioned in Ablation Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-b-additional-qualitative-results">Appendix B Additional Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html">2411.00774_Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#model">2. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#experience">3. Experience</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#conclusion-and-future-work">4. Conclusion and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html">2412.04468_NVILA: Efficient Frontier Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#more-capabilities">4. More Capabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2412.04468_NVILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html">2502.13923_Qwen2.5-VL</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html">2503.20215_Qwen2.5-Omni Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#archtecture">2. Archtecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#id2">3 预训练</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#post-training">4 后训练（Post-training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#id4">3. Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#results-and-analyses">5. Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#id9">3 Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#data-construction">3.2.1 Data Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#results-and-analyses">5 Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-c-case-study">Appendix C Case Study</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id3">LLM 音频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html">2005.08100_Conformer: Convolution-augmented Transformer for Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#conformer-encoder">2 Conformer Encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2005.08100_Conformer.html#conclusion">4 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html">2106.07447_HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#i-introduction">I Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#ii-method">II Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#iii-related-work">III Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#iv-experimental-details">IV Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#v-results">V Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2106.07447_HuBERT.html#vi-conclusion">VI Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html">2112.02418_YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#id1">关键概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#yourtts-model">2. YourTTS Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#results-and-discussion">4. Results and Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#zero-shot-voice-conversion">5. Zero-Shot Voice Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#speaker-adaptation">6. Speaker Adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2112.02418_YourTTS.html#conclusions-limitations-and-future-work">7. Conclusions, limitations and future work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html">2212.04356_whisper: Robust Speech Recognition via Large-Scale Weak Supervision</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#analysis-and-ablations">4. Analysis and Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#limitations-and-future-work">6. Limitations and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#conclusions">7. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#a-evaluation-datasets">A. Evaluation Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#b-compared-models">B Compared Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2212.04356_whisper.html#c-text-standardization">C. Text Standardization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html">2301.02111_Vall-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#background-speech-quantization">3. Background: Speech Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#id9">4. VALL-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2301.02111_Vall-E.html#conclusion-limitations-and-future-work">6. Conclusion, Limitations, and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html">2303.03926_VALL-E_X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#cross-lingual-codec-language-model">3 Cross-Lingual Codec Language Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#vall-e-x-application">4. VALL-E X Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2303.03926_VALL-E_X.html#a-appendix">A. Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html">2406.05370_VALL-E2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#id5">3. VALL-E 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2406.05370_VALL-E2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html">2407.05407_CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#cosyvoice-a-scalable-tts-model-using-supervised-semantic-tokens">2. CosyVoice: A Scalable TTS model using Supervised Semantic Tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#dataset">3. Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#experimental-settings">4. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.05407_CosyVoice.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html">2407.10759_Qwen2-Audio Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html">2410.00037_Moshi: a speech-text foundation model for real-time dialogue</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#model">3.Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#datasets-and-training">4. Datasets and Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#safety">6.Safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2410.00037_Moshi.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html">2412.10117_CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#instroduction">1. Instroduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#id5">2. CosyVoice 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-settings">3. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-results">4. Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2412.10117_CosyVoice2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html">2501.06282_MinMo: A Multimodal Large Language Model for Seamless Voice Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#instruction">1.Instruction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#id9">3.MinMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#conclusion">5.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2501.06282_MinMo.html#a-prompts-for-voice-understanding-tasks">A. Prompts for Voice Understanding Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html">2505.02707_Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#voila-voice-language-foundation-models">3. Voila: Voice-Language Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.02707_Voila.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html">2505.17589_CosyVoice3: Towards In-the-wild Speech Generation via Scaling-up and Post-training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#id3">2.CosyVoice 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#the-multilingual-data-pipeline">3.The Multilingual Data Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-settings">4.Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-results">5.Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#conclusion">6.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMAudio/2505.17589_CosyVoice3.html#limitations">7.Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id4">LLM 视频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html">2301.12597_BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#method">3 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#experiment">4 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#limitation">5 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2301.12597_BLIP-2.html#conclusion">6 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html">2308.01390_OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#id1">OpenFlamingo_ An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#related-work">2 Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#approach">3 Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-a-extended-results">Appendix A Extended results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-b-additional-notes-on-filtering-mmc4">Appendix B Additional notes on filtering MMC4</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-c-synthetic-data-prompt">Appendix C Synthetic data prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-d-image-credits">Appendix D Image credits</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#llm-moe">LLM MoE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMoEs/2408.15664_AUXILIARY-LOSS-FREE_LB.html">2408.15664_AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMMoEs/2410.07490_MoDEM.html">2410.07490_MoDEM: Mixture of Domain Expert Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM.html#id5">商业模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2303.08774_GPT4.html">2303.08774_GPT-4 Technical Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html">2312.11805_Gemini: A Family of Highly Capable Multimodal Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#model-architecture">2. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#training-infrastructure">3. Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#post-training-models">6. Post-Training Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#responsible-deployment">7. Responsible Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2312.11805_Gemini.html#discussion-and-conclusion">8. Discussion and Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2403.05530_Gemini1.5.html">2403.05530_Gemini1.5: Unlocking multimodal understanding across millions of tokens of context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html">2406.02430_Seed-TTS: A Family of High-Quality Versatile Speech Generation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#method">2 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-extensions">4 Model extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-applications-limitations-and-safety">5 Model applications, limitations, and safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#authors-alphabetical-order">6 Authors (alphabetical order)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#acknowledgement">7 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html">2407.04675_Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#motivation">2 Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#methods">3 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#model-and-evaluation">4 Model and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2503.20020_Gemini2.html">2503.20020_Gemini2: Gemini Robotics: Bringing AI into the Physical World</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2504.xxxxx_Seed-Thinking-v1.5.html">2504.xxxxx_Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html">2505.07062_Seed1.5-VL Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#id1">Seed1.5-VL Technical Report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#architecture">2 Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-recipe">3.2 Training Recipe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#post-training">4 Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#hybrid-reinforcement-learning">4.4 Hybrid Reinforcement Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-infrastructure">5 Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation">6 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#video-task-evaluation">6.1.3 Video Task Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#comparison-with-state-of-the-arts">6.3.2 Comparison with State-of-the-arts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#conclusion-and-next-steps">7 Conclusion and Next Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#contributions-and-acknowledgments">8 Contributions and Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#qualitative-examples">9 Qualitative examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#visual-reasoning-visual-pattern-recognition">9.7 Visual Reasoning_ Visual Pattern Recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#failure-cases-combinatorial-search-i">9.19 Failure Cases_ Combinatorial Search I</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation-details">10 Evaluation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#dream-1k">DREAM-1K</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../../LLM_tech.html">LLM 周边技术</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#framework">Framework</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html">1712.05889_Ray: A Distributed Framework for Emerging AI Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#motivation-and-requirements">2. Motivation and Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#programming-and-computation-model">3. Programming and Computation Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#architecture">4. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#discussion-and-experiences">7 Discussion and Experiences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1712.05889_Ray.html#conclusion">8. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html">1910.02054_DeepSpeed_ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#extended-introduction">1. Extended Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#where-did-all-the-memory-go">3 Where Did All the Memory Go?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#zero-insights-and-overview">4 ZeRO: Insights and Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-dp">5 Deep Dive into ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-r">6 Deep Dive into ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-dp">7 Communication Analysis of ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-r">8. Communication Analysis of ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#step-towards-1-trillion-parameters">9. Step Towards 1 Trillion Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#implementation-and-evaluation">10. Implementation and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/1910.02054_DeepSpeed_ZeRO.html#concluding-remarks">11. Concluding Remarks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/19XX_PyTorch.html">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/20XX_Transformers.html">Transformers: State-of-the-Art Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html">2210.XX_Ray v2 Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#architecture-overview">Architecture Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#object-management">Object Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#task-management">Task Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#resource-management-and-scheduling">Resource Management and Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#actor-management">Actor management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#global-control-service">Global Control Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#cluster-management">Cluster Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2210.XX_Ray_v2.html#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html">2309.06180_vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#memory-challenges-in-llm-serving">3. Memory Challenges in LLM Serving</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#method">4. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#implementation">5. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#evaluation">6. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#ablation-studies">7. Ablation Studies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Frameworks/2309.06180_vLLM.html#conclusion">10. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id2">大模型调优</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2101.00190_Prefix-Tuning.html">2101.00190_Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2103.10385_p-tuning.html">2103.10385_p-tuning: GPT Understands, Too</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2104.08691_Prompt_Tuning.html">2104.08691_Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2106.09685_LoRA.html">2106.09685_LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2401.01335_Self-Play.html">2401.01335_Self-Play: Fine-Tuning Converts Weak Language Models to Strong Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2402.09353_DoRA.html">2402.09353_DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2402.12354_LoRA%2B.html">2402.12354_LoRA+: Efficient Low Rank Adaptation of Large Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2403.03507_GaLore.html">2403.03507_GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../FineTunes/2403.13372_LlamaFactory.html">2403.13372_LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../FineTunes/2403.13372_LlamaFactory.html#id2">竞争框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../FineTunes/2403.13372_LlamaFactory.html#efficient-fine-tuning-techniques">3. Efficient Fine-Tuning Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../FineTunes/2403.13372_LlamaFactory.html#llamafactory-framework">4 LlamaFactory Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../FineTunes/2403.13372_LlamaFactory.html#conclusion-and-future-work">6 Conclusion and Future Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="reference internal" href="../../LLM_tech.html#id3">分布式模型</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="1701.06538_MoE.html">1701.06538_MoE: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="1806.03377_PipeDream.html">1806.03377_PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#background-related-work">2. Background &amp; Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#parallel-training-in-pipedream">3. Parallel Training in PipeDream</a></li>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#implementation">4. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="1806.03377_PipeDream.html#evaluation">5. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1811.06965_GPipe.html">1811.06965_GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#the-gpipe-library">2. The GPipe Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#performance-analyses">3. Performance Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#image-classification">4. Image Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#massive-massively-multilingual-machine-translation">5. Massive Massively Multilingual Machine Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="1811.06965_GPipe.html#design-features-and-trade-offs">6. Design Features and Trade-Offs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="1909.08053_Megatron-LM.html">1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="1909.08053_Megatron-LM.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="1909.08053_Megatron-LM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="1909.08053_Megatron-LM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="1909.08053_Megatron-LM.html#background-and-challenges">2. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="1909.08053_Megatron-LM.html#model-parallel-transformers">3. Model Parallel Transformers</a></li>
</ul>
</li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#background-and-related-work">2. BACKGROUND AND RELATED WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pipeline-parallelism">3. 流水线并行(PIPELINE PARALLELISM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">4. 实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">6. 结论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2006.09503_PipeDream-2BW.html">2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2006.09503_PipeDream-2BW.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2006.15704DataParallel.html">2006.15704_PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="2006.16668_GShard.html">2006.16668_GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="2104.04473_Megatron-LM2.html">2104.04473_Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2104.04473_Megatron-LM2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2205.14135_FlashAttention.html">2205.14135_FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#background">2 Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#flashattention-algorithm-analysis-and-extensions">3. FLASHATTENTION: Algorithm, Analysis, and Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#limitations-and-future-directions">5. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#appendix-b-algorithm-details">Appendix B Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#appendix-c-proofs">Appendix C Proofs</a></li>
<li class="toctree-l4"><a class="reference internal" href="2205.14135_FlashAttention.html#appendix-d-extension-details">Appendix D Extension Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2307.08691_FlashAttention2.html">2307.08691_FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#flashattention-2-algorithm-parallelism-and-work-partitioning">3. FlashAttention-2: Algorithm, Parallelism, and Work Partitioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#empirical-validation">4. Empirical Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="2307.08691_FlashAttention2.html#discussion-and-future-directions">5. Discussion and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="normal.html">通用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id4">LLM 量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/0normal.html#id2">混合精度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/0normal.html#id3">浮点数格式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/0normal.html#weight-only-quantization">weight-only quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html">2110.02861_bitsandbytes: 8-bit Optimizers via Block-wise Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#background">1. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#bit-optimizers">2. 8-bit Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#bit-vs-32-bit-optimizer-performance-for-common-benchmarks">3. 8-bit vs 32-bit Optimizer Performance for common Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#analysis">4. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2110.02861_bitsandbytes.html#related-work">5. Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html">2206.01861_ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#relative-work">2. Relative Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#background-and-challenges">3. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#methodology">4. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#results">5. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#appendix-a-background">Appendix A Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.01861_ZeroQuant.html#appendix-d-details-about-system-optimization">Appendix D Details about System Optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html">2206.09557_LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#design-methodology-of-lut-gemm">3. Design Methodology of LUT-GEMM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#accelerating-quantized-opt-175b">5. Accelerating Quantized OPT-175B</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#appendix-a-llm-inference-latency-breakdown">Appendix A LLM Inference Latency Breakdown</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2206.09557_LUT-GEMM.html#appendix-b-detailed-implementation">Appendix B Detailed Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html">2208.07339_LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#id1">相关参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#int8-matrix-multiplication-at-scale">3. Int8 Matrix Multiplication at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#emergent-large-magnitude-features-in-transformers-at-scale">4. Emergent Large Magnitude Features in Transformers at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#discussion-and-limitations">6. Discussion and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#broader-impacts">7. Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2208.07339_LLM.int8.html#id17">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html">2209.05433_FP8: FP8 Formats For Deep Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#aspects-of-fp8-usage-in-deep-learning">2. Aspects of FP8 Usage in Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#fp8-binary-interchange-format">3. FP8 Binary Interchange Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#id3">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#empirical-results">4. Empirical Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2209.05433_FP8.html#conclusions">5. Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html">2210.17323_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#background">3. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#the-gptq-algorithm">4. The GPTQ Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#experimental-validation">5. Experimental Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2210.17323_GPTQ.html#summary-and-limitations">6. Summary and Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html">2211.10438_SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#review-of-quantization-difficulty">3. Review of Quantization Difficulty</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#id9">4. SmoothQuant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2211.10438_SmoothQuant.html#appendix-a-discussion-on-weight-only-quantization">Appendix A. Discussion on Weight-Only Quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html">2305.14314_QLoRA: Efficient Finetuning of Quantized LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#qlora-finetuning">3. QLoRA Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#qlora-vs-standard-finetuning">4. QLoRA vs. Standard Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#pushing-the-chatbot-state-of-the-art-with-qlora">5. Pushing the Chatbot State-of-the-art with QLoRA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#qualitative-analysis">6. Qualitative Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#related-work">7. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2305.14314_QLoRA.html#limitations-and-discussion">8. Limitations and Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html">2306.00978_AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#awq-activation-aware-weight-quantization">3. AWQ: Activation-aware Weight Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#tinychat-mapping-awq-onto-edge-platforms">4. TinyChat: Mapping AWQ onto Edge Platforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2306.00978_AWQ.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html">2309.05516_AutoRound: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#methodology">3. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Quantizations/2309.05516_AutoRound.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id5">LLM 安全</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Securitys/2312.06674_Llama_Guard.html">2312.06674_Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id6">LLM强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RLs/1703.03864_EvolutionStrategies.html">1703.03864_Evolution Strategies: as a Scalable Alternative to Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html">2504.02495_DeepSeek-GRM: Inference-Time Scaling for Generalist Reward Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#self-principled-critique-tuning-spct">3. Self-Principled Critique Tuning (SPCT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#inference-time-scaling-with-spct">4. Inference-Time Scaling with SPCT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#results-on-reward-modeling-benchmarks">5. Results on Reward Modeling Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#a-additional-related-work">A. Additional Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#b-limitations-and-future-directions">B. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RLs/2504.02495_DeepSeek_GRM.html#g-prompt-templates">G. Prompt Templates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RLs/2504.13958_ToolRL.html">2504.13958_ToolRL: Reward is All Tool Learning Needs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LLM_tech.html#id7">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../others/2203.02155_InstructGPT.html">2203.02155_Training language models to follow instructions with human feedback(InstructGPT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#methods-and-experimental-details">3. Methods and experimental details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#discussion">5. Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#appendix-a-additional-prompt-data-details">Appendix A Additional prompt data details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#appendix-b-additional-human-data-collection-details">Appendix B Additional human data collection details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#appendix-c-additional-model-details">Appendix C Additional model details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2203.02155_InstructGPT.html#appendix-d-automatic-evaluation-details">Appendix D Automatic evaluation details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/2305.20050_LetsVerifyStepbyStep.html">2305.20050_Let’s Verify Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/2305.20050_LetsVerifyStepbyStep.html#id2">1. 研究背景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2305.20050_LetsVerifyStepbyStep.html#id3">2. 监督方法对比</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2305.20050_LetsVerifyStepbyStep.html#id4">3. 核心发现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2305.20050_LetsVerifyStepbyStep.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html">2408.03314_Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#how-to-scale-test-time-computation-optimally">3. How to Scale Test-Time Computation Optimally</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#scaling-test-time-compute-via-verifiers">5. Scaling Test-Time Compute via Verifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#refining-the-proposal-distribution">6. Refining the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#id7">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html">2412.14135_Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#fromgpt">FromGPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#id2">3. Policy Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#id3">4. Reward Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#id5">5. Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#id8">6. Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#open-source-o1-project">7 Open-source o1 Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/2412.14135_Scaling_of_Search_and_Learning.html#future-directions">8. Future Directions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../ML.html">机器学习</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../ML.html#ml-vision">ML Vision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1506.02640_YOLO.html">1506.02640_You Only Look Once: Unified, Real-Time Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/1506.02640_YOLO.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1612.08242_YOLO9000.html">1612.08242_YOLO9000: Better, Faster, Stronger</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/1612.08242_YOLO9000.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/1804.02767_YOLOv3.html">1804.02767_YOLOv3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2004.10934_YOLOv4.html">2004.10934_YOLOv4: Optimal Speed and Accuracy of Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2004.10934_YOLOv4.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html">2205.00159_SVTR: Scene Text Recognition with a Single Visual Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#method">2. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2205.00159_SVTR.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2207.02696_YOLOv7.html">2207.02696_YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2207.02696_YOLOv7.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2303.05499_GroundingDINO.html">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2304.08485_VisualInstructionTuning.html">2304.08485_Visual Instruction Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2402.13616_YOLOv9.html">2402.13616_YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2402.13616_YOLOv9.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2405.14458_YOLOv10.html">2405.14458_YOLOv10: Real-Time End-to-End Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2405.14458_YOLOv10.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html">2411.15858_SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#id1">定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#methods">3. Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/MLVisions/2411.15858_SVTRv2.html#more-detail-of-real-world-datasets">8. More detail of real-world datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../ML.html#ml">ML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2112.09332_WebGPT.html">2112.09332_WebGPT: Browser-assisted question-answering with human feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2203.11147_GopherCite.html">2203.11147_GopherCite: Teaching language models to support answers with verified quotes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2304.09848_Generative_Search.html">2304.09848_Generative_Search: Evaluating Verifiability in Generative Search Engines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2305.14251_FActScore.html">2305.14251_FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html">2305.14627_ALCE: Enabling Large Language Models to Generate Text with Citations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html#nli">NLI 在引用质量评估中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../MLs/ML_normals/2305.14627_ALCE.html#prompt">论文中用的prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2307.02185_Citation.html">2307.02185_Citation: A Key to Building Responsible and Accountable Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../MLs/ML_normals/2307.16883_HAGRID.html">2307.16883_HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../Agent.html">AI Agent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agent">通用 Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2210.03629_ReAct.html">2210.03629_ReAct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html">2303.08268_Chat-with-the-Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html#id2">正文</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.11366_Reflexion.html">2303.11366_Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html">2303.16434_TaskMatrix.AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id2">大脑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id3">接口平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#api">API 选择器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2304.03442_Generative-Agents.html">2304.03442_Generative-Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2304.03442_Generative-Agents.html#generative-agent-architecture">Generative Agent Architecture</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2307.07924_ChatDev.html">2307.07924_ChatDev: Communicative Agents for Software Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.00352_MetaGPT.html">2308.00352_MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.04026_AgentSims.html">2308.04026_AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.08155_AutoGen.html">2308.08155_AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2308.10848_AgentVerse.html">2308.10848_AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2308.10848_AgentVerse.html#id2">理念</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2310.06117_Step-Back.html">2310.06117_Step-Back: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2402.18679_MetaGPT_DI.html">2402.18679_MetaGPT_DI: Data Interpreter: An LLM Agent For Data Science</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2402.18679_MetaGPT_DI.html#introduction">INTRODUCTION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html">2407.07061_IoA: Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#overview-of-ioa">2.1 OVERVIEW OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#architecture-of-ioa">2.2 ARCHITECTURE OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#key-mechanisms">2.3 KEY MECHANISMS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2407.07061_IoA.html#putting-it-all-together">2.5 Putting It All Together</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2408.08435_ADAS.html">2408.08435_ADAS: Automated Design of Agentic Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2408.08435_ADAS.html#prompt">Prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html">2408.08435_ADAS: Automating Agentic Workflow Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html#introduce">Introduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.10762_AFlow.html#preliminary">PRELIMINARY</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html">2410.17238_SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.17238_SELA.html#method">3 Method</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2410.21012_FACT.html">2410.21012_FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2410.21012_FACT.html#introduce">Introduce</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2504.01990_foundation-agents.html">2504.01990_Advances and Challenges in Foundation Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html">2506.12508_AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#agentorchestra">3.AgentOrchestra</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_normals/2506.12508_AgentOrchestra.html#experiments">4.Experiments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agent-aios">视觉 Agent&amp;AIOS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html">2108.03353_ Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#dataset-creation">3. Dataset Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#model-design">4. Model Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2108.03353_Screen2Words.html#id3">其它</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html">2209.08199_ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#problem-setting-tasks-and-metrics">3. Problem Setting: Tasks and Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#data-annotation">4. Data Annotation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#dataset-analysis">5. Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#experiments-and-baselines">6. Experiments and Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#limitations">8. Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#ethical-considerations">9. Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#a-data-annotation-details">A. Data Annotation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2209.08199_ScreenQA.html#b-data-examples">B. Data Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html">2212.06817_RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#preliminaries">3. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#system-overview">4. System Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#rt-1-robotics-transformer">5. RT-1: ROBOTICS TRANSFORMER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#experiments">6. EXPERIMENTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#conclusions-limitations-and-future-work">7. CONCLUSIONS, LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#b-model-card">B. MODEL CARD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#c-model-and-data">C. MODEL AND DATA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2212.06817_RT-1.html#d-experiments">D. EXPERIMENTS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html">2312.13771_AppAgent: Multimodal Agents as Smartphone Users</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#environment-and-action-space">3.1 Environment and Action Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#exploration-phase">3.2 Exploration Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2312.13771_AppAgent.html#deployment-phase">3.3 Deployment Phase</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html">2401.10935_SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#screenspot-a-grounding-benchmark">4. ScreenSpot: A Grounding Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#ethical-considerations">Ethical considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#a-details-of-seeclick-pre-training">A. Details of SeeClick Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#b-screenspot-annotation-evaluation">B ScreenSpot Annotation &amp; Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2401.10935_SeeClick.html#c-downstream-agent-tasks">C. Downstream Agent Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html">2402.04615_ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#automatic-data-generation">3. Automatic data generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#data-mixtures">4. Data Mixtures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#experiments-and-results">5. Experiments and Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#a-definitions-of-metrics">A Definitions of Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#b-screen-schema-examples">B. Screen Schema Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#c-prompts-for-llm-generated-content">C. Prompts For LLM Generated Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#d-screen-navigation-generated-examples">D. Screen Navigation Generated Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#f-screenqa-short-answers-generation">F. ScreenQA Short Answers Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#g-complex-question-answering-datasets">G. Complex Question Answering Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.04615_ScreenAI.html#h-new-benchmarks-repositories">H. New Benchmarks Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html">2402.07939_UFO: A UI-Focused Agent for Windows OS Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#the-design-of-ufo">3.The Design of UFO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#experiment">4.Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#limitations-lessons-learned">5.Limitations &amp; Lessons Learned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2402.07939_UFO.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html">2403.16971_AIOS: LLM Agent Operating System</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#the-architecture-of-aios">2. The Architecture of AIOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#aios-kernel">3. AIOS Kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2403.16971_AIOS.html#appendix-e-discussion">Appendix E Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2406.01014_Mobile-Agent-v2.html">2406.01014_Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2411.02059_TableGPT2.html">2411.02059_TableGPT2: A Large Multimodal Model with Tabular Data Integration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2411.02059_TableGPT2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html">2501.11733_Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#mobile-agent-e">2. Mobile-Agent-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#conclusion-and-future-work">6. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-a-full-trajectory-comparison-example-with-previous-sota">Appendix A Full Trajectory Comparison Example with Previous SOTA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-b-error-recovery-with-escalation-to-manager">Appendix B Error Recovery with Escalation to Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-c-remaining-limitations">Appendix C Remaining Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-d-all-tasks-in-mobile-eval-e-benchmark">Appendix D All Tasks in Mobile-Eval-E Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-e-atomic-operation-space">Appendix E Atomic Operation Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-f-full-list-of-self-evolved-shortcuts">Appendix F Full list of Self-Evolved Shortcuts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-g-full-list-of-self-evolved-tips">Appendix G Full list of Self-Evolved Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html">2501.12326_UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#evolution-path-of-gui-agents">2. Evolution Path of GUI Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#core-capabilities-of-native-agent-model">3. Core Capabilities of Native Agent Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#ui-tars">4. UI-TARS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#experiment">5. Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2501.12326_UI-TARS.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html">2502.14282_PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#pc-agent">2. PC-Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#related-work">4. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2502.14282_PC-Agent.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html">2504.14603_UFO2: The Desktop AgentOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#background">2.Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#system-design-of-ufo2">3.System Design of UFO2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#picture-in-picture-interface">4.Picture-in-Picture Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#implementation-and-specialized-engineering-design">5.Implementation and Specialized Engineering Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#evaluation">6.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#discussion-future-work">7.Discussion &amp; Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Agent_Visions/2504.14603_UFO2.html#conclusion">9.Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#id2">记忆</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html">2505.22101_MemOS: An Operating System for Memory-Augmented Generation (MAG) in LLM (Short Version)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memos">4 MemOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id2"><strong>4.1 MemOS 中的记忆类型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#memcube"><strong>4.2 记忆立方体（MemCube）：核心资源</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id3"><strong>4.3 MemOS 架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id4"><strong>4.4 系统执行流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Memorys/2505.22101_MemOS.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#tools">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2205.00445_MRKL.html">2205.00445_MRKL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2302.04761_Toolformer.html">2302.04761_Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2303.17580_HuggingGPT.html">2303.17580_HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html">2307.16789_ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#dataset-construction">2 Dataset Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../Agents/Tools/2307.16789_ToolLLM.html#appendix-a-implementation-details">Appendix A Implementation Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Agent.html#agi">AGI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/AGIs/1905.10985_AI-GA.html">1905.10985_AI-GA: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Agents/AGIs/2408.06292_AI-Scientist.html">2408.06292_The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../RAG.html">RAG</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2005.11401_RAG_for_KI_NLP_task.html">2005.11401_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html">2312.10997_Retrieval-Augmented Generation for Large Language Models: A Survey</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-overview-of-rag">II. Overview of RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-a-naive-rag">II-A Naive RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-b-advanced-rag">II-B Advanced RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-c-modular-rag">II-C Modular RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#ii-d-rag-vs-fine-tuning">II-D RAG vs Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-retrieval">III. Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-a-retrieval-source">III-A Retrieval Source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-b-indexing-optimization">III-B Indexing Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-c-query-optimization">III-C Query Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-d-embedding">III-D Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iii-e-adapter">III-E Adapter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-generation">IV. Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-a-context-curation">IV-A Context Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#iv-b-llm-fine-tuning">IV-B LLM Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-augmentation-process-in-rag">V. Augmentation process in RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-a-iterative-retrieval">V-A Iterative Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-b-recursive-retrieval">V-B Recursive Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#v-c-adaptive-retrieval">V-C Adaptive Retrieval</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-task-and-evaluation">VI. Task and Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-a-downstream-task">VI-A Downstream Task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-b-evaluation-target">VI-B Evaluation Target</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-c-evaluation-aspects">VI-C Evaluation Aspects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vi-d-evaluation-benchmarks-and-tools">VI-D Evaluation Benchmarks and Tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-discussion-and-future-prospects">VII. Discussion and Future Prospects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-a-rag-vs-long-context">VII-A RAG vs Long Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-b-rag-robustness">VII-B RAG Robustness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-c-hybrid-approaches">VII-C Hybrid Approaches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-d-scaling-laws-of-rag">VII-D Scaling laws of RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-e-production-ready-rag">VII-E Production-Ready RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2312.10997_RAG_for_LLM.html#vii-f-multi-modal-rag">VII-F Multi-modal RAG</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2401.15884_CRAG.html">2401.15884_CRAG: Corrective Retrieval Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2403.14403_Adaptive-RAG.html">2403.14403_Adaptive-RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html">2404.16130_GraphRAG: From Local to Global: A GraphRAG Approach to Query-Focused Summarization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#background">2 Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#rag">2.1 RAG方法与系统</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#llmrag">2.2 知识图谱在LLM与RAG中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id2">2.3 自适应基准测试</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id3">2.4 RAG评估标准</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#methods">3 Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#graphrag"><strong>3.1 GraphRAG 工作流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id4"><strong>3.2 全局理解问题生成</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id5"><strong>3.3 全局理解评估标准</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id6"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#analysis">4 Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id7">4.1 实验1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id11">4.2 实验2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id14">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#results">5 Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id15">5.1 实验一：不同方法在摘要任务中的表现比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id18">5.2 实验二：基于声明的指标评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id20">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#discussion">6 Discussion</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id21">6.1 评估方法的局限性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id22">6.2 未来工作</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id23">更广泛的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-a-entity-and-relationship-extraction-approach">Appendix A Entity and Relationship Extraction Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id24"><strong>1. 实体与关系抽取方法</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#self-reflection"><strong>2. 自我反思（Self-Reflection）技术</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id25"><strong>3. 分块大小与抽取效果的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id26"><strong>4. 实验结果（图3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-b-example-community-detection">Appendix B Example Community Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-c-context-window-selection">Appendix C Context Window Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-d-example-answer-comparison">Appendix D Example Answer Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-e-system-prompts">Appendix E System Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-1-element-instance-generation"><strong>E.1 实体实例生成（Element Instance Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-2-community-summary-generation"><strong>E.2 社区摘要生成（Community Summary Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-3-community-answer-generation"><strong>E.3 社区问题回答生成（Community Answer Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#e-4-global-answer-generation"><strong>E.4 全局问题回答生成（Global Answer Generation）</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-f-evaluation-prompts">Appendix F Evaluation Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#f-1-relative-assessment-prompt">F.1 Relative Assessment Prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#f-2-relative-assessment-metrics">F.2 Relative Assessment Metrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#appendix-g-statistical-analysis">Appendix G Statistical Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id38">统计方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id39">主要结果总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id40">总体趋势：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2404.16130_GraphRAG.html#id41">重要结论：</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2406.13213_Multi-Meta-RAG.html">2406.13213_Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html">2410.05779_LightRAG: Simple and Fast Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#retrieval-augmented-generation">2 Retrieval-Augmented Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#the-lightrag-architecture">3 The LightRAG Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#lightrag">一、LightRAG架构概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#graph-based-text-indexing">二、基于图的文本索引（Graph-based Text Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#dual-level-retrieval-paradigm">三、双层检索范式（Dual-level Retrieval Paradigm）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#retrieval-augmented-answer-generation">四、检索增强的答案生成（Retrieval-Augmented Answer Generation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id2">五、复杂度分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id3">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#evaluation">4 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#experimental-settings"><strong>1. 实验设置（4.1 Experimental Settings）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#lightrag-rag-4-2-rq1"><strong>2. LightRAG 与现有 RAG 方法的对比（4.2 RQ1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq2"><strong>3. 消融实验（4.3 RQ2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id8"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#case-study-rq3">4.4 Case Study (RQ3)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq3">4.4 案例研究（RQ3）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#rq4">4.5 模型成本与适应性分析（RQ4）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id9">总体结论：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#related-work">5 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#id10">第5章 相关工作（总结）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.05779_LightRAG.html#appendix">7 Appendix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html">2410.10450_KBLaM: Knowledge Base augmented Language Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#related-work">2. Related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#background">3. Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#self-attention-layer">Self-attention layer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#augmenting-llm-with-the-kb">4. Augmenting LLM with the KB</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#knowledge-tokens">Knowledge tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#rectangular-attention-injecting-knowledge-token-into-prompt-tokens">Rectangular Attention: Injecting knowledge token into prompt tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#kb-length-generalization-through-attention-score-scaling">KB length generalization through attention score scaling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#kb-instruction-tuning">5. KB instruction tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiments">6. EXPERIMENTS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiment-setting">6.1 EXPERIMENT SETTING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#experiment-results">6.2 EXPERIMENT RESULTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#id2">总结亮点</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#conclusion">7. CONCLUSION</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#limitations-and-future-work">8. LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-b-ablation-study">Appendix B Ablation study</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#appendix-c-sample-kb">Appendix C Sample KB</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#sample-q-a">SAMPLE Q&amp;A</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt">PROMPT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-synthetic-kb-generation">PROMPT FOR SYNTHETIC KB GENERATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-open-ended-q-a-generation">Prompt for open-ended Q&amp;A generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-gpt-evaluation-of-open-ended-q-a">PROMPT FOR GPT EVALUATION OF OPEN-ENDED Q&amp;A</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#prompt-for-llama-evaluation">PROMPT FOR LLAMA EVALUATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#question-template">QUESTION TEMPLATE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#sample-output">SAMPLE OUTPUT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#synthetic-kb">SYNTHETIC KB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2410.10450_KBLaM.html#enron">ENRON</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html">2504.03137_LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#related-work">Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#llm-prompt-engineering">LLM Prompt Engineering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#kg-based-llm-reasoning">KG-based LLM Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#preliminaries">Preliminaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#knowledge-graph-kg">1. Knowledge Graph (KG)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#anchor-entities">2. Anchor Entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#relation-link">3. Relation Link</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#reasoning-path">4. Reasoning Path</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#methodology">Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage1-reasoning-graph-retrieval">Stage1: Reasoning Graph Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage2-knowledge-embedding">Stage2: Knowledge Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#stage3-knowledge-prompts-mixed-reasoning">Stage3: Knowledge Prompts Mixed Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#experiments">Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/2504.03137_LightPROF.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../RAGs/graphrag.html">GraphRAG 官方文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/graphrag.html#indexing">Indexing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#indexing-architecture">&gt; Indexing Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#indexing-dataflow">&gt; Indexing Dataflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../RAGs/graphrag.html#prompt-tuning">&gt; Prompt Tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../RAGs/graphrag.html#query">Query</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../paper_pool.html">论文池</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html">2305.16300_Random-Access Infinite Context Length for Transformers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#llm">LLM 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id1"><strong>研究背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id2"><strong>核心问题</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id3"><strong>主要贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id4"><strong>关键技术点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id5"><strong>实验结果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id6"><strong>意义与应用前景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id7"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#related-work">2 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#methodology">3 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id8"><strong>总体思路</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id9"><strong>方法详解</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id11"><strong>位置编码处理</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id12"><strong>与其他方法的对比</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#memory-computation">3.3 Memory &amp; Computation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#experiments">4 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id14"><strong>4.1 语言建模实验</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id18"><strong>4.2 微调预训练模型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id22"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#future-work">5 Future Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#acknowledgment">Acknowledgment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-a-grouped-softmax-example">Appendix A Grouped Softmax Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-b-dataset-description">Appendix B Dataset Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-c-number-of-unique-retrieved-blocks">Appendix C Number of Unique Retrieved Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-d-context-miss-token">Appendix D Context Miss Token</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-e-positional-augmentation">Appendix E Positional Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-f-additional-extensions-and-details">Appendix F Additional Extensions and Details</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#masked-language-modeling">1. <strong>掩码语言建模（Masked Language Modeling）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#flash-attention">2. <strong>与 Flash Attention 的结合</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id23">3. <strong>检索块数量与块大小的权衡</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#id24">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2305.16300.html#appendix-g-offloading-kv-cache-to-cpu">Appendix G Offloading KV Cache to CPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html">2311.18743_AlignBench: Benchmarking Chinese Alignment of Large Language Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id1">主要内容总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id2">总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id3"><strong>1. 背景与挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#alignbench"><strong>2. AlignBench的设计目标</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id4"><strong>3. AlignBench的主要特点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id5"><strong>4. AlignBench的应用与成果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id6"><strong>5. 总体贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id7"><strong>6. 表格对比</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#dataset">2 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#methods">3 Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#human-evaluation-on-alignbench">4 Human Evaluation on AlignBench</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#agreement-evaluation">一、一致性评估（Agreement Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#quality-evaluation">二、解释质量评估（Quality Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id8">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#alignbench-benchmarking-results">5 AlignBench: Benchmarking Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-2-prompts-and-details-of-methods">A.2 Prompts and Details of Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-2">A.2 提示模板与方法细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-3">A.3 各维度表现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#a-4">A.4 案例分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id10">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id11">一、核心问题：参考材料缺失导致评估困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id12">二、数学积分问题对比分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2311.18743_AlignBench.html#id13">三、总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html">2401.15391_MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id1"><strong>背景与动机</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id2"><strong>贡献</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id3"><strong>方法概览</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id4"><strong>实验结果</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id6">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id7">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#rag-with-multi-hop-queries">2 RAG with multi-Hop queries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#rag">2.1 RAG（检索增强生成）概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#multi-hop-queries">2.2 多跳查询（Multi-Hop Queries）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id8">2.3 评估指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#a-benchmarking-dataset-multihop-rag">3 A Benchmarking Dataset: MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#multihop-rag">一、MultiHop-RAG 数据集构建流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id10">二、MultiHop-RAG 数据集统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id11">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#benchmarking-rag-system-using-multihop-rag">4 Benchmarking RAG system using MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#retrieval-related-task">一、检索相关任务（Retrieval-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#generation-related-task">二、生成相关任务（Generation-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#other-use-cases">三、其他潜在改进方向（Other Use Cases）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#id12">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#related-work">5 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#appendix-a-appendix-a-gpt-4-prompts-used-for-data-generation">Appendix A Appendix A: GPT-4 Prompts Used for Data Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2401.15391_MultiHop-RAG.html#appendix-b-appendix-b-dataset-examples">Appendix B Appendix B: Dataset Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#related-work">2 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#prompt-tuning">2.1 Prompt Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#llms">2.2 LLMs在图相关任务中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id1">2.3 图上的检索方法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#problem-formalization">3 Problem Formalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#methodology">4 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id2">概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id3">4.1 文本子图检索</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#indexing">文本子图索引（Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#ranking">文本子图排序（Ranking）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#soft-pruning">文本子图软剪枝（Soft Pruning）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id4">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#textual-graph-augmented-generation">4.2 Textual Graph Augmented Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#text-view-of-textual-graphs">1. 文本视图（Text View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#graph-view-of-textual-graphs">2. 图视图（Graph View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#generation-phase">3. 生成阶段（Generation Phase）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#experiments">5 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id6">总结：第五章 实验部分</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#limitations">7 Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#a"><strong>附录A 总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2405.16506_GRAG.html#id12"><strong>总结</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html">2407.01178_Memory3: Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#language-modeling-with-explicit-memory">Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id1">研究背景与动机：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id2">主要内容与方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id3">实验与结果：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id4">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#abstract">Abstract</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id5">核心思想</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory3">Memory3 模型特点</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id6">实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id7">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#introduction">1 _ Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#retrieval-augmented-training">1.1.1 _ Retrieval-augmented Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id8">1.1.1 | 基于检索的训练（Retrieval-augmented Training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#sparse-computation">1.1.2 | 稀疏计算（Sparse Computation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#parameter-as-memory">1.1.3 | 参数即记忆（Parameter as Memory）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory-circuitry-theory">2 _ Memory Circuitry Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id10">核心概念总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id11">总体贡献：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#definition-2">Definition 2.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id12">1. <strong>定义与核心概念：计算图、同构与知识（电路）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id13">2. <strong>知识的实例</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id14">3. <strong>知识的外部化与记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id15">4. <strong>结论与断言</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id16">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#remark-1">Remark 1.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id17">1. <strong>电路构造的关键性质</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#llm">2. <strong>记忆增强 LLM 的形式化定义</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id18">3. <strong>写入代价与读取代价的权衡（记忆层次结构）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id19">4. <strong>知识使用频率与记忆分配</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id20">5. <strong>图示与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id21">小结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#design">3 _ Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id22"><strong>3 | Design</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id25"><strong>3.1 | 推理过程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id27"><strong>3.2 | 写入与读取记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id28"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#memory-sparsification-and-storage">3.3 _ Memory Sparsification and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id29">一、显式记忆的存储挑战</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id30">二、各维度的稀疏化策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id31">三、压缩效果</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id32">四、部署方式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id33">五、补充说明与建议</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id36">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#model-shape">3.4 _ Model Shape</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id37">3.4 | 模型结构（Model Shape）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#training-designs">3.5 | 训练设计（Training Designs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id38">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#two-stage-pretrain">3.6 _ Two-stage Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id39">一、预训练的两个阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#continual-train">二、对 continual train 的优化</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id40">三、防止信息泄露</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id41">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#pretraining-data">4 _ Pretraining Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#data-collection"><strong>4.1 数据收集（Data Collection）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#filtering"><strong>4.2 数据过滤（Filtering）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#tokenizer"><strong>4.3 分词器（Tokenizer）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#knowledge-base"><strong>4.4 知识库（Knowledge Base）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id42"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#pretrain">5 _ Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id43">1. 预训练总体设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#set-up">2. 训练设置（Set-up）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#warmup-stage">3. 预热阶段（Warmup Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#continual-train-stage">4. 持续训练阶段（Continual Train Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id44">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#fine-tuning-and-alignment">6 _ Fine-tuning and Alignment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#supervised-finetuning-sft">6.1 监督微调（Supervised Finetuning, SFT）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#direct-preference-optimization-dpo">6.2 直接偏好优化（Direct Preference Optimization, DPO）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#evaluation">7 _ Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id45">7.1 通用能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id46">7.2 对话能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id47">7.3 幻觉与事实性评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id48">7.4 专业任务评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id49">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#inference-speed">7.5 _ Inference Speed</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id50">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id51">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#conclusion">8 _ Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-a-cost-estimation">Appendix A Cost Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id52">模型参数设定</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#implicit-memory">隐式记忆（Implicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#explicit-memory">显式记忆（Explicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#external-information-rag">外部信息（External Information，如 RAG）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id53">综合比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#id54">拓展讨论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-b-vector-compression">Appendix B Vector Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2407.01178_Memory3.html#appendix-c-supplementary-evaluation-results">Appendix C Supplementary Evaluation Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html">2505.14683_Emerging Properties in Unified Multimodal Pretraining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id1">核心内容总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id2">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#model">2 Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id3">1. 模型架构概览</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id4">2. 生成策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id5">3. 模型细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#generalized-causal-attention">4. 广义因果注意力（Generalized Causal Attention）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#transformer">5. Transformer结构选择与实验</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id6">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#data">3 Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id7">数据特点与目标</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id8">数据来源与统计</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id9">数据构建方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id16">数据训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id17">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#training">4 Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id18">1. 多阶段训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id19">2. 关键超参数调整</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id22">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#evaluation">5 Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#emerging-properties">6 Emerging Properties</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id23">1. <strong>新兴属性的定义与研究背景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id24">2. <strong>任务表现与训练阶段的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id25">3. <strong>多模态特征的重要性</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id26">4. <strong>定性分析与生成质量提升</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id27">5. <strong>核心发现与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id28">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#main-results">7 Main Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id29">7.1 图像理解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id30">7.2 图像生成</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id31">7.3 图像编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id32">7.4 带有推理的生成/编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id33">7.5 世界建模</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#id34">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#more-qualitative-results">7.6 More Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2505.14683.html#acknowledgement">9 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html">MemOS: A Memory OS for AI System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#llm">LLM 总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id1"><strong>1. 背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id2"><strong>2. 现有方法的不足</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id3"><strong>3. 四大典型挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos"><strong>4. MemOS的提出与核心理念</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id4"><strong>5. 总结与意义</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id5">总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id6"><strong>一、记忆研究的四个阶段</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id7"><strong>二、第一阶段：记忆定义与探索</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id14"><strong>三、MemOS 的初步构想</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id15"><strong>四、总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-1"><strong>2.1 显式长期记忆的建立（Stage 1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-2"><strong>2.2 人脑式记忆机制的引入（Stage 2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-3"><strong>2.3 基于工具的记忆管理（Stage 3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#stage-4"><strong>2.4 系统化记忆治理（Stage 4）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id16"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-3-1-vision-of-memos">一、MemOS 的愿景（3.1 Vision of MemOS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#from-computer-os-to-memory-os">二、从传统操作系统到记忆操作系统（3.2 From Computer OS to Memory OS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id17">三、总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-modeling-in-memos">4 Memory Modeling in MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id18"><strong>4.1 内存类型与语义演化路径</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memory-cube-memcube"><strong>4.2 Memory Cube（MemCube）：内存的核心资源单元</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id19"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#architecture-of-memos">5 Architecture of MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id20">总结：MemOS 架构与执行流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id24">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id25">5.5.1 MemGovernance（内存治理模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memvault">5.5.2 MemVault（内存存储与路由基础设施）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memloader-memdumper">5.5.3 MemLoader 与 MemDumper（内存加载与导出模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memstore">5.5.4 MemStore（内存存储与分发接口）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id26">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#evaluation">6 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#end-to-end-evaluation-on-locomo"><strong>1. 整体系统评估（End-to-End Evaluation on LOCOMO）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#evaluation-of-memory-retrieval"><strong>2. 内存检索评估（Evaluation of Memory Retrieval）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#kv-evaluation-of-kv-based-memory-acceleration"><strong>3. KV缓存加速评估（Evaluation of KV-Based Memory Acceleration）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#memos-for-architecture-innovation-and-applications">7 MemOS for Architecture Innovation and Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id28">一、MemOS推动的架构创新</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id29">二、MemOS的应用场景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#id30">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../zzz_paper_pools/2507.03724_MemOS.html#conclusion">8 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../other.html">其他</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../other.html#id3">数据集&amp;数据蒸馏</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../others/DataSets/1811.10959v3_Dataset_Distillation.html">1811.10959v3_Dataset Distillation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/1811.10959v3_Dataset_Distillation.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/1811.10959v3_Dataset_Distillation.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/1811.10959v3_Dataset_Distillation.html#introduction">1. INTRODUCTION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/1811.10959v3_Dataset_Distillation.html#approach">3. APPROACH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/DataSets/2502.20653_Dataset_Distillation.html">2502.20653_Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/2502.20653_Dataset_Distillation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/2502.20653_Dataset_Distillation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/2502.20653_Dataset_Distillation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/2502.20653_Dataset_Distillation.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/DataSets/normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/DataSets/normal.html#dataset-distillation">Dataset distillation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html#d">3D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html">2003.08934_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#neural-radiance-field-scene-representation">3. Neural Radiance Field Scene Representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#volume-rendering-with-radiance-fields">4. Volume Rendering with Radiance Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#optimizing-a-neural-radiance-field">5. Optimizing a Neural Radiance Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#result">6. Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2003.08934_NeRF.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html">2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#id1">概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#geometric-priors-for-vp-detection">3. Geometric priors for VP detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2203.08586_VanishingPointEstimation.html#conclusion-and-limitations">5. Conclusion and limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html">2312.14132_DUSt3R: Geometric 3D Vision Made Easy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#id2">相关概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#experiments-with-dust3r">4. Experiments with DUSt3R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-a">Appendix A <strong>附录概览</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-b-qualitative-results">Appendix B.  Qualitative results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-c-extended-related-work">Appendix C. Extended Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-d-multi-view-pose-estimation">Appendix D. 多视角姿态估计（Multi-view Pose Estimation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-e-visual-localization">Appendix E. 视觉定位（Visual Localization）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2312.14132_DUSt3R.html#appendix-f-training-details">Appendix F. Training details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html">2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#id1">前言</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#id2">🧠 思维导图式总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#related-works">2. Related works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#id3">🧠 总结思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#appendix-a-additional-qualitative-results">Appendix A Additional Qualitative Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#b-fast-reciprocal-matching">B. Fast Reciprocal Matching</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#c-coarse-to-fine">C. Coarse-to-Fine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2406.09756_MASt3R.html#d-detailed-experimental-settings">D. Detailed experimental settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html">2412.09401_SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#id1">术语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#id14">6. 致谢</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#appendix-a-implementation-details">Appendix A Implementation details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#appendix-b-details-for-experimental-settings">Appendix B Details for experimental settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#appendix-c-additional-comparisons-and-analyses">Appendix C Additional comparisons and analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.09401_SLAM3R.html#d-more-visual-results">D. More visual results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html">2412.12392_MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#gpt">GPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#id1">先验知识</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#limitations-and-future-work">5. Limitations and Future Work（局限与未来工作）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#conclusion">🧾 6. Conclusion（总结）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#id32">🧠 总结一句话版：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#initialisation">8. Initialisation（初始化）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#runtime-breakdown">9. Runtime Breakdown（运行时分析）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#evaluation-setup">10. Evaluation Setup（评估设置）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2412.12392_MASt3R-SLAM.html#id35">11. EuRoC 结果总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html">2503.11651_VGGT: Visual Geometry Grounded Transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#discussions">5. Discussions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#appendix-a-formal-definitions">Appendix A Formal Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#appendix-c-additional-experiments">Appendix C Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#appendix-d-qualitative-examples">Appendix D Qualitative Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/3D/2503.11651_VGGT.html#appendix-e-related-work">Appendix E Related Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../other.html#id4">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html">A PAINLESS GUIDE TO CRC ERROR DETECTION ALGORITHMS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#the-basic-idea-behind-crc-algorithms">The Basic Idea Behind CRC Algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#polynomical-arithmetic">Polynomical Arithmetic</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#binary-arithmetic-with-no-carries">Binary Arithmetic with No Carries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id2">一个可用的实例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#choosing-a-poly">Choosing A Poly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-straightforward-crc-implementation">A Straightforward CRC Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-table-driven-implementation">A Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-slightly-mangled-table-driven-implementation">A Slightly Mangled Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../others/others/Distributed%20Representations%20of%20Sentences%20and%20Documents.html">Distributed Representations of Sentences and Documents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../LLM_tech.html">LLM 周边技术</a> &raquo;</li>
        
      <li>19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/LLM_techs/Parallelism/1910_PipeDream2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</a><ul>
<li><a class="reference internal" href="#id2">收集</a></li>
<li><a class="reference internal" href="#abstract">ABSTRACT</a></li>
<li><a class="reference internal" href="#introduction">1. Introduction</a></li>
<li><a class="reference internal" href="#background-and-related-work">2. BACKGROUND AND RELATED WORK</a><ul>
<li><a class="reference internal" href="#intra-batch-parallelism">2.1 Intra-batch Parallelism</a></li>
<li><a class="reference internal" href="#inter-batch-parallelism">2.2 Inter-batch Parallelism</a></li>
<li><a class="reference internal" href="#dnn-model-and-hardware-diversity">2.3 DNN Model and Hardware Diversity</a></li>
<li><a class="reference internal" href="#id3">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pipeline-parallelism">3. 流水线并行(PIPELINE PARALLELISM)</a><ul>
<li><a class="reference internal" href="#work-partitioning">3.1 挑战 1: 工作分区(Work Partitioning)</a></li>
<li><a class="reference internal" href="#work-scheduling">3.2 挑战 2: 工作调度(Work Scheduling)</a><ul>
<li><a class="reference internal" href="#pipedream">解决方案: PipeDream的调度策略</a></li>
</ul>
</li>
<li><a class="reference internal" href="#effective-learning">3.3 挑战 3: 有效学习(Effective Learning)</a><ul>
<li><a class="reference internal" href="#weight-stashing">解决方案: 权重缓存(Weight Stashing)</a></li>
<li><a class="reference internal" href="#vertical-sync">垂直同步(Vertical Sync)</a></li>
<li><a class="reference internal" href="#staleness">过时性(Staleness)</a></li>
<li><a class="reference internal" href="#id4">总结</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id5">4. 实现</a></li>
<li><a class="reference internal" href="#id6">6. 结论</a></li>
</ul>
</li>
</ul>

            </nav>
  <table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
<section id="xx-pipedream-generalized-pipeline-parallelism-for-dnn-training">
<h1>19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training<a class="headerlink" href="#xx-pipedream-generalized-pipeline-parallelism-for-dnn-training" title="此标题的永久链接">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://deepakn94.github.io/assets/papers/pipedream-sosp19.pdf">https://deepakn94.github.io/assets/papers/pipedream-sosp19.pdf</a></p></li>
<li><p><a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3341301.3359646">https://dl.acm.org/doi/abs/10.1145/3341301.3359646</a></p></li>
<li><p>2019.10.27</p></li>
<li><p>组织: Microsoft Research, Carnegie Mellon University, Stanford University</p></li>
<li><p>标签: Pipeline_Parallelism</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>进一步扩展了PipeDream的概念，提出了将批内并行与批间流水线相结合的方法，以进一步提高并行训练的吞吐量。</p>
</div>
<section id="id2">
<h2>收集<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>GPipe需要等所有的microbatch前向传播完成后，才会开始反向传播。PipeDream则是当一个microbatch的前向传播完成后，立即进入反向传播阶段。理论上，反向传播完成后就可以丢弃掉对应microbatch缓存的激活。由于PipeDream的反向传播完成的要比GPipe早，因此也会减少显存的需求。</p></li>
</ul>
</section>
<section id="abstract">
<h2>ABSTRACT<a class="headerlink" href="#abstract" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>【原始内容】DNN（深度神经网络）训练极其耗时，因此需要高效的多加速器并行化。当前并行化训练的方法主要使用 <strong>批内并行(intra-batch parallelization)</strong> ，其中单次训练迭代被分配到可用的工作节点上，但在工作节点数量增加时，往往会出现收益递减的情况。我们提出了PipeDream，一个将 <strong>批间流水线并行(inter-batch pipelining)</strong> 与批内并行相结合的系统，旨在进一步提高并行训练的吞吐量，帮助更好地重叠计算和通信，并在可能的情况下减少通信量。与传统的流水线不同，DNN训练是双向的，计算图的前向传播之后紧接着进行反向传播，反向传播需要使用在前向传播过程中计算的状态和中间数据。因此，简单的流水线可能导致前向传播和反向传播中使用的状态版本不匹配，或者过多的流水线刷新，进而降低硬件效率。为了解决这些挑战，PipeDream通过版本化模型参数来确保梯度计算的准确性，并且在不同的工作节点上并行调度不同小批量的前向传播和反向传播，最大限度地减少流水线停滞。PipeDream还自动将DNN的层分配给工作节点，以平衡工作负载并最小化通信。通过在多种DNN任务、模型和硬件配置下进行的广泛实验，结果表明，PipeDream训练模型的精度与常用的批内并行技术相比，速度提高了最多5.3倍。</p></li>
<li><dl class="simple">
<dt>背景：</dt><dd><ul>
<li><p>DNN训练时间非常长，因此需要高效的多加速器并行化来加速训练过程。</p></li>
<li><p>现有的并行训练方法主要通过 批内并行（intra-batch parallelism） 来将单次训练迭代分割到多个工作节点上。</p></li>
<li><p>但这种方法在增加工作节点数量时存在 回报递减（diminishing returns） 的问题，即扩展工作节点的数量效果会逐渐下降。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream 的创新：</dt><dd><ul>
<li><p>PipeDream 提出了一种 <strong>跨批次流水线（inter-batch pipelining）</strong> 和 <strong>批内并行（intra-batch parallelism）</strong> 相结合的方式，进一步提高并行训练的吞吐量。</p></li>
<li><p>其核心思想是帮助更好地 重叠计算与通信，并在可能的情况下减少通信量。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>流水线的挑战：</dt><dd><ul>
<li><dl class="simple">
<dt>与传统的流水线不同，DNN训练的过程是 双向的（bi-directional），即：</dt><dd><ul>
<li><p>前向传播（forward pass）：计算通过计算图的结果。</p></li>
<li><p>反向传播（backward pass）：使用前向传播过程中计算得到的中间数据和状态，进行梯度计算。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>如果简单地实现流水线，可能会导致 前向和反向传播使用不匹配的状态版本，或者 流水线刷新过于频繁，导致硬件效率降低。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream 的解决方案：</dt><dd><ul>
<li><p>版本化模型参数：为了确保梯度计算的正确性，PipeDream 在进行反向传播时，确保使用正确版本的模型参数。</p></li>
<li><p>并行调度前后传播：它并行地调度不同小批次（minibatches）的前向和反向传播，尽量减少流水线停顿（pipeline stalls）。</p></li>
<li><p>自动分配DNN层：PipeDream 会自动将 DNN 层分配到不同的工作节点，以 平衡工作量并最小化通信。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>总结：PipeDream 通过结合 跨批次流水线 和 批内并行 的方法，提高了 DNN 训练的效率，能够更好地利用硬件资源，减少不必要的通信，从而加速训练过程，显著提高吞吐量。</p>
</div>
</section>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="此标题的永久链接">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>详细介绍了 PipeDream 系统，它通过结合 批内并行（intra-batch parallelism） 和 跨批并行（inter-batch parallelism） 来加速深度神经网络（DNN）的训练过程。</p>
</div>
<ul class="simple">
<li><dl class="simple">
<dt>背景：</dt><dd><ul>
<li><p>DNN的广泛应用：DNN在图像分类、翻译、语言建模和视频字幕等领域取得了巨大的进展。但随着DNN的广泛应用，其训练过程变得越来越 计算密集，需要在多个加速器（如GPU）之间并行执行。</p></li>
<li><p>DNN训练流程：DNN训练包括前向传播和反向传播。每次训练迭代都会处理一个小批次（minibatch）数据并更新模型参数。为了加速训练，目前的做法主要是通过 并行化每次优化迭代。</p></li>
<li><p>并行化的挑战：在大规模训练时，批内并行（尤其是数据并行）会面临高通信成本，特别是当工作节点数量增加时，通信开销变得非常大。例如，在32个GPU上，某些模型的通信开销可能高达90%。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream的提出：</dt><dd><ul>
<li><p>PipeDream的核心创新：PipeDream通过引入 流水线并行（Pipeline Parallelism），结合 批内并行 和 跨批并行 来减少通信开销并加速训练。</p></li>
<li><p>PipeDream将模型划分为多个连续的操作组（称为“层”），并将这些层分配给不同的工作节点。然后，它通过流水线方式重叠不同输入的计算和通信。</p></li>
<li><p>通信优化：在PipeDream中，通信仅限于不同工作节点之间相邻层的输入和输出（前向传播中的激活和反向传播中的梯度），这大大减少了通信量，并且通信是点对点的，而不是全对全的通信方式。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>持久挑战与PipeDream的解决方案：</dt><dd><ul>
<li><dl class="simple">
<dt>DNN训练的双向性问题：</dt><dd><ul>
<li><p>DNN训练过程中，前向传播和反向传播需要使用相同的模型权重和中间结果。</p></li>
<li><p>流水线操作要确保前向传播和反向传播的数据同步，以保证梯度计算的正确性。</p></li>
<li><p>传统流水线调度方法可能会导致统计效率低下，或者无法达到预期的训练精度。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream的调度方法：</dt><dd><ul>
<li><p>PipeDream使用了一种名为 1F1B（1 Forward 1 Backward） 的调度算法，该算法确保每个工作节点在流水线的“稳定状态”下严格交替执行前向和反向传播。</p></li>
<li><p>这种调度方法能够保证硬件的高效利用，并且没有流水线停顿（pipeline stalls）。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>1F1B算法的细节：</dt><dd><ul>
<li><p>每个工作节点严格交替执行前向和反向传播，保证硬件的高效利用。</p></li>
<li><p>通过使用不同版本的模型权重，保持与数据并行相似的统计效率。</p></li>
<li><p>每次反向传播都会更新权重，下一次前向传播会使用最新的权重版本，并“存储”这些权重用于后续的反向传播。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>流水线负载均衡：</dt><dd><ul>
<li><p>PipeDream还能够自动平衡不同阶段的计算负载，并通过短时间的单GPU性能分析来决定如何划分模型操作。</p></li>
<li><p>它能有效应对模型多样性（计算和通信的不同需求）和平台多样性（不同的互联拓扑结构和带宽）。</p></li>
<li><p>如果某些阶段的计算量过大，PipeDream可能会使用数据并行来为该阶段分配多个工作节点，从而并行处理不同的小批次。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>总结：PipeDream通过结合流水线并行和批内并行，显著提高了DNN训练的效率。其 1F1B调度算法 和 自动负载均衡 机制能够有效地减少通信开销，同时确保训练过程的高硬件利用率和高统计效率，使得训练速度大幅提升。</p>
</div>
</section>
<section id="background-and-related-work">
<h2>2. BACKGROUND AND RELATED WORK<a class="headerlink" href="#background-and-related-work" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>这段内容讨论了 DNN训练的并行化方法，包括批内并行（intra-batch parallelism）和跨批并行（inter-batch parallelism），并介绍了与DNN模型和硬件多样性相关的挑战。</p></li>
</ul>
<section id="intra-batch-parallelism">
<h3>2.1 Intra-batch Parallelism<a class="headerlink" href="#intra-batch-parallelism" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id8">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/Rh4sOa.png" src="https://img.zhaoweiguo.com/uPic/2025/02/Rh4sOa.png" />
<figcaption>
<p><span class="caption-text">Figure 2: Model parallel training with 4 workers. Numbers indicate batch ID, and backward passes takes twice as long as forward passes. For simplicity, we assume that communicating activations/gradients across workers has no overhead.</span><a class="headerlink" href="#id8" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><dl class="simple">
<dt>批内并行是当前最常见的DNN训练方式，它通过将单个训练迭代拆分到多个工作节点（例如GPU）上来进行加速。</dt><dd><ul>
<li><dl class="simple">
<dt>数据并行（Data Parallelism）：</dt><dd><ul>
<li><dl class="simple">
<dt>原理：</dt><dd><ul>
<li><p>在数据并行中，输入数据被划分给不同的工作节点，每个节点维护一个本地模型副本，独立训练自己分配的数据部分，并定期与其他节点同步模型权重。</p></li>
<li><p>通信的量主要取决于模型权重的大小以及参与训练的工作节点数量。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>挑战：</dt><dd><ul>
<li><dl class="simple">
<dt>通信开销：</dt><dd><ul>
<li><p>即使使用现代多GPU服务器和高效的通信库（如NCCL），在数据并行训练中，通信开销依然较高。</p></li>
<li><p>例如，在32个GPU上训练时，某些模型的通信开销可能高达90%，尤其是在使用跨服务器的高延迟通信时。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>缩放问题：</dt><dd><ul>
<li><p>数据并行在多个GPU或多个服务器间扩展时，通信瓶颈变得更加严重，特别是在GPU的计算能力不断提升时，通信开销却相应增大。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>其他优化：</dt><dd><ul>
<li><dl class="simple">
<dt>异步并行训练（ASP）：</dt><dd><ul>
<li><p>允许每个工作节点在接收到前一个小批次的梯度之前，开始处理下一个小批次。这样可以提高硬件效率，但可能会引入梯度陈旧问题，导致统计效率降低。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>梯度量化：</dt><dd><ul>
<li><p>通过量化梯度减少通信量，但这种方法并非对所有模型都有效。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>模型并行（Model Parallelism）：</dt><dd><ul>
<li><dl class="simple">
<dt>原理：</dt><dd><ul>
<li><p>在模型并行中，DNN模型的不同操作被分配到不同的工作节点，每个节点只负责自己分配的部分模型进行前向和反向传播。</p></li>
<li><dl class="simple">
<dt>模型并行可以用于训练非常大的模型，但它面临着两个主要问题：</dt><dd><ul>
<li><p>计算资源利用率低：每个工作节点只能处理模型的一部分，因此有时会出现资源闲置的情况。</p></li>
<li><p>模型划分难度大：模型并行需要开发者手动分配模型操作到不同的工作节点，导致优化过程变得复杂且不易扩展。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>混合并行（Hybrid Parallelism）：</dt><dd><ul>
<li><p>一些方法尝试在单次优化迭代中同时使用数据并行和模型并行，例如OWT和FlexFlow。</p></li>
<li><p>尽管这些方法尝试提高计算效率，但通常并没有结合流水线并行，导致性能并未充分发挥。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="inter-batch-parallelism">
<h3>2.2 Inter-batch Parallelism<a class="headerlink" href="#inter-batch-parallelism" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id9">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/om8KJS.png" src="https://img.zhaoweiguo.com/uPic/2025/02/om8KJS.png" />
<figcaption>
<p><span class="caption-text">Figure 3: GPipe’s inter-batch parallelism approach. Frequent pipeline flushes lead to increased idle time.</span><a class="headerlink" href="#id9" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>跨批并行尝试将训练过程中的不同小批次并行化，这通常涉及在模型并行训练的框架下进行流水线操作。</p></li>
<li><dl class="simple">
<dt>GPipe：</dt><dd><ul>
<li><p>GPipe是一个使用流水线并行的系统，它在模型并行训练中对每个小批次进行分割，先执行前向传播，再执行反向传播。</p></li>
<li><p>GPipe通过将小批次分割成多个微批次来优化内存使用，</p></li>
<li><p>但其主要问题在于如果微批次较小，会导致频繁的流水线刷新，影响硬件效率。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream的优势：PipeDream与GPipe相比，解决了几个关键问题，特别是：</dt><dd><ul>
<li><p>更高的硬件效率：PipeDream将流水线并行与批内并行结合，确保每个工作节点得到充分利用，从而提高硬件效率。</p></li>
<li><p>自动化模型划分：PipeDream能够自动根据硬件配置和模型特点来划分DNN模型，而不像GPipe那样依赖手动划分。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="dnn-model-and-hardware-diversity">
<h3>2.3 DNN Model and Hardware Diversity<a class="headerlink" href="#dnn-model-and-hardware-diversity" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt>DNN模型本身在架构上有很大的差异，常见的有卷积层、LSTM、注意力层和全连接层等，每种类型的模型在并行化时表现不同，因此选择合适的并行化策略高度依赖于目标模型和硬件配置。</dt><dd><ul>
<li><dl class="simple">
<dt>硬件多样性：</dt><dd><ul>
<li><p>不同类型的硬件（如GPU、ASIC、FPGA）具有不同的计算能力，且加速器之间的互联结构和带宽差异也很大。</p></li>
<li><p>例如，云服务器使用的网络带宽可能在10Gbps到100Gbps之间，而服务器内部的GPU可能通过PCIe连接（10到15GBps），或者使用高端的NVLink（30GBps）。</p></li>
<li><p>这种硬件的多样性使得选择最佳的并行化策略变得非常复杂。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream的自动化：</dt><dd><ul>
<li><p>由于硬件和模型多样性带来的复杂性，PipeDream通过自动化的方式来解决模型划分和并行化的挑战，帮助实现高效的训练。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="id3">
<h3>总结<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h3>
<p>这段内容深入探讨了当前DNN训练的并行化技术，特别是批内并行和跨批并行的不同方法。它分析了数据并行、模型并行和混合并行的优缺点，并指出传统的并行化方法（如数据并行和模型并行）在大规模训练中面临的通信瓶颈和计算资源浪费问题。PipeDream通过结合流水线并行和批内并行，并自动化模型划分，成功解决了这些挑战，从而实现了更高效的训练过程。</p>
</section>
</section>
<section id="pipeline-parallelism">
<h2>3. 流水线并行(PIPELINE PARALLELISM)<a class="headerlink" href="#pipeline-parallelism" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>主要介绍了 PipeDream（一种用于深度神经网络训练的并行化策略）的核心概念和挑战，特别是其在使用流水线并行（Pipeline Parallelism, PP）时如何优化计算和通信。</p></li>
<li><dl class="simple">
<dt>流水线并行（Pipeline Parallelism, PP）</dt><dd><ul>
<li><p>将 批内并行（intra-batch parallelism）和 批间并行（inter-batch parallelism）结合在一起。</p></li>
<li><dl class="simple">
<dt>流水线并行的核心思想是将深度神经网络（DNN）的各层分为多个阶段，每个阶段由模型中的连续层组成，每个阶段分配到一块独立的 GPU 来执行。</dt><dd><ul>
<li><p>前向传播：每个阶段处理自己的输入数据并将结果传递给下一个阶段。</p></li>
<li><p>反向传播：最后一个阶段在完成前向传播后，立即开始反向传播，并将梯度传回前一个阶段。</p></li>
<li><p>在这个过程中，各个阶段并行工作，一个阶段的计算和下一个阶段的计算可以重叠进行。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>流水线并行的优势</dt><dd><ul>
<li><dl class="simple">
<dt>流水线并行相比传统的 数据并行（Data Parallelism, DP）有两个主要的优势：</dt><dd><ul>
<li><p>减少通信量：在数据并行中，需要频繁地交换整个模型的梯度，而流水线并行则仅需传递每个阶段部分的梯度和激活值，这大大减少了通信量。</p></li>
<li><p>计算与通信重叠：流水线并行通过异步地进行前向激活值和反向梯度的传递，使得计算和通信可以同时进行，从而提高了效率。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
<figure class="align-default" id="id10">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/zODtDu.png" src="https://img.zhaoweiguo.com/uPic/2025/02/zODtDu.png" />
<figcaption>
<p><span class="caption-text">Figure 4: An example PipeDream pipeline with 4 workers, showing startup and steady states. In this example, the backward pass takes twice as long as the forward pass.</span><a class="headerlink" href="#id10" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id11">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/VVYRDQ.png" src="https://img.zhaoweiguo.com/uPic/2025/02/VVYRDQ.png" />
<figcaption>
<p><span class="caption-text">Figure 5: An example pipeline-parallel assignment with four GPUs and an example timeline at one of the GPUs (worker 3), highlighting the temporal overlap of computation and activation/gradient communication.</span><a class="headerlink" href="#id11" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id12">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/iSbTLv.png" src="https://img.zhaoweiguo.com/uPic/2025/02/iSbTLv.png" />
<figcaption>
<p><span class="caption-text">Figure 6: PipeDream’s automated mechanism to partition DNN layers into stages. PipeDream first profiles the input DNN, to get estimates for each layer’s compute time and output size. Using these estimates, PipeDream’s optimizer partitions layers across available machines, which is then executed by PipeDream’s runtime.</span><a class="headerlink" href="#id12" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<section id="work-partitioning">
<h3>3.1 挑战 1: 工作分区(Work Partitioning)<a class="headerlink" href="#work-partitioning" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id13">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/g9s4AH.png" src="https://img.zhaoweiguo.com/uPic/2025/02/g9s4AH.png" />
<figcaption>
<p><span class="caption-text">Figure 7: An example 2-level hardware topology. Solid green boxes represent GPUs. Each server (dashed yellow boxes) has 4 GPUs connected internally by links of bandwidth B1; each server is connected by links of bandwidth B2. In real systems, B1 &gt; B2. Figure best seen in color.</span><a class="headerlink" href="#id13" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><dl class="simple">
<dt>问题：在流水线中，如果各阶段的计算速度不一致，会导致一些阶段等待数据，造成资源浪费。并且，过多的通信也会拖慢训练速度。</dt><dd><ul>
<li><p>PipeDream将模型训练视为一个计算流水线，每个工作节点（worker）执行模型的一部分（stage）。</p></li>
<li><p>流水线的吞吐量（throughput）由最慢的阶段决定。如果每个阶段的处理速度差异过大，就会导致流水线中出现空闲时间（bubbles），使得一些较快的阶段没有足够的工作负载，从而导致资源未得到充分利用。</p></li>
<li><p>过度的工作节点间通信也会降低整个训练流水线的吞吐量。</p></li>
<li><p>为了有效地进行工作分配，每个阶段的分配需要考虑模型特性和硬件配置。某些情况下，简单地将模型划分到多个GPU上并不能同时满足减少通信和完美负载平衡的需求。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>解决方案：</dt><dd><ul>
<li><dl class="simple">
<dt>整体：</dt><dd><ul>
<li><p>PipeDream 使用优化算法根据 DNN 模型的层、硬件拓扑结构等因素，将模型划分为多个阶段，使每个阶段的计算速度大致相同，并尽量减少通信量。</p></li>
<li><p>此外，PipeDream 还通过数据并行方法在某些阶段进行阶段复制来进一步提高负载均衡。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>PipeDream的优化器通过以下方式来平衡流水线：</dt><dd><ul>
<li><dl class="simple">
<dt>分配模型层到多个阶段：算法将深度神经网络（DNN）的层划分为多个阶段，确保每个阶段的计算时间大致相同，同时尽量减少工作节点之间的通信。</dt><dd><ul>
<li><p>比如：将大输出的激活值发送到带宽较高的通信链路中。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>阶段复制：为了进一步优化负载均衡，PipeDream允许对某些阶段进行数据并行性（data parallelism）处理，即同一个阶段可以在多个工作节点上进行复制处理。</p></li>
<li><dl class="simple">
<dt>最小化慢阶段的时间：这就相当于将问题转化为一个“最小化流水线中最慢阶段的时间”问题。</dt><dd><ul>
<li><p>这个问题有最优子问题性质，即：一个最大化吞吐量的流水线由若干个小的子流水线组成，每个子流水线也是最大化吞吐量的。</p></li>
<li><p>PipeDream使用动态规划（Dynamic Programming, DP）来寻找这个最优解。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>DNN训练时间稳定性：PipeDream还利用了DNN训练中的计算时间变化小这一特性，即不同输入的计算时间差异不大。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>总结：这段内容主要讲解了PipeDream如何通过优化工作分配来解决流水线并行中的瓶颈问题。它通过动态规划和硬件拓扑信息，在减少通信的同时平衡负载，从而提高流水线的吞吐量。这个优化过程对训练时间和通信开销进行了精细的控制，并最终实现了高效的并行化。</p></li>
</ul>
</section>
<section id="work-scheduling">
<h3>3.2 挑战 2: 工作调度(Work Scheduling)<a class="headerlink" href="#work-scheduling" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id14">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/ReggtW.png" src="https://img.zhaoweiguo.com/uPic/2025/02/ReggtW.png" />
<figcaption>
<p><span class="caption-text">Figure 8: An example PipeDream pipeline with 3 workers and 2 stages. We assume that forward and backward passes in the first stage take two time units, while forward and backward passes in the second stage take only a single time unit. The first stage in this pipeline is replicated twice so that each stage sustains roughly the same throughput. Here, we assume that forward and backward passes take equal time, but this is not a requirement of our approach.</span><a class="headerlink" href="#id14" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>与传统的单向流水线不同，PipeDream的训练流水线是双向的，即每个输入的小批次（minibatch）首先进行前向传播（forward pass），然后进行反向传播（backward pass）。</p></li>
<li><dl class="simple">
<dt>每个在流水线中的活动小批次可能处于不同的阶段，既可能在进行前向传播，也可能在进行反向传播。由于这一点，每个工作节点（worker）必须决定：</dt><dd><ul>
<li><p>是否执行该阶段的小批次的前向传播，并将小批次推送到下游工作节点，还是</p></li>
<li><p>是否执行该阶段的小批次的反向传播，并将小批次推送到上游工作节点。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>阶段复制（例如，某些阶段的数据并行处理）进一步增加了工作调度的复杂性，需要决定如何在不同的复制阶段之间路由小批次。</p></li>
</ul>
<section id="pipedream">
<h4>解决方案: PipeDream的调度策略<a class="headerlink" href="#pipedream" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt>启动阶段（Startup Phase）：</dt><dd><ul>
<li><p>在启动阶段，输入阶段会接纳足够多的小批次，以确保流水线在稳定状态下始终是满的。</p></li>
<li><p>为了达到这一目标，PipeDream根据前面提到的划分算法，计算出每个输入阶段复制体应接纳的小批次数量，</p></li>
<li><p>公式: <span class="math notranslate nohighlight">\(\text{NUM_OPT_ACTIVE_MINIBATCHES} (\text{NOAM}) = \left\lceil \frac{\text{# workers}}{\text{# of replicas in the input stage}} \right\rceil\)</span></p></li>
<li><dl class="simple">
<dt>其中：</dt><dd><ul>
<li><p>NUM_OPT_ACTIVE_MINIBATCHES (NOAM)：即在稳定状态下，每个输入阶段的复制体应接纳的最优小批次数量。</p></li>
<li><p>该公式确保了在流水线的启动阶段，每个输入阶段的工作节点在稳定状态下能够有足够的小批次进行处理。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>稳定状态下的调度：</dt><dd><ul>
<li><dl class="simple">
<dt>一旦流水线进入稳定状态，每个阶段会交替进行前向传播和反向传播，我们称之为一前一后调度（1F1B schedule）。</dt><dd><ul>
<li><p>在1F1B调度中，每个阶段交替处理不同的小批次的前向传播和反向传播。</p></li>
<li><p>这种调度确保了每个GPU都能被占用，并且流水线能够平衡地处理每个阶段，所有阶段大致以相同的速度生成输出。</p></li>
<li><p>1F1B调度还确保了反向传播从输入开始时能够按照规律的时间间隔应用到每个阶段。</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>例如，在图4的示意图中，稳定状态下，每个工作节点会交替进行前向传播和反向传播，且每个阶段的处理是平衡的。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>数据并行配置中的调度：</dt><dd><ul>
<li><dl class="simple">
<dt>当一个阶段以数据并行方式运行（即该阶段被复制到多个GPU上），PipeDream采用了确定性轮询负载均衡，根据小批次的标识符来分配工作。具体来说：</dt><dd><ul>
<li><p>每个小批次会被轮询地分配到工作节点上，确保相同的小批次ID在前向传播和反向传播时始终由同一个工作节点处理。</p></li>
<li><p>这种机制被称为一前一后轮询调度（1F1B-RR），是一个静态策略，不需要进行复杂的分布式协调。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>例如，在图8中展示了一个简单的2-1配置，第一个阶段被复制成两个副本，第二个阶段没有复制：</dt><dd><ul>
<li><p>第一阶段：所有偶数ID的小批次由工作节点1处理，所有奇数ID的小批次由工作节点2处理。</p></li>
<li><p>第二阶段：所有小批次由工作节点3处理。</p></li>
<li><p>所有工作节点都会先执行前向传播，然后执行反向传播，且每个阶段会处理不同的小批次。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>1F1B-RR调度的有效性：</dt><dd><ul>
<li><p>1F1B-RR调度的有效性不依赖于前向传播和反向传播的时间必须相等。</p></li>
<li><p>实际上，通常情况下，反向传播的时间会比前向传播的时间长。因此，即使前向传播时间较短，1F1B-RR仍然是有效的调度机制。</p></li>
<li><p>图4中的示意图进一步展示了在这种调度下，系统如何高效地进行工作分配。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<ul class="simple">
<li><dl class="simple">
<dt>小结</dt><dd><ul>
<li><p>与传统的单向流水线不同，PipeDream 中的训练涉及双向流水线，其中输入小批量首先向前，然后向后通过计算流水线。</p></li>
<li><p>在启动阶段，输入阶段接收足够的小批量，以使流水线在稳定状态下保持满负荷。</p></li>
<li><p>1F1B 调度确保每个 GPU 都被一个小批量占用，并确保输入的后向传递以规律的时间间隔应用。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
</section>
<section id="effective-learning">
<h3>3.3 挑战 3: 有效学习(Effective Learning)<a class="headerlink" href="#effective-learning" title="此标题的永久链接">¶</a></h3>
<figure class="align-default" id="id15">
<img alt="https://img.zhaoweiguo.com/uPic/2025/02/wk5hUx.png" src="https://img.zhaoweiguo.com/uPic/2025/02/wk5hUx.png" />
<figcaption>
<p><span class="caption-text">Figure 9: Weight stashing as minibatch 5 flows across stages. Arrows point to weight versions used for forward and backward passes for minibatch 5 at the first and third stages.</span><a class="headerlink" href="#id15" title="此图像的永久链接">¶</a></p>
</figcaption>
</figure>
<ul class="simple">
<li><p>在传统的流水线训练系统中，如果一个阶段的前向传播使用了某一版本的参数，而对应的反向传播却使用了另一个版本的参数，可能会导致无效的梯度，从而影响模型的收敛性。具体来说，假设在Pipeline中有4个工作节点（workers），并且没有阶段复制（stage replication）。</p></li>
<li><dl class="simple">
<dt>图4展示了这个问题：</dt><dd><ul>
<li><p>在第1个阶段，对小批次5的前向传播是在小批次1的更新之后进行的，但是反向传播却是基于小批次2、3和4的更新来计算的。</p></li>
<li><p>结果就是，在小批次5的反向传播中使用的权重版本和前向传播时使用的权重版本不同，这会导致无效梯度，并可能妨碍模型的收敛。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<section id="weight-stashing">
<h4>解决方案: 权重缓存(Weight Stashing)<a class="headerlink" href="#weight-stashing" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>PipeDream提出了一个技术叫做权重缓存（Weight Stashing），来解决前向传播和反向传播中权重版本不一致的问题。</p></li>
<li><dl class="simple">
<dt>具体步骤如下：</dt><dd><ul>
<li><dl class="simple">
<dt>权重缓存技术通过为每个活动的小批次维护多个版本的权重来避免这种不匹配。</dt><dd><ul>
<li><p>每个阶段会使用最新版本的权重来进行前向传播。</p></li>
<li><p>前向传播完成后，PipeDream会将使用的权重缓存起来，并在反向传播时使用相同版本的权重来计算梯度和更新。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>每个阶段(within a stage)在处理某个小批次时，会确保前向传播和反向传播使用相同的权重版本，从而避免了前面提到的梯度不一致问题。</dt><dd><ul>
<li><p>如图9所示</p></li>
<li><p>假设小批次5在第1个机器上使用了小批次1的更新，在第2个机器上使用了小批次2的更新。</p></li>
<li><p>在这种情况下，前向传播和反向传播会使用相同的权重版本，从而确保梯度计算的一致性。</p></li>
<li><p>Weight stashing 并不保证跨 stage 的一致性</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="vertical-sync">
<h4>垂直同步(Vertical Sync)<a class="headerlink" href="#vertical-sync" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><dl class="simple">
<dt><strong>垂直同步（Vertical Sync）</strong> 是PipeDream中的一个可选技术，旨在消除各个阶段(stage)之间的潜在不一致性。</dt><dd><ul>
<li><p>在图4的例子中，使用垂直同步后，小批次5的前向传播和反向传播都使用小批次1更新的权重版本。</p></li>
<li><p>具体来说，每个进入流水线的小批次( <span class="math notranslate nohighlight">\(b_i\)</span> ）都会与最新的权重版本( <span class="math notranslate nohighlight">\(w^(i-x)\)</span> ）关联，并且这个信息随着小批次的前向传播和反向传播在流水线中传播。</p></li>
<li><p>各个阶段在进行前向传播时，都会使用缓存的权重( <span class="math notranslate nohighlight">\(w^(i-x)\)</span> )，而不是最新的权重更新。</p></li>
<li><p>在完成反向传播后，每个阶段独立地进行权重更新，生成最新的权重( <span class="math notranslate nohighlight">\(w^(i)\)</span> )。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>需要注意的是，垂直同步和传统的数据并行（Data Parallelism）有所不同。</dt><dd><ul>
<li><p>在传统的数据并行中，梯度会在所有的活动小批次之间进行汇总，而垂直同步并不会这样做，它只确保在不同阶段使用相同版本的权重来计算梯度。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="staleness">
<h4>过时性(Staleness)<a class="headerlink" href="#staleness" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>在上述技术中，我们可以正式定义不同权重更新的“过时性”（staleness）。</p></li>
<li><p>假设在一个直线流水线中（没有阶段复制），模型被划分成多个阶段，权重分别为w1, w2, …, wn，且每个阶段的权重更新具有一定的“延迟”。</p></li>
<li><p>没有权重缓存时，梯度的计算会使用不同版本的权重，这会导致梯度计算无效。</p></li>
<li><p>有权重缓存时，梯度计算会根据不同的阶段使用不同延迟的权重版本。这意味着在每个阶段计算梯度时，使用的是多个小批次前的权重，而不是当前版本的权重。</p></li>
<li><p>这种延迟效应会导致过时性，但通过引入垂直同步，可以减少这种过时性，使得在各个阶段使用的权重版本保持一致，从而得到有效的梯度计算。</p></li>
</ul>
</section>
<section id="id4">
<h4>总结<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>这段内容描述了PipeDream如何解决模型训练中的权重更新不一致问题，提出了 <strong>权重缓存（Weight Stashing）</strong> 和 <strong>垂直同步（Vertical Sync）</strong> 两种技术，确保了在流水线中的每个阶段前向和反向传播使用一致的权重版本，避免了无效梯度的问题。同时，PipeDream还通过合理的内存管理，确保了在流水线并行下的内存效率。</p></li>
</ul>
<ul class="simple">
<li><dl class="simple">
<dt>小结</dt><dd><ul>
<li><p>在简单的流水线系统中，一个小批量在每个阶段的前向传递使用一个版本的参数执行，而其后向传递使用不同版本的参数执行。</p></li>
<li><p>PipeDream 使用一种称为权重存储的技术来避免在前向和后向传递中使用的权重版本之间存在根本不匹配。</p></li>
<li><p>权重存储确保在阶段内，给定小批量的前向和后向传递使用相同版本的模型参数。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
</section>
</section>
<section id="id5">
<h2>4. 实现<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">备注</p>
<p>介绍了PipeDream的实现细节，主要包括如何管理设备内存、调度工作、处理通信等。</p>
</div>
<ul class="simple">
<li><dl class="simple">
<dt>模型分割和优化</dt><dd><ul>
<li><dl class="simple">
<dt>模型配置：</dt><dd><ul>
<li><p>首先，PipeDream会在单个GPU上对模型进行分析，使用训练数据集的子集进行前期的测试。</p></li>
<li><p>之后，它会通过优化算法（在第3节提到的算法）来将深度神经网络（DNN）模型分割为多个阶段（可能还包括阶段复制），使得每个阶段的计算负载平衡。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>生成计算图：</dt><dd><ul>
<li><p>分割完成后，PipeDream会生成一个注解的操作符图（annotated operator graph），其中每个模型层会被映射到一个“阶段ID”。</p></li>
<li><p>接着，PipeDream会通过图的广度优先搜索（BFS）遍历它，生成每个阶段的代码，并保证各个操作的输入输出依赖关系不被破坏。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Parameter State(参数状态管理)</dt><dd><ul>
<li><p>GPU内存：PipeDream会把每个阶段（以及任何复制的阶段）的参数直接保存在GPU内存中，确保每个阶段能访问到所需的最新参数版本。</p></li>
<li><p>参数同步：在阶段复制的情况下，参数更新会在复制阶段之间同步。如果有新版本的参数，PipeDream会等到该版本的参数用于完成反向传播后才会丢弃旧版本。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Intermediate State(中间状态管理)</dt><dd><ul>
<li><p>数据复制和存储：每个阶段的输入输出数据会被赋予一个唯一的标识符（blob ID）。</p></li>
<li><p>当阶段接收到前一阶段的中间数据时，PipeDream会将这些数据复制到GPU内存中，并将其放入工作队列。</p></li>
<li><p>数据在前向传播期间不会丢弃，直到对应的反向传播完成。</p></li>
<li><p>反向传播中的中间数据会在使用完毕后被释放，或者在需要时发送到下一个阶段。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>阶段复制（Stage Replication）</dt><dd><ul>
<li><dl class="simple">
<dt>数据并行：</dt><dd><ul>
<li><p>PipeDream通过使用PyTorch的DistributedDataParallel库来同步数据并行阶段的参数。</p></li>
<li><p>每个数据并行阶段会根据所需的计算量进行复制，并通过快速无等待反向传播来传递梯度。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>NCCL和Gloo：</dt><dd><ul>
<li><p>在数据并行训练中，NCCL用于GPU间大规模的张量通信，而Gloo则更适合处理小型张量的交换，尤其是激活值和梯度的传递。</p></li>
<li><p>在PipeDream的流水线并行训练中，默认使用Gloo来进行各GPU之间的通信。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>检查点（Checkpointing）</dt><dd><ul>
<li><p>容错机制：为了提高容错能力，PipeDream支持周期性地保存模型的检查点。</p></li>
<li><p>检查点不会需要全局的协调，因此每个阶段在一个epoch的最后一批数据的反向传播完成后，会本地保存模型参数。</p></li>
<li><p>若发生故障，训练可以从最后一次成功保存的检查点恢复。</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>总结</dt><dd><ul>
<li><p>PipeDream的实现通过优化模型分割、内存管理和通信调度来支持高效的流水线并行训练。</p></li>
<li><p>它能够扩展到多个GPU并进行数据并行训练，同时还提供了容错机制，以确保训练过程中的鲁棒性。通过合理的阶段划分和阶段复制，PipeDream能够减少通信开销并提高训练效率。</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</section>
<section id="id6">
<h2>6. 结论<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h2>
<ul class="simple">
<li><p>流水线并行 DNN 训练有助于减少可能阻碍批处理内并行性的通信开销。</p></li>
<li><p>与最先进的方法相比，PipeDream 在一系列 DNN 和硬件配置中完成训练的速度提高了 5.3 倍。</p></li>
</ul>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference external" href="/index.html">主页</a></p></td>
<td><p><a class="reference internal" href="../../genindex.html"><span class="std std-ref">索引</span></a></p></td>
<td><p><a class="reference internal" href="../../py-modindex.html"><span class="std std-ref">模块索引</span></a></p></td>
<td><p><a class="reference internal" href="../../search.html"><span class="std std-ref">搜索页面</span></a></p></td>
</tr>
</tbody>
</table>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="2006.09503_PipeDream-2BW.html" class="btn btn-neutral float-right" title="2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="1909.08053_Megatron-LM.html" class="btn btn-neutral" title="1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../../_static/copybutton.js"></script>
      <script type="text/javascript" src="../../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="../../None"></script>
      <script type="text/javascript" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>