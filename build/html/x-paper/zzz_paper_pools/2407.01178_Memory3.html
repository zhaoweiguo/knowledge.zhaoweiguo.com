

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="zh-CN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh-CN" > <!--<![endif]-->
<head>


<!-- start added 2025-04-14   增加对markdown中公式的支持 -->
<script>
window.MathJax = {
    tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
    },
    options: {
        ignoreHtmlClass: "tex2jax_ignore|mathjax_ignore",
        processHtmlClass: "tex2jax_process|mathjax_process|math|output_area"
    }
};
</script>
<script defer="defer" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- end added 2025-04-14   增加对markdown中公式的支持 -->


  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>2407.01178_Memory3: Language Modeling with Explicit Memory &mdash; 新溪-gordon V2025.07 文档</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="2505.14683_Emerging Properties in Unified Multimodal Pretraining" href="2505.14683.html" />
    <link rel="prev" title="2405.16506_GRAG: Graph Retrieval-Augmented Generation" href="2405.16506_GRAG.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>
  <script src="../_static/js/jquery.min.js"></script>


<!-- 评论插件 gittalk start -->
<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script> -->
<!-- 评论插件 gittalk end -->


</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> 新溪-gordon
          

          
          </a>

          
            
            
              <div class="version">
                V2025.07
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0normal.html">通用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../0normals/normal.html">通用</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../0normals/normal.html#id3">如何看一个论文是不是重要</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../0normals/website.html">学术网站</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../0normals/website.html#id2">整体分析</a></li>
<li class="toctree-l3"><a class="reference internal" href="../0normals/website.html#id3">1. 学术搜索平台（核心功能：检索与发现文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#google-scholar">Google Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#semantic-scholar">Semantic Scholar</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#web-of-science">Web of Science</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#id4">百度学术</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../0normals/website.html#id5">2. 资源共享平台（核心功能：免费获取付费文献）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#sci-hub">Sci-Hub</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#library-genesis-libgen">Library Genesis (LibGen)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#unpaywall">Unpaywall</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../0normals/website.html#id6">论文数据库（核心功能：存储与提供文献原文）</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#acl-anthology">ACL Anthology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#arxiv">ArXiv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#cnki">知网 CNKI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../0normals/website.html#id10">万方数据库</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Benchmarking.html">评测基准</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id3">评测基准</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html">02xx.xxxxx_BLEU: a Method for Automatic Evaluation of Machine Translation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#id8">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-baseline-bleu-metric">2.The Baseline BLEU Metric</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-bleu-evaluation">3.The BLEU Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#the-human-evaluation">4.The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#bleu-vs-the-human-evaluation">5.BLEU vs The Human Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/02xx.xxxxx_BLEU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html">0401.xxxxx_ROUGE: A Package for Automatic Evaluation of Summaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-n-n-gram-co-occurrence-statistics">2.ROUGE-N: N-gram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-l-longest-common-subsequence">3.ROUGE-L: Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-w-weighted-longest-common-subsequence">4 ROUGE-W: Weighted Longest Common Subsequence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#rouge-s-skip-bigram-co-occurrence-statistics">5.ROUGE-S: Skip-Bigram Co-Occurrence Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#evaluations-of-rouge">6 Evaluations of ROUGE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/0401.xxxxx_ROUGE.html#conclusions">7 Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Standards/1803.01937_ROUGE2.html">1803.01937_ROUGE2.0: Updated and Improved Measures for Evaluation of Summarization Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1803.01937_ROUGE2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1803.01937_ROUGE2.html#problems-with-the-current-rouge-measures">1. Problems with the current ROUGE measures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1803.01937_ROUGE2.html#rouge-2-0">2. ROUGE 2.0</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html">1804.08771_SacreBLEU: A Call for Clarity in Reporting BLEU Scores</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#bleu">BLEU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#id3">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#problem-description">2 Problem Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#a-way-forward">3 A way forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/1804.08771_SacreBLEU.html#summary">4 Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html">2306.05685_Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#mt-bench-and-chatbot-arena">2 MT-Bench and Chatbot Arena</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#llm-as-a-judge">3 LLM as a Judge</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#agreement-evaluation">4 Agreement Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#human-preference-benchmark-and-standardized-benchmark">5 Human Preference Benchmark and Standardized Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-a-prompt-templates">Appendix A Prompt templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-b-case-study">Appendix B Case Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-c-data-collection">Appendix C Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-d-additional-experimental-results">Appendix D Additional Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-e-training-details-of-vicuna-models">Appendix E Training Details of Vicuna Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Standards/2306.05685_LLM-as-a-Judge.html#appendix-f-exploring-vicuna-as-a-judge">Appendix F Exploring Vicuna as a judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#agent">数据集-Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html">2312.14033_T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#t-eval">2 T-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-a-t-eval-benchmark-details">Appendix A T-Eval Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-c-detailed-evaluation-metrics">Appendix C Detailed Evaluation Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2312.14033_T-Eval.html#appendix-d-api-documentation">Appendix D API Documentation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html">2406.12045_τ-bench: A Benchmark for Tool-Agent-User</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#bench-a-benchmark-for-t-ool-a-gent-u-ser-interaction">3.τ-bench: A benchmark for T ool-A gent-U ser Interaction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#benchmark-construction">4. Benchmark Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#experiments">5.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2406.12045_%CF%84-bench.html#disscussion">6.Disscussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html">2506.07982_𝜏²-Bench: Evaluating Conversational Agents in a Dual-Control Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#tau-2-bench-evaluating-agents-in-a-dual-control-environment">3 <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench: Evaluating Agents in a Dual-Control Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#broader-impact">Broader Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-a-telecom-domain">Appendix A Telecom Domain</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-b-verifying-original-tau-2-bench">Appendix B Verifying Original <span class="math notranslate nohighlight">\(\tau^{2}\)</span>-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-d-domain-policies">Appendix D Domain Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Agents/2506.07982_%F0%9D%9C%8F%C2%B2-Bench.html#appendix-e-user-simulator-quality">Appendix E User Simulator Quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#qa">数据集-QA</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html">1809.09600_HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#data-collection">2 Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#processing-and-benchmark-settings">3 Processing and Benchmark Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#dataset-analysis">4 Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#conclusions">7 Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-a-data-collection-details">Appendix A Data Collection Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#a">附录A 数据收集细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-b-further-data-analysis">Appendix B Further Data Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/1809.09600_HotpotQA.html#appendix-c-full-wiki-setting-details">Appendix C Full Wiki Setting Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html">2109.07958_TruthfulQA: Measuring How Models Mimic Human Falsehoods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#the-truthfulqa-benchmark">2 The TruthfulQA Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#ethics-and-impact">8 Ethics and Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-a-additional-examples-from-truthfulqa">Appendix A Additional examples from TruthfulQA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-b-additional-results">Appendix B Additional results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-c-dataset-construction">Appendix C Dataset construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-d-human-evaluations">Appendix D Human evaluations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-e-prompts">Appendix E Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2109.07958_TruthfulQA.html#appendix-f-checking-for-data-quality-and-disagreement">Appendix F Checking for data quality and disagreement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html">2311.12022_GPQA: A Graduate-Level Google-Proof Q&amp;A Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#data-collection">2.Data Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#dataset-analysis">3.Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#baseline">4.Baseline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#related-work">5.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2311.12022_GPQA.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html">2411.04368_SimpleQA: Measuring short-form factuality in large language models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#data-collection-and-verification">2.Data Collection and Verification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#measuring-calibration">4.Measuring calibration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_QAs/2411.04368_SimpleQA.html#appendix-b-guessing-strategy-and-f-score">Appendix B Guessing strategy and F-score</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id4">数据集-编程</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html">2107.03374_HumanEval: Evaluating Large Language Models Trained on Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#evaluation-framework">2.Evaluation Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#code-fine-tuning">3.Code Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#supervised-fine-tuning">4.Supervised Fine-Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#docstring-generation">5.Docstring Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#broader-impacts-and-hazard-analysis">7.Broader Impacts and Hazard Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2107.03374_HumanEval.html#conclusions">9.Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html">2108.07732_MBPP: Program Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#datasets">2 Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#model-and-methods">3 Model and Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mbpp-synthesis-results">4 MBPP Synthesis Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#human-model-collaboration-results">5 Human-Model Collaboration Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#program-execution-results">6 Program Execution Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#mathqa-results">7 MathQA Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#related-work">8 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#risks-and-limitations">9 Risks and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#conclusion">10 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2108.07732_MBPP.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html">2310.06770_SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#id7">2 SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#swe-llama-fine-tuning-codellama-for-swe-bench">3 SWE-Llama: Fine-tuning CodeLlama for SWE-bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#experimental-setup">4 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#discussion">7 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#ethics-statement">8 Ethics Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#reproducibility-statement">9 Reproducibility Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-a-benchmark-details">Appendix A Benchmark Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-b-additional-details-on-training-swe-llama">Appendix B Additional Details on Training SWE-Llama</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-c-additional-results">Appendix C Additional Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-d-additional-experimental-details">Appendix D Additional Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-e-societal-impact">Appendix E Societal Impact</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2310.06770_SWE-bench.html#appendix-f-in-depth-analysis-of-swe-llama-generations">Appendix F In-depth Analysis of SWE-Llama Generations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html">2402.16694_HumanEval-XL: A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#a-multilingual-code-generation-benchmark-for-cross-lingual-natural-language-generalization">A Multilingual Code Generation Benchmark for Cross-lingual Natural Language Generalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#introduction">1.   Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#related-work">2.   Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#humaneval-xl">3.   HumanEval-XL</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#experiments">4.   Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#conclusion">5.   Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-a-experiment-settings">Appendix A Experiment Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2402.16694_HumanEval-XL.html#appendix-b-comprehensive-experiment-results">Appendix B Comprehensive Experiment Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html">2403.07974_LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#holistic-evaluation">2 Holistic Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#benchmark-curation">3 Benchmark Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#experiment-setup">4 Experiment Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#results">5 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-b-ui">Appendix B UI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-d-results">Appendix D Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2403.07974_LiveCodeBench.html#appendix-e-qualitative-examples">Appendix E Qualitative Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html">2407.10499_CIBench: Evaluating Your LLMs with a Code Interpreter Plugin</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#cibench">3 CIBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-b-construction-prompts-and-rules">Appendix B Construction Prompts and Rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-c-experiment-example-demo">Appendix C Experiment Example Demo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-d-subjective-visualization-evaluation">Appendix D Subjective Visualization Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-e-dataset-error-analysis">Appendix E Dataset Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-f-human-annotator">Appendix F Human Annotator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2407.10499_CIBench.html#appendix-g-ethical-consideration">Appendix G Ethical Consideration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html">2410.03859_SWE-bench-Multimodal: Do AI Systems Generalize to Visual Software Domains?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#swe-bench-multimodal">2 SWE-bench Multimodal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#evaluating-on-swe-bench-m">3 Evaluating on SWE-bench M</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-a-dataset">Appendix A Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-b-collection">Appendix B Collection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-c-experiments">Appendix C Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-d-human-validation">Appendix D Human Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.03859_SWE-bench-Multimodal.html#appendix-e-limitations">Appendix E Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html">2410.06992_SWE-Bench+: Enhanced Coding Benchmark for LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-analysis-of-swe-bench">2 Robustness Analysis of SWE-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#building-swe-bench">3 Building SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#robustness-of-swe-bench">4 Robustness of SWE-Bench+</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#effectiveness-aware-evaluation">5 Effectiveness-aware Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2410.06992_SWE-Bench%2B.html#conclusion">7 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html">2501.01257_CodeForces: Benchmarking Competition-level Code Generation of LLMs on CodeForces</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#codeforces-benchmark">3 CodeForces Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#evaluation-on-existing-llms">4 Evaluation on Existing LLMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#analysis-experiments">5 Analysis Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#discussion">6 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#ethical-statement">8 Ethical Statement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-a-model-cards">Appendix A Model Cards</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-b-decoding-hyperparameters">Appendix B Decoding Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-c-analysis-of-our-elo-rating-calculation-system">Appendix C Analysis of Our Elo Rating Calculation System</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-d-human-comparable-elo-rating">Appendix D Human-comparable Elo Rating</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-e-problem-demonstration">Appendix E Problem Demonstration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Codes/2501.01257_CodeForces.html#appendix-f-special-judge">Appendix F Special Judge</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id5">数据集-长文本</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html">2402.05136_LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#lv-eval-benchmark">3 LV-Eval Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-c-detailed-evaluation-results">Appendix C Detailed Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.05136_LV-Eval.html#appendix-d-detailed-ablation-results">Appendix D Detailed Ablation Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html">2402.17753_LoCoMo: Evaluating Very Long-Term Conversational Memory of LLM Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#generative-pipeline-for-locomo">3 Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#locomo-evaluation-benchmark">4 LoCoMo Evaluation Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-setup">5 Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#experimental-results">6 Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#broader-impacts">9 Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-overview">Appendix Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-a-generative-pipeline-for-locomo">Appendix A Generative Pipeline for LoCoMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-b-dataset">Appendix B Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-c-experimental-setup">Appendix C Experimental Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2402.17753_LoCoMo.html#appendix-d-results">Appendix D Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html">2404.06654_RULER: What’s the Real Context Size of Your Long-Context Language Models?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#the-ruler-benchmark">3 The Ruler Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#experiments-results">4 Experiments &amp; Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#task-error-analysis">5 Task Error Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#model-analysis">6 Model Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#limitations">8 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-a-models">Appendix A Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-b-task-configurations">Appendix B Task Configurations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-c-task-correlation-analysis">Appendix C Task Correlation Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-d-prompt-templates">Appendix D Prompt Templates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-e-passkey-retrieval-and-vanilla-niah-results">Appendix E Passkey Retrieval and Vanilla NIAH Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2404.06654_RULER.html#appendix-f-additional-results">Appendix F Additional Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html">2407.11963_NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#tasks-and-datasets">3 Tasks and Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#impact-of-language-which-model-performs-better-under-the-bilingual-scenario">4.1.5 Impact of Language_ Which Model Performs Better under the Bilingual Scenario_</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#conclusion-and-future-work">5 Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-a-evaluated-models">Appendix A Evaluated Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-b-needlebench-prompt-examples">Appendix B NeedleBench Prompt Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_LongCtxs/2407.11963_NeedleBench.html#appendix-c-error-analysis-examples">Appendix C Error Analysis Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id6">数据集-数学</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Maths/2103.03874_MATH.html">2103.03874_MATH: Measuring Mathematical Problem Solving With the MATH Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html">2110.14168_GSM8K: Training Verifiers to Solve Math Word Problems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#dataset">2 Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#related-work">3 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#methods">4 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#additional-experiments">5 Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-a-dataset-details">Appendix A Dataset Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-b-hyperparameters">Appendix B Hyperparameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-c-calculator-annotations">Appendix C Calculator Annotations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-d-example-model-solutions">Appendix D Example Model Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-e-verifier-details">Appendix E Verifier Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2110.14168_GSM8K.html#appendix-f-verifier-visualization">Appendix F Verifier Visualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html">2405.12209_MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#experiments-and-analysis">3 Experiments and Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#discussion">4 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#related-work">5 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#limitations">7 Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#ethical-considerations">8 Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-a-mathbench-statistics">Appendix A MathBench Statistics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-b-detailed-experimental-results">Appendix B Detailed Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Maths/2405.12209_MathBench.html#appendix-c-extra-analysis">Appendix C Extra Analysis</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id7">数据集-图片</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html">2306.13394_MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#mme-evaluation-suite">2 MME Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#analysis">4 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2306.13394_MME.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html">2307.06281_MMBench: Is Your Multi-modal Model an All-around Player?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#the-construction-of-mmbench">3 The construction of MMBench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-strategy">4 Evaluation Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#evaluation-results">5 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-a-more-details-about-the-data">Appendix A More Details about the Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-b-more-details-on-mmbench-construction">Appendix B More Details on MMBench Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-c-more-details-on-llm-based-choice-extraction">Appendix C More Details on LLM-based Choice Extraction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.06281_MMBench.html#appendix-d-evaluation-settings-and-results">Appendix D Evaluation Settings and Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html">2307.16125_SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#id7">3 SEED-Bench</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#evaluation-results">4 Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2307.16125_SEED-Bench.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html">2311.12793_ShareGPT4V: Improving Large Multi-Modal Models with Better Captions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-dataset">3 ShareGPT4V Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sharegpt4v-7b-model">4 ShareGPT4V-7B Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id11"><strong>4.1 模型架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id12"><strong>4.2 预训练</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#sft"><strong>4.3 监督微调（SFT）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#experiments">5 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-a-data-sources">Appendix A Data Sources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-b-caption-analysis">Appendix B Caption Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-c-prompts">Appendix C Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2311.12793_ShareGPT4V.html#appendix-d-examples">Appendix D Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html">2506.18095_ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#sharegpt-4o-image">2 ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#janus-4o-fine-tuning-with-sharegpt-4o-image">3 Janus-4o: Fine-Tuning with ShareGPT-4o-Image</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#conclusion">5 conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-b-image-generation-categories">Appendix B Image Generation Categories</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-c-prompts-for-generation">Appendix C Prompts for Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-d-document-pipeline">Appendix D Document Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/DS_Images/2506.18095_ShareGPT-4o-Image.html#appendix-e-ethical-considerations-and-societal-impact">Appendix E Ethical Considerations and Societal Impact</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Benchmarking.html#id8">数据集</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#id2">评测标准</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#accuracy">准确率(Accuracy)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#precision">精确率(Precision, 精准率)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#recall">召回率(Recall)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#f1-score">F1 Score</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/0normal.html#id3">可视化精度和召回率</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html">2009.03300_MMLU: Measuring Massive Multitask Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#a-multitask-test">3.A Multitask Test</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2009.03300_MMLU.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html">2305.08322_C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#id2">C-Eval_ A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#the-c-eval-evaluation-suite">2 The C-Eval Evaluation Suite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#experiment">3 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-a-author-contributions">Appendix A Author Contributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-b-detailed-stats-of-c-eval">Appendix B Detailed Stats of C-Eval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-c-explanation-data-generation">Appendix C Explanation Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-d-evaluation-prompts">Appendix D Evaluation Prompts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-e-details-of-the-models-being-evaluated">Appendix E Details of the models being evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-f-breakdown-of-model-performance">Appendix F Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-g-option-bias">Appendix G Option Bias</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2305.08322_C-Eval.html#appendix-h-compute-and-resources-used-for-evaluation">Appendix H Compute and Resources Used for Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html">2306.09212_CMMLU: Measuring massive multitask language understanding in Chinese</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#cmmlu">3 CMMLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#impact-of-model-size-on-performance">Impact of model size on performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-a-comparison-to-concurrent-benchmarks">Appendix A Comparison to concurrent benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-b-cmmlu-subjects">Appendix B CMMLU Subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-c-cmmlu-examples">Appendix C CMMLU Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-d-cmmlu-difficulty-distribution">Appendix D CMMLU Difficulty Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-e-emergent-ability-shown-in-cmmlu-subjects">Appendix E Emergent Ability shown in CMMLU subjects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-f-models-being-evaluated">Appendix F Models being Evaluated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-g-strategies-for-estimating-model-choices">Appendix G Strategies for Estimating Model Choices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-h-regular-expressions-matching-algorithmsl">Appendix H Regular expressions matching algorithmsl</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-i-correlation-to-other-benchmarks">Appendix I Correlation to other Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#appendix-j-breakdown-of-model-performance">Appendix J Breakdown of Model Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2306.09212_CMMLU.html#j-3-the-effect-of-chain-of-thought-prompt">J.3 The effect of chain-of-thought prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html">2307.15020_SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#superclue-benchmark">3 SuperCLUE Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#additional-analysis">5 Additional Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-a-evaluation-process">Appendix A Evaluation Process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2307.15020_SuperCLUE.html#appendix-b-capability-categories">Appendix B Capability Categories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html">2311.12983_GAIA: a benchmark for General AI Assistants</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#related-work">2.Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#id4">3.GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#llms-results-on-gaia">4.LLMs results on GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#discussion">5.Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-c-extended-description-of-gaia">Appendix C Extended description of GAIA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2311.12983_GAIA.html#appendix-d-extended-description-of-our-question-design-framework">Appendix D Extended description of our question design framework</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html">2404.07972_OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-environment">2. OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#osworld-benchmark">3. OSWORLD Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#benchmarking-llm-and-vlm-agent-baselines">4. Benchmarking LLM and VLM Agent Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#analysis">5. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#a-details-of-osworld-environment">A. Details of OSWORLD Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#c-details-of-baseline-methods">C. Details of Baseline Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2404.07972_OSWorld.html#d-examples-of-qualitative-analysis">D. Examples of Qualitative Analysis</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html">2501.14249_HLE: Humanity’s Last Exam</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#dataset">3.Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#evaluation">4.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Benchmarkings/Datasets/2501.14249_HLE.html#discussion">5.Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../LLM.html">LLM 模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#nlp">NLP 模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/1810.04805_BERT.html">1810.04805_BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/1810.04805_BERT.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/1810.04805_BERT.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/1810.04805_BERT.html#bert">3 BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/1810.04805_BERT.html#appendix-a-additional-details-for-bert">Appendix A Additional Details for BERT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html">18xx_GPT1: Improving Language Understanding by Generative Pre-Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#framework">3. Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#analysis">5 Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#id3">引文口碑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/18_GPT1.html#id4">要点解读</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/19_GPT2.html">19xx_GPT2: Language Models are Unsupervised Multitask Learners</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/19_GPT2.html#the-illustrated-gpt-2">The Illustrated GPT-2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/19_GPT2.html#id2">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2012.00413_CPM.html">2012.00413_CPM: A Large-scale Generative Chinese Pre-trained Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2302.13971_LLaMA.html">2302.13971_LLaMA: Open and Efficient Foundation Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2307.09288_Llama2.html">2307.09288_Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html">2309.16609_Qwen Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#pretraining">2. Pretraining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#alignment">3. Alignment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#code-qwen-specialized-model-for-coding">4. CODE-QWEN: SPECIALIZED MODEL FOR CODING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#math-qwen-specialized-model-for-mathematics-reasoning">5. MATH-QWEN: SPECIALIZED MODEL FOR MATHEMATICS REASONING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-1-more-training-details">A.1 MORE TRAINING DETAILS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2309.16609_Qwen.html#a-2-evaluation">A.2 EVALUATION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html">2310.19341_Skywork: A More Open Bilingual Foundation Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#methodology">2 Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#limitation">6 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-a-details-on-gpt-7b-vs-llama-7b-experiment">Appendix A Details on GPT-7B vs. LLaMA-7B Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-b-preliminary-experiments-on-distributed-training">Appendix B Preliminary Experiments on Distributed Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-c-more-benchmark-results">Appendix C More Benchmark Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2310.19341_Skywork.html#appendix-d-details-on-lm-test-sets">Appendix D Details on LM Test Sets</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2401.14196_DeepSeek-Coder.html">2401.14196_DeepSeek-Coder: When the Large Language Model Meets Programming – The Rise of Code Intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2404.06395_MiniCPM.html">2404.06395_MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#two-stage-pre-training-strategy">5. Two Stage Pre-training Strategy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#model">6. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2404.06395_MiniCPM.html#minicpm-family">7 MiniCPM Family</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2405.04434_DeepSeek-V2.html">2405.04434_DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2406.12793_ChatGLM.html">2406.12793_ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html">2407.10671_Qwen2 Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#tokenizer-model">2. Tokenizer &amp; Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2407.10671_Qwen2.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html">2412.15115_Qwen2.5</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#architecture-and-tokenizer">2. Architecture and Tokenizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2412.15115_Qwen2.5.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html">2505.09388_Qwen3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#pre-training">3. Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#post-training">4. Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLM_NLPs/2505.09388_Qwen3.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#id2">多模态模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html">2112.15093_CTR: Benchmarking Chinese Text Recognition: Datasets, Baselines, and an Empirical Study</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#datasets">3. Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#baselines">4. Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#an-empirical-study">5. An Empirical Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-a-details-of-prab">Appendix A Details of PRAB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2112.15093_CTR.html#appendix-c-visualization-of-failure-cases">Appendix C Visualization of Failure Cases.</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html">2304.08485_LLaVA: Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#gpt-assisted-visual-instruction-data-generation">3. GPT-assisted Visual Instruction Data Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#visual-instruction-tuning">4. Visual Instruction Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2304.08485_LLaVA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html">2308.12966_Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#methodology">Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#evaluation">Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2308.12966_Qwen-VL.html#b-data-format-details-of-training">B. Data Format Details of Training</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html">2310.03744_LLaVA2: Improved Baselines with Visual Instruction Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#empirical-evaluation">4. Empirical Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#open-problems-in-lmms">5. Open Problems in LMMs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#a-implementation-details">A. Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2310.03744_LLaVA2.html#b-qualitative-results">B. Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html">2312.07533_VILA: On Pre-training for Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#on-pre-training-for-visual-language-models">3. On Pre-training for Visual Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2312.07533_VILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html">2403.05525_DeepSeek-VL: Towards Real-World Vision-Language Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2403.05525_DeepSeek-VL.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html">2408.01800_MiniCPM-V: A GPT-4V Level MLLM on Your Phone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#model-architecture">3. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#end-side-deployment">5. End-side Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#experiments">6. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2408.01800_MiniCPM-V.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html">2409.17146_Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#architecture">2. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#data">3. Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#training">4. Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#ablations">6. Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-a-model-details">Appendix A: Model Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-b-training-details">Appendix B: Training Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-c-evaluation-results">Appendix C: Evaluation Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-d-result-details">Appendix D: Result Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-e-ablations-details">Appendix E Ablations Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-f-data-details">Appendix F Data Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-g-dataset-examples">Appendix G Dataset Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2409.17146_Molmo_and_PixMo.html#appendix-h-related-work">Appendix H Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html">2410.13848_Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#janus-a-simple-unified-and-flexible-multimodal-framework">3 Janus: A Simple, Unified and Flexible Multimodal Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-a-details-of-semantic-tokenizer-mentioned-in-ablation-study">Appendix A Details of Semantic Tokenizer Mentioned in Ablation Study</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2410.13848_Janus.html#appendix-b-additional-qualitative-results">Appendix B Additional Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html">2411.00774_Freeze-Omni: A Smart and Low Latency Speech-to-speech Dialogue Model with Frozen LLM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#model">2. Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#experience">3. Experience</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2411.00774_Freeze-Omni.html#conclusion-and-future-work">4. Conclusion and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html">2412.04468_NVILA: Efficient Frontier Visual Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#more-capabilities">4. More Capabilities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2412.04468_NVILA.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html">2502.13923_Qwen2.5-VL</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2502.13923_Qwen2.5-VL.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html">2503.20215_Qwen2.5-Omni Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#archtecture">2. Archtecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#id2">3 预训练</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#post-training">4 后训练（Post-training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2503.20215_Qwen2.5-Omni.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#id4">3. Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#results-and-analyses">5. Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html">2506.13642_Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#id9">3 Stream-Omni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#data-construction">3.2.1 Data Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#results-and-analyses">5 Results and Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-a-construction-of-instructomni">Appendix A Construction of InstructOmni</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-b-construction-of-spokenvisit">Appendix B Construction of SpokenVisIT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMMultimodals/2506.13642_Stream-Omni2.html#appendix-c-case-study">Appendix C Case Study</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#id3">LLM 音频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html">2005.08100_Conformer: Convolution-augmented Transformer for Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#conformer-encoder">2 Conformer Encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2005.08100_Conformer.html#conclusion">4 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html">2106.07447_HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#llm">LLM 总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#i-introduction">I Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#ii-method">II Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#iii-related-work">III Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#iv-experimental-details">IV Experimental Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#v-results">V Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2106.07447_HuBERT.html#vi-conclusion">VI Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html">2112.02418_YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#id1">关键概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#yourtts-model">2. YourTTS Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#results-and-discussion">4. Results and Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#zero-shot-voice-conversion">5. Zero-Shot Voice Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#speaker-adaptation">6. Speaker Adaptation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2112.02418_YourTTS.html#conclusions-limitations-and-future-work">7. Conclusions, limitations and future work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html">2212.04356_whisper: Robust Speech Recognition via Large-Scale Weak Supervision</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#approach">2. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#analysis-and-ablations">4. Analysis and Ablations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#limitations-and-future-work">6. Limitations and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#conclusions">7. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#a-evaluation-datasets">A. Evaluation Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#b-compared-models">B Compared Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2212.04356_whisper.html#c-text-standardization">C. Text Standardization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html">2301.02111_Vall-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#background-speech-quantization">3. Background: Speech Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#id9">4. VALL-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2301.02111_Vall-E.html#conclusion-limitations-and-future-work">6. Conclusion, Limitations, and Future Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html">2303.03926_VALL-E_X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#cross-lingual-codec-language-model">3 Cross-Lingual Codec Language Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#vall-e-x-application">4. VALL-E X Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2303.03926_VALL-E_X.html#a-appendix">A. Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html">2406.05370_VALL-E2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#id5">3. VALL-E 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2406.05370_VALL-E2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html">2407.05407_CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#cosyvoice-a-scalable-tts-model-using-supervised-semantic-tokens">2. CosyVoice: A Scalable TTS model using Supervised Semantic Tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#dataset">3. Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#experimental-settings">4. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.05407_CosyVoice.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html">2407.10759_Qwen2-Audio Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2407.10759_Qwen2-Audio.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html">2410.00037_Moshi: a speech-text foundation model for real-time dialogue</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#model">3.Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#datasets-and-training">4. Datasets and Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#safety">6.Safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2410.00037_Moshi.html#conclusion">7.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html">2412.10117_CosyVoice2: Scalable Streaming Speech Synthesis with Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#instroduction">1. Instroduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#id5">2. CosyVoice 2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-settings">3. Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#experimental-results">4. Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2412.10117_CosyVoice2.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html">2501.06282_MinMo: A Multimodal Large Language Model for Seamless Voice Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#instruction">1.Instruction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#id9">3.MinMo</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#experiments">4.Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#conclusion">5.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#limitations">6.Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2501.06282_MinMo.html#a-prompts-for-voice-understanding-tasks">A. Prompts for Voice Understanding Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html">2505.02707_Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#voila-voice-language-foundation-models">3. Voila: Voice-Language Foundation Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.02707_Voila.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html">2505.17589_CosyVoice3: Towards In-the-wild Speech Generation via Scaling-up and Post-training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#id3">2.CosyVoice 3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#the-multilingual-data-pipeline">3.The Multilingual Data Pipeline</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-settings">4.Experimental Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#experimental-results">5.Experimental Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#conclusion">6.Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMAudio/2505.17589_CosyVoice3.html#limitations">7.Limitations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#id4">LLM 视频</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html">2301.12597_BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#related-work">2 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#method">3 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#experiment">4 Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#limitation">5 Limitation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2301.12597_BLIP-2.html#conclusion">6 Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html">2308.01390_OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#id1">OpenFlamingo_ An Open-Source Framework for Training Large Autoregressive Vision-Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#related-work">2 Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#approach">3 Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#results">4 Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#discussion">5 Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-a-extended-results">Appendix A Extended results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-b-additional-notes-on-filtering-mmc4">Appendix B Additional notes on filtering MMC4</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-c-synthetic-data-prompt">Appendix C Synthetic data prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMVideos/2308.01390_OpenFlamingo.html#appendix-d-image-credits">Appendix D Image credits</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#llm-moe">LLM MoE</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMoEs/2408.15664_AUXILIARY-LOSS-FREE_LB.html">2408.15664_AUXILIARY-LOSS-FREE LOAD BALANCING STRATEGY FOR MIXTURE-OF-EXPERTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMMoEs/2410.07490_MoDEM.html">2410.07490_MoDEM: Mixture of Domain Expert Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM.html#id5">商业模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2303.08774_GPT4.html">2303.08774_GPT-4 Technical Report</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html">2312.11805_Gemini: A Family of Highly Capable Multimodal Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#model-architecture">2. Model Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#training-infrastructure">3. Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#post-training-models">6. Post-Training Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#responsible-deployment">7. Responsible Deployment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2312.11805_Gemini.html#discussion-and-conclusion">8. Discussion and Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2403.05530_Gemini1.5.html">2403.05530_Gemini1.5: Unlocking multimodal understanding across millions of tokens of context</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html">2406.02430_Seed-TTS: A Family of High-Quality Versatile Speech Generation Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#method">2 Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-extensions">4 Model extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#model-applications-limitations-and-safety">5 Model applications, limitations, and safety</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#authors-alphabetical-order">6 Authors (alphabetical order)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2406.02430_Seed-TTS.html#acknowledgement">7 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html">2407.04675_Seed-ASR: Understanding Diverse Speech and Contexts with LLM-based Speech Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#motivation">2 Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#methods">3 Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#model-and-evaluation">4 Model and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2407.04675_Seed-ASR.html#appendix-a-appendix">Appendix A Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2503.20020_Gemini2.html">2503.20020_Gemini2: Gemini Robotics: Bringing AI into the Physical World</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2504.xxxxx_Seed-Thinking-v1.5.html">2504.xxxxx_Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html">2505.07062_Seed1.5-VL Technical Report</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#id1">Seed1.5-VL Technical Report</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#architecture">2 Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#pre-training">3 Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-recipe">3.2 Training Recipe</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#post-training">4 Post-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#hybrid-reinforcement-learning">4.4 Hybrid Reinforcement Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#training-infrastructure">5 Training Infrastructure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation">6 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#video-task-evaluation">6.1.3 Video Task Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#comparison-with-state-of-the-arts">6.3.2 Comparison with State-of-the-arts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#conclusion-and-next-steps">7 Conclusion and Next Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#contributions-and-acknowledgments">8 Contributions and Acknowledgments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#qualitative-examples">9 Qualitative examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#visual-reasoning-visual-pattern-recognition">9.7 Visual Reasoning_ Visual Pattern Recognition</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#failure-cases-combinatorial-search-i">9.19 Failure Cases_ Combinatorial Search I</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#evaluation-details">10 Evaluation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLMs/LLMCommercials/2505.07062_Seed1.5-VL.html#dream-1k">DREAM-1K</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../LLM_tech.html">LLM 周边技术</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#framework">Framework</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html">1712.05889_Ray: A Distributed Framework for Emerging AI Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#motivation-and-requirements">2. Motivation and Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#programming-and-computation-model">3. Programming and Computation Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#architecture">4. Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#evaluation">5. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#related-work">6 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#discussion-and-experiences">7 Discussion and Experiences</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1712.05889_Ray.html#conclusion">8. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html">1910.02054_DeepSpeed_ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#extended-introduction">1. Extended Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#where-did-all-the-memory-go">3 Where Did All the Memory Go?</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#zero-insights-and-overview">4 ZeRO: Insights and Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-dp">5 Deep Dive into ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#deep-dive-into-zero-r">6 Deep Dive into ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-dp">7 Communication Analysis of ZeRO-DP</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#communication-analysis-of-zero-r">8. Communication Analysis of ZeRO-R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#step-towards-1-trillion-parameters">9. Step Towards 1 Trillion Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#implementation-and-evaluation">10. Implementation and Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/1910.02054_DeepSpeed_ZeRO.html#concluding-remarks">11. Concluding Remarks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/19XX_PyTorch.html">PyTorch: An Imperative Style, High-Performance Deep Learning Library</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/20XX_Transformers.html">Transformers: State-of-the-Art Natural Language Processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html">2210.XX_Ray v2 Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#overview">Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#architecture-overview">Architecture Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#object-management">Object Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#task-management">Task Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#resource-management-and-scheduling">Resource Management and Scheduling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#actor-management">Actor management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#global-control-service">Global Control Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#cluster-management">Cluster Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2210.XX_Ray_v2.html#appendix">Appendix</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html">2309.06180_vLLM: Efficient Memory Management for Large Language Model Serving with PagedAttention</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#id2">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#memory-challenges-in-llm-serving">3. Memory Challenges in LLM Serving</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#method">4. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#implementation">5. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#evaluation">6. Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#ablation-studies">7. Ablation Studies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Frameworks/2309.06180_vLLM.html#conclusion">10. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id2">大模型调优</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2101.00190_Prefix-Tuning.html">2101.00190_Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2103.10385_p-tuning.html">2103.10385_p-tuning: GPT Understands, Too</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2104.08691_Prompt_Tuning.html">2104.08691_Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2106.09685_LoRA.html">2106.09685_LoRA: Low-Rank Adaptation of Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2401.01335_Self-Play.html">2401.01335_Self-Play: Fine-Tuning Converts Weak Language Models to Strong Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2402.09353_DoRA.html">2402.09353_DoRA: Weight-Decomposed Low-Rank Adaptation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2402.12354_LoRA%2B.html">2402.12354_LoRA+: Efficient Low Rank Adaptation of Large Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2403.03507_GaLore.html">2403.03507_GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/FineTunes/2403.13372_LlamaFactory.html">2403.13372_LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#id2">竞争框架</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#efficient-fine-tuning-techniques">3. Efficient Fine-Tuning Techniques</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#llamafactory-framework">4 LlamaFactory Framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/FineTunes/2403.13372_LlamaFactory.html#conclusion-and-future-work">6 Conclusion and Future Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id3">分布式模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/1701.06538_MoE.html">1701.06538_MoE: Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html">1806.03377_PipeDream: Fast and Efficient Pipeline Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#background-related-work">2. Background &amp; Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#parallel-training-in-pipedream">3. Parallel Training in PipeDream</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#implementation">4. Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1806.03377_PipeDream.html#evaluation">5. Evaluation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html">1811.06965_GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#the-gpipe-library">2. The GPipe Library</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#performance-analyses">3. Performance Analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#image-classification">4. Image Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#massive-massively-multilingual-machine-translation">5. Massive Massively Multilingual Machine Translation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1811.06965_GPipe.html#design-features-and-trade-offs">6. Design Features and Trade-Offs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html">1909.08053_Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#background-and-challenges">2. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1909.08053_Megatron-LM.html#model-parallel-transformers">3. Model Parallel Transformers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html">19xx_PipeDream: Generalized Pipeline Parallelism for DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#id2">收集</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#background-and-related-work">2. BACKGROUND AND RELATED WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#pipeline-parallelism">3. 流水线并行(PIPELINE PARALLELISM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#id5">4. 实现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/1910_PipeDream2.html#id6">6. 结论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2006.09503_PipeDream-2BW.html">2006.09503_PipeDream-2BW: Memory-Efficient Pipeline-Parallel DNN Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2006.09503_PipeDream-2BW.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2006.15704DataParallel.html">2006.15704_PyTorch Distributed: Experiences on Accelerating Data Parallel Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2006.16668_GShard.html">2006.16668_GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2104.04473_Megatron-LM2.html">2104.04473_Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2104.04473_Megatron-LM2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html">2205.14135_FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#background">2 Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#flashattention-algorithm-analysis-and-extensions">3. FLASHATTENTION: Algorithm, Analysis, and Extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#limitations-and-future-directions">5. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-a-related-work">Appendix A Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-b-algorithm-details">Appendix B Algorithm Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-c-proofs">Appendix C Proofs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2205.14135_FlashAttention.html#appendix-d-extension-details">Appendix D Extension Details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html">2307.08691_FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#flashattention-2-algorithm-parallelism-and-work-partitioning">3. FlashAttention-2: Algorithm, Parallelism, and Work Partitioning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#empirical-validation">4. Empirical Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Parallelism/2307.08691_FlashAttention2.html#discussion-and-future-directions">5. Discussion and Future Directions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Parallelism/normal.html">通用</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id4">LLM 量化</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/0normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/0normal.html#id2">混合精度</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/0normal.html#id3">浮点数格式</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/0normal.html#weight-only-quantization">weight-only quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html">2110.02861_bitsandbytes: 8-bit Optimizers via Block-wise Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#background">1. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#bit-optimizers">2. 8-bit Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#bit-vs-32-bit-optimizer-performance-for-common-benchmarks">3. 8-bit vs 32-bit Optimizer Performance for common Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#analysis">4. Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2110.02861_bitsandbytes.html#related-work">5. Related Work</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html">2206.01861_ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#relative-work">2. Relative Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#background-and-challenges">3. Background and Challenges</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#methodology">4. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#results">5. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#appendix-a-background">Appendix A Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.01861_ZeroQuant.html#appendix-d-details-about-system-optimization">Appendix D Details about System Optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html">2206.09557_LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#instructions">1. Instructions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#design-methodology-of-lut-gemm">3. Design Methodology of LUT-GEMM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#accelerating-quantized-opt-175b">5. Accelerating Quantized OPT-175B</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#appendix-a-llm-inference-latency-breakdown">Appendix A LLM Inference Latency Breakdown</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2206.09557_LUT-GEMM.html#appendix-b-detailed-implementation">Appendix B Detailed Implementation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html">2208.07339_LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#id1">相关参考</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#int8-matrix-multiplication-at-scale">3. Int8 Matrix Multiplication at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#emergent-large-magnitude-features-in-transformers-at-scale">4. Emergent Large Magnitude Features in Transformers at Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#discussion-and-limitations">6. Discussion and Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#broader-impacts">7. Broader Impacts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2208.07339_LLM.int8.html#id17">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html">2209.05433_FP8: FP8 Formats For Deep Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#aspects-of-fp8-usage-in-deep-learning">2. Aspects of FP8 Usage in Deep Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#fp8-binary-interchange-format">3. FP8 Binary Interchange Format</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#id3">示例讲解</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#empirical-results">4. Empirical Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2209.05433_FP8.html#conclusions">5. Conclusions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html">2210.17323_GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#background">3. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#the-gptq-algorithm">4. The GPTQ Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#experimental-validation">5. Experimental Validation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2210.17323_GPTQ.html#summary-and-limitations">6. Summary and Limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html">2211.10438_SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#review-of-quantization-difficulty">3. Review of Quantization Difficulty</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#id9">4. SmoothQuant</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2211.10438_SmoothQuant.html#appendix-a-discussion-on-weight-only-quantization">Appendix A. Discussion on Weight-Only Quantization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html">2305.14314_QLoRA: Efficient Finetuning of Quantized LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#qlora-finetuning">3. QLoRA Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#qlora-vs-standard-finetuning">4. QLoRA vs. Standard Finetuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#pushing-the-chatbot-state-of-the-art-with-qlora">5. Pushing the Chatbot State-of-the-art with QLoRA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#qualitative-analysis">6. Qualitative Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#related-work">7. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2305.14314_QLoRA.html#limitations-and-discussion">8. Limitations and Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html">2306.00978_AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#awq-activation-aware-weight-quantization">3. AWQ: Activation-aware Weight Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#tinychat-mapping-awq-onto-edge-platforms">4. TinyChat: Mapping AWQ onto Edge Platforms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2306.00978_AWQ.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html">2309.05516_AutoRound: Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#methodology">3. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/Quantizations/2309.05516_AutoRound.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id5">LLM 安全</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/Securitys/2312.06674_Llama_Guard.html">2312.06674_Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id6">LLM强化学习</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/RLs/1703.03864_EvolutionStrategies.html">1703.03864_Evolution Strategies: as a Scalable Alternative to Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html">2504.02495_DeepSeek-GRM: Inference-Time Scaling for Generalist Reward Modeling</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#preliminaries">2. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#self-principled-critique-tuning-spct">3. Self-Principled Critique Tuning (SPCT)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#inference-time-scaling-with-spct">4. Inference-Time Scaling with SPCT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#results-on-reward-modeling-benchmarks">5. Results on Reward Modeling Benchmarks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#related-work">6. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#conclusion-and-future-work">7. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#a-additional-related-work">A. Additional Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#b-limitations-and-future-directions">B. Limitations and Future Directions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/RLs/2504.02495_DeepSeek_GRM.html#g-prompt-templates">G. Prompt Templates</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/RLs/2504.13958_ToolRL.html">2504.13958_ToolRL: Reward is All Tool Learning Needs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../LLM_tech.html#id7">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html">2203.02155_Training language models to follow instructions with human feedback(InstructGPT)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#methods-and-experimental-details">3. Methods and experimental details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#discussion">5. Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#appendix-a-additional-prompt-data-details">Appendix A Additional prompt data details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#appendix-b-additional-human-data-collection-details">Appendix B Additional human data collection details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#appendix-c-additional-model-details">Appendix C Additional model details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2203.02155_InstructGPT.html#appendix-d-automatic-evaluation-details">Appendix D Automatic evaluation details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html">2305.20050_Let’s Verify Step by Step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id2">1. 研究背景</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id3">2. 监督方法对比</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id4">3. 核心发现</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2305.20050_LetsVerifyStepbyStep.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html">2408.03314_Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#how-to-scale-test-time-computation-optimally">3. How to Scale Test-Time Computation Optimally</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#scaling-test-time-compute-via-verifiers">5. Scaling Test-Time Compute via Verifiers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#refining-the-proposal-distribution">6. Refining the Proposal Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2408.03314_Scaling_LLM_Test-Time_Compute_Optimally.html#id7">其他</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html">2412.14135_Scaling of Search and Learning: A Roadmap to Reproduce o1 from Reinforcement Learning Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#fromgpt">FromGPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#background">2. Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id2">3. Policy Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id3">4. Reward Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id5">5. Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#id8">6. Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#open-source-o1-project">7 Open-source o1 Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="../LLM_techs/others/2412.14135_Scaling_of_Search_and_Learning.html#future-directions">8. Future Directions</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../ML.html">机器学习</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../ML.html#ml-vision">ML Vision</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/1506.02640_YOLO.html">1506.02640_You Only Look Once: Unified, Real-Time Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/1506.02640_YOLO.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/1612.08242_YOLO9000.html">1612.08242_YOLO9000: Better, Faster, Stronger</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/1612.08242_YOLO9000.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/1804.02767_YOLOv3.html">1804.02767_YOLOv3</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2004.10934_YOLOv4.html">2004.10934_YOLOv4: Optimal Speed and Accuracy of Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2004.10934_YOLOv4.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html">2205.00159_SVTR: Scene Text Recognition with a Single Visual Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html#method">2. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2205.00159_SVTR.html#conclusion">4. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2207.02696_YOLOv7.html">2207.02696_YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2207.02696_YOLOv7.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2303.05499_GroundingDINO.html">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2304.08485_VisualInstructionTuning.html">2304.08485_Visual Instruction Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2402.13616_YOLOv9.html">2402.13616_YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2402.13616_YOLOv9.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2405.14458_YOLOv10.html">2405.14458_YOLOv10: Real-Time End-to-End Object Detection</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2405.14458_YOLOv10.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html">2411.15858_SVTRv2: CTC Beats Encoder-Decoder Models in Scene Text Recognition</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#id1">定义</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#methods">3. Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#experiments">4 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/MLVisions/2411.15858_SVTRv2.html#more-detail-of-real-world-datasets">8. More detail of real-world datasets</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../ML.html#ml">ML</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2112.09332_WebGPT.html">2112.09332_WebGPT: Browser-assisted question-answering with human feedback</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2203.11147_GopherCite.html">2203.11147_GopherCite: Teaching language models to support answers with verified quotes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2304.09848_Generative_Search.html">2304.09848_Generative_Search: Evaluating Verifiability in Generative Search Engines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2305.14251_FActScore.html">2305.14251_FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2305.14627_ALCE.html">2305.14627_ALCE: Enabling Large Language Models to Generate Text with Citations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../MLs/ML_normals/2305.14627_ALCE.html#nli">NLI 在引用质量评估中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../MLs/ML_normals/2305.14627_ALCE.html#prompt">论文中用的prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2307.02185_Citation.html">2307.02185_Citation: A Key to Building Responsible and Accountable Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../MLs/ML_normals/2307.16883_HAGRID.html">2307.16883_HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../Agent.html">AI Agent</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Agent.html#agent">通用 Agent</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2210.03629_ReAct.html">2210.03629_ReAct</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html">2303.08268_Chat-with-the-Environment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2303.08268_Chat-with-the-Environment.html#id2">正文</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2303.11366_Reflexion.html">2303.11366_Reflexion: Language Agents with Verbal Reinforcement Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html">2303.16434_TaskMatrix.AI</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id2">大脑</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#id3">接口平台</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2303.16434_TaskMatrix.AI.html#api">API 选择器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2304.03442_Generative-Agents.html">2304.03442_Generative-Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2304.03442_Generative-Agents.html#generative-agent-architecture">Generative Agent Architecture</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2307.07924_ChatDev.html">2307.07924_ChatDev: Communicative Agents for Software Development</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2308.00352_MetaGPT.html">2308.00352_MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2308.04026_AgentSims.html">2308.04026_AgentSims: An Open-Source Sandbox for Large Language Model Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2308.08155_AutoGen.html">2308.08155_AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2308.10848_AgentVerse.html">2308.10848_AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2308.10848_AgentVerse.html#id2">理念</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2310.06117_Step-Back.html">2310.06117_Step-Back: Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2402.18679_MetaGPT_DI.html">2402.18679_MetaGPT_DI: Data Interpreter: An LLM Agent For Data Science</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2402.18679_MetaGPT_DI.html#introduction">INTRODUCTION</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2407.07061_IoA.html">2407.07061_IoA: Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2407.07061_IoA.html#overview-of-ioa">2.1 OVERVIEW OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2407.07061_IoA.html#architecture-of-ioa">2.2 ARCHITECTURE OF IOA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2407.07061_IoA.html#key-mechanisms">2.3 KEY MECHANISMS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2407.07061_IoA.html#putting-it-all-together">2.5 Putting It All Together</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2408.08435_ADAS.html">2408.08435_ADAS: Automated Design of Agentic Systems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2408.08435_ADAS.html#prompt">Prompt</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2410.10762_AFlow.html">2408.08435_ADAS: Automating Agentic Workflow Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.10762_AFlow.html#introduce">Introduce</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.10762_AFlow.html#preliminary">PRELIMINARY</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2410.17238_SELA.html">2410.17238_SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.17238_SELA.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.17238_SELA.html#related-works">2 Related Works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.17238_SELA.html#method">3 Method</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2410.21012_FACT.html">2410.21012_FACT: Examining the Effectiveness of Iterative Context Rewriting for Multi-fact Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2410.21012_FACT.html#introduce">Introduce</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2504.01990_foundation-agents.html">2504.01990_Advances and Challenges in Foundation Agents</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_normals/2506.12508_AgentOrchestra.html">2506.12508_AgentOrchestra: A Hierarchical Multi-Agent Framework for General-Purpose Task Solving</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2506.12508_AgentOrchestra.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2506.12508_AgentOrchestra.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2506.12508_AgentOrchestra.html#agentorchestra">3.AgentOrchestra</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_normals/2506.12508_AgentOrchestra.html#experiments">4.Experiments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Agent.html#agent-aios">视觉 Agent&amp;AIOS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html">2108.03353_ Screen2Words: Automatic Mobile UI Summarization with Multimodal Learning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#dataset-creation">3. Dataset Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#model-design">4. Model Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2108.03353_Screen2Words.html#id3">其它</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html">2209.08199_ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#problem-setting-tasks-and-metrics">3. Problem Setting: Tasks and Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#data-annotation">4. Data Annotation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#dataset-analysis">5. Dataset Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#experiments-and-baselines">6. Experiments and Baselines</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#conclusion">7. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#limitations">8. Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#ethical-considerations">9. Ethical Considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#a-data-annotation-details">A. Data Annotation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2209.08199_ScreenQA.html#b-data-examples">B. Data Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html">2212.06817_RT-1: ROBOTICS TRANSFORMER FOR REAL-WORLD CONTROL AT SCALE</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#preliminaries">3. Preliminaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#system-overview">4. System Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#rt-1-robotics-transformer">5. RT-1: ROBOTICS TRANSFORMER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#experiments">6. EXPERIMENTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#conclusions-limitations-and-future-work">7. CONCLUSIONS, LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#b-model-card">B. MODEL CARD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#c-model-and-data">C. MODEL AND DATA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2212.06817_RT-1.html#d-experiments">D. EXPERIMENTS</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2312.13771_AppAgent.html">2312.13771_AppAgent: Multimodal Agents as Smartphone Users</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2312.13771_AppAgent.html#environment-and-action-space">3.1 Environment and Action Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2312.13771_AppAgent.html#exploration-phase">3.2 Exploration Phase</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2312.13771_AppAgent.html#deployment-phase">3.3 Deployment Phase</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html">2401.10935_SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#related-work">2. Related work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#approach">3. Approach</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#screenspot-a-grounding-benchmark">4. ScreenSpot: A Grounding Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#experiments">5. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#conclusion">6. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#limitations">Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#ethical-considerations">Ethical considerations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#a-details-of-seeclick-pre-training">A. Details of SeeClick Pre-training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#b-screenspot-annotation-evaluation">B ScreenSpot Annotation &amp; Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2401.10935_SeeClick.html#c-downstream-agent-tasks">C. Downstream Agent Tasks</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html">2402.04615_ScreenAI: A Vision-Language Model for UI and Infographics Understanding</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#methodology">2. Methodology</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#automatic-data-generation">3. Automatic data generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#data-mixtures">4. Data Mixtures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#experiments-and-results">5. Experiments and Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#a-definitions-of-metrics">A Definitions of Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#b-screen-schema-examples">B. Screen Schema Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#c-prompts-for-llm-generated-content">C. Prompts For LLM Generated Content</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#d-screen-navigation-generated-examples">D. Screen Navigation Generated Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#f-screenqa-short-answers-generation">F. ScreenQA Short Answers Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#g-complex-question-answering-datasets">G. Complex Question Answering Datasets</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.04615_ScreenAI.html#h-new-benchmarks-repositories">H. New Benchmarks Repositories</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html">2402.07939_UFO: A UI-Focused Agent for Windows OS Interaction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#related-work">2.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#the-design-of-ufo">3.The Design of UFO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#experiment">4.Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#limitations-lessons-learned">5.Limitations &amp; Lessons Learned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2402.07939_UFO.html#conclusion">6.Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html">2403.16971_AIOS: LLM Agent Operating System</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#the-architecture-of-aios">2. The Architecture of AIOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#aios-kernel">3. AIOS Kernel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#evaluation">4 Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2403.16971_AIOS.html#appendix-e-discussion">Appendix E Discussion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2406.01014_Mobile-Agent-v2.html">2406.01014_Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2411.02059_TableGPT2.html">2411.02059_TableGPT2: A Large Multimodal Model with Tabular Data Integration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2411.02059_TableGPT2.html#abstract">Abstract</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html">2501.11733_Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#mobile-agent-e">2. Mobile-Agent-E</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#related-work">5. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#conclusion-and-future-work">6. Conclusion and Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-a-full-trajectory-comparison-example-with-previous-sota">Appendix A Full Trajectory Comparison Example with Previous SOTA</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-b-error-recovery-with-escalation-to-manager">Appendix B Error Recovery with Escalation to Manager</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-c-remaining-limitations">Appendix C Remaining Limitations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-d-all-tasks-in-mobile-eval-e-benchmark">Appendix D All Tasks in Mobile-Eval-E Benchmark</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-e-atomic-operation-space">Appendix E Atomic Operation Space</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-f-full-list-of-self-evolved-shortcuts">Appendix F Full list of Self-Evolved Shortcuts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.11733_Mobile-Agent-E.html#appendix-g-full-list-of-self-evolved-tips">Appendix G Full list of Self-Evolved Tips</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html">2501.12326_UI-TARS: Pioneering Automated GUI Interaction with Native Agents</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#evolution-path-of-gui-agents">2. Evolution Path of GUI Agents</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#core-capabilities-of-native-agent-model">3. Core Capabilities of Native Agent Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#ui-tars">4. UI-TARS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#experiment">5. Experiment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2501.12326_UI-TARS.html#conclusion">6. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html">2502.14282_PC-Agent: A Hierarchical Multi-Agent Collaboration Framework for Complex Task Automation on PC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#pc-agent">2. PC-Agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#experiments">3. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#related-work">4. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2502.14282_PC-Agent.html#conclusion">5. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html">2504.14603_UFO2: The Desktop AgentOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#introduction">1.Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#background">2.Background</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#system-design-of-ufo2">3.System Design of UFO2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#picture-in-picture-interface">4.Picture-in-Picture Interface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#implementation-and-specialized-engineering-design">5.Implementation and Specialized Engineering Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#evaluation">6.Evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#discussion-future-work">7.Discussion &amp; Future Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#related-work">8.Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Agent_Visions/2504.14603_UFO2.html#conclusion">9.Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Agent.html#id2">记忆</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html">2505.22101_MemOS: An Operating System for Memory-Augmented Generation (MAG) in LLM (Short Version)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#memos">4 MemOS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#id2"><strong>4.1 MemOS 中的记忆类型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#memcube"><strong>4.2 记忆立方体（MemCube）：核心资源</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#id3"><strong>4.3 MemOS 架构</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#id4"><strong>4.4 系统执行流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Memorys/2505.22101_MemOS.html#conclusion">5 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Agent.html#tools">Tools</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Tools/2205.00445_MRKL.html">2205.00445_MRKL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Tools/2302.04761_Toolformer.html">2302.04761_Toolformer: Language Models Can Teach Themselves to Use Tools</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Tools/2303.17580_HuggingGPT.html">2303.17580_HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html">2307.16789_ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#id1">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#introduction">1 Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#dataset-construction">2 Dataset Construction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#experiments">3 Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#related-work">4 Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#conclusion">5 Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Agents/Tools/2307.16789_ToolLLM.html#appendix-a-implementation-details">Appendix A Implementation Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Agent.html#agi">AGI</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Agents/AGIs/1905.10985_AI-GA.html">1905.10985_AI-GA: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Agents/AGIs/2408.06292_AI-Scientist.html">2408.06292_The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../RAG.html">RAG</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2005.11401_RAG_for_KI_NLP_task.html">2005.11401_Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html">2312.10997_Retrieval-Augmented Generation for Large Language Models: A Survey</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#ii-overview-of-rag">II. Overview of RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#ii-a-naive-rag">II-A Naive RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#ii-b-advanced-rag">II-B Advanced RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#ii-c-modular-rag">II-C Modular RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#ii-d-rag-vs-fine-tuning">II-D RAG vs Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-retrieval">III. Retrieval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-a-retrieval-source">III-A Retrieval Source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-b-indexing-optimization">III-B Indexing Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-c-query-optimization">III-C Query Optimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-d-embedding">III-D Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iii-e-adapter">III-E Adapter</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iv-generation">IV. Generation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iv-a-context-curation">IV-A Context Curation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#iv-b-llm-fine-tuning">IV-B LLM Fine-tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#v-augmentation-process-in-rag">V. Augmentation process in RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#v-a-iterative-retrieval">V-A Iterative Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#v-b-recursive-retrieval">V-B Recursive Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#v-c-adaptive-retrieval">V-C Adaptive Retrieval</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vi-task-and-evaluation">VI. Task and Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vi-a-downstream-task">VI-A Downstream Task</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vi-b-evaluation-target">VI-B Evaluation Target</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vi-c-evaluation-aspects">VI-C Evaluation Aspects</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vi-d-evaluation-benchmarks-and-tools">VI-D Evaluation Benchmarks and Tools</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-discussion-and-future-prospects">VII. Discussion and Future Prospects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-a-rag-vs-long-context">VII-A RAG vs Long Context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-b-rag-robustness">VII-B RAG Robustness</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-c-hybrid-approaches">VII-C Hybrid Approaches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-d-scaling-laws-of-rag">VII-D Scaling laws of RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-e-production-ready-rag">VII-E Production-Ready RAG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2312.10997_RAG_for_LLM.html#vii-f-multi-modal-rag">VII-F Multi-modal RAG</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2401.15884_CRAG.html">2401.15884_CRAG: Corrective Retrieval Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2403.14403_Adaptive-RAG.html">2403.14403_Adaptive-RAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html">2404.16130_GraphRAG: From Local to Global: A GraphRAG Approach to Query-Focused Summarization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#background">2 Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#rag">2.1 RAG方法与系统</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#llmrag">2.2 知识图谱在LLM与RAG中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id2">2.3 自适应基准测试</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id3">2.4 RAG评估标准</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#methods">3 Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#graphrag"><strong>3.1 GraphRAG 工作流程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id4"><strong>3.2 全局理解问题生成</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id5"><strong>3.3 全局理解评估标准</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id6"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#analysis">4 Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id7">4.1 实验1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id11">4.2 实验2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id14">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#results">5 Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id15">5.1 实验一：不同方法在摘要任务中的表现比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id18">5.2 实验二：基于声明的指标评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id20">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#discussion">6 Discussion</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id21">6.1 评估方法的局限性</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id22">6.2 未来工作</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id23">更广泛的影响</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-a-entity-and-relationship-extraction-approach">Appendix A Entity and Relationship Extraction Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id24"><strong>1. 实体与关系抽取方法</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#self-reflection"><strong>2. 自我反思（Self-Reflection）技术</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id25"><strong>3. 分块大小与抽取效果的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id26"><strong>4. 实验结果（图3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-b-example-community-detection">Appendix B Example Community Detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-c-context-window-selection">Appendix C Context Window Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-d-example-answer-comparison">Appendix D Example Answer Comparison</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-e-system-prompts">Appendix E System Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#e-1-element-instance-generation"><strong>E.1 实体实例生成（Element Instance Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#e-2-community-summary-generation"><strong>E.2 社区摘要生成（Community Summary Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#e-3-community-answer-generation"><strong>E.3 社区问题回答生成（Community Answer Generation）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#e-4-global-answer-generation"><strong>E.4 全局问题回答生成（Global Answer Generation）</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-f-evaluation-prompts">Appendix F Evaluation Prompts</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#f-1-relative-assessment-prompt">F.1 Relative Assessment Prompt</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#f-2-relative-assessment-metrics">F.2 Relative Assessment Metrics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#appendix-g-statistical-analysis">Appendix G Statistical Analysis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id38">统计方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id39">主要结果总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id40">总体趋势：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2404.16130_GraphRAG.html#id41">重要结论：</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2406.13213_Multi-Meta-RAG.html">2406.13213_Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html">2410.05779_LightRAG: Simple and Fast Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id1">总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#retrieval-augmented-generation">2 Retrieval-Augmented Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#the-lightrag-architecture">3 The LightRAG Architecture</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#lightrag">一、LightRAG架构概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#graph-based-text-indexing">二、基于图的文本索引（Graph-based Text Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#dual-level-retrieval-paradigm">三、双层检索范式（Dual-level Retrieval Paradigm）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#retrieval-augmented-answer-generation">四、检索增强的答案生成（Retrieval-Augmented Answer Generation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id2">五、复杂度分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id3">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#evaluation">4 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#experimental-settings"><strong>1. 实验设置（4.1 Experimental Settings）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#lightrag-rag-4-2-rq1"><strong>2. LightRAG 与现有 RAG 方法的对比（4.2 RQ1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#rq2"><strong>3. 消融实验（4.3 RQ2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id8"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#case-study-rq3">4.4 Case Study (RQ3)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#rq3">4.4 案例研究（RQ3）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#rq4">4.5 模型成本与适应性分析（RQ4）总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id9">总体结论：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#related-work">5 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#id10">第5章 相关工作（总结）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.05779_LightRAG.html#appendix">7 Appendix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html">2410.10450_KBLaM: Knowledge Base augmented Language Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#introduction">1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#related-work">2. Related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#background">3. Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#self-attention-layer">Self-attention layer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#augmenting-llm-with-the-kb">4. Augmenting LLM with the KB</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#knowledge-tokens">Knowledge tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#rectangular-attention-injecting-knowledge-token-into-prompt-tokens">Rectangular Attention: Injecting knowledge token into prompt tokens</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#kb-length-generalization-through-attention-score-scaling">KB length generalization through attention score scaling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#kb-instruction-tuning">5. KB instruction tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#experiments">6. EXPERIMENTS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#experiment-setting">6.1 EXPERIMENT SETTING</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#experiment-results">6.2 EXPERIMENT RESULTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#id2">总结亮点</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#conclusion">7. CONCLUSION</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#limitations-and-future-work">8. LIMITATIONS AND FUTURE WORK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#appendix-a-extended-related-work">Appendix A Extended related work</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#appendix-b-ablation-study">Appendix B Ablation study</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#appendix-c-sample-kb">Appendix C Sample KB</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#sample-q-a">SAMPLE Q&amp;A</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#prompt">PROMPT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#prompt-for-synthetic-kb-generation">PROMPT FOR SYNTHETIC KB GENERATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#prompt-for-open-ended-q-a-generation">Prompt for open-ended Q&amp;A generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#prompt-for-gpt-evaluation-of-open-ended-q-a">PROMPT FOR GPT EVALUATION OF OPEN-ENDED Q&amp;A</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#prompt-for-llama-evaluation">PROMPT FOR LLAMA EVALUATION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#question-template">QUESTION TEMPLATE</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#sample-output">SAMPLE OUTPUT</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#synthetic-kb">SYNTHETIC KB</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2410.10450_KBLaM.html#enron">ENRON</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html">2504.03137_LightPROF: A Lightweight Reasoning Framework for Large Language Model on Knowledge Graph</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#related-work">Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#llm-prompt-engineering">LLM Prompt Engineering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#kg-based-llm-reasoning">KG-based LLM Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#preliminaries">Preliminaries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#knowledge-graph-kg">1. Knowledge Graph (KG)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#anchor-entities">2. Anchor Entities</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#relation-link">3. Relation Link</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#reasoning-path">4. Reasoning Path</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#methodology">Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#stage1-reasoning-graph-retrieval">Stage1: Reasoning Graph Retrieval</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#stage2-knowledge-embedding">Stage2: Knowledge Embedding</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#stage3-knowledge-prompts-mixed-reasoning">Stage3: Knowledge Prompts Mixed Reasoning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#experiments">Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/2504.03137_LightPROF.html#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../RAGs/graphrag.html">GraphRAG 官方文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/graphrag.html#indexing">Indexing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/graphrag.html#indexing-architecture">&gt; Indexing Architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/graphrag.html#indexing-dataflow">&gt; Indexing Dataflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="../RAGs/graphrag.html#prompt-tuning">&gt; Prompt Tuning</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../RAGs/graphrag.html#query">Query</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../paper_pool.html">论文池</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="2305.16300.html">2305.16300_Random-Access Infinite Context Length for Transformers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#llm">LLM 总结</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id1"><strong>研究背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id2"><strong>核心问题</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id3"><strong>主要贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id4"><strong>关键技术点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id5"><strong>实验结果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id6"><strong>意义与应用前景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id7"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#related-work">2 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#methodology">3 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id8"><strong>总体思路</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id9"><strong>方法详解</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id11"><strong>位置编码处理</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id12"><strong>与其他方法的对比</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id13"><strong>总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#memory-computation">3.3 Memory &amp; Computation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#experiments">4 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id14"><strong>4.1 语言建模实验</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id18"><strong>4.2 微调预训练模型</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id22"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#future-work">5 Future Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#acknowledgment">Acknowledgment</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-a-grouped-softmax-example">Appendix A Grouped Softmax Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-b-dataset-description">Appendix B Dataset Description</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-c-number-of-unique-retrieved-blocks">Appendix C Number of Unique Retrieved Blocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-d-context-miss-token">Appendix D Context Miss Token</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-e-positional-augmentation">Appendix E Positional Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-f-additional-extensions-and-details">Appendix F Additional Extensions and Details</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#masked-language-modeling">1. <strong>掩码语言建模（Masked Language Modeling）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#flash-attention">2. <strong>与 Flash Attention 的结合</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id23">3. <strong>检索块数量与块大小的权衡</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2305.16300.html#id24">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2305.16300.html#appendix-g-offloading-kv-cache-to-cpu">Appendix G Offloading KV Cache to CPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2311.18743_AlignBench.html">2311.18743_AlignBench: Benchmarking Chinese Alignment of Large Language Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#id1">主要内容总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#id2">总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id3"><strong>1. 背景与挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#alignbench"><strong>2. AlignBench的设计目标</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id4"><strong>3. AlignBench的主要特点</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id5"><strong>4. AlignBench的应用与成果</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id6"><strong>5. 总体贡献</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id7"><strong>6. 表格对比</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#dataset">2 Dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#methods">3 Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#human-evaluation-on-alignbench">4 Human Evaluation on AlignBench</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#agreement-evaluation">一、一致性评估（Agreement Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#quality-evaluation">二、解释质量评估（Quality Evaluation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id8">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#alignbench-benchmarking-results">5 AlignBench: Benchmarking Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#related-work">6 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#conclusion">7 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="2311.18743_AlignBench.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#a-2-prompts-and-details-of-methods">A.2 Prompts and Details of Methods</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#a-2">A.2 提示模板与方法细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#a-3">A.3 各维度表现</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#a-4">A.4 案例分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id10">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id11">一、核心问题：参考材料缺失导致评估困难</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id12">二、数学积分问题对比分析</a></li>
<li class="toctree-l4"><a class="reference internal" href="2311.18743_AlignBench.html#id13">三、总结</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2401.15391_MultiHop-RAG.html">2401.15391_MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id1"><strong>背景与动机</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id2"><strong>贡献</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id3"><strong>方法概览</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id4"><strong>实验结果</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id5"><strong>总结</strong></a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id6">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id7">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#rag-with-multi-hop-queries">2 RAG with multi-Hop queries</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#rag">2.1 RAG（检索增强生成）概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#multi-hop-queries">2.2 多跳查询（Multi-Hop Queries）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id8">2.3 评估指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#a-benchmarking-dataset-multihop-rag">3 A Benchmarking Dataset: MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#multihop-rag">一、MultiHop-RAG 数据集构建流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id10">二、MultiHop-RAG 数据集统计信息</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id11">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#benchmarking-rag-system-using-multihop-rag">4 Benchmarking RAG system using MultiHop-RAG</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#retrieval-related-task">一、检索相关任务（Retrieval-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#generation-related-task">二、生成相关任务（Generation-related Task）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#other-use-cases">三、其他潜在改进方向（Other Use Cases）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#id12">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#related-work">5 Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#limitations">Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#appendix-a-appendix-a-gpt-4-prompts-used-for-data-generation">Appendix A Appendix A: GPT-4 Prompts Used for Data Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2401.15391_MultiHop-RAG.html#appendix-b-appendix-b-dataset-examples">Appendix B Appendix B: Dataset Examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2405.16506_GRAG.html">2405.16506_GRAG: Graph Retrieval-Augmented Generation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#introduction">1 Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#related-work">2 Related Work</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#prompt-tuning">2.1 Prompt Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#llms">2.2 LLMs在图相关任务中的应用</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id1">2.3 图上的检索方法</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#problem-formalization">3 Problem Formalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#methodology">4 Methodology</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id2">概述</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id3">4.1 文本子图检索</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#indexing">文本子图索引（Indexing）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#ranking">文本子图排序（Ranking）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#soft-pruning">文本子图软剪枝（Soft Pruning）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id4">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#textual-graph-augmented-generation">4.2 Textual Graph Augmented Generation</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#text-view-of-textual-graphs">1. 文本视图（Text View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#graph-view-of-textual-graphs">2. 图视图（Graph View of Textual Graphs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#generation-phase">3. 生成阶段（Generation Phase）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id5">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#experiments">5 Experiments</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id6">总结：第五章 实验部分</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#conclusion">6 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#limitations">7 Limitations</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#acknowledgments">Acknowledgments</a></li>
<li class="toctree-l3"><a class="reference internal" href="2405.16506_GRAG.html#appendix-a-appendix">Appendix A Appendix</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#a"><strong>附录A 总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2405.16506_GRAG.html#id12"><strong>总结</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">2407.01178_Memory3: Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#language-modeling-with-explicit-memory">Language Modeling with Explicit Memory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">研究背景与动机：</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">主要内容与方法：</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">实验与结果：</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#abstract">Abstract</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id5">核心思想</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory3">Memory3 模型特点</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">实验结果</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#introduction">1 _ Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#retrieval-augmented-training">1.1.1 _ Retrieval-augmented Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id8">1.1.1 | 基于检索的训练（Retrieval-augmented Training）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sparse-computation">1.1.2 | 稀疏计算（Sparse Computation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameter-as-memory">1.1.3 | 参数即记忆（Parameter as Memory）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id9">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#memory-circuitry-theory">2 _ Memory Circuitry Theory</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">核心概念总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id11">总体贡献：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#definition-2">Definition 2.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id12">1. <strong>定义与核心概念：计算图、同构与知识（电路）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id13">2. <strong>知识的实例</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id14">3. <strong>知识的外部化与记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id15">4. <strong>结论与断言</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id16">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#remark-1">Remark 1.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">1. <strong>电路构造的关键性质</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#llm">2. <strong>记忆增强 LLM 的形式化定义</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id18">3. <strong>写入代价与读取代价的权衡（记忆层次结构）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id19">4. <strong>知识使用频率与记忆分配</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id20">5. <strong>图示与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id21">小结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#design">3 _ Design</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id22"><strong>3 | Design</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id25"><strong>3.1 | 推理过程</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id27"><strong>3.2 | 写入与读取记忆</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id28"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#memory-sparsification-and-storage">3.3 _ Memory Sparsification and Storage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id29">一、显式记忆的存储挑战</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id30">二、各维度的稀疏化策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id31">三、压缩效果</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id32">四、部署方式</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id33">五、补充说明与建议</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id36">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#model-shape">3.4 _ Model Shape</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id37">3.4 | 模型结构（Model Shape）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-designs">3.5 | 训练设计（Training Designs）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id38">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#two-stage-pretrain">3.6 _ Two-stage Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id39">一、预训练的两个阶段</a></li>
<li class="toctree-l4"><a class="reference internal" href="#continual-train">二、对 continual train 的优化</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id40">三、防止信息泄露</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id41">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pretraining-data">4 _ Pretraining Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data-collection"><strong>4.1 数据收集（Data Collection）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#filtering"><strong>4.2 数据过滤（Filtering）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#tokenizer"><strong>4.3 分词器（Tokenizer）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#knowledge-base"><strong>4.4 知识库（Knowledge Base）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#id42"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pretrain">5 _ Pretrain</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id43">1. 预训练总体设计</a></li>
<li class="toctree-l4"><a class="reference internal" href="#set-up">2. 训练设置（Set-up）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#warmup-stage">3. 预热阶段（Warmup Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#continual-train-stage">4. 持续训练阶段（Continual Train Stage）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id44">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning-and-alignment">6 _ Fine-tuning and Alignment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#supervised-finetuning-sft">6.1 监督微调（Supervised Finetuning, SFT）</a></li>
<li class="toctree-l4"><a class="reference internal" href="#direct-preference-optimization-dpo">6.2 直接偏好优化（Direct Preference Optimization, DPO）</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation">7 _ Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id45">7.1 通用能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id46">7.2 对话能力评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id47">7.3 幻觉与事实性评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id48">7.4 专业任务评估</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id49">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#inference-speed">7.5 _ Inference Speed</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id50">主要内容总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id51">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#conclusion">8 _ Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="#acknowledgement">Acknowledgement</a></li>
<li class="toctree-l3"><a class="reference internal" href="#appendix-a-cost-estimation">Appendix A Cost Estimation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id52">模型参数设定</a></li>
<li class="toctree-l4"><a class="reference internal" href="#implicit-memory">隐式记忆（Implicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="#explicit-memory">显式记忆（Explicit Memory）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="#external-information-rag">外部信息（External Information，如 RAG）成本</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id53">综合比较</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id54">拓展讨论</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#appendix-b-vector-compression">Appendix B Vector Compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#appendix-c-supplementary-evaluation-results">Appendix C Supplementary Evaluation Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2505.14683.html">2505.14683_Emerging Properties in Unified Multimodal Pretraining</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#llm">LLM 总结</a></li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id1">核心内容总结：</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id2">总结：</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#model">2 Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id3">1. 模型架构概览</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id4">2. 生成策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id5">3. 模型细节</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#generalized-causal-attention">4. 广义因果注意力（Generalized Causal Attention）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#transformer">5. Transformer结构选择与实验</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id6">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#data">3 Data</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id7">数据特点与目标</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id8">数据来源与统计</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id9">数据构建方法</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id16">数据训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id17">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#training">4 Training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id18">1. 多阶段训练策略</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id19">2. 关键超参数调整</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id22">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#evaluation">5 Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#emerging-properties">6 Emerging Properties</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id23">1. <strong>新兴属性的定义与研究背景</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id24">2. <strong>任务表现与训练阶段的关系</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id25">3. <strong>多模态特征的重要性</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id26">4. <strong>定性分析与生成质量提升</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id27">5. <strong>核心发现与结论</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id28">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#main-results">7 Main Results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id29">7.1 图像理解</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id30">7.2 图像生成</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id31">7.3 图像编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id32">7.4 带有推理的生成/编辑</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id33">7.5 世界建模</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#id34">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="2505.14683.html#more-qualitative-results">7.6 More Qualitative Results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#conclusion">8 Conclusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="2505.14683.html#acknowledgement">9 Acknowledgement</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="2507.03724_MemOS.html">MemOS: A Memory OS for AI System</a><ul>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#llm">LLM 总结：</a></li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#abstract">Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#introduction">1 Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id1"><strong>1. 背景与动机</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id2"><strong>2. 现有方法的不足</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id3"><strong>3. 四大典型挑战</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memos"><strong>4. MemOS的提出与核心理念</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id4"><strong>5. 总结与意义</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#memory-in-large-language-models">2 Memory in Large Language Models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id5">总结如下：</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id6"><strong>一、记忆研究的四个阶段</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id7"><strong>二、第一阶段：记忆定义与探索</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id14"><strong>三、MemOS 的初步构想</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id15"><strong>四、总结</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#stage-1"><strong>2.1 显式长期记忆的建立（Stage 1）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#stage-2"><strong>2.2 人脑式记忆机制的引入（Stage 2）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#stage-3"><strong>2.3 基于工具的记忆管理（Stage 3）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#stage-4"><strong>2.4 系统化记忆治理（Stage 4）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id16"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#memos-design-philosophy">3 MemOS Design Philosophy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memos-3-1-vision-of-memos">一、MemOS 的愿景（3.1 Vision of MemOS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#from-computer-os-to-memory-os">二、从传统操作系统到记忆操作系统（3.2 From Computer OS to Memory OS）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id17">三、总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#memory-modeling-in-memos">4 Memory Modeling in MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id18"><strong>4.1 内存类型与语义演化路径</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memory-cube-memcube"><strong>4.2 Memory Cube（MemCube）：内存的核心资源单元</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id19"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#architecture-of-memos">5 Architecture of MemOS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id20">总结：MemOS 架构与执行流程</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id24">总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id25">5.5.1 MemGovernance（内存治理模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memvault">5.5.2 MemVault（内存存储与路由基础设施）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memloader-memdumper">5.5.3 MemLoader 与 MemDumper（内存加载与导出模块）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#memstore">5.5.4 MemStore（内存存储与分发接口）</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id26">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#evaluation">6 Evaluation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#end-to-end-evaluation-on-locomo"><strong>1. 整体系统评估（End-to-End Evaluation on LOCOMO）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#evaluation-of-memory-retrieval"><strong>2. 内存检索评估（Evaluation of Memory Retrieval）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#kv-evaluation-of-kv-based-memory-acceleration"><strong>3. KV缓存加速评估（Evaluation of KV-Based Memory Acceleration）</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id27"><strong>总结</strong></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#memos-for-architecture-innovation-and-applications">7 MemOS for Architecture Innovation and Applications</a><ul>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id28">一、MemOS推动的架构创新</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id29">二、MemOS的应用场景</a></li>
<li class="toctree-l4"><a class="reference internal" href="2507.03724_MemOS.html#id30">总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2507.03724_MemOS.html#conclusion">8 Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../other.html">其他</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../other.html#id3">数据集&amp;数据蒸馏</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../others/DataSets/1811.10959v3_Dataset_Distillation.html">1811.10959v3_Dataset Distillation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/1811.10959v3_Dataset_Distillation.html#abstract">ABSTRACT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/1811.10959v3_Dataset_Distillation.html#llm">LLM总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/1811.10959v3_Dataset_Distillation.html#introduction">1. INTRODUCTION</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/1811.10959v3_Dataset_Distillation.html#approach">3. APPROACH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/DataSets/2502.20653_Dataset_Distillation.html">2502.20653_Dataset Distillation with Neural Characteristic Function: A Minmax Perspective</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/2502.20653_Dataset_Distillation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/2502.20653_Dataset_Distillation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/2502.20653_Dataset_Distillation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/2502.20653_Dataset_Distillation.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/DataSets/normal.html">通用</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/DataSets/normal.html#dataset-distillation">Dataset distillation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../other.html#d">3D</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html">2003.08934_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#neural-radiance-field-scene-representation">3. Neural Radiance Field Scene Representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#volume-rendering-with-radiance-fields">4. Volume Rendering with Radiance Fields</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#optimizing-a-neural-radiance-field">5. Optimizing a Neural Radiance Field</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#result">6. Result</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2003.08934_NeRF.html#conclusion">7. Conclusion</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html">2203.08586: Deep vanishing point detection: Geometric priors make dataset variations vanish</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#id1">概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#geometric-priors-for-vp-detection">3. Geometric priors for VP detection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2203.08586_VanishingPointEstimation.html#conclusion-and-limitations">5. Conclusion and limitations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html">2312.14132_DUSt3R: Geometric 3D Vision Made Easy</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#id1">关键词</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#id2">相关概念</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#experiments-with-dust3r">4. Experiments with DUSt3R</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-a">Appendix A <strong>附录概览</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-b-qualitative-results">Appendix B.  Qualitative results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-c-extended-related-work">Appendix C. Extended Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-d-multi-view-pose-estimation">Appendix D. 多视角姿态估计（Multi-view Pose Estimation）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-e-visual-localization">Appendix E. 视觉定位（Visual Localization）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2312.14132_DUSt3R.html#appendix-f-training-details">Appendix F. Training details</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html">2406.09756_MASt3R: Grounding Image Matching in 3D with MASt3R</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#id1">前言</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#id2">🧠 思维导图式总结</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#related-works">2. Related works</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#id3">🧠 总结思维导图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#experimental-results">4. Experimental results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#appendix-a-additional-qualitative-results">Appendix A Additional Qualitative Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#b-fast-reciprocal-matching">B. Fast Reciprocal Matching</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#c-coarse-to-fine">C. Coarse-to-Fine</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2406.09756_MASt3R.html#d-detailed-experimental-settings">D. Detailed experimental settings</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html">2412.09401_SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#id1">术语</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#conclusion">5. Conclusion</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#id14">6. 致谢</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#appendix">Appendix</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#appendix-a-implementation-details">Appendix A Implementation details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#appendix-b-details-for-experimental-settings">Appendix B Details for experimental settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#appendix-c-additional-comparisons-and-analyses">Appendix C Additional comparisons and analyses</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.09401_SLAM3R.html#d-more-visual-results">D. More visual results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html">2412.12392_MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#gpt">GPT</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#id1">先验知识</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#results">4. Results</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#limitations-and-future-work">5. Limitations and Future Work（局限与未来工作）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#conclusion">🧾 6. Conclusion（总结）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#id32">🧠 总结一句话版：</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#initialisation">8. Initialisation（初始化）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#runtime-breakdown">9. Runtime Breakdown（运行时分析）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#evaluation-setup">10. Evaluation Setup（评估设置）</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2412.12392_MASt3R-SLAM.html#id35">11. EuRoC 结果总结</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html">2503.11651_VGGT: Visual Geometry Grounded Transformer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#abstract">Abstract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#introduction">1. Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#related-work">2. Related Work</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#method">3. Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#experiments">4. Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#discussions">5. Discussions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#conclusions">6. Conclusions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#appendix-a-formal-definitions">Appendix A Formal Definitions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#appendix-b-implementation-details">Appendix B Implementation Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#appendix-c-additional-experiments">Appendix C Additional Experiments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#appendix-d-qualitative-examples">Appendix D Qualitative Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/3D/2503.11651_VGGT.html#appendix-e-related-work">Appendix E Related Work</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../other.html#id4">其他</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html">A PAINLESS GUIDE TO CRC ERROR DETECTION ALGORITHMS</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#the-basic-idea-behind-crc-algorithms">The Basic Idea Behind CRC Algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#polynomical-arithmetic">Polynomical Arithmetic</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#binary-arithmetic-with-no-carries">Binary Arithmetic with No Carries</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id2">一个可用的实例</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#choosing-a-poly">Choosing A Poly</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-straightforward-crc-implementation">A Straightforward CRC Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-table-driven-implementation">A Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#a-slightly-mangled-table-driven-implementation">A Slightly Mangled Table-Driven Implementation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../others/others/A%20PAINLESS%20GUIDE%20TO%20CRC%20ERROR%20DETECTION%20ALGORITHMS.html#id3">参考</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../others/others/Distributed%20Representations%20of%20Sentences%20and%20Documents.html">Distributed Representations of Sentences and Documents</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">新溪-gordon</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../paper_pool.html">论文池</a> &raquo;</li>
        
      <li>2407.01178_Memory3: Language Modeling with Explicit Memory</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/zzz_paper_pools/2407.01178_Memory3.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            <nav id="local-table-of-contents" role="navigation" aria-labelledby="local-table-of-contents-title">
              <h4 id="local-table-of-contents-title">On This Page</h4>
              <ul>
<li><a class="reference internal" href="#">2407.01178_Memory3: Language Modeling with Explicit Memory</a><ul>
<li><a class="reference internal" href="#language-modeling-with-explicit-memory">Language Modeling with Explicit Memory</a><ul>
<li><a class="reference internal" href="#id1">研究背景与动机：</a></li>
<li><a class="reference internal" href="#id2">主要内容与方法：</a></li>
<li><a class="reference internal" href="#id3">实验与结果：</a></li>
<li><a class="reference internal" href="#id4">总结：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#abstract">Abstract</a><ul>
<li><a class="reference internal" href="#id5">核心思想</a></li>
<li><a class="reference internal" href="#memory3">Memory3 模型特点</a></li>
<li><a class="reference internal" href="#id6">实验结果</a></li>
<li><a class="reference internal" href="#id7">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#introduction">1 _ Introduction</a></li>
<li><a class="reference internal" href="#retrieval-augmented-training">1.1.1 _ Retrieval-augmented Training</a><ul>
<li><a class="reference internal" href="#id8">1.1.1 | 基于检索的训练（Retrieval-augmented Training）</a></li>
<li><a class="reference internal" href="#sparse-computation">1.1.2 | 稀疏计算（Sparse Computation）</a></li>
<li><a class="reference internal" href="#parameter-as-memory">1.1.3 | 参数即记忆（Parameter as Memory）</a></li>
<li><a class="reference internal" href="#id9">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#memory-circuitry-theory">2 _ Memory Circuitry Theory</a><ul>
<li><a class="reference internal" href="#id10">核心概念总结：</a></li>
<li><a class="reference internal" href="#id11">总体贡献：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#definition-2">Definition 2.</a><ul>
<li><a class="reference internal" href="#id12">1. <strong>定义与核心概念：计算图、同构与知识（电路）</strong></a></li>
<li><a class="reference internal" href="#id13">2. <strong>知识的实例</strong></a></li>
<li><a class="reference internal" href="#id14">3. <strong>知识的外部化与记忆</strong></a></li>
<li><a class="reference internal" href="#id15">4. <strong>结论与断言</strong></a></li>
<li><a class="reference internal" href="#id16">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#remark-1">Remark 1.</a><ul>
<li><a class="reference internal" href="#id17">1. <strong>电路构造的关键性质</strong></a></li>
<li><a class="reference internal" href="#llm">2. <strong>记忆增强 LLM 的形式化定义</strong></a></li>
<li><a class="reference internal" href="#id18">3. <strong>写入代价与读取代价的权衡（记忆层次结构）</strong></a></li>
<li><a class="reference internal" href="#id19">4. <strong>知识使用频率与记忆分配</strong></a></li>
<li><a class="reference internal" href="#id20">5. <strong>图示与结论</strong></a></li>
<li><a class="reference internal" href="#id21">小结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#design">3 _ Design</a><ul>
<li><a class="reference internal" href="#id22"><strong>3 | Design</strong></a><ul>
<li><a class="reference internal" href="#id23"><strong>3.1 架构设计</strong></a></li>
<li><a class="reference internal" href="#id24"><strong>3.2 训练目标</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id25"><strong>3.1 | 推理过程</strong></a><ul>
<li><a class="reference internal" href="#id26"><strong>备注与优化</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#id27"><strong>3.2 | 写入与读取记忆</strong></a></li>
<li><a class="reference internal" href="#id28"><strong>总结</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#memory-sparsification-and-storage">3.3 _ Memory Sparsification and Storage</a><ul>
<li><a class="reference internal" href="#id29">一、显式记忆的存储挑战</a></li>
<li><a class="reference internal" href="#id30">二、各维度的稀疏化策略</a><ul>
<li><a class="reference internal" href="#layers">1. 层（Layers）的稀疏化</a></li>
<li><a class="reference internal" href="#heads">2. 头（Heads）的稀疏化</a></li>
<li><a class="reference internal" href="#token">3. Token 的稀疏化</a></li>
<li><a class="reference internal" href="#head-dimension">4. 头维度（Head Dimension）的压缩</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id31">三、压缩效果</a></li>
<li><a class="reference internal" href="#id32">四、部署方式</a></li>
<li><a class="reference internal" href="#id33">五、补充说明与建议</a><ul>
<li><a class="reference internal" href="#id34">备注 4：记忆头的选择方法</a></li>
<li><a class="reference internal" href="#id35">备注 5：自适应稀疏化</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id36">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#model-shape">3.4 _ Model Shape</a><ul>
<li><a class="reference internal" href="#id37">3.4 | 模型结构（Model Shape）</a></li>
<li><a class="reference internal" href="#training-designs">3.5 | 训练设计（Training Designs）</a></li>
<li><a class="reference internal" href="#id38">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#two-stage-pretrain">3.6 _ Two-stage Pretrain</a><ul>
<li><a class="reference internal" href="#id39">一、预训练的两个阶段</a></li>
<li><a class="reference internal" href="#continual-train">二、对 continual train 的优化</a></li>
<li><a class="reference internal" href="#id40">三、防止信息泄露</a></li>
<li><a class="reference internal" href="#id41">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#pretraining-data">4 _ Pretraining Data</a><ul>
<li><a class="reference internal" href="#data-collection"><strong>4.1 数据收集（Data Collection）</strong></a></li>
<li><a class="reference internal" href="#filtering"><strong>4.2 数据过滤（Filtering）</strong></a></li>
<li><a class="reference internal" href="#tokenizer"><strong>4.3 分词器（Tokenizer）</strong></a></li>
<li><a class="reference internal" href="#knowledge-base"><strong>4.4 知识库（Knowledge Base）</strong></a></li>
<li><a class="reference internal" href="#id42"><strong>总结</strong></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pretrain">5 _ Pretrain</a><ul>
<li><a class="reference internal" href="#id43">1. 预训练总体设计</a></li>
<li><a class="reference internal" href="#set-up">2. 训练设置（Set-up）</a></li>
<li><a class="reference internal" href="#warmup-stage">3. 预热阶段（Warmup Stage）</a></li>
<li><a class="reference internal" href="#continual-train-stage">4. 持续训练阶段（Continual Train Stage）</a></li>
<li><a class="reference internal" href="#id44">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#fine-tuning-and-alignment">6 _ Fine-tuning and Alignment</a><ul>
<li><a class="reference internal" href="#supervised-finetuning-sft">6.1 监督微调（Supervised Finetuning, SFT）</a></li>
<li><a class="reference internal" href="#direct-preference-optimization-dpo">6.2 直接偏好优化（Direct Preference Optimization, DPO）</a></li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation">7 _ Evaluation</a><ul>
<li><a class="reference internal" href="#id45">7.1 通用能力评估</a></li>
<li><a class="reference internal" href="#id46">7.2 对话能力评估</a></li>
<li><a class="reference internal" href="#id47">7.3 幻觉与事实性评估</a></li>
<li><a class="reference internal" href="#id48">7.4 专业任务评估</a></li>
<li><a class="reference internal" href="#id49">总结</a></li>
</ul>
</li>
<li><a class="reference internal" href="#inference-speed">7.5 _ Inference Speed</a><ul>
<li><a class="reference internal" href="#id50">主要内容总结如下：</a></li>
<li><a class="reference internal" href="#id51">总结：</a></li>
</ul>
</li>
<li><a class="reference internal" href="#conclusion">8 _ Conclusion</a></li>
<li><a class="reference internal" href="#acknowledgement">Acknowledgement</a></li>
<li><a class="reference internal" href="#appendix-a-cost-estimation">Appendix A Cost Estimation</a><ul>
<li><a class="reference internal" href="#id52">模型参数设定</a></li>
<li><a class="reference internal" href="#implicit-memory">隐式记忆（Implicit Memory）成本</a></li>
<li><a class="reference internal" href="#explicit-memory">显式记忆（Explicit Memory）成本</a></li>
<li><a class="reference internal" href="#external-information-rag">外部信息（External Information，如 RAG）成本</a></li>
<li><a class="reference internal" href="#id53">综合比较</a></li>
<li><a class="reference internal" href="#id54">拓展讨论</a></li>
</ul>
</li>
<li><a class="reference internal" href="#appendix-b-vector-compression">Appendix B Vector Compression</a></li>
<li><a class="reference internal" href="#appendix-c-supplementary-evaluation-results">Appendix C Supplementary Evaluation Results</a></li>
</ul>
</li>
</ul>

            </nav>
  <section class="tex2jax_ignore mathjax_ignore" id="memory3-language-modeling-with-explicit-memory">
<h1>2407.01178_Memory3: Language Modeling with Explicit Memory<a class="headerlink" href="#memory3-language-modeling-with-explicit-memory" title="此标题的永久链接">¶</a></h1>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2407.01178">https://arxiv.org/abs/2407.01178</a></p></li>
<li><p>PDF: <a class="reference external" href="https://arxiv.org/pdf/2407.01178">https://arxiv.org/pdf/2407.01178</a></p></li>
<li><p>引用: 6(2025-07-29)</p></li>
<li><p>组织:</p>
<ul>
<li><p>1Center for LLM, Institute for Advanced Algorithms Research, Shanghai</p></li>
<li><p>2Moqi Inc</p></li>
<li><p>3Center for Machine Learning Research, Peking University</p></li>
</ul>
</li>
</ul>
<section id="language-modeling-with-explicit-memory">
<h2>Language Modeling with Explicit Memory<a class="headerlink" href="#language-modeling-with-explicit-memory" title="此标题的永久链接">¶</a></h2>
<p>文章《Memory3: Language Modeling with Explicit Memory》主要探讨了一种名为 Memory3 的语言模型，其核心特点是引入了<strong>显式记忆机制</strong>（Explicit Memory），以增强模型在处理语言任务时的推理能力与信息存储能力。</p>
<section id="id1">
<h3>研究背景与动机：<a class="headerlink" href="#id1" title="此标题的永久链接">¶</a></h3>
<p>传统语言模型（如 GPT、Transformer 等）依赖于隐式学习（implicit learning）来理解语言，虽然在生成任务中表现出色，但在需要长期记忆或复杂推理的任务中存在局限。作者提出 Memory3，旨在通过引入显式记忆模块，使模型能够<strong>主动存储和检索信息</strong>，从而提升其在需要记忆的任务中的表现。</p>
</section>
<section id="id2">
<h3>主要内容与方法：<a class="headerlink" href="#id2" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>显式记忆模块</strong>：</p>
<ul class="simple">
<li><p>Memory3 引入了一个<strong>显式的记忆存储器</strong>（memory bank），用于存储模型在处理过程中主动选择的重要信息。</p></li>
<li><p>模型可以根据当前输入内容，<strong>主动决定哪些信息需要存储</strong>，并在后续处理中<strong>检索这些信息进行推理</strong>。</p></li>
</ul>
</li>
<li><p><strong>记忆的编码与检索</strong>：</p>
<ul class="simple">
<li><p>记忆信息通过可学习的编码机制进行表示，便于后期检索。</p></li>
<li><p>检索机制使用注意力机制（attention）从记忆中找到与当前输入最相关的条目，辅助模型生成更准确的输出。</p></li>
</ul>
</li>
<li><p><strong>训练策略</strong>：</p>
<ul class="simple">
<li><p>作者设计了合适的训练目标，使模型在训练过程中学会如何有效使用记忆模块，包括何时存储、何时检索等。</p></li>
</ul>
</li>
</ol>
</section>
<section id="id3">
<h3>实验与结果：<a class="headerlink" href="#id3" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>在多个需要记忆和推理能力的自然语言处理任务上（如问答、对话、逻辑推理等），Memory3 显示出优于传统语言模型的性能。</p></li>
<li><p>实验表明，显式记忆机制可以显著提升模型在处理长文本、多轮对话和逻辑推理任务中的表现。</p></li>
</ul>
</section>
<section id="id4">
<h3>总结：<a class="headerlink" href="#id4" title="此标题的永久链接">¶</a></h3>
<p>《Memory3: Language Modeling with Explicit Memory》提出了一种新的语言模型架构，通过引入显式记忆机制，增强了模型的记忆和推理能力。该方法在多个任务中取得了良好的效果，为未来构建更智能、更具推理能力的语言模型提供了新的研究方向。</p>
</section>
</section>
<section id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="此标题的永久链接">¶</a></h2>
<p>该论文提出了一种名为 <strong>Memory3</strong> 的大语言模型（LLM），其核心创新在于引入<strong>显式记忆（explicit memory）</strong>，以降低训练和推理成本。论文主要观点和贡献如下：</p>
<section id="id5">
<h3>核心思想<a class="headerlink" href="#id5" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>受人脑记忆层级启发</strong>，作者提出为LLM引入显式记忆，作为比模型参数（隐式记忆）和上下文缓存（工作记忆）更经济的知识存储方式。</p></li>
<li><p>显式记忆可以被<strong>检索利用</strong>，从而减少对模型参数规模的依赖，降低训练和推理成本。</p></li>
<li><p>模型的知识可以分为“抽象知识”（由参数存储）和“显式知识”（由记忆存储），显式知识的外化使得模型规模可以更小，但性能仍可保持。</p></li>
</ul>
</section>
<section id="memory3">
<h3>Memory3 模型特点<a class="headerlink" href="#memory3" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>从头训练了一个2.4B参数的模型，<strong>在性能上超越了更大规模的LLM和RAG模型</strong>，同时在推理速度上优于RAG。</p></li>
<li><p>采用了<strong>两阶段预训练方案</strong>和<strong>记忆稀疏化机制</strong>，帮助模型高效地形成和存储显式记忆。</p></li>
<li><p>提出<strong>记忆电路理论</strong>（memory circuitry theory），支持知识从模型参数转移到显式记忆的可行性。</p></li>
</ul>
</section>
<section id="id6">
<h3>实验结果<a class="headerlink" href="#id6" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>图2（左）</strong>：展示了在多个基准测试中的模型性能与模型规模的关系，Memory3在较小规模下仍表现出色。</p></li>
<li><p><strong>图2（右）</strong>：展示了在专业任务上的性能与推理速度对比，Memory3在保持高性能的同时，推理速度优于RAG模型。</p></li>
<li><p>作者强调该实验为初步探索，尚未对预训练数据质量和推理效率进行优化，因此结果可能与最先进模型（SOTA）不完全可比。</p></li>
</ul>
</section>
<section id="id7">
<h3>总结<a class="headerlink" href="#id7" title="此标题的永久链接">¶</a></h3>
<p>该研究通过引入<strong>显式记忆机制</strong>，探索了一种新的大语言模型架构，试图以更小的模型规模实现高性能和低资源消耗。这为未来LLM的效率优化提供了新的思路，特别是在知识存储与利用方式上。</p>
</section>
</section>
<section id="introduction">
<h2>1 _ Introduction<a class="headerlink" href="#introduction" title="此标题的永久链接">¶</a></h2>
<p>本文主要探讨了大规模语言模型（LLMs）在训练和推理过程中所面临的高昂成本问题，并提出了一种新的优化思路——通过优化“知识存储”来降低整体成本。传统的优化方法大多集中在模型结构、数据质量、并行计算、硬件等方面，而本文提出了一种全新的“显式记忆”（explicit memory）机制，旨在提升知识处理的效率和模型性能。</p>
<p>文章的核心思想是将LLM的训练和推理成本建模为“知识写入成本”与“知识读取成本”的总和。通过引入一种新的记忆形式——显式记忆，作者构建了一个多级记忆层次结构（RAG → 显式记忆 → 模型参数），显式记忆在写入和读取成本之间取得了良好平衡，适合存储中等频率使用的知识。</p>
<p>显式记忆通过将知识编码为稀疏的注意力键值对，并在推理过程中直接调用这些记忆，减少了对模型参数的依赖。这种设计使得现有大多数基于Transformer的模型在少量微调后即可支持显式记忆，具有良好的通用性和扩展性。作者将其方法命名为Memory3（第三种记忆形式），并展示了其在减少预训练成本、提升推理效率、增强事实性、减少幻觉、支持长文本处理等方面的优势。</p>
<p>此外，作者还类比人类记忆系统，指出显式记忆的引入可以类比于人类将知识从工作记忆转化为长期记忆的过程，从而提高知识处理效率。通过实验验证，Memory3模型在多种任务中表现出色，超越了更大规模的SOTA模型，并在事实性和推理速度方面取得了显著提升。</p>
<p>最后，文章概述了论文的结构，后续章节将分别介绍Memory3的理论基础、设计细节、训练过程和性能评估等内容。</p>
</section>
<section id="retrieval-augmented-training">
<h2>1.1.1 _ Retrieval-augmented Training<a class="headerlink" href="#retrieval-augmented-training" title="此标题的永久链接">¶</a></h2>
<p>本节对近年来与语言模型中知识表示和高效计算相关的研究进行了系统总结，主要分为三个部分：</p>
<hr class="docutils" />
<section id="id8">
<h3>1.1.1 | 基于检索的训练（Retrieval-augmented Training）<a class="headerlink" href="#id8" title="此标题的永久链接">¶</a></h3>
<p>研究者在语言模型的预训练和微调阶段引入文本检索机制，以增强模型对知识的访问能力。</p>
<ul class="simple">
<li><p><strong>预训练阶段</strong>：</p>
<ul>
<li><p><strong>REALM</strong> 在 BERT 模型中引入单步检索，用于解决问答任务。</p></li>
<li><p><strong>Retro</strong> 在解码过程中每 64 个 token 进行一次检索，并通过一个两层编码器和跨注意力层将检索内容注入模型。</p></li>
<li><p><strong>Retro++</strong> 对 Retro 的扩展性进行了探索，实现了高达 9.5B 参数规模的模型。</p></li>
<li><p>与这些模型不同，Retro 采用<strong>实时编码检索内容</strong>，但为减少计算开销采用浅层编码器和少量检索结果，导致知识利用率受限。</p></li>
</ul>
</li>
<li><p><strong>微调阶段</strong>：</p>
<ul>
<li><p><strong>WebGPT</strong> 通过模仿学习在网页浏览环境中训练模型使用搜索引擎。</p></li>
<li><p><strong>Toolformer</strong> 利用多个工具（包括搜索引擎）进行解码，并通过模型自身生成微调数据。</p></li>
</ul>
</li>
<li><p><strong>长上下文建模</strong>：</p>
<ul>
<li><p><strong>Memorizing Transformer</strong> 通过 kNN 查找缓存中的键值对扩展上下文长度。</p></li>
<li><p><strong>LongLlama</strong> 和 <strong>LONGMEM</strong> 则通过对比学习和架构设计解决记忆过时问题。</p></li>
<li><p>这些方法受限于缓存空间，难以应用于大规模知识库。本文提出的方法通过<strong>更激进的稀疏记忆方法</strong>克服了这一问题。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="sparse-computation">
<h3>1.1.2 | 稀疏计算（Sparse Computation）<a class="headerlink" href="#sparse-computation" title="此标题的永久链接">¶</a></h3>
<p>为了解决知识检索与计算效率之间的矛盾，研究者探索了稀疏性架构，以减少每 token 的参数使用量。</p>
<ul class="simple">
<li><p><strong>Mixture-of-Experts (MoE)</strong>：</p>
<ul>
<li><p>使用稀疏路由机制，将每个 token 分配给少量专家模块，从而在参数规模和计算成本之间取得平衡。</p></li>
<li><p>常见设计中每个 Transformer 块包含多个 MLP 层，通过线性分类器选择最相关的专家。</p></li>
<li><p>进一步的优化如 <strong>QMoE</strong> 引入压缩技术以减少内存负担。</p></li>
<li><p>尽管参数效率有所提升（如 Arctic 模型的活跃参数比例约 3.5%），但提升上限通常在 4~32 倍之间。</p></li>
</ul>
</li>
<li><p><strong>深度混合架构（Mixture of Depth）</strong>：</p>
<ul>
<li><p>通过早期退出（early exit）或 top-k 路由机制，仅使用模型中部分层级进行计算，可将计算量降低至 12.5~50%。</p></li>
</ul>
</li>
<li><p><strong>更细粒度的稀疏性</strong>：</p>
<ul>
<li><p><strong>Deja Vu</strong> 为每层 MLP 或注意力层训练一个低成本网络，预测每个神经元或注意力头对当前 token 的相关性。</p></li>
<li><p>推理时保留每个 token 的前 5~15% MLP 神经元和 20~50% 注意力头，实现更细粒度的计算节省。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="parameter-as-memory">
<h3>1.1.3 | 参数即记忆（Parameter as Memory）<a class="headerlink" href="#parameter-as-memory" title="此标题的永久链接">¶</a></h3>
<p>部分研究将模型参数视为<strong>隐式记忆</strong>，认为某些神经元可以存储知识。</p>
<ul class="simple">
<li><p><strong>GPT 中的 MLP 神经元行为</strong>：</p>
<ul>
<li><p>神经元可被视为键值对（Key-Value），其中第一层权重作为“键”，第二层权重作为“值”。</p></li>
<li><p>激活的键通常对应某些人类可理解的模式，值则用于生成最可能的下一个 token。</p></li>
</ul>
</li>
<li><p><strong>基于注意力的 GPT 变体</strong>：</p>
<ul>
<li><p><strong>[[108]]</strong> 提出仅由注意力层构成的 GPT 变体，将 MLP 的键值对作为“持久记忆”嵌入注意力机制中。</p></li>
</ul>
</li>
<li><p><strong>知识神经元</strong>：</p>
<ul>
<li><p><strong>[[29]]</strong> 发现 BERT 中的事实知识往往集中在少数 MLP 神经元中，称为“知识神经元”，可通过操作这些神经元更新模型知识。</p></li>
</ul>
</li>
<li><p><strong>神经网络中的超合（Superposition）现象</strong>：</p>
<ul>
<li><p><strong>[[38]]</strong> 研究发现，一个神经元可以存储多个不相关的概念，这种多语义性机制是神经网络表达复杂知识的一种方式。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id9">
<h3>总结<a class="headerlink" href="#id9" title="此标题的永久链接">¶</a></h3>
<p>本节综述了当前语言模型中提升知识访问效率和计算效率的主要技术路线，包括：</p>
<ol class="arabic simple">
<li><p><strong>检索增强训练</strong>：在预训练和微调中引入检索机制，提升模型对知识的访问能力。</p></li>
<li><p><strong>稀疏计算架构</strong>：通过稀疏路由、混合专家和细粒度剪枝等方法减少计算开销。</p></li>
<li><p><strong>参数作为隐式知识载体</strong>：揭示模型参数中存储知识的机制，为知识更新和模型优化提供了理论基础。</p></li>
</ol>
<p>这些研究为本文提出的高效知识访问与存储方法提供了背景与对比。</p>
</section>
</section>
<section id="memory-circuitry-theory">
<h2>2 _ Memory Circuitry Theory<a class="headerlink" href="#memory-circuitry-theory" title="此标题的永久链接">¶</a></h2>
<p>本节介绍了<strong>记忆电路理论（Memory Circuitry Theory）</strong>，旨在在大型语言模型（LLM）的背景下定义知识与记忆。该理论通过将LLM的计算过程分解为可重复的模块（称为“电路”），帮助我们理解哪些知识可以被提取为<strong>显式记忆（explicit memory）</strong>，并指导适合读写显式记忆的模型架构设计。</p>
<hr class="docutils" />
<section id="id10">
<h3>核心概念总结：<a class="headerlink" href="#id10" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>知识与电路的定义</strong>：</p>
<ul class="simple">
<li><p><strong>知识</strong>定义为输入输出关系及其对应的<strong>内部计算路径（电路）</strong>。</p></li>
<li><p><strong>电路</strong>被看作是“时空现象”，其因果结构局部存在于特定的MLP神经元和注意力头，并作用于特定的token上。</p></li>
<li><p>电路是多个计算图中相似子图的<strong>等价类</strong>，用于描述知识的实现机制。</p></li>
</ul>
</li>
<li><p><strong>知识提取的动机</strong>：</p>
<ul class="simple">
<li><p>通过识别和提取电路，可以将LLM中的知识和技能模块化，从而支持<strong>显式记忆的外部化</strong>，实现轻量化的模型骨干和记忆层级结构。</p></li>
</ul>
</li>
<li><p><strong>行为主义与白盒方法对比</strong>：</p>
<ul class="simple">
<li><p>行为主义方法仅关注输入输出关系，但不足以确定知识的内在机制。</p></li>
<li><p>白盒方法通过分析模型内部的计算路径（如注意力头与MLP层的交互）来定义知识，更具解释力。</p></li>
</ul>
</li>
<li><p><strong>知识示例与机制</strong>：</p>
<ul class="simple">
<li><p><strong>事实性知识（如“China的首都是Beijing”）</strong>：</p>
<ul>
<li><p>由“移动头（mover head）”将上下文信息移动到最后的token，并通过MLP层中的“知识神经元”映射到答案。</p></li>
</ul>
</li>
<li><p><strong>算术推理</strong>：</p>
<ul>
<li><p>依赖模型内部的计算路径，如通过注意力头提取表达式特征，再由MLP计算结果。</p></li>
</ul>
</li>
<li><p><strong>“搜索、复制、粘贴”机制</strong>（induction head）：</p>
<ul>
<li><p>通过两个注意力头实现类如“…[a][b]…[a]→[b]”的模式识别与输出。</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>计算图与子图定义</strong>：</p>
<ul class="simple">
<li><p><strong>计算图</strong>：描述LLM内部所有token在所有层中的隐藏状态及其激活路径。</p></li>
<li><p><strong>子图</strong>：代表一个知识的电路，其输入输出序列可通过梯度或因果干预识别。</p></li>
<li><p><strong>等价类定义电路</strong>：多个任务中结构相似的子图被视为同一电路的实例。</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id11">
<h3>总体贡献：<a class="headerlink" href="#id11" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>提出了一种基于<strong>计算图与子图分析</strong>的框架，用于识别和提取LLM中的知识。</p></li>
<li><p>定义了“记忆电路理论”作为理解LLM内部记忆机制的指导原则。</p></li>
<li><p>为后续设计具有<strong>显式记忆能力的模型架构</strong>提供了理论支持。</p></li>
</ul>
<p>该理论对LLM的可解释性、模块化构建以及记忆机制研究具有重要意义。</p>
</section>
</section>
<section id="definition-2">
<h2>Definition 2.<a class="headerlink" href="#definition-2" title="此标题的永久链接">¶</a></h2>
<p>这篇文章的章节内容总结如下：</p>
<hr class="docutils" />
<section id="id12">
<h3>1. <strong>定义与核心概念：计算图、同构与知识（电路）</strong><a class="headerlink" href="#id12" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>计算图（Computation Graphs）</strong>：用于描述大型语言模型（LLM）中信息处理路径的结构，包括注意力头（head）和 MLP 神经元（neuron）等节点。</p></li>
<li><p><strong>同构映射（Homomorphism）</strong>：定义了两个计算图子图之间的结构关系，包括深度、位置和边（注意力或神经元连接）的对应性。这种映射允许节点不唯一，但保持结构一致性。</p></li>
<li><p><strong>知识（Circuit / Knowledge）</strong>：</p>
<ul>
<li><p>一组相互同构的子图，构成一个“电路”。</p></li>
<li><p>所有子图中的边权重非可忽略，表示有实际作用。</p></li>
<li><p>输入输出对（tin, tout）在语义上具有一致性，即具有可解释的模式或意义。</p></li>
<li><p>知识分为<strong>具体知识（specific knowledge）<strong>和</strong>抽象知识（abstract knowledge）</strong>：</p>
<ul>
<li><p><strong>具体知识</strong>：输入语义一致，输出固定或差异很小。</p></li>
<li><p><strong>抽象知识</strong>：输出可能变化较大，但输入有明确模式。</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id13">
<h3>2. <strong>知识的实例</strong><a class="headerlink" href="#id13" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>具体知识的例子</strong>：如“中国首都是北京”的知识神经元，输入是“中国”和“首都”，输出总是“北京”。</p></li>
<li><p><strong>抽象知识的例子</strong>：如诱导头（induction head），输入是类似“[a][b]…[a]”的模式，输出为“[b]”，虽然输入有规律，但输出可以不同。</p></li>
<li><p><strong>更复杂的抽象知识</strong>：已发现用于上下文学习（in-context learning）和间接对象识别（indirect object identification）。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id14">
<h3>3. <strong>知识的外部化与记忆</strong><a class="headerlink" href="#id14" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>知识的实现（Realization）</strong>：表示能够激活某个知识的文本，即在该文本的计算图中存在该知识的子图。</p></li>
<li><p><strong>可分离性（Separability）</strong>：一个知识如果可以从模型参数中分离并存储在外部记忆中，则称为可分离的。分离的条件包括：</p>
<ul>
<li><p>某模型M在没有该知识的情况下无法生成输出。</p></li>
<li><p>通过提供一个示例文本（prefix），模型M可以生成该知识的输出。</p></li>
</ul>
</li>
<li><p><strong>可模仿性（Imitability）</strong>：如果任何实现文本都可以作为示例文本来辅助模型生成输出，则该知识是可模仿的。可模仿性是可分离性的一个特例。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id15">
<h3>4. <strong>结论与断言</strong><a class="headerlink" href="#id15" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>断言 1（Claim 1）</strong>：所有<strong>具体知识</strong>都是可模仿的，因此也是可分离的。这表明许多知识可以从模型参数中移除，并通过外部示例来实现。</p></li>
<li><p><strong>分离性与模仿性</strong>：分离性更宽泛，例如可以使用抽象描述作为前缀（如指令提示），而不仅仅是具体示例。</p></li>
<li><p><strong>知识的组织</strong>：文章提出，LLM 的效率本质上来源于这些知识的有效组织，而问题的核心是将这些知识从庞大模型中分离，以构建更高效的带记忆结构的模型。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id16">
<h3>总结<a class="headerlink" href="#id16" title="此标题的永久链接">¶</a></h3>
<p>本节从计算图的结构出发，定义了知识的结构和语义特性，并通过同构关系建立了知识的统一表示。随后通过具体与抽象知识的例子展示了知识在模型中的表现形式，并提出了知识外部化的概念，包括可分离性和可模仿性。最后，文章指出许多知识（尤其是具体知识）可以被提取并存储在外部，从而为构建更高效的语言模型提供了理论依据。</p>
</section>
</section>
<section id="remark-1">
<h2>Remark 1.<a class="headerlink" href="#remark-1" title="此标题的永久链接">¶</a></h2>
<p>本节主要围绕**记忆增强语言模型（Memory-Augmented LLM）**的理论模型展开，重点包括以下几个方面：</p>
<hr class="docutils" />
<section id="id17">
<h3>1. <strong>电路构造的关键性质</strong><a class="headerlink" href="#id17" title="此标题的永久链接">¶</a></h3>
<p>作者在 Claim 1 的证明中总结了三个对后续设计有指导意义的细节：</p>
<ol class="arabic simple">
<li><p><strong>单一注意力头</strong>：构建的电路只有一个注意力头，该头从当前文本 𝐭 中关注参考文本 𝐭′，其他计算则局限于各自文本。</p></li>
<li><p><strong>注意力边的稀疏性</strong>：该注意力头只需关注参考文本中少数几个标记（从 [b] 到 [a’] 的边），说明其对参考内容的依赖较为有限。</p></li>
<li><p><strong>自注意力机制</strong>：参考文本只需关注自身，不需要外部信息，简化了注意力结构。</p></li>
</ol>
<p>这些性质为后续的架构设计提供了指导，特别是在设计高效记忆访问机制方面。</p>
</section>
<hr class="docutils" />
<section id="llm">
<h3>2. <strong>记忆增强 LLM 的形式化定义</strong><a class="headerlink" href="#llm" title="此标题的永久链接">¶</a></h3>
<p>作者提出了一种通用的记忆增强 LLM 模型定义：</p>
<ul class="simple">
<li><p><strong>输入</strong>：前缀（t₀…tᵢ₋₁）、非可分离知识集合 {𝒦₁,…,𝒦ₙ} 以及不同格式的参考文本集合 X₁,…,Xₘ。</p></li>
<li><p><strong>输出</strong>：一个基于前缀的 token 分布。</p></li>
<li><p><strong>记忆格式</strong>：每个 Xⱼ 由函数 fⱼ 从参考文本 𝐭ⱼ,k 中编码得到，每个 fⱼ 被称为“写函数（write function）”，表示该记忆格式的数据如何被写入模型。</p></li>
<li><p>如果某个知识 𝒦 被写入格式 j，则表示它以该格式被模型读取和使用。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id18">
<h3>3. <strong>写入代价与读取代价的权衡（记忆层次结构）</strong><a class="headerlink" href="#id18" title="此标题的永久链接">¶</a></h3>
<p>作者提出了一个关键的理论假设：</p>
<ul class="simple">
<li><p><strong>写入代价</strong>（costwrite）与<strong>读取代价</strong>（costread）存在<strong>负相关关系</strong>：写入越便宜，读取越昂贵；反之亦然。</p></li>
<li><p>这种关系被称为<strong>记忆层次结构（memory hierarchy）</strong>，类似于人类的记忆系统：常用表达（如母语）写入代价高（需长期学习），但读取代价低；而罕见知识（如书籍内容）写入代价低，读取代价高。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id19">
<h3>4. <strong>知识使用频率与记忆分配</strong><a class="headerlink" href="#id19" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>不同频率使用的知识应分配到不同的记忆格式中：</p>
<ul>
<li><p><strong>高频知识</strong>应分配到<strong>高写入代价、低读取代价</strong>的格式（如隐式记忆）。</p></li>
<li><p><strong>低频知识</strong>应分配到<strong>低写入代价、高读取代价</strong>的格式（如显式记忆或外部信息）。</p></li>
</ul>
</li>
<li><p>添加新的记忆格式可以扩展搜索空间，从而在某些知识的使用频率落在特定区间时，降低总体代价。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id20">
<h3>5. <strong>图示与结论</strong><a class="headerlink" href="#id20" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>图 4 和 图 8 显示了写入与读取代价之间的反比例关系，并展示了不同记忆格式的使用场景。</p></li>
<li><p>人类和 LLM 的记忆分布有相似性：高频知识倾向隐式记忆，低频知识可能需通过显式记忆或外部检索表示。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id21">
<h3>小结<a class="headerlink" href="#id21" title="此标题的永久链接">¶</a></h3>
<p>本节构建了一个形式化的记忆增强 LLM 模型，提出了记忆格式的通用定义，并通过理论分析与图示展示了写入与读取代价之间的权衡关系。这一理论框架为设计具备高效显式记忆机制的语言模型提供了理论支持。</p>
</section>
</section>
<section id="design">
<h2>3 _ Design<a class="headerlink" href="#design" title="此标题的永久链接">¶</a></h2>
<p>本章主要介绍了 <strong>Memory3</strong> 的设计，包括其架构、训练方案以及推理过程。目标是通过引入一种显式的记忆机制，使 Transformer 大型语言模型（LLM）能够在保持较低写入和读取成本的前提下，提升模型对知识的利用效率。</p>
<hr class="docutils" />
<section id="id22">
<h3><strong>3 | Design</strong><a class="headerlink" href="#id22" title="此标题的永久链接">¶</a></h3>
<section id="id23">
<h4><strong>3.1 架构设计</strong><a class="headerlink" href="#id23" title="此标题的永久链接">¶</a></h4>
<p>Memory3 的目标是为 Transformer 架构设计一种显式记忆机制，具有较低的写入和读取成本，同时尽量减少对原有 Transformer 架构的改动，不引入新的可训练参数，从而使得大多数现有的 Transformer LLM 可以通过少量微调转换为 Memory3 模型。</p>
<ul class="simple">
<li><p><strong>写入成本（Write cost）</strong>：</p>
<ul>
<li><p>在推理之前，LLM 会将每个参考文本写入显式记忆中，存储在磁盘上。</p></li>
<li><p>显式记忆是从自注意力层的 key-value 向量中选取的，因此写入过程不需要训练。</p></li>
<li><p>每个参考文本独立处理，避免了长上下文注意力的开销。</p></li>
</ul>
</li>
<li><p><strong>读取成本（Read cost）</strong>：</p>
<ul>
<li><p>在推理时，显式记忆从磁盘中检索，并与常规上下文的 key-value 一起通过自注意力机制处理。</p></li>
<li><p>每个记忆只包含少量注意力头的 key-value，从而大大减少了额外的计算、GPU 存储、磁盘存储和加载时间。</p></li>
<li><p>使得 LLM 能够频繁检索大量参考文档，而对解码速度影响较小。</p></li>
</ul>
</li>
</ul>
</section>
<section id="id24">
<h4><strong>3.2 训练目标</strong><a class="headerlink" href="#id24" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>希望通过显式记忆机制，使得 LLM 只需要学习抽象知识，而将具体知识外部化存储到记忆库中。</p></li>
<li><p>通过减少预训练的数据量，使训练成本与模型参数中存储的知识量成比例，从而更接近人类的学习效率。</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id25">
<h3><strong>3.1 | 推理过程</strong><a class="headerlink" href="#id25" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>参考文本定义</strong>：将可分离的知识实例称为“参考”（References）。</p></li>
<li><p><strong>知识库</strong>：由 1.1×10⁸ 个长度不超过 128 个 token 的文本块组成。</p></li>
<li><p><strong>显式记忆结构</strong>：每个参考可转换为一个张量，形状为 (22,2,8,8,80)，其中 2 表示 key 和 value，其他为层数、头数、稀疏 token 数和维度。</p></li>
<li><p><strong>推理阶段</strong>：</p>
<ul>
<li><p>所有参考在推理前被转换为显式记忆并存储在磁盘中。</p></li>
<li><p>推理时，当需要一个参考时，其对应的显式记忆被加载到 GPU，并用于计算。</p></li>
<li><p>由于每个参考在编码时不需关注其他文本，因此可以独立编码，减少注意力计算。</p></li>
<li><p>支持“冷启动”（首次检索即转换为记忆）和“热启动”（提前预处理）两种方式。</p></li>
</ul>
</li>
<li><p><strong>检索机制</strong>：</p>
<ul>
<li><p>使用向量搜索和余弦相似度来检索参考。</p></li>
<li><p>利用 BGE-M3 模型进行嵌入，FAISS 构建索引。</p></li>
<li><p>每生成 64 个 token，模型会丢弃旧记忆，检索 5 个新记忆。</p></li>
</ul>
</li>
<li><p><strong>内存缓存</strong>：</p>
<ul>
<li><p>使用固定大小的 RAM 缓存来存储最近使用的显式记忆，以减少磁盘加载时间。</p></li>
<li><p>相邻 chunk 常用相同参考，缓存有助于提升效率。</p></li>
</ul>
</li>
</ul>
<section id="id26">
<h4><strong>备注与优化</strong><a class="headerlink" href="#id26" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p><strong>备注2</strong>：理想情况下，应使用 LLM 自身的隐藏特征进行检索，因为这样更接近人类的内部检索机制。一种方法是使用稀疏注意力查询直接搜索显式记忆，无需微调模型。</p></li>
<li><p><strong>备注3</strong>：传统 RAG 方法中，参考多为文本块而非完整文档，导致缺乏上下文。Memory3 通过将完整文档编码后分块处理，使得 key-value 能够关注其上下文，从而克服这一问题。</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id27">
<h3><strong>3.2 | 写入与读取记忆</strong><a class="headerlink" href="#id27" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>显式记忆构成</strong>：每个记忆是参考文本编码后自注意力层中某些注意力头的 key-value 子集。</p></li>
<li><p><strong>读取方式</strong>：推理时，显式记忆通过自注意力层与常规上下文的 key-value 拼接后处理。</p></li>
<li><p><strong>公式说明</strong>：模型在读取记忆时，其注意力输出公式中引入了显式记忆的 key 和 value，与常规上下文 key-value 结合进行 softmax 计算。</p></li>
<li><p><strong>BOS 标记调整</strong>：在编码参考时，BOS 标记改为“⟨s⟩Reference:”，以帮助模型区分常规文本与参考文本。推理时该标记用于区分参考与上下文。</p></li>
<li><p><strong>并行位置编码</strong>：</p>
<ul>
<li><p>使用 RoPE（Rotary Positional Encoding）对所有显式记忆的 key 进行编码。</p></li>
<li><p>位置信息被保留，以便模型理解参考中 token 的相对位置。</p></li>
<li><p>通过并行位置编码避免“中间被忽略”现象（middle loss），并增强上下文学习和长上下文建模能力。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id28">
<h3><strong>总结</strong><a class="headerlink" href="#id28" title="此标题的永久链接">¶</a></h3>
<p>Memory3 的设计旨在通过将具体知识外部化为显式记忆，使得 LLM 能更专注于学习抽象知识，从而降低训练成本并提升推理效率。其核心特点包括：</p>
<ul class="simple">
<li><p>显式记忆机制，支持高效检索和读取；</p></li>
<li><p>尽量最小化对 Transformer 架构的改动；</p></li>
<li><p>使用缓存和并行位置编码优化检索和注意力处理；</p></li>
<li><p>支持“冷启动”与“热启动”推理模式；</p></li>
<li><p>通过参考文档的完整编码解决上下文缺失问题。</p></li>
</ul>
<p>该设计为实现“人脑式”学习提供了可行的模型结构基础。</p>
</section>
</section>
<section id="memory-sparsification-and-storage">
<h2>3.3 _ Memory Sparsification and Storage<a class="headerlink" href="#memory-sparsification-and-storage" title="此标题的永久链接">¶</a></h2>
<p>本节介绍了<strong>Memory3</strong> 模型中<strong>显式记忆的稀疏化与存储</strong>方法，旨在解决显式记忆占用存储和显存空间过大的问题。主要内容总结如下：</p>
<hr class="docutils" />
<section id="id29">
<h3>一、显式记忆的存储挑战<a class="headerlink" href="#id29" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>显式记忆的关键值对（attention keys-values）占用大量空间，不仅消耗磁盘存储成本，也在推理时占用 GPU 显存，影响批量大小和生成吞吐量。</p></li>
<li><p>需要对显式记忆进行<strong>高强度压缩</strong>，压缩维度包括：<strong>层（layers）、头（heads）、token 数量、头维度（head dimension）</strong>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id30">
<h3>二、各维度的稀疏化策略<a class="headerlink" href="#id30" title="此标题的永久链接">¶</a></h3>
<section id="layers">
<h4>1. 层（Layers）的稀疏化<a class="headerlink" href="#layers" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>仅将<strong>前半部分注意力层</strong>设置为<strong>记忆层（memory layers）</strong>，用于生成和访问显式记忆。</p></li>
<li><p>更合理的做法是将**中间层（如 25%~75% 深度范围）**设置为记忆层，因为这些层更可能关注远处的上下文。</p></li>
<li><p>支持这一选择的研究表明，中间层更常参与长距离注意力。</p></li>
</ul>
</section>
<section id="heads">
<h4>2. 头（Heads）的稀疏化<a class="headerlink" href="#heads" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>将每个记忆层的<strong>所有 key-value 头设为记忆头（memory heads）</strong>，然后通过**分组查询注意力（GQA）**将其数量减少（如从 40 个头压缩到 8 个头）。</p></li>
<li><p>也可以通过选择<strong>对读取记忆最有帮助的头部子集</strong>（非均匀选择）来进一步压缩。</p></li>
<li><p>提供了多种选择记忆头的方法，详见备注 4。</p></li>
</ul>
</section>
<section id="token">
<h4>3. Token 的稀疏化<a class="headerlink" href="#token" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>对每个 key-value 头，从 128 个 token 中<strong>选择前 8 个 token</strong>，因为参考文献显示注意力集中在少量 token 上。</p></li>
<li><p>选择依据是每个 token 被其他 token 注意力权重的总和。</p></li>
<li><p>排除 BOS token 和填充 token。</p></li>
<li><p>不使用因果掩码（causal mask）或位置编码，以保证所有位置 token 被公平考虑。</p></li>
<li><p>实现中使用了近似计算加快速度。</p></li>
</ul>
</section>
<section id="head-dimension">
<h4>4. 头维度（Head Dimension）的压缩<a class="headerlink" href="#head-dimension" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>可选地使用**向量量化器（vector quantizer）**对每个 key 和 value 向量进行压缩。</p></li>
<li><p>采用 FAISS 实现的<strong>残差量化</strong>，压缩比约为 11.4。</p></li>
<li><p>压缩后的显式记忆在推理前需先解压。</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id31">
<h3>三、压缩效果<a class="headerlink" href="#id31" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>原始显式记忆存储需求约 7.17PB（7340TB），压缩后为 45.9TB（或 4.02TB，含向量压缩），适合 GPU 集群存储。</p></li>
<li><p>评估表明，向量压缩对性能影响可忽略不计。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id32">
<h3>四、部署方式<a class="headerlink" href="#id32" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>在终端设备（如手机、笔记本）上部署时，可将<strong>显式记忆库和向量索引放在云服务器</strong>，设备端仅存储模型参数和向量解码器。</p></li>
<li><p>推理时设备将查询向量发送给服务器，服务器检索后返回压缩记忆。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id33">
<h3>五、补充说明与建议<a class="headerlink" href="#id33" title="此标题的永久链接">¶</a></h3>
<section id="id34">
<h4>备注 4：记忆头的选择方法<a class="headerlink" href="#id34" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>可通过微调预训练模型来选择记忆头。</p></li>
<li><p>一种方法是通过验证集测试每个头对长距离上下文性能的影响，选择对长距离注意力贡献最大的头。</p></li>
<li><p>通常这类头只占所有头的不到 10%。</p></li>
</ul>
</section>
<section id="id35">
<h4>备注 5：自适应稀疏化<a class="headerlink" href="#id35" title="此标题的永久链接">¶</a></h4>
<ul class="simple">
<li><p>每个参考文献可能只需一个头来处理长距离注意力，不同参考文献可能使用不同头。</p></li>
<li><p>可借鉴 MoE（Mixture of Experts）机制，为每个参考文献<strong>动态分配 1-2 个记忆头</strong>，以进一步提升稀疏度和节省空间。</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id36">
<h3>总结<a class="headerlink" href="#id36" title="此标题的永久链接">¶</a></h3>
<p>本节通过<strong>多维度的稀疏化策略</strong>（层、头、token、维度）大幅压缩显式记忆的存储需求，使其在实际应用中可行，并提出了多种有效且灵活的实现方式和优化思路。</p>
</section>
</section>
<section id="model-shape">
<h2>3.4 _ Model Shape<a class="headerlink" href="#model-shape" title="此标题的永久链接">¶</a></h2>
<p>本节内容主要围绕 <strong>Memory3 模型的设计</strong>，特别聚焦于 <strong>模型结构（Model Shape）</strong> 和 <strong>训练设计（Training Designs）</strong>，其核心思想是通过将具体知识外部化为显式记忆，来优化模型参数的使用，从而更高效地学习抽象知识。以下是总结：</p>
<hr class="docutils" />
<section id="id37">
<h3>3.4 | 模型结构（Model Shape）<a class="headerlink" href="#id37" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>设计目标：</strong></p>
<ul class="simple">
<li><p>由于具体知识可以外部化存储为显式记忆，模型参数（即隐式记忆）只需负责存储抽象知识和高频使用的具体知识。</p></li>
<li><p>由此，模型结构的设计目标是在固定的参数量（24 亿）下，最大化模型对抽象知识的存储能力与对具体知识存储能力的比值。</p></li>
</ul>
</li>
<li><p><strong>模型结构选择：</strong></p>
<ul class="simple">
<li><p>选择 Transformer 的层数 <span class="math notranslate nohighlight">\(L\)</span>、头数 <span class="math notranslate nohighlight">\(H\)</span>、每头维度 <span class="math notranslate nohighlight">\(d_h\)</span> 和 MLP 层宽度 <span class="math notranslate nohighlight">\(W\)</span>。</p></li>
<li><p>参考相关研究：</p>
<ul>
<li><p>具体知识的存储容量与参数量成正比，约为每个参数存储 2 bit。</p></li>
<li><p>抽象知识的学习能力则与模型的结构（尤其是层数和头数）相关。</p></li>
</ul>
</li>
<li><p>因此，模型应优先增加 <span class="math notranslate nohighlight">\(L\)</span> 和 <span class="math notranslate nohighlight">\(H\)</span>。</p></li>
</ul>
</li>
<li><p><strong>结构参数确定：</strong></p>
<ul class="simple">
<li><p>通过实验和控制变量（图10）发现，当 <span class="math notranslate nohighlight">\(L:H ≈ 1:1\)</span> 时，损失下降更快，因此采用此比例。</p></li>
<li><p>每头维度 <span class="math notranslate nohighlight">\(d_h\)</span> 不能太小（小于 64 时训练不稳定），最终设置为 80。</p></li>
<li><p>MLP 宽度 <span class="math notranslate nohighlight">\(W\)</span> 设置为 <span class="math notranslate nohighlight">\(H \times d_h = 40 \times 80 = 3200\)</span>。</p></li>
<li><p>采用分组查询注意力（GQA），设置键值头数 <span class="math notranslate nohighlight">\(H_{kv} = 8\)</span>。</p></li>
<li><p>最终模型结构为：<span class="math notranslate nohighlight">\(L=44, H=40, H_{kv}=8, d_h=80, W=3200\)</span>，总非嵌入参数为 2.4B。</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="training-designs">
<h3>3.5 | 训练设计（Training Designs）<a class="headerlink" href="#training-designs" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>训练目标：</strong></p>
<ul class="simple">
<li><p>由于具体知识已外部化，模型训练目标应转向学习抽象知识，从而减少训练计算资源需求。</p></li>
<li><p>这要求重新设计传统的预训练设置（如数据、初始化、正则化）。</p></li>
</ul>
</li>
<li><p><strong>数据选择：</strong></p>
<ul class="simple">
<li><p>应优先选择富含抽象知识、少含具体知识的数据。</p></li>
<li><p>研究表明，具体知识会抑制模型学习抽象能力。</p></li>
<li><p>类似 Phi-3 模型，选择强调推理、避免过多具体信息的数据。</p></li>
</ul>
</li>
<li><p><strong>参数初始化：</strong></p>
<ul class="simple">
<li><p>使用较小的标准差初始化参数，有助于模型学习组合推理而非记忆。</p></li>
<li><p>实验表明，小初始化鼓励模型在稀疏解中学习抽象结构。</p></li>
</ul>
</li>
<li><p><strong>权重衰减（Weight Decay）：</strong></p>
<ul class="simple">
<li><p>使用较大的权重衰减可以促进泛化而非记忆，有助于模型更快地掌握可泛化的解决方案。</p></li>
<li><p>生成模型需要更强的正则化，以防止分布塌缩到训练数据。</p></li>
</ul>
</li>
<li><p><strong>当前实践：</strong></p>
<ul class="simple">
<li><p>由于本工作是 Memory3 的初步版本，目前仍采用传统训练设置。</p></li>
<li><p>未来版本将尝试上述提到的优化设计，以进一步提升模型性能。</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id38">
<h3>总结<a class="headerlink" href="#id38" title="此标题的永久链接">¶</a></h3>
<p>3.4 节通过理论分析和实验验证，确定了最优的模型结构，使模型在有限参数下最大化抽象知识学习能力。<br />
3.5 节则从训练角度提出了优化方向，包括数据选择、初始化策略和权重衰减设置，旨在提升模型对抽象知识的学习效率与泛化能力。<br />
尽管目前尚未完全应用这些优化策略，但它们为 Memory3 模型的进一步发展提供了明确方向。</p>
</section>
</section>
<section id="two-stage-pretrain">
<h2>3.6 _ Two-stage Pretrain<a class="headerlink" href="#two-stage-pretrain" title="此标题的永久链接">¶</a></h2>
<p>该章节“3.6 | Two-stage Pretrain”主要介绍了 Memory3 模型在预训练阶段采用的“两阶段预训练”方法，具体包括以下内容：</p>
<hr class="docutils" />
<section id="id39">
<h3>一、预训练的两个阶段<a class="headerlink" href="#id39" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>Warmup 阶段</strong>（预热阶段）</p>
<ul class="simple">
<li><p>在该阶段，模型采用传统语言模型的预训练方式，<strong>不引入显式记忆</strong>。</p></li>
<li><p>作者发现，如果在预训练初期就引入显式记忆，模型可能不会有效利用这些记忆，反而导致训练损失不再下降。</p></li>
<li><p>图11a 显示，使用显式记忆的预训练模型（绿色曲线）在 warmup 阶段训练损失没有明显优势。</p></li>
</ul>
</li>
<li><p><strong>Continual Train 阶段</strong>（持续训练阶段）</p>
<ul class="simple">
<li><p>在此阶段，模型开始使用显式记忆机制。</p></li>
<li><p>从 warmup 阶段的检查点继续训练，加入显式记忆后，模型的训练损失明显下降，说明 warmup 是必要的。</p></li>
<li><p>图11b 显示，在 warmup 检查点基础上启用显式记忆（红色曲线），训练损失显著降低。</p></li>
</ul>
</li>
</ol>
<blockquote>
<div><p><strong>结论</strong>：Memory3 模型需要先进行一个不涉及显式记忆的 warmup 阶段，再进行带有显式记忆的 continual train 阶段，这种两阶段策略有助于模型逐步适应和有效利用显式记忆。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="continual-train">
<h3>二、对 continual train 的优化<a class="headerlink" href="#continual-train" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>内存共享机制</strong></p>
<ul class="simple">
<li><p>为减少 continual train 的计算开销，采用<strong>共享参考内容</strong>的方式。</p></li>
<li><p>每个 64-token 的 chunk 在训练时只检索一个参考（reference），但可以关注前四个 chunk 的 reference，这样每个 chunk 就可以利用多个参考，而不会显著增加计算量。</p></li>
<li><p>一个训练序列长度为 2048 token，分为 32 个 chunk，每个 chunk 对应一个 128-token 的 reference，共 4096 token 的 reference 输入。reference 的隐藏状态在通过最后一个记忆层后会被丢弃，以节省内存。</p></li>
</ul>
</li>
<li><p><strong>时间开销</strong></p>
<ul class="simple">
<li><p>continual train 每个步骤的时间大约是 warmup 的两倍。</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="id40">
<h3>三、防止信息泄露<a class="headerlink" href="#id40" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>问题</strong>：如果训练序列与其检索到的 reference 内容高度重合，模型可能只是“记忆”而不会真正学习。</p></li>
<li><p><strong>解决方案</strong>：</p>
<ul>
<li><p>不允许训练序列与 reference 有超过 90% 的重叠。</p></li>
<li><p>重叠度通过最长公共子序列（LCS）的长度占 reference 长度的比例来衡量。</p></li>
<li><p>公式（6）给出了具体计算 overlap 的方法。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id41">
<h3>总结<a class="headerlink" href="#id41" title="此标题的永久链接">¶</a></h3>
<p>本节提出 Memory3 模型的预训练采用“两阶段”策略：</p>
<ul class="simple">
<li><p>第一阶段（warmup）：传统预训练，不使用显式记忆；</p></li>
<li><p>第二阶段（continual train）：引入显式记忆机制，使用共享参考以减少计算开销；</p></li>
<li><p>同时，通过限制训练序列与 reference 的重叠，防止信息泄露，确保模型真正学习而不是“记忆”。</p></li>
</ul>
<p>这种结构化、分阶段的训练方式，有助于模型更有效地利用显式记忆，提高性能。</p>
</section>
</section>
<section id="pretraining-data">
<h2>4 _ Pretraining Data<a class="headerlink" href="#pretraining-data" title="此标题的永久链接">¶</a></h2>
<p>本节主要介绍了 <strong>Memory3 模型</strong> 的 <strong>预训练数据（Pretraining Data）</strong> 相关内容，包括数据收集、过滤、分词器（Tokenizer）以及知识库（Knowledge Base）的构建。</p>
<hr class="docutils" />
<section id="data-collection">
<h3><strong>4.1 数据收集（Data Collection）</strong><a class="headerlink" href="#data-collection" title="此标题的永久链接">¶</a></h3>
<p>预训练数据主要来源于 <strong>英文和中文文本数据集</strong>，包括网页、书籍、代码、有监督微调（SFT）数据和合成数据。具体如下：</p>
<ul class="simple">
<li><p><strong>英文数据</strong>：主要来自 RedPajamaV2、SlimPajama 和 The Piles，总计约 200TB（过滤前）。</p></li>
<li><p><strong>中文数据</strong>：主要来自 Wanjuan、Wenshu 和 MNBVC，总计约 500TB（过滤前）。</p></li>
<li><p><strong>代码数据</strong>：来自 GitHub，选取高星数（高受欢迎度）的代码仓库。</p></li>
<li><p><strong>SFT 数据</strong>：在预训练中作为普通文本处理，所有 token 都参与损失计算，而非仅答案 token。</p></li>
<li><p><strong>合成数据</strong>：用于补充数据多样性。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="filtering">
<h3><strong>4.2 数据过滤（Filtering）</strong><a class="headerlink" href="#filtering" title="此标题的永久链接">¶</a></h3>
<p>为了提高数据质量，采用了三步过滤流程：</p>
<ol class="arabic simple">
<li><p><strong>去重（Deduplication）</strong>：<br />
使用 <strong>MinHash</strong> 方法对大部分数据集进行去重，RedPajamaV2 数据集本身已提供去重标签。</p></li>
<li><p><strong>基于规则的过滤（Rule-based Filtering）</strong>：<br />
设计了一系列启发式规则，去除不适合训练的文本，例如：</p>
<ul class="simple">
<li><p>文档长度小于 50 词；</p></li>
<li><p>单词平均长度超过 10 个字符；</p></li>
<li><p>70% 以上的上下文为非字母字符；</p></li>
<li><p>唯一词比例异常高；</p></li>
<li><p>单字词熵值异常低等。</p></li>
</ul>
</li>
<li><p><strong>基于模型的过滤（Model-based Filtering）</strong>：<br />
使用一个微调后的 <strong>Tiny-BERT</strong> 模型，依据由 XinYu-70B 模型生成的“信息量”评分对数据进行筛选。Tiny-BERT 模型通过优化超参数训练，用于对整个数据集进行评分并选择高质量文本。</p></li>
</ol>
<blockquote>
<div><p><strong>备注</strong>：为了与 Memory3 的设计理念（强调抽象知识、减少具体知识）一致，未来版本将转向更注重推理性和减少具体信息的模型过滤策略。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="tokenizer">
<h3><strong>4.3 分词器（Tokenizer）</strong><a class="headerlink" href="#tokenizer" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>分词器支持中英文混合文本。</p></li>
<li><p>英文词汇基于 LLaMA2 的 32000 个 token。</p></li>
<li><p>中文词汇通过 BPE（Byte Pair Encoding）在 20GB 的中文新闻和电子书语料上训练。</p></li>
<li><p>最终字典包含 60,299 个 token。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="knowledge-base">
<h3><strong>4.4 知识库（Knowledge Base）</strong><a class="headerlink" href="#knowledge-base" title="此标题的永久链接">¶</a></h3>
<p>知识库在训练和推理中作为显式记忆的来源，其构建原则如下：</p>
<ul class="simple">
<li><p><strong>质量优先</strong>：包含英文维基百科、WikiHow、中文百科、学术书籍、中文新闻、高质量代码和合成数据。</p></li>
<li><p><strong>切片处理</strong>：所有文本被切分为 128 个 token 的片段。</p></li>
<li><p><strong>总量</strong>：最终知识库包含约 1.1 × 10⁸ 个参考片段。</p></li>
</ul>
<blockquote>
<div><p><strong>防止数据泄露</strong>：在评估过程中，若检索到的参考片段与问题重叠度过高（≥80%），则被过滤，确保评估公平性。阈值设为 2/3，基于重叠度分析设定。</p>
</div></blockquote>
<blockquote>
<div><p><strong>备注</strong>：当前知识库的构建依赖于人工偏好，未来计划采用模型导向的方法，通过验证集损失的减少来衡量参考片段的实际效果。</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="id42">
<h3><strong>总结</strong><a class="headerlink" href="#id42" title="此标题的永久链接">¶</a></h3>
<p>本节全面介绍了 Memory3 模型预训练数据的构建流程，包括数据的来源、过滤策略、分词器设置和知识库的构建。重点在于通过多步过滤和模型评估确保数据质量，同时兼顾模型训练效率与推理能力。方法上结合了去重、规则过滤和模型评分，体现了对数据质量与模型可学习性的双重重视。</p>
</section>
</section>
<section id="pretrain">
<h2>5 _ Pretrain<a class="headerlink" href="#pretrain" title="此标题的永久链接">¶</a></h2>
<p>本节总结了论文中关于预训练（Pretrain）过程的详细内容，主要包括预训练的两个阶段：<strong>预热阶段（Warmup Stage）</strong> 和 <strong>持续训练阶段（Continual Train Stage）</strong>，以及在训练过程中使用的技术设置和遇到的问题。</p>
<section id="id43">
<h3>1. 预训练总体设计<a class="headerlink" href="#id43" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>两阶段预训练</strong> 和 <strong>内存增强数据</strong> 的设计参考了论文第3.6节。</p></li>
<li><p><strong>预热阶段</strong> 主要提升模型的<strong>阅读理解能力</strong>，为后续<strong>内存形成</strong>做准备。</p></li>
<li><p><strong>持续训练阶段</strong> 引入<strong>显式记忆（Explicit Memory）</strong>，增强模型的外部存储和调用能力。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="set-up">
<h3>2. 训练设置（Set-up）<a class="headerlink" href="#set-up" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>训练框架</strong>：使用 Megatron-DeepSpeed，采用混合精度训练（bfloat16 参数和激活，float32 AdamW 状态）。</p></li>
<li><p><strong>批量大小</strong>：约 400 万训练 token，序列长度为 2048（不包括参考 token）。</p></li>
<li><p><strong>权重衰减</strong>：使用常见的 0.1。</p></li>
<li><p><strong>学习率调度</strong>：采用 MiniCPM 中的“预热-稳定-衰减”（warmup-stable-decay）学习率调度，优于常规的 cosine 调度，但在实际训练中因<strong>损失尖峰和发散</strong>问题，不得不<strong>手动降低学习率</strong>以稳定训练。</p></li>
<li><p><strong>训练数据量</strong>：原计划两个阶段均使用 4T token 的预训练数据，但因训练稳定性问题，实际训练提前终止。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="warmup-stage">
<h3>3. 预热阶段（Warmup Stage）<a class="headerlink" href="#warmup-stage" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>无显式记忆</strong>，仅训练基础语言建模能力。</p></li>
<li><p><strong>学习率调度</strong>：先线性上升，保持稳定，最后 10% 步骤快速衰减。</p></li>
<li><p><strong>训练问题</strong>：频繁出现<strong>损失发散</strong>，每次发散后需从最近的检查点恢复并<strong>降低学习率</strong>。</p></li>
<li><p><strong>训练终止</strong>：在约 3.1T token 时，即使降低学习率也无法避免发散，训练被迫终止。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="continual-train-stage">
<h3>4. 持续训练阶段（Continual Train Stage）<a class="headerlink" href="#continual-train-stage" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>引入显式记忆模块</strong>，模型在训练时实时将训练数据中的参考内容编码为显式记忆。</p></li>
<li><p><strong>训练速度下降</strong>：每一步训练时间约为预热阶段的 <strong>两倍</strong>。</p></li>
<li><p><strong>学习率调度</strong>：仍采用类似 warmup 阶段的调度方式，初始学习率较低以稳定训练。</p></li>
<li><p><strong>训练问题</strong>：在约 120B token 时出现<strong>不可挽回的损失发散</strong>，远低于原计划的 4T token。</p></li>
<li><p><strong>可能原因</strong>：该阶段从 warmup 阶段<strong>即将发散前的检查点初始化</strong>，模型本身已处于不稳定状态，即使降低学习率也仅能<strong>延缓发散</strong>。</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id44">
<h3>总结<a class="headerlink" href="#id44" title="此标题的永久链接">¶</a></h3>
<p>预训练阶段分为两个主要阶段：预热阶段用于提升模型基础能力，持续训练阶段引入显式记忆以增强模型的外部存储功能。尽管采用了优化的学习率调度和训练设置，但在实际训练中仍面临严重的<strong>损失发散问题</strong>，导致训练提前终止。这表明在引入显式记忆后，训练稳定性面临较大挑战，可能需要更细致的策略或更强的硬件支持以完成整个 4T token 的训练目标。</p>
</section>
</section>
<section id="fine-tuning-and-alignment">
<h2>6 _ Fine-tuning and Alignment<a class="headerlink" href="#fine-tuning-and-alignment" title="此标题的永久链接">¶</a></h2>
<p>本章主要介绍了 <strong>Memory3 模型的微调与对齐</strong> 过程，具体包括 <strong>监督微调（SFT）</strong> 和 <strong>直接偏好优化（DPO）</strong> 两个阶段。以下是总结：</p>
<hr class="docutils" />
<section id="supervised-finetuning-sft">
<h3>6.1 监督微调（Supervised Finetuning, SFT）<a class="headerlink" href="#supervised-finetuning-sft" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>数据来源</strong>：使用了多个公开的 SFT 数据集，包括 UltraChat、WizardLM、SlimOrca、ShareGPT、Capybara、Deita 和 MetaMathQA，这些数据集均来自 Hugging Face Hub。</p></li>
<li><p><strong>合成数据</strong>：此外还加入了人工生成的数据，重点涵盖多轮对话、数学、常识和知识领域。</p></li>
<li><p><strong>数据处理</strong>：每个样本包含一个或多个问答轮次，去除超过八轮的样本，最终组成总计约 70 万条样本的数据集（详见表 15）。</p></li>
<li><p><strong>训练设置</strong>：</p>
<ul>
<li><p>学习率调度：余弦退火（cosine schedule），最大学习率为 <span class="math notranslate nohighlight">\(5 \times 10^{-5}\)</span>，前 10% 是线性热身阶段；</p></li>
<li><p>权重衰减：0.1；</p></li>
<li><p>批大小：512；</p></li>
<li><p>最大序列长度：2048 个 token；</p></li>
<li><p>微调轮数：3 轮。</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="direct-preference-optimization-dpo">
<h3>6.2 直接偏好优化（Direct Preference Optimization, DPO）<a class="headerlink" href="#direct-preference-optimization-dpo" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>目的</strong>：进一步对齐模型与人类偏好，提升其对话能力。</p></li>
<li><p><strong>数据集</strong>：包括通用对话（UltraFeedback Binarized）、数学问题（Distilabel Math）和代码相关问题（Synth Code）。</p></li>
<li><p><strong>训练设置</strong>：</p>
<ul>
<li><p>学习率调度：余弦退火，最大学习率为 <span class="math notranslate nohighlight">\(4 \times 10^{-6}\)</span>；</p></li>
<li><p>DPO 损失中的逆温参数 <span class="math notranslate nohighlight">\(\beta\)</span> 设置为 0.01；</p></li>
<li><p>微调后的效果在 <strong>第 7.2 节（对话能力评估）</strong> 中有展示。</p></li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p><strong>总结</strong>：<br />
Memory3 模型通过监督微调和直接偏好优化两个阶段进行精细化调整，前者基于多样化的高质量数据集确保模型的通用能力，后者则专注于对齐人类偏好以提升对话质量。两阶段的设置在训练参数和数据选择上均做了详细优化，为模型的最终性能提供了坚实支持。</p>
</section>
</section>
<section id="evaluation">
<h2>7 _ Evaluation<a class="headerlink" href="#evaluation" title="此标题的永久链接">¶</a></h2>
<p><strong>章节总结：7. Evaluation（评估）</strong></p>
<p>本节对 Memory3 模型进行全面评估，涵盖其通用能力、对话技能、专业能力、事实性与幻觉控制，以及解码速度。评估对象包括 Memory3 与其他 SOTA 大型语言模型（LLMs）及基于 RAG（检索增强生成）的方法进行比较。</p>
<section id="id45">
<h3>7.1 通用能力评估<a class="headerlink" href="#id45" title="此标题的永久链接">¶</a></h3>
<p>使用 Huggingface leaderboard 上的任务以及两个中文任务，评估 Memory3 的通用能力。结果表明，尽管 Memory3 的模型参数仅为 2.4B，其平均性能优于许多更大参数规模的模型，如 Llama2-7B 与 Llama2-13B 的性能差异为 4.91%，而 Memory3 的性能提升达 2.51%，相当于增加了约 51% 的“有效模型规模”。此外，即使将显式记忆的向量压缩至原始大小的 8.75%，模型性能仍无明显下降。对比 Retro++ XXL（9.5B 参数）的零样本结果，Memory3 在多个任务上表现更优。</p>
</section>
<section id="id46">
<h3>7.2 对话能力评估<a class="headerlink" href="#id46" title="此标题的永久链接">¶</a></h3>
<p>在 MT-Bench 多轮对话基准测试中，Memory3 的表现优于多个主流模型，如 Vicuna-7B、Falcon-40B-Instruct 和 ChatGLM2-6B，尽管其模型参数较小（2.4B）。通过 DPO（直接偏好优化）微调，Memory3 的对话性能进一步提升。</p>
</section>
<section id="id47">
<h3>7.3 幻觉与事实性评估<a class="headerlink" href="#id47" title="此标题的永久链接">¶</a></h3>
<p>通过 TruthfulQA、HaluEval 和 HalluQA 等数据集测试幻觉问题。结果显示，Memory3 在多个任务上的得分高于其他主流模型，表明其显式记忆机制有助于减少幻觉，提高输出的事实准确性。尤其在中文任务中表现突出。</p>
</section>
<section id="id48">
<h3>7.4 专业任务评估<a class="headerlink" href="#id48" title="此标题的永久链接">¶</a></h3>
<p>Memory3 在法律和医学领域的专业任务中表现出色。通过引入领域相关文档作为显式记忆，模型无需微调即可快速适应新领域。在法律任务（JEC-QA）和医学任务（C-Eval、MMLU、CMMLU）中，Memory3 的性能优于多个基于 RAG 的方法，证明其在专业领域中的高效性和准确性。</p>
</section>
<section id="id49">
<h3>总结<a class="headerlink" href="#id49" title="此标题的永久链接">¶</a></h3>
<p>Memory3 凭借显式记忆机制，在通用能力、对话技能、幻觉控制以及专业任务表现上均优于或可比现有 SOTA 模型。其参数效率高，计算成本低，且在多领域适应性方面具有显著优势，展示了其在实际应用中的潜力。</p>
</section>
</section>
<section id="inference-speed">
<h2>7.5 _ Inference Speed<a class="headerlink" href="#inference-speed" title="此标题的永久链接">¶</a></h2>
<p>本章节主要评估了 <strong>Memory3</strong> 模型在推理速度（Inference Speed）方面的表现，并将其与传统的 RAG（Retrieval-Augmented Generation）模型进行比较，以衡量显式记忆机制在检索效率上的优势。</p>
<section id="id50">
<h3>主要内容总结如下：<a class="headerlink" href="#id50" title="此标题的永久链接">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>评估目标</strong>：</p>
<ul class="simple">
<li><p>评估 Memory3 的解码速度（以 tokens 每秒为单位）。</p></li>
<li><p>与基于文本检索的 RAG 模型比较，衡量 Memory3 使用显式记忆的加速效果。</p></li>
</ul>
</li>
<li><p><strong>实验设置</strong>：</p>
<ul class="simple">
<li><p>使用 A800 GPU，所有模型均启用 Flash Attention。</p></li>
<li><p>输入输出长度统一为 128 tokens，批量大小为 32。</p></li>
<li><p>Memory3 每 64 tokens 进行一次检索，平均每生成 64 tokens 进行 3 次检索。</p></li>
<li><p>实验涵盖本地服务器和边缘设备（Jetson AGX Orin）两种场景。</p></li>
</ul>
</li>
<li><p><strong>实验结果</strong>：</p>
<ul class="simple">
<li><p>表 21 展示了不同模型在本地服务器和边缘设备上的吞吐量（with/without retrieval）。</p></li>
<li><p>Memory3-2B 模型吞吐量约为 733 tokens/s（带检索），而相同模型不带检索时约为 1131 tokens/s，说明检索带来约 35.2% 的性能下降。</p></li>
<li><p>相较之下，其他模型如 Gemma、Llama、Qwen 等也在表中列出，供对比分析。</p></li>
</ul>
</li>
<li><p><strong>性能下降原因分析</strong>：</p>
<ul class="simple">
<li><p>Memory3 的速度下降主要归因于两点：</p>
<ol class="arabic simple">
<li><p><strong>从存储加载记忆键值对到 GPU 的开销</strong>：由于 Memory3 高频检索，此操作带来显著延迟。</p></li>
<li><p><strong>逐块注意力的 Python 实现</strong>：由于每个块需要关注不同的记忆，当前使用 for 循环处理，效率较低。</p></li>
</ol>
</li>
<li><p>实际计算中，嵌入向量计算和向量索引检索的时间开销相对较小。</p></li>
</ul>
</li>
<li><p><strong>优化方向</strong>：</p>
<ul class="simple">
<li><p>作者表示计划优化代码，例如用 CUDA 实现逐块注意力，以将与记忆相关的开销降低至理论值 0.228% 附近。</p></li>
</ul>
</li>
</ol>
</section>
<section id="id51">
<h3>总结：<a class="headerlink" href="#id51" title="此标题的永久链接">¶</a></h3>
<p>本节通过对 Memory3 和其他 LLM 模型在推理速度上的对比，揭示了显式记忆机制在提升模型性能的同时，也带来了额外的计算开销。研究指出当前性能瓶颈，并提出了可能的优化策略，为后续改进提供了方向。</p>
</section>
</section>
<section id="conclusion">
<h2>8 _ Conclusion<a class="headerlink" href="#conclusion" title="此标题的永久链接">¶</a></h2>
<p>本章总结了研究的主要目标、方法和实验成果，同时提出了未来的改进方向。</p>
<p><strong>研究目标与方法</strong>：<br />
本文旨在降低大型语言模型（LLM）的训练与推理成本，构建一个在性能上可媲美更大更慢模型但效率更高的LLM。研究通过“知识操作”的新视角分析模型，将模型成本视为“知识”在不同记忆格式之间传输的开销。研究识别出两个效率低下的原因：知识的放置不当和知识遍历问题，并通过引入一种新颖的记忆格式——<strong>显式记忆（explicit memory）</strong>，配合新的训练方案和架构予以解决。</p>
<p><strong>实验结果</strong>：<br />
初步实验模型 <strong>Memory3-2B</strong> 在性能和速度上超越了许多更大规模的SOTA模型以及基于RAG的模型，展示了显式记忆设计的有效性。</p>
<p><strong>未来工作方向</strong>：</p>
<ol class="arabic simple">
<li><p><strong>高效的抽象知识训练</strong>：通过筛选训练数据，最大化抽象知识并减少具体知识，使训练成本更接近人类的学习效率，理想情况下模型应能自行评估数据质量，忽略无用信息。</p></li>
<li><p><strong>类人能力的实现</strong>：利用显式记忆，模型可实现无限上下文处理、记忆巩固（显式转隐式记忆）和有意识推理等认知功能，从而提升推理能力和效率。</p></li>
<li><p><strong>显式记忆的紧凑表示</strong>：借鉴人类记忆的分类（情景记忆与语义记忆），当前模型更接近情景记忆，未来可尝试加入从情景记忆中归纳出的语义记忆，以增强推理能力。</p></li>
</ol>
<p><strong>工程改进方向</strong>：<br />
还包括若干工程优化方向，如稀疏注意力匹配、更稀疏的记忆头、完整保留上下文的记忆提取、基于机器偏好的知识库编译、减少显式记忆的时间消耗等。</p>
<p>总体而言，本文提出了一个通过显式记忆机制提高模型效率和能力的创新架构，并为未来的研究和优化提供了清晰的路线图。</p>
</section>
<section id="acknowledgement">
<h2>Acknowledgement<a class="headerlink" href="#acknowledgement" title="此标题的永久链接">¶</a></h2>
<p>本章节为致谢部分，内容总结如下：</p>
<p>该研究得到了中国国家自然科学基金委（NSFC）“可解释与通用的下一代人工智能”重大项目（编号：92270001）的支持。作者还感谢多位教授和研究人员在研究过程中提供的有益讨论，包括徐志强教授、林周涵教授、刘芳睿、杭良凯、陶子阳、王霄星、王铭泽、金永奇、何昊天、黄冠华、胡祎荣等。</p>
</section>
<section id="appendix-a-cost-estimation">
<h2>Appendix A Cost Estimation<a class="headerlink" href="#appendix-a-cost-estimation" title="此标题的永久链接">¶</a></h2>
<p>本章节 <strong>Appendix A: Cost Estimation</strong> 主要对 Memory3 模型中三种知识存储方式（<strong>隐式记忆</strong>、<strong>显式记忆</strong> 和 <strong>外部信息</strong>）的计算成本进行估算，并通过比较来展示显式记忆在成本上的优势。</p>
<section id="id52">
<h3>模型参数设定<a class="headerlink" href="#id52" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p>使用 2.4B 参数的 <strong>Memory3 模型</strong> 作为主干模型。</p></li>
<li><p>模型结构包括：</p>
<ul>
<li><p>Transformer 层数 <span class="math notranslate nohighlight">\( L = 44 \)</span></p></li>
<li><p>查询头数 <span class="math notranslate nohighlight">\( H = 40 \)</span>，键值头数 <span class="math notranslate nohighlight">\( H_{kv} = 8 \)</span></p></li>
<li><p>头维度 <span class="math notranslate nohighlight">\( d_h = 80 \)</span>，隐藏维度 <span class="math notranslate nohighlight">\( d = 3200 \)</span></p></li>
<li><p>词汇表大小 <span class="math notranslate nohighlight">\( n_{\text{vocab}} = 60416 \)</span></p></li>
<li><p>显式记忆层数 <span class="math notranslate nohighlight">\( L_{\text{mem}} = 22 \)</span></p></li>
<li><p>记忆长度 <span class="math notranslate nohighlight">\( l_{\text{mem}} = 8 \)</span>，参考长度 <span class="math notranslate nohighlight">\( l_{\text{ref}} = 128 \)</span>，块长度 <span class="math notranslate nohighlight">\( l_{\text{chunk}} = 64 \)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="implicit-memory">
<h3>隐式记忆（Implicit Memory）成本<a class="headerlink" href="#implicit-memory" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>写入成本</strong>：通过训练模型参数来存储知识，假设每个知识只需一次训练步。计算结果约为 <strong>2.24 TFlops</strong>。</p></li>
<li><p><strong>读取成本</strong>：由于隐式记忆的参数会在每次模型调用中使用，其读取成本难以准确估算，但这里下界设为 <strong>0 TFlops</strong>（为了显示其竞争力）。</p></li>
<li><p>总成本为 <span class="math notranslate nohighlight">\( c_{\text{implicit}}(n) \geq 2.24 \)</span>，与使用次数无关。</p></li>
</ul>
</section>
<section id="explicit-memory">
<h3>显式记忆（Explicit Memory）成本<a class="headerlink" href="#explicit-memory" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>写入成本</strong>：包括自注意力层、MLP 层和 token 稀疏化操作。计算结果约为 <strong>0.308 TFlops</strong>。</p></li>
<li><p><strong>读取成本</strong>：涉及块与记忆之间的注意力计算，计算结果约为 <strong>0.000144 × n</strong>，其中 <span class="math notranslate nohighlight">\( n \)</span> 为使用次数。</p></li>
<li><p>总成本为 <span class="math notranslate nohighlight">\( c_{\text{explicit}}(n) = 0.308 + 0.000144n \)</span>，随着使用次数增加而增长。</p></li>
</ul>
</section>
<section id="external-information-rag">
<h3>外部信息（External Information，如 RAG）成本<a class="headerlink" href="#external-information-rag" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>写入成本</strong>：为 <strong>0 TFlops</strong>，因为外部信息以纯文本形式存储。</p></li>
<li><p><strong>读取成本</strong>：考虑到引用文本对模型的影响，估算为 <strong>0.624 × n</strong>。</p></li>
<li><p>总成本为 <span class="math notranslate nohighlight">\( c_{\text{external}}(n) \geq 0.624n \)</span>，随着使用次数线性增长。</p></li>
</ul>
</section>
<section id="id53">
<h3>综合比较<a class="headerlink" href="#id53" title="此标题的永久链接">¶</a></h3>
<p>三种存储方式的成本函数分别为：</p>
<ul class="simple">
<li><p>隐式记忆：<span class="math notranslate nohighlight">\( c_{\text{implicit}}(n) \geq 2.24 \)</span></p></li>
<li><p>显式记忆：<span class="math notranslate nohighlight">\( c_{\text{explicit}}(n) = 0.308 + 0.000144n \)</span></p></li>
<li><p>外部信息：<span class="math notranslate nohighlight">\( c_{\text{external}}(n) \geq 0.624n \)</span></p></li>
</ul>
<p>通过比较可以发现：</p>
<ul class="simple">
<li><p>当知识的使用次数 <span class="math notranslate nohighlight">\( n \in (0.494, 13400) \)</span> 时，<strong>显式记忆是最优选择</strong>，其成本远低于隐式记忆和外部信息。</p></li>
</ul>
</section>
<section id="id54">
<h3>拓展讨论<a class="headerlink" href="#id54" title="此标题的永久链接">¶</a></h3>
<ul class="simple">
<li><p><strong>知识保留问题</strong>：模型参数更新（如微调）会导致隐式和显式记忆的丢失。为此，通常会在微调中保留部分预训练数据。</p></li>
<li><p><strong>研究方向</strong>：设计更鲁棒的架构，使记忆对模型更新具有更强的稳定性，是一个值得探索的方向。</p></li>
</ul>
<p><strong>总结</strong>：本附录通过详细的计算和比较，展示了显式记忆在成本上的显著优势，特别是在知识使用次数中等的场景下，显式记忆是更高效的选择。</p>
</section>
</section>
<section id="appendix-b-vector-compression">
<h2>Appendix B Vector Compression<a class="headerlink" href="#appendix-b-vector-compression" title="此标题的永久链接">¶</a></h2>
<p>本章节总结如下：</p>
<p>在附录B中，作者介绍了用于向量压缩的向量量化器（vector quantizer），其采用了FAISS库中的复合索引，索引类型为 <strong>OPQ20x80-Residual2x14-PQ8x10</strong>。该量化器能够将一个80维的bfloat16向量压缩为14维的uint8向量，压缩比约为11.4。</p>
<p>为了训练该量化器，作者从知识库中均匀独立地采样参考文本，并使用 <strong>Memory3-2B-SFT</strong> 模型将其编码为显式记忆（key-value向量），然后输入到量化器中进行训练。这种训练方式避免了对特定评估任务所检索参考内容的偏向性。</p>
</section>
<section id="appendix-c-supplementary-evaluation-results">
<h2>Appendix C Supplementary Evaluation Results<a class="headerlink" href="#appendix-c-supplementary-evaluation-results" title="此标题的永久链接">¶</a></h2>
<p>本附录补充评估了 Memory3-2B 模型在不同训练阶段和参数设置下的性能表现：</p>
<ol class="arabic simple">
<li><p><strong>不同训练阶段的性能变化</strong>（表22）：</p>
<ul class="simple">
<li><p>Memory3-2B 在三个训练阶段（warmup、continual train、SFT）的测试分数逐步提升。</p></li>
<li><p>例如，在英文任务中的平均得分从 warmup 阶段的 42.13 提升到 continual train 阶段的 45.12，最终在 SFT 阶段达到 63.31。</p></li>
<li><p>作者指出，若在 warmup 阶段减少损失差异，可以进一步提升 continual train 阶段的性能。</p></li>
</ul>
</li>
<li><p><strong>检索过滤阈值的影响</strong>（表23）：</p>
<ul class="simple">
<li><p>在评估任务中引入过滤机制以防止复制行为，过滤阈值设置为 2/3。</p></li>
<li><p>表23 显示不同阈值（如 no filter、80%、2/3）对测试分数的影响较小，说明大多数任务的问题未在知识库中重复出现。</p></li>
</ul>
</li>
<li><p><strong>few-shot 与 0-shot 模式的影响</strong>（表24）：</p>
<ul class="simple">
<li><p>在 few-shot 模式下，显式记忆带来的性能提升约为 2.51%。</p></li>
<li><p>在 0-shot 模式下，显式记忆的提升增加至 3.70%，尤其在 GSM8k 任务中提升显著（从 10.46% 提升至 13.50%）。</p></li>
</ul>
</li>
</ol>
<p>总结来看，Memory3 的性能在不同训练阶段和任务设定下表现出稳定的提升，且显式记忆在 0-shot 模式下更能发挥优势。</p>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="2505.14683.html" class="btn btn-neutral float-right" title="2505.14683_Emerging Properties in Unified Multimodal Pretraining" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="2405.16506_GRAG.html" class="btn btn-neutral" title="2405.16506_GRAG: Graph Retrieval-Augmented Generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>
  
  <div id="gitalk-container"></div>
  <div role="contentinfo">
    <p>
        &copy; Copyright 2010-2025, 新溪-gordon.

    </p>
  </div>
  <div>备案号 <a href="http://www.beian.miit.gov.cn">京ICP备16018553号</a></div><div>Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a></div>. 


</footer>

<script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?042289284b8eb33866001347a3e0b129";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
</script>     
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'V2025.07',
            LANGUAGE:'zh-CN',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../_static/clipboard.min.js"></script>
      <script type="text/javascript" src="../_static/copybutton.js"></script>
      <script type="text/javascript" src="../_static/translations.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="../None"></script>
      <script type="text/javascript" src="https://fastly.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });


      // var gitalk = new Gitalk({
      //         clientID: '565177626b5d46427009',
      //         clientSecret: 'b2a36e67e1d2a73e43667f46d571c2624f8e1026',
      //         repo: 'knowledge',
      //         owner: 'zhaoweiguo',
      //         admin: ['zhaoweiguo'],
      //         id: location.pathname,      // Ensure uniqueness and length less than 50
      //         distractionFreeMode: false  // Facebook-like distraction free mode
      //       })
      // gitalk.render('gitalk-container')

  </script>


<script type="text/javascript" src="../_static/js/table-of-contents-sidebar.js"></script>
<!-- <script type="text/javascript" src="https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/table-of-contents-sidebar.js"></script> -->
<script type="text/javascript">
    window.onload = function(e){
        TableOfContents.init({
            basePath: "https://table-of-contents-sidebar.github.io/table-of-contents-sidebar-lib/",
            querySelector: "body" // or other css querySelector
        });
    }
</script> 

</body>
</html>