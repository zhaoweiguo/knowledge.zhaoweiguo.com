2209.08199_ScreenQA: Large-Scale Question-Answer Pairs over Mobile App Screenshots
##################################################################################

* https://arxiv.org/abs/2209.08199
* 组织: Google
* GoogleScholar(star: 28)
* GitHub: https://github.com/google-research-datasets/screen_qa


Abstract
========

* ScreenQA：一种新颖的基准测试数据集，旨在通过问答来推动屏幕内容理解。
* 现有的屏幕数据集专注于低水平的结构和组件理解，或者是特别高级别的复合任务，例如自主代理的导航和任务完成。ScreenQA 试图填补中间的空缺。
* 通过在 RICO 数据集上注释 86K 的问题回答，我们的目标是基于屏幕阅读理解能力，从而为基于屏幕截图的基于视觉的自动化奠定了基础。
* 我们的注释包含完整答案，简短的答案短语和带有边界框的对应的 UI 内容，从而使四个子任务能够解决各种应用程序方案。
* 我们在zero-shot，微调和转移学习设置中使用开放量和专有模型评估数据集的功效。我们进一步证明了对 Web 应用程序的积极转移，突出了其超出移动应用程序的潜力。

* 本章介绍了 ScreenQA，一个用于 屏幕内容理解 的 问答数据集，并强调其创新性和用途。

1. ScreenQA 的目标  
    - ScreenQA 是一个 基准测试（benchmarking）数据集，旨在通过 问答任务 提高对 屏幕内容（screen content） 的理解能力。  
    - 现有的屏幕数据集要么关注 低级别的结构和组件理解，要么涉及 更高级别的任务（如自动化导航和任务完成），缺少中间层面的内容理解。ScreenQA 试图弥补这一空白。  

2. 数据集构建  
    - 基于 RICO 数据集（一个广泛使用的 UI 界面数据集），标注了 86,000 组问答数据。  
    - 这些标注内容包括：  
        - 完整答案（full answers）  
        - 短答案（short answer phrases）  
        - 对应 UI 内容的边界框（bounding boxes）  

4. 评估方式  
    - 研究人员使用 开源模型（open-weight models） 和 专有模型（proprietary models） 进行评测，采用三种方式：  
        - 零样本学习（zero-shot）：直接测试模型的泛化能力，无需额外训练。  
        - 微调（fine-tuning）：在 ScreenQA 数据集上进一步训练模型，以提高任务特定能力。  
        - 迁移学习（transfer learning）：在一个数据集上训练，然后应用到另一个相关任务。  

5. 实际应用场景  
    - 研究表明，ScreenQA 不仅适用于移动应用（mobile applications），还可以 迁移到 Web 应用（web applications），说明其在 基于截图的视觉自动化（vision-based automation） 方面有广泛的潜力。  

.. note:: 【总结】ScreenQA 是一个面向 屏幕内容理解 的 问答数据集，提供 86k 组问答对，并支持多个子任务。它填补了低级 UI 结构理解和高级任务自动化之间的空白，可用于评估和训练 AI 在屏幕截图上的阅读理解能力，并有望应用于 网页和移动端的自动化任务。


1. Introduction
===============

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/jItqrq.png

    Figure 1: ScreenQA examples.

a) Four corresponding tasks (Section 3): 
    1) Short answer (SQA-S): "1,1". 
    2) Long answer (SQA-L): "There is 1 like and 1 comment". 
    3) UI Content (SQA-UIC): [“1”, “1”]. 
    4) UI Content w/ Bounding Boxes (SQA-UIC-BB): also with bounding box coordinates (in green).
b) Both high and low temperatures in two Saturdays are relevant answers. 
c) Lacking the content to answer the question.




* 研究背景
    - 近年 VLM（视觉大语言模型） 和 MLLM（多模态大模型） 的进步，推动了许多基于移动屏幕的 AI 应用，
    - 比如：
        - 语音助手（支持无手操作）
        - UI 代码自动生成
        - 自适应界面
        - 自动广告创作
        - 移动应用测试

* 现有研究的局限：
    - 虽然已有研究从不同角度分析移动应用截图（比如 UI 结构分析、按钮预测、屏幕分类等），但 直接从像素级别理解屏幕内容 仍然是个未充分研究的领域。
    - 现有方法往往依赖 View Hierarchies（VHs，UI 结构数据），但这些数据并不总是可靠或可用的。

* ScreenQA 数据集核心特点：
    - 直接使用 像素 作为 UI 屏幕的表示，不依赖 VHs。
    - 以 RICO 数据集（包含 66k 个截图）为基础，标注了 85,984 组问答对。
    - 设定了 四类任务，并定义了评估指标。
* 为什么选择 RICO 作为基础？
    1. 数据量大（66k 个截图）
    2. 应用种类广（覆盖 9.3k 个应用，27 个类别）
    3. 已有众多基准任务（便于对比和复用）
    4. 在学术界和工业界应用广泛

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/HQSFmp.png

    Table 1: The RICO dataset remains the most suitable mobile app screenshot collection for this work due to its diversity, large scale, and established presence in the field. Although AitW is larger in scale, it lacks app and screenshot diversity due to its sampling method.


* 总结
    - ScreenQA 是一个针对 移动应用 UI 屏幕 的大规模问答数据集，填补了从 像素级别 理解屏幕内容的空白。它提供：
        1. 85k+ 问答对（基于 RICO 数据集）
        2. 四种任务（短/长答案、UI 元素、边界框）
        3. 高质量基准（支持零样本、微调、跨领域学习）



2. Related Work
===============

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/kYxD7f.png

    Table 2: Comparison of ScreenQA with Related Datasets. ScreenQA is the largest QA dataset for mobile screenshots, using entire screenshots rather than cropped answer regions (as in WebSRC) to better represent real-world applications, and including unanswerable questions and bounding boxes. ComplexQA complements ScreenQA with its focus on counting, arithmetic, and comparisons. Not an exhaustive comparison due to space constraints.


2.1 Multimodality
-----------------

* 主要讨论了基于屏幕UI的理解（Screen UI-based understanding）、视觉文档理解（Visual Document Understanding）和封闭领域问答（Closed-domain Question Answering）这三个方向。

1. 基于屏幕UI的理解（Screen UI-based Understanding）
    - 屏幕UI（用户界面）与自然图片不同，它是设计出来用于传递信息和执行操作的。
    - 主要强调AI如何理解UI并用于自动化交互，例如智能浏览器自动化、AI驱动的软件操作等。
    - 相关研究主要关注两个方面：
        - 信息性（Informativeness）：
            - 识别UI元素，如图标检测、组件（Widget）描述等（Deka et al. 2017, Li et al. 2020b）。
            - 通过指代表达（Referring Expression）识别UI组件，在分类、表示学习和生成方面的应用（Wu et al. 2023, Bai et al. 2021）。
        - 可操作性（Actionability）：
            - 主要涉及任务完成和自主智能体（AI代理）。
            - MoTIF、VisualWebArena 和 Android-in-the-Wild 提供交互式的应用环境，用于评估基于视觉的屏幕代理（screen agents）。


2. 视觉文档理解（Visual Document Understanding）
    - 这部分涉及对扫描或拍摄的文档的理解
    - 这部分与OCR（光学字符识别）增强的AI模型相关，可用于自动阅读发票、合同、报表等结构化或半结构化文档。
    - 重点在于：
        - 提取关键信息：
            - DocVQA（Mathew et al. 2021）用于提取文档中的特定内容，类似于问答系统（QA）。
            - TextVQA（Singh et al. 2019）和其他数据集研究如何将文本的二维布局与语义关联，即让AI理解文本的位置和内容的含义。
        - 信息图表（Infographics）理解：
            - 研究如何解析图表、图示（如折线图、柱状图等）以及周围的文本（Mathew et al. 2022, Kahou et al. 2017）。


2.2 Question Answering
----------------------


3. 封闭领域问答（Closed-domain Question Answering）
    - 封闭领域问答指基于特定语境回答问题（而非开放式的搜索引擎问答）
    - 这部分强调AI如何在特定场景中回答问题，如客服AI、法律文档问答、企业内部知识库搜索等。
    - 包含：
        - 不同答案类型：
            - Span（从文本中提取一段内容，如SQuAD数据集）。
            - 实体（识别答案是某个命名实体，如人名、地名）。
            - 多项选择（AI需要选择正确答案）。
            - 生成式问答（AI自由生成答案）。
        - 不同推理能力：
            - 阅读理解（AI从文本中直接找答案）。
            - 多跳推理（Multi-hop Reasoning）（AI需要综合多个信息片段才能得出答案）。
            - 逻辑推理（要求AI基于逻辑关系进行推理）。
            - 常识推理（结合现实世界的知识回答问题）。

总结
----

1. Screen UI-based Understanding 关注AI如何理解UI界面，用于自动操作和交互。
2. Visual Document Understanding 研究AI如何解析文档、表格、图表，提升信息提取能力。
3. Closed-domain QA 讨论AI如何在特定场景下精准回答问题，涉及推理和知识整合能力。




3. Problem Setting: Tasks and Metrics
=====================================

* 主要介绍了ScreenQA任务的问题设置（Problem Setting），包括评估指标和四种任务类型。
* 整体来看，ScreenQA 是一个面向移动应用界面的问答数据集，对自动 UI 解析、智能问答、屏幕阅读器等技术有很大帮助。



评估指标(Evaluation Metrics)
----------------------------

* 由于验证集和测试集的每个示例可能有多个标注者提供的不同正确答案（Ground Truths），因此评估方式如下：

    1. 计算每个示例的最佳匹配值：对于每个问题 \( i \)，计算其所有标注答案中与模型预测答案 \( A_i \) 最匹配的得分（取最大值）。
    2. 计算所有示例的平均值：对整个数据集的所有问题进行平均。

公式如下：

.. math::

    \text{avg(metric)} = \frac{1}{N} \sum_{i=1}^{N} \max_{j} \left[ \text{metric}(A_i, A^g_{i,j}) \right]

* 其中：
    - :math:` N` 是数据集中问题的总数。
    - :math:` A_i` 是模型对第 i 个问题的预测答案。
    - :math:` A^g_{i,j}` 是第 i 个问题的第 j 个正确答案。
    - :math:`\max_{j}` 确保对于每个问题，选择最符合的 ground truth 进行评估。

* 这个评估方式考虑到同一个问题可能有多个合理答案，因此不会因为答案措辞不同而过于苛刻。


ScreenQA 任务类型
-----------------

* ScreenQA（简称 SQA）涉及四个任务，涵盖不同的应用场景：

1) ScreenQA Short Answer (SQA-S)
    * 任务目标：
        - 给定屏幕截图和问题，输出简短的答案。
        - 如果屏幕中没有答案，则输出 `<<<no answer>>>`。
    * 评估指标：
        - Exact Match (EM)：答案必须与 ground truth 完全匹配。
        - F1-Score：允许词序变化或同义表达的匹配方式，提高鲁棒性。
    * 核心应用：
        - QA 问答系统（如手机助手）。
        - 适用于精确的 UI 内容查找。

2) ScreenQA Long Answer (SQA-L)
    - 任务目标：
        - 与 SQA-S 相同，但要求输出完整句子，而非简短答案。
    - 评估指标：
        - ROUGE-{1,2,L}：评估生成的长文本是否涵盖 ground truth 的主要信息。
    - 核心应用：
        - 虚拟助手：如 Siri、Google Assistant、智能客服等。
        - 信息摘要：要求回答更具可读性和连贯性。

3) ScreenQA UI Content (SQA-UIC)
    - 任务目标：
        - 给定屏幕截图和问题，输出包含答案的 UI 元素列表，每个 UI 元素用文本表示。
        - 如果没有答案，则返回空列表。
    - 评估指标：
        - Exact Match & F1-score：匹配 UI 元素的文本内容。
    - 特殊性：
        - 与 OCR 识别不同，这里的输出是UI 元素级别的列表，而非连续的文本。
        - 适用于从屏幕 UI 解析信息（如按钮、菜单、标签等）。
    - 核心应用：
        - 自动化 UI 解析：如无障碍辅助、自动化测试。

4) ScreenQA UI Content with Bounding Boxes (SQA-UIC-BB)
    - 任务目标：
        - 与 SQA-UIC 相同，但输出每个 UI 元素的边界框（Bounding Box），用于定位答案位置。
    - 评估指标：
        - IoU（Intersection over Union）：边界框匹配度大于 0.1 视为正确匹配。
        - Exact Match & F1-score：文本内容仍需匹配。
    - 核心应用：
        - 屏幕内容自动操作（如点击、滑动）。
        - 无障碍辅助技术（如屏幕阅读器）。
        - 智能 UI 解析（如自动 UI 测试）。


总结
----

- 评估方式采用最大匹配策略，确保每个预测答案都能与最合理的 ground truth 对比。
- ScreenQA 包含 4 个任务，分别针对短答案、长答案、UI 元素、UI 元素+边界框，适用于不同的 AI UI 解析应用。
- 评估指标：短答案使用EM & F1，长答案用ROUGE，UI 解析用F1 & IoU。




4. Data Annotation
==================

* 本章描述了 ScreenQA（一个针对移动应用截图的问答数据集）的标注过程，共分为 五个阶段，每个阶段有不同的目的和操作方法
* 总结-这五个阶段确保了 ScreenQA 数据集的高质量：
    - 第一阶段（预筛选）清理了低质量数据。
    - 第二阶段（问题标注）确保问题多样性和针对性。
    - 第三阶段（答案标注）提供准确答案，并处理不同的 UI 结构。
    - 第四阶段（无法回答问题）提高模型对 拒答问题 的适应能力。
    - 第五阶段（短答案生成）提高答案的易读性和一致性。


.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/Q0cCxw.png

    Table 3: Counts of distinct screenshot (SS) and questions (Q) for annotation stages and data splitting.


1. 预筛选（Prefiltering）
    - 目的：清理数据，确保数据质量。
    - 操作：
        1. 移除非英语应用的截图，避免语言干扰。
        2. 移除视图层次结构（View Hierarchies, VH）不同步的截图，以减少 UI 结构错误带来的噪音。
    - 结果：
        - 由 27 名标注员完成筛选。
        - 允许 UI 过渡时的遮挡或重影（只要主要内容区域可读）。
2. 问题标注（Question Annotation）
    - 目的：生成与截图内容相关的问题，模拟用户查询。
    - 操作：
        1. 每张截图 最多创建 8 个问题，采用 两步标注法：
            - 第一位标注员创建最多 5 个问题。
            - 第二位标注员在已有问题基础上，补充最多 3 个问题。
        2. 限制问题类型：
            - 只基于屏幕上直接可见的信息。
            - 排除推理、计算、广告相关问题，以避免受大模型（LLM）推理能力的影响。
        3. 屏幕内容贫乏的截图（如登录页、广告）可跳过。

3. 答案标注（Answer Annotation）
    - 目的：为每个问题提供正确答案，并确保答案格式一致。
    - 操作：
        1. 修正问题的语法错误。
        2. 提供答案：
            - 标注 答案所在的 UI 元素的边界框（Bounding Box）。
            - 按阅读顺序排列边界框，确保答案可读性。
            - 提供完整的长文本答案（long answer）。
        3. 特殊情况：
            - 如果问题不合理，标记为 "无效问题"。
            - 如果截图无法回答该问题，标记为 "无法回答"。
        4. 训练集每个问题只标注 1 个答案，但验证集和测试集 每个问题标注 3 个答案，提高评估质量。

4. “无法回答” 问题标注（Not-Answerable Question Annotation）
    - 目的：增加数据集中 无法回答的问题，以测试模型在拒答能力上的表现。
    - 操作：
        1. 在现有数据集中，随机抽取 5000 张截图，让标注员添加 1 个无法回答的问题。
        2. 确保 至少 10% 的问题是无法回答的。

5. 生成短答案（Short Answer Generation）
    - 目的：统一短答案格式，使其易于阅读，并处理 UI 设计带来的碎片化问题。
    - 操作：
        1. 后处理：
            - 由于 UI 设计可能把一个答案拆分到多个元素中
            - （如 `Oct. 15, 2024` 被拆成 `"Oct."`、`"15"`、`"2024"`），需合并成可读短答案。
        2. 处理答案的不同表示方式：
            - 处理 数字 vs. 文字（如 `3` vs. `three`）。
            - 处理 日期格式（如 `Oct. 15, 2024` vs. `2024-10-15`）。
            - 处理 可选单位（如 `3 reviews` vs. `3`）。
        3. 使用大模型生成短答案：
            - 使用 PaLM 2（Google 训练的大型语言模型）。
            - 采用 few-shot prompting 方式（给定一些示例，让模型学习如何生成答案）。
            - 部分答案由人工验证，确保质量。


5. Dataset Analysis
===================

* 主要分析了 ScreenQA 数据集 的数据划分、问题类型分布，以及答案注释的特点。

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/lpwg8g.png

    Table 4: Top question category distribution (≥ 2.0%) and examples (See Appendix C for remaining categories).



* 数据集划分（Dataset Splitting）
    - ScreenQA 数据集 共包含 85,984 个问题，来源于 35,352 张独立的截图。
    - 数据按照截图进行划分，训练集、验证集和测试集的比例约为 80-10-10。

* 问题分析（Question Analysis）
    1. 问题类别分布
        - 数据集中 83.8% 的问题 属于以下主要类别（见表 4）
        - 由于数据采集方式的影响，某些类别占比较高。例如：
            - 登录页面 常出现在 RICO 爬取过程中，因此“应用名称”和“邮箱地址”相关问题较多。
    2. 问题重复度
        - 在 86k 个问题中，有 47.5k 是唯一的（基于 SQuAD EM 评测）。
        - 许多问题会在不同的截图中反复出现，比如：
            - “哪个选项被选中了？”
            - “邮箱地址是什么？”
        - 信息量较多的截图往往会产生更多问题。
        - 截图的问题数量分布呈现轻微的指数衰减（即，大多数截图只有少量问题，少数截图有很多问题）。

* 答案分析（Answer Analysis）
    1. 答案的标注方式
        - 主要分析了答案标注的两个维度：
            1. 是否需要多个框（bounding boxes）来标注答案（复杂度）
            2. 是否能直接使用视图层级（VH）中的 UI 叶节点来标注答案（VH 的可靠性）
        - 84% 的答案只需要一个框，其中：
            - 51% 直接使用 VH 叶节点
            - 49% 需要手动绘制框
        - 当所有答案考虑在内时：
            - 51% 仅使用 VH 叶节点
            - 48% 需要手动框选
            - 0.8% 需要多个框组合
        - 组合框通常出现在 多部分答案（如日期“Oct. 15, 2024”）或 包含单位的数值（如“3 reviews”）。
    2. VH 视图层级的局限性
        - VH 不能正确捕获 WebView 和 Canvas 内的 UI 元素，并且在某些屏幕设计中表现不一致。
        - 由于 VH 的这些局限性，研究团队决定使用截图作为 ScreenQA 的主要输入，而不是仅依赖 VH 数据。

* 总结
    1. ScreenQA 数据集的划分 确保了训练、验证和测试的合理性。
    2. 问题主要围绕 UI 操作、数值、时间、价格等，其中 UI 选择（18.1%）是最常见的问题。
    3. 答案标注中，约一半可以直接使用 VH 叶节点，另一半需要手动框选，这说明 VH 并不完全可靠，因此采用截图作为主要数据输入。




6. Experiments and Baselines
============================

* 主要描述了 ScreenQA 数据集上的实验，包括 零样本测试、微调实验、跨领域与迁移学习 以及 多模态与微调的重要性。
* 零样本实验 (Zero-Shot Experiments)
    - 测试模型：研究人员测试了 Fuyu-8B、Gemini 1.5 Flash、Gemini 1.5 Pro 和 GPT-4o 这些大模型。
    - 测试方法：使用相同的提示词（prompt）对模型进行零样本测试（zero-shot，即不微调模型，直接测试）。

* 微调实验 (Fine-Tuning Experiments)  
    - ScreenAI 5B 比 PaliGemma 3B 表现更好，因为它 模型更大、预训练时包含更多 UI 数据。
    - ScreenAI 5B 与 Gemini 1.5 Flash 表现相当。
    - Bounding Box 任务上 ScreenAI 表现更好，因为它是专门为 UI 设计的。
    - PaliGemma 通过微调语言 + 视觉部分，效果提升明显，说明充分利用模型能力很重要。

* 跨领域学习 (CDL) & 迁移学习 (Transfer Learning)
    - 相近领域数据有一定迁移性，但仍然不能完全替代本领域数据。
    - Bounding Box 任务对问答能力没有明显帮助，可能是因为 UI 识别任务与自然语言问答的关联性不够强。

* 多模态 & 微调的重要性
    - 结果：多模态输入远胜于 OCR 纯文本，说明屏幕理解需要结合视觉信息。
    - 另一个发现：零样本 vs. 微调
        - 结果显示：微调后效果大幅提升，说明 屏幕理解是一个特殊领域，不能直接用通用视觉模型来处理。

👉 解读：
    - 屏幕问答任务不仅需要文字内容，还需要视觉上下文（例如 UI 结构、颜色、按钮布局等）。
    - 多模态（图像 + 文字）是关键，单纯依赖 OCR 提取的文本会丢失重要信息。
    - 由于屏幕问答是一个特殊任务，必须经过微调才能取得好效果。

* 总结
    - 这部分主要讲了 大模型在屏幕问答（ScreenQA）任务上的实验：
        1. 零样本实验：Gemini 1.5 Pro > GPT-4o，表明模型本身能力比 prompt 设计更关键。
        2. 微调实验：ScreenAI 5B 在 UI 理解任务上表现突出，PaliGemma 3B 通过训练语言 & 视觉部分效果也很好。
        3. 跨领域 & 迁移学习：
            - 相关领域（如文档问答）数据有一定帮助，但比不上本领域训练数据。
            - Bounding Box 任务的迁移效果有限。
        4. 多模态 vs. 纯文本：
            - 仅靠 OCR 解析文本效果很差，屏幕理解必须结合 图像 + 文本。
            - 微调对于屏幕理解任务至关重要。

    - 这个研究强调了 屏幕问答是一个特殊的 AI 任务，不能直接用现有的通用视觉或语言模型来处理，而是需要专门优化和微调。


7. Conclusion
=============

* 主要总结了 ScreenQA 这个数据集的贡献、独特性以及未来的发展方向。

* ScreenQA 数据集的介绍  
    - 这是一个用于 屏幕内容问答（QA） 任务的数据集。  
    - 数据集共包含 85,984 组问题-答案对。  
    - 除了提供答案外，数据集还包含 丰富的 UI 元素标注，可以用于训练或评估模型的 屏幕理解能力，这对于 UI 界面的高级推理和自动化至关重要。

* ScreenQA 任务的独特挑战  
    - 文本丰富：屏幕内容通常包含大量文字。  
    - 应用多样性：数据涵盖不同的移动应用，场景复杂。  
    - 符号与图标混合：需要处理 UI 组件，如按钮、图标、交互式元素，而不仅仅是纯文本。  
    - 任务要求更高：不仅评估 内容理解（QA），还考察 UI 元素识别能力，即是否能正确理解屏幕结构和元素位置。

.. note:: [总结]ScreenQA 是一个专门针对 屏幕内容问答 设计的数据集，不仅测试文本理解能力，还涉及 UI 组件的识别与推理。研究表明，该领域比传统的视觉-语言任务（如文档理解、VQA）更复杂。研究者希望这个基准能推动 更智能的 UI 理解技术，最终提升 自动化和人机交互体验。



8. Limitations
==============


1. 语言（Language）  
    - 数据集目前 仅支持英文，尚未涵盖其他语言。  
    - 要创建多语言版本，需要收集不同地区（locales）手机设置下的截图，这增加了复杂性。  

2. 复杂布局（Rich Layouts）  
    - 目前数据集主要针对 静态内容，专注于标准化（canonical）的 UI 布局，以便于信息提取。  
    - 但它未能涵盖：
        - 复杂的 UI 交互（如游戏应用的动态界面）。  
        - 包含自然图片和视频的屏幕内容。  

3. 推理能力（Reasoning）  
    - 数据集主要用于 信息查找（look-up）和内容重写（rewriting），不涉及：
        - 数学运算  
        - 复杂的组合推理（compositional reasoning）  
    - 也就是说，它更适合 UI 结构和内容的理解，而非高阶逻辑推理。  

4. 多图像（Multi-Image）  
    - 每个样本仅包含 单个截图，因此不支持：
        - 跨多个截图的追踪（trace of screenshots）。  
        - UI 动画或滚动行为（scrolling actions）。  

5. 平台限制（Platform）  
    - 数据集 仅包含 Android 设备截图，没有涵盖 iOS 或其他设备。  
    - 由于问题主要基于屏幕主要内容（main content area），而 不涉及手势或交互操作，研究者认为不同操作系统间不会有太大性能差异。  
    - 研究表明，ScreenQA 还能促进其他任务的模型迁移学习（如 VisaulWebBench-WebQA）。  

9. Ethical Considerations
=========================

1. 仅用于信息检索（Information Retrieval Only）  
    - ScreenQA 仅用于 屏幕内容的信息检索，旨在提高 屏幕内容理解能力。  
    - 它不会用于 自动决策（decision-making），也不会 代表用户执行操作，从而避免潜在风险。  

2. 隐私保护（Privacy）  
    - ScreenQA 需要访问屏幕内容，权限类似于 辅助功能（accessibility applications）。  
    - 由于部分 ScreenQA 训练的模型可在本地设备运行，这有助于减少隐私风险（无需上传数据到服务器）。  

3. 应用类别公平性（App Category Fairness）  
    - 数据集涵盖 27 类移动应用，是目前多样性最高的数据集之一。  
    - 研究表明，微调后的 PaliGemma 896 模型 在不同类别应用上的 SQuAD-F1 评分保持一致，说明数据集具有 公平性（fair support），不会偏向特定类型的应用。  


A. Data Annotation Details
==========================

* 主要讲解了 数据标注的细节，包括标注规则、质量控制、后处理方法，以及如何生成短答案。


A.1 VH Out-of-Sync Rules
------------------------

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/agEAbk.png

    Figure 4: View hierarchies (VHs) are overlaid on the screenshots with class names and the first few characters printed to assist annotators to determine whether the VHs for the main contents are in sync.

* VH（View Hierarchy）指的是 UI 界面元素的层级结构。
* 在标注过程中，如果截图的 VH 和界面显示的内容不同步（out-of-sync），就需要标记出来。
* 主要有以下几种情况：
    1. UI 元素被遮挡（occluded）（见 Figure 4(a)）：
        - 如果 VH 中的元素因为某种 UI 设计而被部分遮挡，但仍然可用，这种情况被认为是同步的（in-sync）。
    2. “鬼影” VH（ghosting VH）（见 Figure 4(b)）：
        - 由于 UI 动画（比如菜单滑入/弹出），可能会出现多个 VH 叠加的现象。这种情况也是允许的。
    3. 完全不同步的 VH（见 Figure 4(c)）：
        - 如果 VH 完全无法映射到当前屏幕的主要内容（例如 VH 只包含一些无关的 UI 元素），
        - 那么这个情况才会被标记为 out-of-sync，并从数据集中移除。

* 总结：只有在 VH 和屏幕主要内容完全不匹配 的情况下，才会被标记为不同步并移除，部分遮挡或动画导致的 VH 重叠是可以接受的。

A.2 Question Annotation UI
--------------------------

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/AGz86x.png

    Figure 5: Annotation interfaces: (a)Question annotation UI


- 问题标注（A.2）：
    - 由多个标注人员逐步完善。
    - 可以看到之前标注的问题，避免重复，提高问题多样性。
    - 采用 逐步反馈和培训，提高标注质量。


A.3 Answer Annotation UI
------------------------

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/2AdkE6.png

    Figure 5: Annotation interfaces: (b)Answer annotation UI

- 答案标注（A.3）- 标注员的主要任务：
    1. 判断问题是否有效、是否可以从屏幕内容回答。
    2. 选择或绘制 UI 元素的边界框。
    3. 填写文本、排序答案。
    4. 提供完整的句子回答。
    5. 纠正问题中的拼写或语法错误。

- 总结：问题和答案的标注是一个多步骤流程，通过逐步反馈和质量控制来保证标注的准确性和多样性。


A.4 Annotation quality control
------------------------------

- 采用 评分平台 确保标注员经过训练，达到 90% 以上的质量标准 才能正式标注。
- 在整个标注过程中，持续监测质量，最终质量 保持在 95% 以上。


A.5 Annotation post-processing
------------------------------

- 删除无效问题 或 没有可用答案的问题。
- 训练数据：如果答案标注员对问题进行了修改，则采用修改后的问题。
- 验证集 & 测试集：
- 由于每个问题有 3 个不同的答案标注员，他们可能会有不同的修改。
- 计算标注一致性（inter-annotator agreement）：
    - 97.9% 的问题，所有答案一致。
    - 1.9% 的问题，2 个答案一致（部分一致）。
    - 0.2% 的问题，完全没有一致的答案（被移除）。
- 对于部分一致的情况，删除不一致的答案，并选择最多人同意的修改。

- 总结：严格的质量控制和后处理，确保数据的一致性和可靠性。

A.6 Short Answer Generation Prompts
-----------------------------------

- 使用 PaLM 2 语言模型 生成短答案。
- 输入数据包括：
    - 问题
    - UI 元素文本列表
    - 完整句子的答案
- 提示词模板：
  - 生成尽可能简短的答案，避免冗余信息。

prompt template is give below::

    List various ways to rephrase the answer. 
    The answer should be as short as possible, without extra words from the question. 
    Use all provided elements in each answer. 
    Provide the output in square brackets.

    {examples}

    Now is your turn.
    Question: {question}
    Answer elements: {list of text of UI elements}
    Full answer: {full-sentence answer}
    Rephrases:

An example of single-UI-element answer is as below::

    Here is another example:
    Question: 'What is the gender?'
    Answer elements: ['Male']
    Full answer: 'The gender is male.'
    Rephrases: ['male']

An example of multiple-UI-element answer is as below:

    Here is another example:
    Question: 'What is the name?'
    Answer elements: ['Jon', 'Brown']
    Full answer: 'The name is Jon Brown.'
    Rephrases: ['Jon Brown']



> 总结：PaLM 2 通过示例学习（few-shot learning）生成短答案，优化答案的简洁性。

总结
----

* 这部分内容主要介绍了 ScreenQA 数据集的标注流程，重点包括
    1. VH 同步性规则：确保 UI 结构和屏幕内容匹配，只删除完全不同步的情况。
    2. 问题和答案标注：多轮标注+反馈机制，确保高质量数据。
    3. 质量控制 & 后处理：采用严格的训练、监测和一致性检查，最终质量达 95% 以上。
    4. 短答案生成：使用 PaLM 2 语言模型，生成高质量、简洁的短答案。


B. Data Examples
================

* Each example contains
    • A screenshot
    • A question
    • Multiple annotations of lists of UI elements relevant to the question, each annotation of which is completed by an annotator
    • Multiple annotations of long full-sentence answers corresponding to the UI element annotations
    • Multiple short answers, generated by the procedure outlined in Section 4.5


.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/SJbS4w.png

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/gI2d6h.png

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/lv9neQ.png

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/npJn70.png









 




