2112.09332_WebGPT: Browser-assisted question-answering with human feedback
##########################################################################


* https://arxiv.org/abs/2112.09332
* 组织：OpenAI
* 作者：Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, ...
* We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56% of the time to those of our human demonstrators, and 69% of the time to the highest-voted answer from Reddit.
* 我们对 GPT-3 进行微调，以使用基于文本的网络浏览环境回答长篇问题，该环境允许模型搜索和导航网络。通过设置任务使其可以由人类执行，我们能够使用模仿学习在任务上训练模型，然后通过人类反馈优化答案质量。为了使人类更容易评估事实准确性，模型必须在浏览时收集参考资料以支持其答案。我们在 ELI5 上训练和评估我们的模型，ELI5 是 Reddit 用户提出的问题数据集。我们最好的模型是通过使用行为克隆微调 GPT-3 获得的，然后对经过训练以预测人类偏好的奖励模型执行拒绝抽样。这个模型的答案在56%的时间里比我们的人类演示者的答案更受欢迎，而在69%的时间里，人类更喜欢Reddit上得票最高的答案。






NLP 中一个日益严峻的挑战是长篇问答 （LFQA），其中在回答开放式问题时生成一个段落长度的答案。LFQA 系统有可能成为人们了解世界的主要方式之一，但目前落后于人类的表现 [Krishna 等人，2021 年]。现有的工作往往集中在任务的两个核心组成部分上，即信息检索和综合。在这项工作中，我们利用现有的解决方案来构建这些组件：我们将文档检索外包给 Microsoft Bing Web Search API，并利用无监督预训练通过微调 GPT-3 来实现高质量的合成 [Brown et al.， 2020]。我们不是试图改进这些成分，而是专注于使用更忠实的训练目标将它们结合起来。我们使用人类反馈直接优化答案质量，使我们能够实现与人类竞争的性能。



* 目标：WebGPT项目的主要目标是通过结合GPT-3的能力与文本基础的网页浏览环境，来改进长形式问答的质量。该项目旨在使模型能够搜索和导航互联网，以获取更准确和信息丰富的答案，从而提高回答的准确性和可靠性。此外，WebGPT还通过人类反馈进行微调，以进一步提升其回答质量和用户体验
* 该模型如何通过人类反馈来提高答案质量：该模型通过多种方式利用人类反馈来提高答案质量。首先，模型在训练过程中使用了人类示范的行为克隆（即监督微调），这使得模型能够学习如何在特定任务中表现得更好。其次，模型还通过比较不同模型生成的答案，进行奖励建模，从而优化答案的质量。具体来说，WebGPT会在浏览过程中收集参考资料，以支持其答案，这样可以使人类评估答案的事实准确性变得更容易。最终，模型的答案在与人类示范者的答案进行比较时，能够被人类偏好56%的时间，这表明其在回答质量上达到了人类水平。通过这种方式，WebGPT能够不断改进其回答的准确性和相关性。


































