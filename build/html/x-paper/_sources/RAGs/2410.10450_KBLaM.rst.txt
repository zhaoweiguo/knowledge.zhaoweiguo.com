2410.10450_KBLaM: Knowledge Base augmented Language Model
#########################################################


* https://arxiv.org/abs/2410.10450
* ç»„ç»‡: Microsoft Research

* å…³é”®è¯
    - Knowledge Base augmented Language Model (KBLaM)
    - KB: Knowledge Base

Abstract
========

* In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a new method for augmenting Large Language Models (LLMs) with external knowledge. 
* KBLaM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and integrating them into pre-trained LLMs via a specialized rectangular attention mechanism. 
* Unlike Retrieval-Augmented Generation, KBLaM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB(knowledge base) size rather than quadratically. 
* Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. 
* Experiments demonstrate KBLaMâ€™s effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its use of the augmented knowledge.



1. Introduction
===============

.. note:: KBLaM é€šè¿‡ç»“æ„åŒ–çŸ¥è¯†åº“ï¼Œå°†å¤–éƒ¨çŸ¥è¯†é«˜æ•ˆåœ°ç¼–ç æˆ LLM å¯ç›´æ¥ç”¨çš„ key-value å‘é‡ï¼Œé¿å… RAG çš„æ£€ç´¢å¤æ‚æ€§ï¼Œä¹Ÿè§£å†³äº† in-context learning èµ„æºæ¶ˆè€—é«˜çš„é—®é¢˜ï¼Œå®ç°äº†åŠ¨æ€ã€é«˜æ•ˆã€å¯è§£é‡Šçš„çŸ¥è¯†å¢å¼ºã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/f5Nzfd.png

    Figure 1:Overview of the KBLaM pipeline and comparison with existing approaches. KBLaM augments knowledge into a pre-trained LLM in the form of knowledge tokens, a set of continuous key-value vectors, using a modified rectangular attention structure. Unlike RAG, KBLaM does not rely on separate retriever module at inference time and unlike in-context learning, KBLaMâ€™s computation and memory overhead scales linearly rather than quadratically with the size of the KB.


* æ ¸å¿ƒæ€æƒ³ï¼š
    - å…ˆç”¨å·¥å…·æŠŠå¤–éƒ¨è¯­æ–™ï¼ˆæ— ç»“æ„ï¼‰è½¬æ¢æˆ ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆKBï¼‰ï¼Œå½¢å¼æ˜¯ä¸‰å…ƒç»„ï¼š
        - ``<name>ï¼ˆå®ä½“åç§°ï¼‰, <property>ï¼ˆå±æ€§ï¼‰, <value>ï¼ˆå€¼ï¼‰``
        - æ¯”å¦‚ï¼š("China", "capital", "Beijing")
* KBLaMå…·ä½“åšæ³•ï¼š
    - æŠŠæ¯ä¸ªä¸‰å…ƒç»„å˜æˆä¸€ä¸ªå›ºå®šé•¿åº¦çš„ key-value å‘é‡å¯¹ï¼Œå«åš knowledge tokenï¼Œæ ¼å¼è·Ÿ LLM çš„ KV cache ä¸€æ ·ã€‚
    - <name> å’Œ <property> ç¼–ç æˆ key vectorã€‚
    - <value> ç¼–ç æˆ value vectorã€‚
    - è¿™äº› knowledge tokens ç›´æ¥æ³¨å…¥åˆ° LLM çš„æ³¨æ„åŠ›å±‚ï¼Œæ¯ä¸ªçŸ¥è¯†ä¸‰å…ƒç»„éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œäº’ä¸å½±å“ã€‚

* ä¼˜ç¼ºç‚¹
    - ä¼˜ç‚¹ï¼šä¸‰å…ƒç»„ â†’ å›ºå®šå‘é‡å¯¹ï¼Œæ³¨å…¥Attentionå±‚ï¼Œçº¿æ€§å¢é•¿ 
        - åŠ¨æ€æ›´æ–°èƒ½åŠ›ï¼šæ·»åŠ /åˆ é™¤/ä¿®æ”¹çŸ¥è¯†ï¼Œåªéœ€æ›´æ–°å¯¹åº”çš„ knowledge tokenï¼Œæ— éœ€å¾®è°ƒæ¨¡å‹æƒé‡æˆ–é‡å»ºKV cacheã€‚
        - å¯è§£é‡Šæ€§å¥½ï¼šæ³¨æ„åŠ›çŸ©é˜µå¯ä»¥æ¸…æ¥šçœ‹åˆ°æ¨¡å‹å¦‚ä½•ä½¿ç”¨æ¯ä¸ª knowledge tokenã€‚
        - è®­ç»ƒæ–¹å¼ï¼š
            - ä½¿ç”¨ åˆæˆæ•°æ®è®­ç»ƒ linear adapterï¼Œä¸å…³å¿ƒçœŸå®çŸ¥è¯†å†…å®¹ï¼Œåªéœ€è®©æ¨¡å‹å­¦ä¼šå¦‚ä½•æŠ•å½± sentence encoder â†’ LLM embeddingã€‚
            - ä¸æ¶‰åŠâ€œè®°ä½çŸ¥è¯†â€ï¼Œé¿å… fine-tuning çš„ç¾éš¾æ€§é—å¿˜ã€‚
    - ç¼ºç‚¹ï¼šéœ€è¦å…ˆç»“æ„åŒ–æ•°æ®ä¸ºKBï¼›ä½†æ”¯æŒåŠ¨æ€å¢åˆ æ”¹ï¼Œè§£é‡Šæ€§å¼ºã€‚


2. Related work
===============

.. note:: KBLaMçš„æ ¸å¿ƒåˆ›æ–°æ˜¯åœ¨ç»§æ‰¿RAGéšå¼æ£€ç´¢ã€å¤šæ¨¡æ€é€‚é…å™¨è®­ç»ƒã€KV cacheé«˜æ•ˆæ€§è¿™äº›æ€æƒ³çš„åŸºç¡€ä¸Šï¼Œç”¨ä¸€ä¸ªæ— éœ€æ–°æ¨¡å—ã€å¯è§£é‡Šã€åŠ¨æ€å¯æ›´æ–°çš„attentionæœºåˆ¶ï¼ŒæŠŠç»“æ„åŒ–çŸ¥è¯†åº“ç›´æ¥åµŒå…¥åˆ°å·²æœ‰LLMä¸­ï¼Œæ—¢ä¿æŒäº†æ•ˆç‡ï¼Œåˆå¤§å¹…é™ä½äº†å¤æ‚åº¦å’Œè®­ç»ƒæˆæœ¬ã€‚

* ä¸ RAG çš„å…³ç³»
    - ç›¸ä¼¼ç‚¹
        + éšå¼æ£€ç´¢: KBLaM æ²¡æœ‰å•ç‹¬çš„æ£€ç´¢å™¨ï¼Œè€Œæ˜¯é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å®Œæˆâ€œæ£€ç´¢â€
        + æ€ä¹ˆåšçš„: è¾“å…¥queryé€šè¿‡æ³¨æ„åŠ›å±‚å’ŒçŸ¥è¯†tokençš„keyè¿›è¡ŒåŒ¹é…ï¼Œå¾—åˆ°ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆquery-key similarityï¼‰ï¼Œç„¶åå–çŸ¥è¯†tokençš„valueçš„åŠ æƒå¹³å‡ï¼Œç±»ä¼¼â€œè½¯æ£€ç´¢â€ï¼ˆsoft retrievalï¼‰ã€‚
    - ä¸åŒç‚¹
        + RAGï¼šä¾èµ–å¤–éƒ¨æ£€ç´¢æ¨¡å—ï¼ˆretrieverï¼‰ â†’ å…ˆæ£€ç´¢ â†’ å†å–‚æ¨¡å‹ã€‚
        + KBLaMï¼šå®Œå…¨é€šè¿‡Attentionæœºåˆ¶å†…åŒ–å®Œæˆï¼Œä¸éœ€è¦æ£€ç´¢æ¨¡å— â†’ ç«¯åˆ°ç«¯å¤„ç†ã€‚

* ä¸ å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ çš„å…³ç³»
    - MLMsï¼šæŠŠå›¾ç‰‡ã€è§†é¢‘ç­‰éæ–‡æœ¬æ•°æ®ï¼Œé€šè¿‡ä¸“é—¨çš„ encoder/adapters å˜æ¢æˆ LLMå¯ä»¥ç†è§£çš„å‘é‡ï¼Œå†ç”¨ instruction tuning å¾®è°ƒã€‚
    - KBLaM ç±»æ¯”ï¼š
        + å¯ä»¥æŠŠ ç»“æ„åŒ–çŸ¥è¯†åº“ï¼ˆKBï¼‰ çœ‹ä½œæ˜¯å¦ä¸€ç§â€œæ¨¡æ€â€ï¼Œå®ƒè¢«ç¼–ç æˆè¿ç»­å‘é‡ï¼ˆknowledge tokensï¼‰ã€‚
        + è¿™ä¸ªè¿‡ç¨‹é‡Œçš„ encoder + adapter çš„è®¾è®¡ï¼Œæ­£æ˜¯å€Ÿé‰´äº† MLM é¢†åŸŸå·²æœ‰æ–¹æ³•ã€‚
        + åŒæ ·ç”¨ instruction tuning æ¥è®­ç»ƒ adapterï¼Œé€‚é…é¢„è®­ç»ƒå¥½çš„LLMã€‚



3. Background
=============

* Knowledge base in the form of triples(ç¤ºä¾‹å‚è§ Appendix Eä¸­çš„table1å’Œtable2)

.. math::

    {(<name>_m; <property>_m; <value>_m)}^M_{m=1}


Self-attention layer
--------------------

.. note:: æ€»ç»“ï¼šSelf-Attention å±‚é€šè¿‡ Queryã€Keyã€Value è®¡ç®— token é—´å…³ç³»ï¼Œæ—¶é—´å’Œå†…å­˜æ¶ˆè€—éšç€åºåˆ—é•¿åº¦ $N$ å¢é•¿ä¸º $O(N^2)$ï¼Œæ‰€ä»¥é•¿åºåˆ—æ—¶ä¼šé¢ä¸´é«˜è®¡ç®—å’Œé«˜å†…å­˜å¼€é”€ã€‚



* Transformer æœ‰ L å±‚ï¼Œæ¯ä¸€å±‚æœ‰ ä¸‰ä¸ªæƒé‡çŸ©é˜µï¼Œä¹Ÿå« æŠ•å½±å¤´ï¼ˆprojection headsï¼‰ï¼š
    - æŸ¥è¯¢æƒé‡ (Query weight)ï¼š :math:`\boldsymbol{W}_{Q}^{l} \in \mathbb{R}^{D \times D}`
    - é”®æƒé‡ (Key weight)ï¼š :math:`\boldsymbol{W}_{K}^{l} \in \mathbb{R}^{D \times D}`
    - å€¼æƒé‡ (Value weight)ï¼š :math:`\boldsymbol{W}_{V}^{l} \in \mathbb{R}^{D \times D}`
    - è¿™é‡Œ D æ˜¯ embedding çš„ç»´åº¦ï¼Œæ¯”å¦‚ 512 æˆ– 768 ç»´ã€‚

*  è¾“å…¥æ˜¯ä»€ä¹ˆ
    - è¾“å…¥æ˜¯ä¸€ä¸ª ç”¨æˆ· promptï¼ˆæ¯”å¦‚ä¸€ä¸ªé—®é¢˜ï¼‰ï¼Œå®ƒæ˜¯ä¸€ä¸ªé•¿åº¦ä¸º N çš„ token åºåˆ—ã€‚
    - å¯¹åº”æ¯ä¸ª tokenï¼Œæœ‰ä¸€ä¸ª embedding å‘é‡ï¼š
    - ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸€å±‚ self-attentionï¼Œè¾“å…¥æ˜¯ N ä¸ª D ç»´å‘é‡ã€‚

.. math::

    \boldsymbol{x}^{l} = \left[\boldsymbol{x}_{1}^{l}, \ldots, \boldsymbol{x}_{n}^{l}, \ldots, \boldsymbol{x}_{N}^{l}\right]^{\top} \in \mathbb{R}^{N \times D}

* Query, Key, Value çš„è®¡ç®—
    - å¯¹äºç¬¬ n ä¸ª token çš„ embedding
    - æ¯ä¸ª token é€šè¿‡ä¸‰å¥—æƒé‡çŸ©é˜µè¢«æ˜ å°„æˆ Query, Key, Value ä¸‰ä¸ªå‘é‡ã€‚

.. math::

    \begin{matrix}
        \boldsymbol{q}_{n}^{l} = \boldsymbol{W}_{Q}^{l} \boldsymbol{x}_{n}^{l} \quad (Query) \\
        \boldsymbol{k}_{n}^{l} = \boldsymbol{W}_{K}^{l} \boldsymbol{x}_{n}^{l} \quad (Key) \\
        \boldsymbol{v}_{n}^{l} = \boldsymbol{W}_{V}^{l} \boldsymbol{x}_{n}^{l} \quad (Value) 
    \end{matrix}

* Self-Attention çš„æ ¸å¿ƒè®¡ç®—
    - å¯¹äºæ¯ä¸ª token nï¼Œè¾“å‡º :math:`\boldsymbol{y}_{n}^{l}`
        - è‡ªæ³¨æ„åŠ›å…¬å¼: :math:`\boldsymbol{y}_{n}^{l} = \frac{\sum_{i=1}^{n} \exp(w_{n, i}) \boldsymbol{v}_{i}^{l}}{\sum_{i=1}^{n} \exp(w_{n, i})}`
    - å…¶ä¸­æƒé‡ :math:`w_{n,i}` æ˜¯ï¼š
        - :math:`w_{n, i} = \frac{\langle \boldsymbol{q}_{n}^{l}, \boldsymbol{k}_{i}^{l} \rangle}{\sqrt{D}}`
        - ä¹Ÿå°±æ˜¯è¯´ï¼š
            - å½“å‰ token çš„ Query ä¸ æ‰€æœ‰ä¹‹å‰ token çš„ Key åš ç‚¹ç§¯ï¼Œç„¶åé™¤ä»¥ :math:`\sqrt{D}` ï¼ˆé˜²æ­¢æ¢¯åº¦è¿‡å¤§ï¼‰ã€‚
            - è¿™äº›ç‚¹ç§¯é€šè¿‡ softmax å½’ä¸€åŒ–ï¼ˆå…¶å®å°±æ˜¯åŠ äº† :math:`\exp` ç„¶åå†å½’ä¸€åŒ–ï¼‰ã€‚
            - ç„¶åç”¨è¿™äº›æƒé‡åŠ æƒä¹‹å‰æ‰€æœ‰ token çš„ Valueã€‚

    - å› ä¸ºè¿™æ˜¯ decoder-based Transformerï¼Œå®ƒçš„ attention æ˜¯ maskedï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå½“å‰ token åªèƒ½çœ‹åˆ°è‡ªå·±å’Œä¹‹å‰çš„ tokenï¼Œä¸èƒ½çœ‹åˆ°åé¢çš„ï¼ˆé˜²æ­¢â€œå·çœ‹â€æœªæ¥ï¼‰ã€‚

* è¾“å‡º
    - æœ€ç»ˆå¾—åˆ°çš„ :math:`\boldsymbol{y}_{n}^{l}` ï¼Œä¼šå†ç»è¿‡ å‰é¦ˆç¥ç»ç½‘ç»œ (FFN) å¤„ç†ï¼Œè¿›ä¸€æ­¥æå‡è¡¨è¾¾èƒ½åŠ›ã€‚

* è®¡ç®— & å†…å­˜å¤æ‚åº¦
    1. Attention æƒé‡è®¡ç®—ï¼š
        - æ¯ä¸ª token éœ€è¦è·Ÿæ‰€æœ‰ token è®¡ç®—ç‚¹ç§¯ï¼ˆN ä¸ª tokenï¼Œä¸¤ä¸¤ä¹‹é—´ï¼‰ã€‚
        - å› æ­¤ï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯ :math:`\mathcal{O}(N^2 D)`
    2. å­˜å‚¨æ‰€æœ‰æƒé‡ :math:`w_{n,i}` ï¼š
        - éœ€è¦å­˜å‚¨ :math:`N \times N` çš„çŸ©é˜µã€‚
        - æ‰€ä»¥ å†…å­˜å¤æ‚åº¦æ˜¯ :math:`\mathcal{O}(N^2)`
    3. å‰é¦ˆç¥ç»ç½‘ç»œ (FFN)ï¼š
        - å¤æ‚åº¦æ˜¯ :math:`\mathcal{O}(N D^2)`




4. Augmenting LLM with the KB
=============================

.. note:: KBLaM é€šè¿‡â€œçŸ¥è¯† token + Rectangular Attentionâ€æ–¹å¼ï¼Œå°†å¤–éƒ¨çŸ¥è¯†åº“çš„ä¸‰å…ƒç»„é«˜æ•ˆæ˜ å°„è¿› LLM çš„æ¯å±‚ attentionï¼Œä½¿æ¨¡å‹å…·å¤‡é¢å¤–çš„çŸ¥è¯†è®°å¿†ï¼Œä½†é¿å…äº†ä¼ ç»Ÿä¸Šä¸‹æ–‡æ‹¼æ¥æ–¹å¼çš„å†…å­˜é—®é¢˜ã€‚



* æ•´ä½“æµç¨‹(å‚è§ä¸‹å›¾: Figure 2:Overview of KBLaMâ€™s KB augmentation process)
    1ï¸âƒ£ å°†çŸ¥è¯†åº“ä¸­çš„ä¸‰å…ƒç»„è½¬æ¢æˆå‘é‡å½¢å¼ â€”â€” knowledge token
        - é€šè¿‡ pre-trained sentence encoder followed by linear adapters å®ç°
    2ï¸âƒ£ æŠŠè¿™äº› knowledge token æ³¨å…¥åˆ° LLM çš„æ¯ä¸€ä¸ª attention å±‚ä¸­
        - é€šè¿‡ä¸€ç§ Rectangular Attention ç»“æ„å®ç°

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/9O1Nwx.png

    (a)Step 1. KB encoding:  For each triple from a KB of M triples, we first construct a key and value string (left most white boxes). Then both strings are processed by a pre-trained sentence encoder followed by a learned linear adapter middle two blue boxes). The acquired knowledge tokens (right-most small boxes) are then stored on disk for later usage. Note that all :math:`\widetilde{k}_m^ls` and :math:`\widetilde{v}_m^ls` have a fixed dimension of D, i.e. identical to that of the key and value embeddings of a token, regardless of input length.



.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/NOhQNM.png

    (b)Step 2. Augmenting knowledge tokens into attention  For layer l, given an sequence of N D-dimensional embeddings from the prompt ï¼ˆ :math:`\left(\boldsymbol{x}_{1}^{l}, \ldots, \boldsymbol{x}_{N}{ }^{l}\right)` , e.g. a question about the KB), augmented with M knowledge tokens as context :math:`\left(\left\{\left(\widetilde{\boldsymbol{k}}_{m}{ }^{l}, \widetilde{\boldsymbol{v}}_{m}{ }^{l}\right)\right\}_{m=1}{ }^{M}\right)` , KBLaMâ€™s attention outputs N embedding vectors with each element :math:`\widetilde{\boldsymbol{y}}_{n}{ }^{l} \in \mathbb{R}^{D}`  (top right panel) under a rectangular attention matrix (bottom right panel): Blue regions show the extra components introduced by KBLaM whereas the red parts are from standard self-attention. Note that the input sizes of the FFN stay unchanged, the only additional overhead introduced by KBLaM comes from the blue parts in the equation and thematrix, which scales as ğ’ª(M).


Knowledge tokens
----------------

è¾“å…¥::

    çŸ¥è¯†åº“ä¸­å­˜å‚¨çš„æ˜¯ä¸‰å…ƒç»„
    (å®ä½“, å±æ€§, æè¿°)
    (Name, Property, Value)
    å¦‚ï¼š("Apple", "founded_year", "1976")

* è½¬æ¢è¿‡ç¨‹:
    1. Sentence Encoderï¼š
        - ç”¨ä¸€ä¸ª pre-trained Sentence Encoder æ¨¡å‹(å¦‚ :math:`f(Â·)` )ï¼ŒæŠŠæ¯ä¸ªä¸‰å…ƒç»„çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºä¸€ä¸ª P ç»´çš„è¿ç»­å‘é‡ã€‚
        - å¾—åˆ°çš„æ˜¯ä¸€ä¸ª key embedding å’Œä¸€ä¸ª value embeddingï¼ˆç±»ä¼¼ transformer é‡Œçš„ key-value pairï¼‰ã€‚
        - :math:`k_m = f(The <property>_m of <name>_m) \in \mathbb{R}^P`
        - :math:`v_m = f(<value>_m) \in \mathbb{R}^P`
    2. Linear Adapterï¼š
        - ç”¨ä¸¤ä¸ª çº¿æ€§å˜æ¢å™¨ (adapter)ï¼Œå°†åˆšæ‰å¾—åˆ°çš„ key å’Œ value å‘é‡ï¼Œè¿›ä¸€æ­¥æ˜ å°„åˆ° LLM å„å±‚çš„ key/value embedding ç©ºé—´ï¼Œä»¥é€‚é…æ¯ä¸€å±‚çš„æ³¨æ„åŠ›ã€‚
        - è½¬æ¢åï¼Œæ¯ä¸ªä¸‰å…ƒç»„å¾—åˆ°çš„ key å’Œ value embedding åœ¨ æ¯ä¸€å±‚éƒ½æœ‰ä¸€ä¸ªå¯¹åº”ç‰ˆæœ¬
        - :math:`\widetilde{\boldsymbol{W}}_{K} \in \mathbb{R}^{L \times D \times P}`
        - :math:`\widetilde{\boldsymbol{W}}_{V} \in \mathbb{R}^{L \times D \times P}`
        - å…¶ä¸­: 
            + L: the number of attention layers in a model
            + With the adapters, we map ğ—¸_m and ğ˜ƒ_m from the sentence encoderâ€™s space to LLMâ€™s key and value embedding space at each attention layer.
            + å‚è§ä¸‹é¢å…¬å¼
            + å°† :math:`(\widetilde{\boldsymbol{k}}_{m}, \widetilde{\boldsymbol{v}}_{m})` å¯¹ç§°ä¸º ``knowledge token``
    3. è¿™ç§æ˜ å°„çš„æœ¬è´¨ï¼š
        - æŠŠçŸ¥è¯†ä¸‰å…ƒç»„çš„å¤š token è¡¨ç¤ºï¼Œå‹ç¼©æˆä¸€ä¸ªç‰¹åˆ«çš„ çŸ¥è¯† token
        - å®ƒåœ¨æ¯å±‚ attention é‡Œéƒ½æœ‰å›ºå®šçš„ key-value embeddingï¼Œè€Œä¸éœ€è¦å‚ä¸è‡ªæ³¨æ„åŠ›ã€‚
        - This encoding process is applied to all triples in the KB, which transforms the information from a KB into a collection of knowledge tokens
        - :math:`\{(<name>_m, <property>_m, <value>_m\}^M_{m=1} \overset{Encode}{\rightarrow} \{(\widetilde{\boldsymbol{k}}_{m}, \widetilde{\boldsymbol{v}}_{m})\}^M_{m=1}`

.. math::

    \begin{array}{l}
        \widetilde{\boldsymbol{k}}_{m}=\left[\widetilde{\boldsymbol{k}}_{m}{ }^{1}, \ldots, \widetilde{\boldsymbol{k}}_{m}^{l}, \ldots, \widetilde{\boldsymbol{k}}_{m}{ }^{L}\right]^{\top}=\widetilde{\boldsymbol{W}}_{K} \mathbf{k}_{m} \in \mathbb{R}^{L \times D}, \\
        \widetilde{\boldsymbol{v}}_{m}=\left[{\boldsymbol{v}_{m}}^{1}, \ldots, \widetilde{\boldsymbol{v}}_{m}{ }^{l}, \ldots, \widetilde{\boldsymbol{v}}_{m}{ }^{L}\right]^{\top}=\widetilde{\boldsymbol{W}}_{V} \mathbf{v}_{m} \in \mathbb{R}^{L \times D} .
    \end{array}


Rectangular Attention: Injecting knowledge token into prompt tokens
-------------------------------------------------------------------

* å…³é”®ç‚¹ï¼š
    - è¾“å…¥ prompt æœ‰ N ä¸ª token
    - çŸ¥è¯†åº“äº§ç”Ÿäº† M ä¸ª knowledge tokens

* åœ¨æ¯ä¸€å±‚ attention é‡Œï¼Œä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶ï¼Œè®©ï¼š
    - Prompt tokens å¯ä»¥ï¼š
        - æ­£å¸¸ self-attentionï¼ˆå½¼æ­¤ä¹‹é—´æ³¨æ„ï¼‰
        - è¿˜å¯ä»¥é¢å¤– attend åˆ°æ‰€æœ‰ knowledge tokens
    - Knowledge tokensï¼š
        - ä¸ä¼šå½¼æ­¤ä¹‹é—´ self-attend
        - å®ƒä»¬æ²¡æœ‰ query embeddingï¼Œä¸ä¼šä¸»åŠ¨å»å…³æ³¨ promptï¼Œåªæ˜¯æä¾›é™æ€çš„ key-value

* å…¬å¼è§£é‡Šï¼š

.. math::

    \widetilde{\boldsymbol{y}}_{n}^{l}=\frac
    {
        \sum_{m=1}^{M} \exp \left(\widetilde{w}_{n, m}^{l}\right) \widetilde{\boldsymbol{v}}_{m}^{l}
            +\sum_{i=1}^{n} \exp \left(w_{n, i}^{l}\right) \boldsymbol{v}_{i}^{l}
    }
    {
        \sum_{m=1}{ }^{M} \exp \left(\widetilde{w}_{n, m}^{l}\right)+\sum_{i=1}^{n} 
        \exp \left(w_{n, i}^{l}\right)
    }


- è¯´æ˜
    - attention matrix of rectangular shape of size (M +N)Ã—N, where M is the number of triples in the KB.
    - å½“M=0æ—¶(knowledge tokens are not introduced)ï¼Œä¼šé€€åŒ–æˆæ™®é€šçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
    - :math:`\boldsymbol{y}_{n}^{l} = \frac{\sum_{i=1}^{n} \exp(w_{n, i}) \boldsymbol{v}_{i}^{l}}{\sum_{i=1}^{n} \exp(w_{n, i})}`

* è¯¦ç»†æ¥è¯´ï¼š
    - è‡ªèº«æ³¨æ„åˆ†æ•°ï¼š<query, prompt key>
    - çŸ¥è¯†æ³¨æ„åˆ†æ•°ï¼š<query, knowledge key>
    - çŸ¥è¯† token é€šè¿‡å•ç‹¬çš„ query å¤´å’Œçº¿æ€§å˜æ¢å¾—åˆ°
    - æœ€ç»ˆè¾“å‡ºæ˜¯ prompt token å’ŒçŸ¥è¯† token çš„ weighted sum


.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/JnjAl4.png

    Figure 3:Memory overhead of different methods. Given a KB of M triples (with each triple K-token long on average). In-context learningâ€™s memory scales with (KM)^2, whereas KBLaMâ€™s memory scales with M.




KB length generalization through attention score scaling
--------------------------------------------------------


* ä¸ºä»€ä¹ˆå« Rectangular Attentionï¼Ÿ
    - ä¼ ç»Ÿ self-attention çš„çŸ©é˜µæ˜¯ NÃ—Nï¼ˆprompt token è‡ªå·±ä¹‹é—´äº’ç›¸æ³¨æ„ï¼‰ã€‚
    - è€Œè¿™é‡Œï¼š
    - - prompt tokens å’Œ knowledge tokens ä¹‹é—´æ³¨æ„ -> (M+N)Ã—Nï¼Œæ˜¯ä¸€ä¸ªé•¿æ–¹å½¢çŸ©é˜µã€‚
    - ä¸”çŸ¥è¯† token éƒ¨åˆ†æ˜¯ä¸äº’ç›¸æ³¨æ„çš„ï¼Œæ‰€ä»¥åœ¨çŸ©é˜µä¸­å‘ˆç°ç‰¹å®šç¨€ç–ç»“æ„ã€‚

* ä¼˜åŠ¿ï¼š
    - å†…å­˜æ•ˆç‡é«˜ï¼šä¸åƒ in-context learning é‚£æ ·éœ€è¦æ˜¾å¼æŠŠæ‰€æœ‰ KB ç›´æ¥æ‹¼è¿› promptï¼Œé¿å… O((KM)Â²) çš„å†…å­˜è†¨èƒ€ã€‚
    - çŸ¥è¯† token çš„ embedding æ˜¯ç›´æ¥æ³¨å…¥åˆ° attention è®¡ç®—ä¸­ï¼Œä¸éšç€ä¸Šä¸‹æ–‡é•¿åº¦æš´æ¶¨ã€‚




5. KB instruction tuning
========================

.. note:: KBLaM é€šè¿‡ freeze å¤§æ¨¡å‹å‚æ•°ï¼Œä»…è®­ç»ƒè½»é‡ adapterï¼ˆQuery/Key/Value å¤´ï¼‰ï¼Œåˆ©ç”¨åˆæˆ KB å’Œ QA å¯¹è¿›è¡Œ instruction tuningï¼Œé¿å…è¿‡æ‹Ÿåˆï¼ŒåŒæ—¶ä¿ç•™ LLM æ¨ç†èƒ½åŠ›ï¼Œä¸”è®­ç»ƒæˆæœ¬ä½ã€è§£é‡Šæ€§å¼ºã€‚

* KBLaM çš„å¯å­¦ä¹ å‚æ•°æœ‰å“ªäº›ï¼Ÿ
    * KBLaM é‡Œå¹¶ æ²¡æœ‰ fine-tune æ•´ä¸ªå¤§æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œè€Œæ˜¯å¼•å…¥äº† è½»é‡å¯å­¦ä¹ å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
    * :math:`\theta = \{ \tilde{\mathbf{W}}_K, \tilde{\mathbf{W}}_V, \{\tilde{\mathbf{W}}_Q^l\}_{l=1}^L \}`
    * å«ä¹‰ï¼š
        - :math:`\tilde{\mathbf{W}}_K` ï¼šKey adapter æƒé‡
        - :math:`\tilde{\mathbf{W}}_V` ï¼šValue adapter æƒé‡
        - :math:`\tilde{\mathbf{W}}_Q^l` ï¼šæ¯ä¸€å±‚ç¬¬ l å±‚çš„ Query adapter æƒé‡

    * ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ˜¯ é¢å¤–çš„çº¿æ€§ adapter/å¤´ï¼Œé€šå¸¸æ˜¯ä½ç§©ï¼ˆå‚æ•°é‡è¿œå°äºå®Œæ•´ LLM çš„å‚æ•°ï¼‰ã€‚
    * ç›®çš„ï¼š  
        - åªè®­ç»ƒè¿™äº› adapterï¼Œè€Œä¸åŠ¨ LLM æœ¬èº«çš„å‚æ•°ã€‚

* ä¸ºä»€ä¹ˆè¦è¿™æ ·åšï¼Ÿ
    - çµæ„Ÿæ¥è‡ªå¤šæ¨¡æ€å¤§æ¨¡å‹çš„åšæ³•ï¼ˆæ¯”å¦‚ `LLaVA <https://openreview.net/forum?id=w0H2xGHlkw>`_ è¿™ç±»ï¼‰ï¼Œå…¶å¥½å¤„æœ‰ï¼š
        - ä¿æŒ LLM çš„åŸå§‹æ¨ç†èƒ½åŠ›  
          â†’ å› ä¸ºä¸æ”¹åŠ¨ LLM çš„å¤§éƒ¨åˆ†å‚æ•°
        - å‡å°‘è¿‡æ‹Ÿåˆå’Œè®°å¿†è®­ç»ƒæ•°æ®çš„é£é™©  
          â†’ ä¼ ç»Ÿ fine-tune å¾ˆå®¹æ˜“ç›´æ¥è®°ä½è®­ç»ƒæ•°æ®ï¼Œè€Œåªè°ƒ adapter å°±å¯ä»¥é¿å…è¿™ä¸ªé—®é¢˜
        - è®­ç»ƒæˆæœ¬ä½  
          â†’ å‚æ•°å°‘ï¼Œè®¡ç®—èµ„æºéœ€æ±‚ä¹Ÿä½

* Instruction Tuning çš„å…·ä½“æ“ä½œ
    - æ ¸å¿ƒè®­ç»ƒç›®æ ‡ï¼š
        - ç»™å®šä¸€ä¸ªçŸ¥è¯†åº“ï¼ˆKBï¼‰å’Œä¸€ä¸ªé—®é¢˜ Qï¼Œæ¨¡å‹éœ€è¦è¾“å‡ºæ­£ç¡®ç­”æ¡ˆ Aã€‚
    - è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼š
        - :math:`\max_{\theta} \log p_{\theta, \phi}(A \mid Q, \text{KB})`
        - :math:`\theta` â†’ å¯è®­ç»ƒçš„ adapter å‚æ•°
        - :math:`\phi` â†’ é¢„è®­ç»ƒ LLM çš„å›ºå®šå‚æ•°ï¼ˆåŒ…æ‹¬åŸæœ‰çš„ QKVã€FFNã€embedding ç­‰ï¼‰
    - é‡ç‚¹æ˜¯ åªæ›´æ–° :math:`\theta` ï¼Œå†»ç»“ :math:`\phi` ã€‚

* ä¸ºä»€ä¹ˆç”¨ synthetic KBï¼Ÿ
    - æ ¸å¿ƒåŸå› ï¼š
        - Instruction tuning é‡ç‚¹ ä¸æ˜¯è®°ä½å…·ä½“ KB å†…å®¹ï¼Œè€Œæ˜¯ï¼š
            - å­¦ä¹ å¦‚ä½•æŠŠ sentence encoder çš„ç©ºé—´æ˜ å°„åˆ° LLM çš„è¯­ä¹‰ç©ºé—´
        - åˆæˆ KB çš„å¤šæ ·æ€§è¶³å¤Ÿï¼Œèƒ½ç¡®ä¿æ¨¡å‹å­¦åˆ°çš„æ˜¯æ³›åŒ–èƒ½åŠ›ï¼Œè€Œéæ­»è®° KBã€‚


6. EXPERIMENTS
==============

* å®éªŒç›®çš„æ¦‚è¿°-éªŒè¯ KBLaM æ¨¡å‹çš„èƒ½åŠ›ï¼š
    1. KBLaM çš„æ³¨æ„åŠ›çŸ©é˜µï¼ˆattention matrixï¼‰æ˜¯å¦å…·å¤‡å¯è§£é‡Šæ€§ï¼Œä»¥åŠæ˜¯å¦èƒ½å‡†ç¡®åœ°ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ä¿¡æ¯ã€‚
    2. å›ç­”é—®é¢˜çš„èƒ½åŠ›ï¼šå’Œä¼ ç»Ÿçš„ In-Context Learning æ–¹æ³•ç›¸æ¯”ï¼ŒKBLaM æ˜¯å¦èƒ½åœ¨ æ›´ä½å†…å­˜æ¶ˆè€—ä¸‹ä¿æŒç±»ä¼¼çš„æ€§èƒ½ï¼Œä¸”æ”¯æŒ æ‰©å±•åˆ° 10K æ¡ triple è€Œæ€§èƒ½ä¸æ˜æ˜¾ä¸‹é™ã€‚
    3. æ‹’ç­”èƒ½åŠ›ï¼šå½“çŸ¥è¯†åº“ä¸­æ²¡æœ‰ç­”æ¡ˆæ—¶ï¼ŒKBLaM æ˜¯å¦èƒ½é€‚å½“åœ°æ‹’ç»å›ç­”ï¼Œè€Œä¸”æ¯” In-Context Learning æ›´å°‘â€œè¿‡åº¦æ‹’ç­”(â€œover-refusalâ€)â€ã€‚
    4. æ¶ˆèå®éªŒï¼šæµ‹è¯• KBLaM è®¾è®¡é‡Œçš„å…³é”®ç»„ä»¶ï¼ŒéªŒè¯å®ƒä»¬å¯¹æ•´ä½“æ•ˆæœçš„å½±å“ã€‚

.. note:: KBLaM åœ¨ä¿è¯æ€§èƒ½çš„åŒæ—¶ï¼Œå¤§å¹…é™ä½å†…å­˜éœ€æ±‚ï¼Œå¹¶é€šè¿‡è®­ç»ƒå­¦ä¼šäº†â€œå¯è§£é‡Šæ€§æ£€ç´¢+æ¨ç†â€èƒ½åŠ›ï¼Œæ˜¯æ¯” In-context Learning æ›´é«˜æ•ˆã€æ‰©å±•æ€§æ›´å¥½çš„ KB-Augmented LLM æ–¹æ¡ˆã€‚


6.1 EXPERIMENT SETTING
----------------------

* æ¨¡å‹è§„æ ¼(Model specification)
    - ä½¿ç”¨ Llama3 8B ä½œä¸ºä¸»æ¨¡å‹ï¼Œå¹¶ç»è¿‡æŒ‡ä»¤å¾®è°ƒã€‚
    - ç”¨ OpenAI çš„ ada-002 sentence embedding model é¢„è®­ç»ƒå¾—åˆ°çŸ¥è¯†åº“ key/value çš„åŸºæœ¬è¡¨ç¤ºï¼ˆç”¨äºåç»­ attention ä¸­çš„ key/valueï¼‰ã€‚

* ä¼˜åŒ–è®¾ç½®(Optimization setting )
    - Key å’Œ Value adapterï¼šéšæœºåˆå§‹åŒ–ã€‚
    - Query headsï¼šä»é¢„è®­ç»ƒ Llama3 8B çš„æƒé‡åŠ è½½ã€‚
    - ä¼˜åŒ–å™¨ï¼šAdamWï¼Œå­¦ä¹ ç‡ä» `5Ã—10^-4` è¡°å‡åˆ° `5Ã—10^-6`ï¼Œè®­ç»ƒ 20K æ¬¡è¿­ä»£ï¼Œæ¯æ‰¹ 400 ä¸ª Q&A æ ·æœ¬ã€‚
    - è®­ç»ƒç¡¬ä»¶ï¼š80GB A100 GPUï¼Œä½¿ç”¨ bfloat16 ç²¾åº¦ã€‚

* è®­ç»ƒæ•°æ®æ„é€ (Construction of the training batches)
    - æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ï¼šä¸€ä¸ª çŸ¥è¯†åº“(KB)ã€ä¸€ä¸ªé—®é¢˜ã€ä¸€ä¸ªç­”æ¡ˆã€‚
    - çŸ¥è¯†åº“æ¥æºï¼šåˆæˆ KB ä¸­ å‰ 120K triplesã€‚
    - æ¯ä¸ª sample çš„ KB æ˜¯ä»ä¸­ éšæœºé€‰å– 10~100 triplesã€‚
    - è®¾è®¡å››ç±» Q&Aï¼š
        1. ç®€å•å•å®ä½“é—®é¢˜
        2. å¤šå®ä½“é—®é¢˜ï¼ˆæ¶‰åŠå¤šä¸ª triplesï¼‰
        3. å¼€æ”¾æ€§é—®é¢˜
        4. æ— ç­”æ¡ˆé—®é¢˜ï¼ˆunanswerableï¼‰
    - æ¯ batch çš„ 20 ä¸ª micro-batchesï¼š
        - 2 ä¸ªæ— ç­”æ¡ˆ
        - 6 ä¸ªç®€å•
        - 6 ä¸ªå¤šå®ä½“
        - 6 ä¸ªå¼€æ”¾æ€§
    - æå‰è®¡ç®—å¥½æ‰€æœ‰ triples çš„ key/value å‘é‡ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚

* éªŒè¯é›†(Evaluation dataset)
    - åˆæˆæ•°æ®ï¼š15000 æ¡æœªç”¨äºè®­ç»ƒçš„ triplesã€‚
    - Enron æ•°æ®é›†ï¼šä» Enron é‚®ä»¶ä¸­æå–çš„ triplesï¼Œç»è¿‡å°æ¨¡å‹å¾®è°ƒ & å»é‡ã€‚

* åŸºçº¿æ–¹æ³•(Baseline)
    1. In-Context Learningï¼šç›´æ¥æŠŠæ‰€æœ‰ triples å±•å¹³åŠ åˆ° prompt é‡Œï¼Œå®¹æ˜“å‡ºç°å†…å­˜ç“¶é¢ˆï¼ˆæœ€å¤šåªèƒ½å¤„ç† 200 triplesï¼‰ã€‚
    2. Zero-shotï¼šä¸æä¾› KBï¼Œé  LLM è‡ªå¸¦çŸ¥è¯†ç›´æ¥å›ç­”ã€‚

* è¯„ä¼°è®¾ç½®(Evaluation setting)
    - æ¯ç§è¯„ä¼°ï¼Œéšæœºè·‘ 5 æ¬¡ï¼Œæ¯æ¬¡ 100 ä¸ªæµ‹è¯•æ ·æœ¬ï¼ŒKB å¤§å°ä¸ç­‰ï¼Œæ€»å…± 500 ä¸ªæµ‹è¯•é—®é¢˜ï¼Œå–å¹³å‡ç»“æœã€‚


6.2 EXPERIMENT RESULTS
----------------------

* æ³¨æ„åŠ›ä½œä¸ºæ£€ç´¢å™¨(KBLAM attention is an accurate retriever)
    - è§‚å¯Ÿåˆ° KBLaM çš„æ³¨æ„åŠ›çŸ©é˜µç¡®å®è¡¨ç°å‡ºæ£€ç´¢è¡Œä¸ºï¼š
    - å…·ä½“è¡¡é‡æ–¹æ³•ï¼šç¬¬ 15 å±‚ï¼ˆLlama3 çš„ä¸­é—´å±‚ï¼‰çš„æ³¨æ„åŠ›å¾—åˆ†ï¼ˆè·¨ 32 ä¸ªå¤´å¹³å‡ï¼‰ä½œä¸ºåˆ†ç±»åˆ†æ•°ï¼Œå»çœ‹ top-1 å’Œ top-5 å‡†ç¡®ç‡ã€‚
    - ç»“æœï¼š
        - åœ¨åˆæˆæ•°æ®å’Œ Enron æ•°æ®ä¸Šï¼ŒKBLaM éƒ½èƒ½å‡†ç¡®æŠŠæ³¨æ„åŠ›æ”¾åœ¨æ­£ç¡® triple ä¸Šï¼ŒEnron è¿™ç§ OOD æ•°æ®è¡¨ç°ç¨å·®ä½†è¿˜èƒ½æ¥å—ã€‚
        - æ³¨æ„ï¼šè¿™ä¸ªæ£€ç´¢èƒ½åŠ›å®Œå…¨é  æŒ‡ä»¤å¾®è°ƒ å­¦å‡ºæ¥çš„ï¼Œæ²¡æœ‰åŠ ä¸“é—¨æ­£åˆ™åŒ–æˆ–æ£€ç´¢ lossã€‚

* KBLaM çš„æ¨ç†èƒ½åŠ›(KBLAM can reason about knowledge)
    - é’ˆå¯¹ä¸‰ç±»é—®é¢˜ï¼ˆç®€å•ã€å¤šå®ä½“ã€å¼€æ”¾æ€§ï¼‰è¯„ä¼°ï¼š
        - ç®€å•ã€å¤šå®ä½“ç”¨ BERT score è¡¡é‡è¾“å‡ºè´¨é‡ã€‚
        - å¼€æ”¾æ€§é—®é¢˜ç”¨ GPT-4 æ‰“åˆ†ï¼ŒèŒƒå›´ 0~5ã€‚
    - ç»“æœï¼š
        - åœ¨åˆæˆæ•°æ®ä¸Šï¼ŒKBLaM å’Œ In-context Learning æ€§èƒ½æ¥è¿‘ï¼Œä½† å†…å­˜å ç”¨è¿œå°ï¼Œæ‰©å±•æ€§æ›´å¥½ã€‚
        - åœ¨ Enron æ•°æ®ï¼ˆOODï¼‰ ä¸Šï¼ŒKBLaM æ€§èƒ½æœ‰ä¸‹é™ï¼Œä½†ä¾ç„¶å¥½äº zero-shotï¼Œè¯´æ˜ KB ä¿¡æ¯ç¡®å®è¢«æœ‰æ•ˆåˆ©ç”¨ã€‚
        - ä»æ ·æœ¬è¾“å‡ºçœ‹ï¼š
        - åˆæˆ KB ä¸Šï¼Œå›ç­”å‡ ä¹å’Œ triple å†…å®¹ä¸€è‡´ã€‚
        - Enron ä¸Šï¼Œå›ç­”æ„æ€å¯¹ï¼Œä½†æªè¾ã€ç»†èŠ‚å·®å¼‚æ›´å¤§ã€‚

æ€»ç»“äº®ç‚¹
--------

- KBLaM å†…å­˜å‹å¥½ï¼Œç›¸æ¯” In-context Learningï¼Œä¸ç”¨é‡å¤è¾“å…¥æ‰€æœ‰ triplesã€‚
- å…·å¤‡ æ£€ç´¢è§£é‡Šæ€§ï¼Œå¯é€šè¿‡æ³¨æ„åŠ›çŸ©é˜µçœ‹å‡ºæ¨¡å‹ä¾èµ–äº†å“ªäº› KB æ¡ç›®ã€‚
- å¯ä»¥é€‚åº” è§„æ¨¡æ›´å¤§çš„ KBï¼ˆ10K triplesï¼‰ï¼Œæ€§èƒ½ç¨³å®šã€‚
- å…·å¤‡ æ‹’ç­”æœºåˆ¶ï¼Œé¢å¯¹æ— ç­”æ¡ˆé—®é¢˜ï¼Œèƒ½æœ‰æ•ˆè¯†åˆ«ã€‚



7. CONCLUSION
=============

.. note:: KBLaM é€šè¿‡æŠŠçŸ¥è¯†åº“ç¼–ç æˆç¨ å¯†å‘é‡ï¼Œå¹¶åˆ©ç”¨ä¸‰å…ƒç»„ä¹‹é—´çš„ç‹¬ç«‹æ€§ï¼Œè®© LLM å¯ä»¥è½»é‡ã€é«˜æ•ˆã€åŠ¨æ€åœ°è°ƒç”¨å¤–éƒ¨çŸ¥è¯†ï¼ŒåŒæ—¶ä¿æŒè§£é‡Šæ€§ï¼Œå¹¶é¿å…å¤§è§„æ¨¡è‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚


* èƒŒæ™¯ï¼šå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æœ¬èº«åŒ…å«äº†å¤§é‡ä¸–ç•ŒçŸ¥è¯†ï¼Œä½†è¿™äº›çŸ¥è¯†æ˜¯é™æ€çš„ï¼Œæ›´æ–°æˆæœ¬é«˜ã€‚è€Œ å¤–éƒ¨çŸ¥è¯†åº“ï¼ˆKnowledge Base, KBï¼‰ æ˜¯ç»“æ„åŒ–æˆ–åŠç»“æ„åŒ–çš„ä¿¡æ¯æ¥æºï¼Œæ¯”å¦‚ä¸‰å…ƒç»„ï¼ˆSubject-Predicate-Objectï¼‰ï¼Œå¯ä»¥æä¾›æ›´å¤š åŠ¨æ€ã€å¯æ›´æ–° çš„çŸ¥è¯†ã€‚
* æ ¸å¿ƒå†…å®¹ï¼šKBLaM æ˜¯ä»–ä»¬æå‡ºçš„ä¸€ä¸ªæ–°æ–¹æ³•ï¼Œç›®çš„æ˜¯ é«˜æ•ˆåœ°å°†å¤–éƒ¨çŸ¥è¯†åº“ä¸é¢„è®­ç»ƒçš„å¤§è¯­è¨€æ¨¡å‹ç»“åˆã€‚

* å…·ä½“åšæ³•
    a. çŸ¥è¯†è¡¨ç¤ºæ–¹å¼
        - ä¼ ç»Ÿçš„åšæ³•æ˜¯ç”¨ åŸå§‹æ–‡æœ¬ æˆ– ç¬¦å·åŒ–ä¸‰å…ƒç»„ å­˜å‚¨çŸ¥è¯†ã€‚
        - KBLaM é€‰æ‹©ç”¨ ç¨ å¯†è¿ç»­å‘é‡ï¼ˆdense continuous vectorsï¼‰ è¡¨ç¤ºçŸ¥è¯†ã€‚
        - å°±æ˜¯æŠŠæ¯ä¸ªçŸ¥è¯†å•å…ƒï¼Œæ¯”å¦‚ä¸‰å…ƒç»„ `(subject, predicate, object)`ï¼Œç¼–ç æˆä¸€ä¸ªå›ºå®šç»´åº¦çš„å‘é‡ã€‚
    b. ç‹¬ç«‹æ€§ç»“æ„
        - ä¸‰å…ƒç»„ä¹‹é—´é€šå¸¸æ˜¯ç‹¬ç«‹çš„ï¼ˆå³ä¸€ä¸ªä¸‰å…ƒç»„çš„ä¿¡æ¯ä¸ä¼šç›´æ¥å½±å“å¦ä¸€ä¸ªï¼‰ã€‚
        - KBLaM åˆ©ç”¨è¿™ç§ç»“æ„ä¼˜åŠ¿ï¼Œé¿å…äº† LLM å¯¹æ‰€æœ‰ä¸‰å…ƒç»„è¿›è¡Œå…¨å±€è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰ã€‚


* å¸¦æ¥çš„å¥½å¤„
    ğŸ”¹ é¿å…äº†é«˜æ˜‚çš„è®¡ç®—å¼€é”€  
        - ä¼ ç»Ÿçš„ LLM éœ€è¦å¯¹æ•´ä¸ªçŸ¥è¯†åº“ï¼ˆå¯èƒ½æ˜¯ç™¾ä¸‡é‡çº§ï¼‰è¿›è¡Œ self-attentionï¼Œè®¡ç®—é‡å·¨å¤§ã€‚è€Œ KBLaM åªéœ€è¦å…³æ³¨ä¸å½“å‰é—®é¢˜ç›¸å…³çš„ç¨ å¯†å‘é‡ï¼Œå‡å°‘äº†ä¸å¿…è¦çš„è®¡ç®—ã€‚
    ğŸ”¹ åŠ¨æ€æ›´æ–°çŸ¥è¯†  
        - å› ä¸ºçŸ¥è¯†æ˜¯ç¨ å¯†å‘é‡å­˜å‚¨çš„ï¼Œæ›´æ–°æŸä¸ªçŸ¥è¯†ç‚¹ä¸éœ€è¦é‡æ–°å¾®è°ƒ LLMï¼Œåªéœ€æ›¿æ¢æˆ–æ–°å¢ç›¸åº”å‘é‡å³å¯ã€‚
    ğŸ”¹ å¯è§£é‡Šæ€§å¢å¼º  
        - æ¯ä¸ªä¸‰å…ƒç»„éƒ½æœ‰æ˜ç¡®çš„å«ä¹‰ï¼Œä¸”æ˜¯ç‹¬ç«‹çš„ã€‚
        - åœ¨æ¨¡å‹æ¨ç†æ—¶ï¼Œèƒ½å¤Ÿæ¸…æ™°åœ°çœ‹åˆ°å“ªäº›å…·ä½“çŸ¥è¯†è¢«ç”¨åˆ°äº†ã€‚
    ğŸ”¹ Attention ç›´æ¥å½“åšæ£€ç´¢æœºåˆ¶  
        - æ¨¡å‹å†…éƒ¨çš„ attention æœºåˆ¶ ä¹Ÿå¯ä»¥ç”¨äºæ£€ç´¢ç›¸å…³çš„çŸ¥è¯†å‘é‡ï¼Œè€Œä¸éœ€è¦é¢å¤–è®¾è®¡å¤æ‚çš„æ£€ç´¢å™¨ã€‚

* In this paper, we propose KBLaM, which efficiently augments pre-trained LLMs with external knowledge bases. 
* KBLaM represents external knowledge as dense continuous vectors and leverages the independence structure between triples, which offers several advantages: 
    * avoiding expensive self-attention over large knowledge sources, 
    * enabling dynamic knowledge updates without fine-tuning, 
    * improving interpretability, and 
    * allowing attention to perform retrieval. 
* These benefits could potentially also be applied to settings using raw text as knowledge representation.




8. LIMITATIONS AND FUTURE WORK
==============================

.. note:: æ€»ç»“ï¼šå½“å‰ KBLaM çš„å±€é™åœ¨äºæ•°æ®å¤šæ ·æ€§ä¸è¶³ã€ä¿¡æ¯å‹ç¼©æœ‰æŸä»¥åŠå¾®è°ƒä»»åŠ¡ç®€å•ï¼Œæœªæ¥å¯ä»¥é€šè¿‡æ”¹è¿›åˆæˆæ•°æ®è´¨é‡ã€è°ƒæ•´ä¿¡æ¯å‹ç¼©ç­–ç•¥ï¼Œä»¥åŠè®¾è®¡æ›´å¤æ‚çš„æ¨ç†æŒ‡ä»¤ï¼Œè®©æ¨¡å‹åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šè¡¨ç°å¾—æ›´ç¨³ã€æ›´å¼ºã€‚


* Higher quality synthetic KBï¼ˆæ›´é«˜è´¨é‡çš„åˆæˆçŸ¥è¯†åº“ï¼‰
    - èƒŒæ™¯ï¼š- KBLaM è¿™ç¯‡è®ºæ–‡é‡Œçš„çŸ¥è¯†åº“ï¼ˆKBï¼‰æ˜¯å®Œå…¨ç”¨åˆæˆæ•°æ®è®­ç»ƒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¸æ˜¯ç”¨çœŸå®çš„ç»´åŸºç™¾ç§‘ã€Freebase è¿™æ ·çš„ç°æˆçŸ¥è¯†åº“ï¼Œè€Œæ˜¯äººé€ æ•°æ®ç”Ÿæˆçš„ã€‚
    - é—®é¢˜ï¼š- å®éªŒè¡¨æ˜ï¼Œè™½ç„¶æ¨¡å‹åœ¨åˆæˆæ•°æ®ä¸Šè¡¨ç°ä¸é”™ï¼Œä½†ä¸€æ—¦æ¢åˆ°ç°å®ä¸–ç•Œçš„ã€æ²¡è§è¿‡çš„æ•°æ®ï¼ˆæ¯”å¦‚ Enron é‚®ä»¶æ•°æ®é›†ï¼‰ï¼Œæ€§èƒ½ä¼šä¸‹é™ï¼Œæ³›åŒ–èƒ½åŠ›ä¸è¶³ã€‚
    - è§£å†³æ–¹å‘ï¼š- æ›´å¤§ã€æ›´å¤šæ ·çš„åˆæˆæ•°æ®é›†ï¼šé€šè¿‡è®©åˆæˆæ•°æ®çš„é£æ ¼ã€å†…å®¹æ›´æ¥è¿‘çœŸå®ä¸–ç•Œï¼Œæ¨¡å‹å¯ä»¥å­¦å¾—æ›´â€œå¹¿â€ã€‚
    - ä»çœŸå® KB å‡ºå‘ç”Ÿæˆåˆæˆæ•°æ®ï¼š- ä¸æ˜¯å®Œå…¨å‡­ç©ºæé€ ï¼Œè€Œæ˜¯ä»¥çœŸå® KB ä¸ºâ€œç§å­â€å»æ‰©å±•ï¼Œèƒ½æ›´å¥½åœ°æ¨¡æ‹ŸçœŸå®åˆ†å¸ƒï¼Œè®©æ¨¡å‹é€‚åº”æ€§æ›´å¼ºã€‚

* Information loss in knowledge tokenï¼ˆçŸ¥è¯† token ä¸­çš„ä¿¡æ¯æŸå¤±ï¼‰
    - èƒŒæ™¯ï¼š- KBLaM é‡Œï¼ŒæŠŠçŸ¥è¯†åº“é‡Œçš„ä¸‰å…ƒç»„ï¼ˆæ¯”å¦‚ï¼š`(subject, predicate, object)`ï¼‰å‹ç¼©æˆå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯â€œçŸ¥è¯† tokenâ€ã€‚
    - é—®é¢˜ï¼š- è¿™ç§å‹ç¼©æœ‰æŸï¼Œå°¤å…¶åœ¨éœ€è¦ç²¾ç¡®æ–‡æœ¬è¾“å‡ºæ—¶ä¼šå‡ºé—®é¢˜ï¼š
        - æ¯”å¦‚éœ€è¦ç”Ÿæˆ å…·ä½“åå­—ã€æ•°å­—ï¼Œå‹ç¼©å‘é‡å¯èƒ½ä¼šä¸¢æ‰å…³é”®ç»†èŠ‚ï¼Œå¯¼è‡´è¾“å‡ºä¸å‡†ç¡®ã€‚
        - ä½†æ˜¯ï¼Œå¯¹äºä¸€äº›å®½æ³›çš„ä¿¡æ¯ï¼ˆå¦‚â€œæè¿°â€ã€â€œç›®æ ‡â€ç±»å±æ€§ï¼‰ï¼Œå¤§è‡´æ„æ€å°±å¤Ÿäº†ã€‚
    - æœªæ¥æ–¹å‘ï¼š- å¼•å…¥ä¸€ä¸ªâ€œå‹ç¼©ç‡è¶…å‚æ•°â€ï¼Œå¯ä»¥æ ¹æ®ä¸åŒä¸‰å…ƒç»„å†…å®¹è°ƒæ•´å‹ç¼©ç¨‹åº¦ã€‚
        - å¦‚æœæ˜¯æ•°å­—ã€ä¸“æœ‰åè¯ â†’ ä½å‹ç¼©ï¼ˆå°½é‡ä¿ç•™ç»†èŠ‚ï¼‰ã€‚
        - å¦‚æœæ˜¯æè¿°æ€§ä¿¡æ¯ â†’ é«˜å‹ç¼©å³å¯ã€‚

* More sophisticated instruction tuningï¼ˆæ›´å¤æ‚çš„æŒ‡ä»¤å¾®è°ƒï¼‰
    - èƒŒæ™¯ï¼š- ç›®å‰ KBLaM çš„æŒ‡ä»¤å¾®è°ƒï¼ˆinstruction tuningï¼‰æ˜¯ç›¸å¯¹ç®€å•çš„ï¼Œä¾‹å¦‚ç®€å•é—®ç­”ã€‚
    - é—®é¢˜ & æœªæ¥æå‡ç©ºé—´ï¼š- æœªæ¥å¯ä»¥æ¢ç´¢æ›´å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œæ¯”å¦‚ï¼š
        - å¤šè·³æ¨ç†ï¼ˆmulti-hopï¼‰ï¼šéœ€è¦è¿ç»­æŸ¥å¤šæ¡ KB è®°å½•æ‰èƒ½å¾—å‡ºç­”æ¡ˆã€‚
        - å¤šæ­¥æ¨ç†ï¼ˆmulti-stepï¼‰ï¼šä¸€æ­¥æ­¥æ¨å¯¼ï¼Œåƒé“¾å¼æ€ç»´ï¼ˆchain-of-thoughtï¼‰ã€‚
        - å¤æ‚çš„ é“¾å¼æ¨ç†ï¼Œä¸ä»…æ˜¯æ£€ç´¢å•æ¡çŸ¥è¯†ï¼Œè¿˜éœ€è¦æ¨ç†é€»è¾‘é“¾ã€‚
    - è§‚ç‚¹ï¼š- ç›¸æ¯” RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ï¼ŒKBLaM æŠŠæ•´ä¸ª KB èè¿›ä¸Šä¸‹æ–‡ï¼Œæœ‰æ½œåŠ›æ›´çµæ´»åœ°è¿›è¡Œå¤æ‚æ¨ç†ï¼Œä¸ä¾èµ–å¤–éƒ¨æ£€ç´¢ã€‚



Appendix A Extended related work
================================

* Memory Augmented Language Modelsï¼ˆè®°å¿†å¢å¼ºå‹è¯­è¨€æ¨¡å‹ï¼‰
    - æœ‰å¾ˆå¤šç ”ç©¶å°è¯•ç»™è¯­è¨€æ¨¡å‹åŠ â€œå¤–éƒ¨è®°å¿†â€ï¼Œé€šå¸¸æ˜¯ ä¸€ç»„å›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚
    - KBLaM çš„çŸ¥è¯† token ä¹Ÿå¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§å¤–éƒ¨è®°å¿†ï¼Œæ¥æºäºçŸ¥è¯†åº“ï¼ˆKBï¼‰ã€‚
    - å¯¹æ¯”ï¼š
    - `Wu et al. (2022) <https://arxiv.org/abs/2203.08913>`_ æœ€æ¥è¿‘ KBLaM çš„æ–¹æ³•ï¼š
    - ä»–ä»¬ç”¨key-value å‘é‡å¯¹æ„å»ºå¤–éƒ¨è®°å¿†ï¼Œå’Œ KBLaM çš„åšæ³•å¾ˆåƒã€‚
    - ä½†æœ‰ä¸¤ä¸ªä¸åŒï¼š
        1. Wu et al. éœ€è¦ä»é›¶è®­ç»ƒ transformerï¼Œä»£ä»·å¤§ã€‚
        2. ä»–ä»¬çš„è®°å¿†æ¥è‡ªäºä¹‹å‰è®­ç»ƒè¿‡ç¨‹ä¸­è§åˆ°çš„ tokenã€‚
        3. è€Œ KBLaM çš„è®°å¿†ç›´æ¥æ¥æºäºå¤–éƒ¨ KBï¼Œæ¨ç†é˜¶æ®µåŠ¨æ€åŠ è¿›å»ï¼Œä¸éœ€è¦é‡æ–°è®­ç»ƒå¤§æ¨¡å‹ã€‚

* Token Compressionï¼ˆToken å‹ç¼©ï¼‰
    - KBLaM æŠŠæ¯ä¸ªä¸‰å…ƒç»„ (subject, predicate, object) ç¼–ç æˆä¸€ä¸ª å›ºå®šé•¿åº¦çš„å‘é‡å¯¹ï¼Œç›¸å½“äºå‹ç¼©æˆå•ä¸ª token çš„é•¿åº¦ã€‚
    - ç±»ä¼¼çš„å‹ç¼©æ–¹æ³•åœ¨å…¶å®ƒå·¥ä½œé‡Œä¹Ÿæœ‰å°è¯•ï¼Œæ¯”å¦‚ï¼š
        1. Ge et al. (2023)ï¼š
            - æŠŠé•¿ prompt æ›¿æ¢æˆç‰¹æ®Š tokenï¼Œå‘Šè¯‰ LLM æ‰§è¡Œç‰¹å®šä»»åŠ¡ã€‚
            - ç¼ºç‚¹ï¼šéœ€è¦å¯¹ LLM å¾®è°ƒï¼Œæ”¹åŠ¨æˆæœ¬é«˜ã€‚
        2. Mu et al. (2024)ï¼š
            - ç”¨å¾®è°ƒè¿‡çš„ LLM æŠŠé•¿ä¸Šä¸‹æ–‡å‹ç¼©æˆâ€œmemory slotsâ€ã€‚
            - è¿™ç§æ–¹æ³•æ›´å¼ºå¤§ï¼Œä½†ä»£ä»·ä¹Ÿæ›´é«˜ã€‚
    - KBLaM çš„ä¼˜ç‚¹ï¼š
        - ç›´æ¥ç”¨é¢„è®­ç»ƒ sentence encoder æŠŠ KB å‹ç¼©ï¼Œä¸å¾®è°ƒä¸» LLMã€‚
    - æœªæ¥æ–¹å‘ï¼š
        - å¯ä»¥å°è¯•æŠŠ Mu et al. é‚£ç§æ›´å¼ºçš„å‹ç¼©å™¨ç»“åˆè¿› KBLaM æ¡†æ¶ï¼Œæå‡å‹ç¼©æ•ˆæœã€‚

* Structured Attentionï¼ˆç»“æ„åŒ–æ³¨æ„åŠ›ï¼‰
    - æœ‰äº›ç ”ç©¶ä¿®æ”¹äº† Transformer çš„æ³¨æ„åŠ›æ©ç ï¼Œä¸ç”¨æ ‡å‡†çš„ ä¸‹ä¸‰è§’ causal maskã€‚
    - ä¸¾çš„ä¾‹å­åŒ…æ‹¬ï¼š
        1. Ratner et al. (2022)ï¼šåœ¨ few-shot learning é‡Œå‡è®¾ä¸åŒç¤ºä¾‹ä¹‹é—´ç›¸äº’ç‹¬ç«‹ï¼Œä¸éœ€è¦äº’ç›¸ attendã€‚
        2. Cai et al. (2023) å’Œ Merth et al. (2024)ï¼šå¤„ç†å¤šæ–‡æ¡£æ—¶å‡è®¾ä¸åŒæ¥æºçš„æ–‡æ¡£æ˜¯ç‹¬ç«‹çš„ï¼Œä¸è®©å®ƒä»¬äº’ç›¸ attendã€‚
    - ä¼˜ç‚¹ï¼š
        - å‡å°‘è®¡ç®—æˆæœ¬ã€‚
        - æé«˜å¯è§£é‡Šæ€§ã€‚
    - KBLaM çš„å¯¹åº”åšæ³•ï¼š
        - ä¹Ÿåšäº†ç‹¬ç«‹æ€§å‡è®¾ï¼š
        - å‡è®¾ä¸åŒ KB ä¸‰å…ƒç»„ç›¸äº’ç‹¬ç«‹ï¼Œattention ç»“æ„ä¸Šè®©å®ƒä»¬å½¼æ­¤ç‹¬ç«‹ã€‚
        - è¿™æ · KBLaM èƒ½æ›´é«˜æ•ˆæ‰©å±•ï¼Œä¸”ç»“æ„æ›´æ¸…æ™°å¯è§£é‡Šã€‚




Appendix B Ablation study
=========================

1ï¸âƒ£ Choice of Encodersï¼ˆç¼–ç å™¨é€‰æ‹©ï¼‰
    - OpenAI embedding æ•ˆæœæ›´å¥½ã€‚
    - ç»´åº¦é«˜çš„åµŒå…¥ æ¯”ä½ç»´åº¦åµŒå…¥è¡¨ç°æ›´å¥½ï¼ˆä¿¡æ¯å®¹é‡å¤§ã€è¡¨è¾¾èƒ½åŠ›å¼ºï¼‰ã€‚

2ï¸âƒ£ Where to Add Knowledge Tokensï¼ˆåœ¨å“ªå±‚åŠ çŸ¥è¯† tokensï¼‰
    - æ—©æœŸå±‚çš„çŸ¥è¯† token è¡¨ç¤ºå˜åŒ–ä¸å¤§ï¼ˆä¸åŒä¸‰å…ƒç»„ä¹‹é—´ç›¸ä¼¼åº¦é«˜ï¼‰ã€‚
    - åæœŸå±‚å˜åŒ–è¾ƒå¤§ï¼Œè¯´æ˜åæœŸå±‚æ›´å¤šå…³æ³¨å…·ä½“ KB å†…å®¹ã€‚

3ï¸âƒ£ Frequency of Knowledge Tokensï¼ˆçŸ¥è¯† token å‡ºç°é¢‘ç‡ï¼‰
    - æ—©æœŸå±‚çš„çŸ¥è¯† tokens å¯èƒ½åƒ è½¯æŒ‡ä»¤ promptï¼Œå‘Šè¯‰æ¨¡å‹è¯¥æ€ä¹ˆç”¨ KBã€‚
    - åŠ å¾—è¶Šå°‘ï¼Œæ¨¡å‹è¶Šå¯èƒ½å¿½è§† KB æˆ–ä¸ç†è§£ KB ç”¨æ³•ã€‚


Appendix C Sample KB
====================

Here we present some example triples from both KBs, where each triple is of format::

    (<name>; <property>; <value>)


.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/bTbhtj.png

    Table 1: Synthetic KB


.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/K71jYo.png

    Table 2: Enron KB




SAMPLE Q&A
==========

.. figure:: https://img.zhaoweiguo.com/uPic/2025/03/h5W0Ja.png

    Figure 12: Examples of instructions tuning dataset. Given a KB of 4 triples (top panel), we generate 4 types of Q&A for instruction tuning. The KB shown is sampled from the synthetic KB; The question and answer for **open-ended Q&A** are also synthesized by GPT, the rest Q&As are generated using formatted string.



PROMPT
======

PROMPT FOR SYNTHETIC KB GENERATION
----------------------------------

system prompt::

    You are a AI system that generates synthetic data examples in JSON format

a list of idea types and a list of object types::

    idea_types = [
        â€™greek lettersâ€™, â€™fiction charactersâ€™, â€™famous rock bandsâ€™,
        â€™birdsâ€™, â€™animalsâ€™, â€™natural phenomenaâ€™, â€™physical locationsâ€™,
        â€™artist namesâ€™, â€™classical musicâ€™, â€™musical instrumentsâ€™,
        â€™music genresâ€™, â€™art stylesâ€™, â€™ancient Roman conceptsâ€™,
        â€™Hindu mythsâ€™, â€™Cthulhu Mythosâ€™, â€™real-world company namesâ€™,
        â€™mythological creaturesâ€™, â€™planets and starsâ€™, â€™historical figuresâ€™,
        â€™literary genresâ€™, â€™botanical namesâ€™, â€™famous landmarksâ€™,
        â€™scientific conceptsâ€™, â€™space missionsâ€™, â€™inventionsâ€™,
        â€™philosophical termsâ€™, â€™chemical elementsâ€™, â€™famous scientistsâ€™,
        â€™marine lifeâ€™, â€™mythological placesâ€™
    ]
    object_types = [
        â€™education companyâ€™, â€™tech companyâ€™, â€™car companyâ€™,
        â€™entertainment companyâ€™, â€™construction companyâ€™, â€™retail companyâ€™,
        â€™finance companyâ€™, â€™healthcare companyâ€™, â€™restaurantâ€™, â€™hotelâ€™,
        â€™github repoâ€™, â€™projectâ€™, â€™meeting roomâ€™, â€™buildingâ€™, â€™labâ€™,
        â€™airlineâ€™, â€™textbookâ€™, â€™websiteâ€™, â€™personal blogâ€™,
        â€™gaming companyâ€™, â€™consulting firmâ€™, â€™biotech companyâ€™, â€™appâ€™,
        â€™software toolâ€™, â€™bookstoreâ€™, â€™e-commerce siteâ€™,
        â€™social media platformâ€™, â€™fitness brandâ€™, â€™fashion brandâ€™,
        â€™non-profit organizationâ€™
    ]



combination of object type and idea type in the list::

    Please randomly generate 50 {object_type} name innovated by {idea_type}.
    The name should have various styles.
    The generated name should be of diverse style and length, for example the
    name could have hyphen, space in it or multiple words.

which gives us 50 different names. Then in the same context, we further prompt GPT with::

    Now for each of the names generated, generate a short desciption, short
    objectives, and a purpose for the data.
    Please ensure that the generated contents has **LOW** correlation with
    the name.

which generates the <value> for three <property>s: description, objectives and purpose. Lastly, we perform one round of polishing, to let GPT diversity the language and style of the generated KB triple::

    Now for each of the name, description, objective and purpose generated,
    make their text style more diverse using a mixture of formal and informal
    language.



Prompt for open-ended Q&A generation
------------------------------------


::

    You are given a question and answer pair, please extend the question to
    be open-ended and generate a short answer. For example, you could
    generate
    What is the objective of xxx and what do you think of it?
    Make sure the answer is **only** based on information provided from the
    QA pair. In addition, please generate in the format of:
    Q: ...
    A: ...


PROMPT FOR GPT EVALUATION OF OPEN-ENDED Q&A
-------------------------------------------

system prompt::

    You are an AI system that evaluates the quality of generated responses.
    Your goal is to return a score between 0 and 5 indicating how accurate
    and useful the response is. An accurate and useful response should get a
    high score of 5.

we use the following prompt, which encourages the model to give chain-of-thought reasoning for scoring::

    A model is given a question about some information and evidence. The
    question is composed of two parts, a part that involves repeating
    information in the evidence and a part that potentially involves openended thinking. 
    Then the model generates a response. Evaluate the
    response based on how grounded it is given the evidence and how
    reasonable it is. Return an integer score and step by step explanation of
    how you arrived at the score. Score of 5 means the response is accurate,
    relevant and reasonable (in that it meets common sense).
    If the response addresses the question and uses the evidence in a
    relevant way, it should get a high score of 5. Score of 0 means the
    response is inaccurate and irrelevant or model is hallucinating. Score
    between 0 and 5 means the response is partially correct and relevant.


Followed by the prompt, we include 5 examples to better help GPT calibrate the scoring rule::

    Example 1:
    Evidence: "The purpose of Alexandria is to extract knowledge."
    Question: "Describe the purpose of Alexandria and how it can benefit users."
    Model output: "The purpose of Alexandria is to extract knowledge, it can benefit users by providing a structured way to organize knowledge."
    Score: 5
    Reason: The modelâ€™s response is accurate and relevant to the question and evidence, the open-ended part is reasonable.

    Example 2:
    Evidence: "The purpose of Alexandria is to extract knowledge."
    Question: "Describe the purpose of Alexandria and what can it extract."
    Model output: "The purpose of Alexandria is to extract knowledge, it can extract knowledge knowledge."
    Score: 5
    Reason: The modelâ€™s response is accurate and relevant to the question and evidence.

    Example 3:
    Evidence: "GreatTool is an app that helps users to be more productive."
    Question: "Describe GreatTool and how it may affect the community."
    Model output: "GreatTool is an app that helps users to be more productive. It may affect the community by helping users to sleep better."
    Score: 3
    Reason: The modelâ€™s response is accurate and relevant to the question and evidence but it is not very reasonable.

    Example 4:
    Evidence: "GreatTool is an app that helps users to be more productive."
    Question: "Describe GreatTool and how it may affect the community."
    Model output: "GreatTool is an app that helps users to be more productive. It may affect the community by helping users to organize their tasks and manage their time better improving their productivity." 
    Score: 5
    Reason: The modelâ€™s response is accurate and relevant to the question and evidence and the open ended part is sensible and reasonable.

    Example 5:
    Evidence: "GreatTool is an app that helps users to be more productive."
    Question: "Describe GreatTool and how it may affect the community."
    Model output: "GreatTool is great tool with many feature"
    Score: 0
    Reason: The modelâ€™s response is not accurate and doesnâ€™t answer the question.


PROMPT FOR LLAMA EVALUATION
---------------------------

For in-context learning, we use the following prompts for evaluation::

    # Simple Q&A
    Please answer questions based on the given text with format: "The {property} of {name} is {description}"

    # Two-entity Q&A
    Please answer questions based on the given text with format: "The {property} of {name1} is {description}; The {property} of {name2} is {description}; ..."

    # Open-ended Q&A
    You are provided a context and a question that has a retrieval part and an open-ended part.
    Please answer the question based on the given text.
    If the information for the open-ended part is not provided in the context, please generate a potential possible answer.

    # Unanserable questions
    Please answer questions based on the given text with format: "The {property} of {name} is {description}", 
        if relevant information cannot be found in the text, please respond "I am sorry I cannot find relevant information in the KB".

For zero-shot learning, we use the following prompt::

    # Simple Q&A
    Please answer the question in a very compact manner with format: The {property} of {name} is {description}

    # Two-entity Q&A
    Please answer the question in a very compact manner with format: "The {property} of {name1} is {description}; The {property} of {name2} is {description}; ...

    # Open-ended Q&A
    Please answer the question based on your knowledge.



QUESTION TEMPLATE
-----------------

For simple Q&A, we use the following templates::

    What <property> does <name> have?,
    What is the <property> of <name>?,
    Tell me about the <property> of <name>.,
    Can you let me know the <property> of <name>?,
    Can you inform me about the <property> of <name>?,
    Describe the <property> of <name>.,
    What details can you share about the <property> of <name>?,
    What kind of <property> does <name> have?,
    Provide details on the <property> of <name>.,
    What features does the <property> of <name> include?,
    Can you elaborate on the <property> of <name>?,
    How would you describe the <property> of <name>?,
    What can you tell me about the <property> characteristics of <name>?,
    Can you explain the <property> of <name>?,
    What insights can you provide about the <property> of <name>?,
    What should I know about the <property> of <name>?,

For multi-entities Q&A involving G triples, :math:`\{(<name>_g, <property>_g, <value>_g)\}^G_{g=1}` , we usethe following template::

    What is the {}
    Tell me {},
    Can you let me know {},
    Can you inform me {},
    Describe {},
    Explain {},
    Could you describe the {},
    What can you tell me about {},
    Could you provide information on {},
    Please enlighten me about {},
    Can you clarify {} for me?,
    Could you give me a detailed description of {},
    I need more information on {},

for each blank in the curly brackets, we fill in the string::

    the <property>_1 of <name>_1, 
    ..., 
    the <property>_g of <name>_g, 
    ...,
    the <property>_G of <name>_G




SAMPLE OUTPUT
=============


SYNTHETIC KB
------------

* Relevant triple:
    - (ReefPulse Jellyfish; objectives; to provide real-time alerts, support remote monitoring, and improve home safety)

* Question:
    - Can you inform me about the objectives of ReefPulse Jellyfish and what do you think of its effectiveness in achieving these goals?

* KBLaM output:
    - The objectives of ReefPulse Jellyfish are to provide real-time monitoring, send alerts, and improve home safety. While the effectiveness of ReefPulse Jellyfish in achieving these goals would depend on various factors such as the accuracy of the monitoring technology, the reliability of the alert system, and the user experience, the outlined objectives suggest a comprehensive approach to enhancing home safety.

* In-context learning output:
    - According to the context, the objectives of ReefPulse Jellyfish are to provide real-time alerts, support remote monitoring, and improve home safety.
    - Based on these objectives, I think ReefPulse Jellyfish is effective in achieving its goals. The real-time alerts feature allows users to stay informed and take prompt action in case of any issues, which can help prevent potential hazards and ensure a safer home environment. The remote monitoring feature enables users to keep track of their homeâ€™s conditions from anywhere, which can be particularly useful for people who are away from home for extended periods. Additionally, the improved home safety feature can help reduce the risk of accidents and injuries, making it a valuable tool for homeowners. Overall, I believe ReefPulse Jellyfish is

* Zero-shot learning output:
    - ReefPulse Jellyfish is a product designed to monitor and control the water parameters in aquariums, specifically targeting jellyfish and other delicate marine life. Based on my knowledge, the objectives of ReefPulse Jellyfish are:
        1. **Accurate Water Parameter Monitoring**: To provide real-time monitoring of water parameters such as pH, ammonia, nitrite, nitrate, and temperature, ensuring that the aquariumâ€™s water conditions are optimal for the jellyfish and other inhabitants.
        2. **Automated Water Parameter Control**: To automatically adjust water parameters to maintain a stable and healthy environment, reducing the need for manual intervention and minimizing the risk of water parameter fluctuations.
        3. **Enhanced Jellyfish Care**: To provide a more precise and controlled environment




ENRON
-----

* Relevant triple:
    - (Sitraka; description; a provider of Internet software technologies for IT organizations)
* Question:
    - What kind of description does Sitraka have and what services do they offer?
* KBLaM output:
    - The description of Sitraka is a software company providing networking solutions for businesses. They offer services that likely include developing and implementing network infrastructure, managing network security, and possibly providing technical support to ensure smooth network operations for their clients.
* In-context learning output:
    - Based on the given text, Sitraka is described as a provider of Internet software technologies for IT organizations. 
    - As for the services they offer, the text does not provide specific information. However, based on the general description, it can be inferred that Sitraka offers software technologies and solutions for IT organizations, possibly related to internet-based services, network management, or other IT-related areas.
* Zero-shot learning output:
    - According to my knowledge, Sitraka is a software company that provides IT service management solutions. They offer a range of services and products that help organizations manage and optimize their IT infrastructure, including:
        * IT Service Management (ITSM) software: Sitrakaâ€™s flagship product is the ITSM software, which provides a comprehensive platform for managing IT services, including incident, problem, and change management.
        * Asset Management: Sitrakaâ€™s asset management solution helps organizations track and manage their IT assets, including hardware, software, and network devices.
        * Service Desk: Sitrakaâ€™s service desk solution provides a centralized platform for managing IT requests, incidents, and problems.
        * Reporting and Analytics: Sitrakaâ€™s reporting and analytics solution provides real












