# 2503.09149_MemVid: Memory-enhanced Retrieval Augmentation for Long Video Understanding

* 首页: <https://arxiv.org/abs/2503.09149>
* PDF: <https://arxiv.org/pdf/2503.09149>
* 引用: 11(2025-12-14)


## From Moonlight

### 三行摘要

1. 🧠 MemVid 提出了一种受人类认知记忆启发的记忆增强型 RAG 框架，通过“记忆-推理-检索-聚焦”四阶段流程，有效解决了长视频理解中现有 LVLM 信息丢失和 RAG 缺乏隐式查询推理能力的问题。
2. 📚 为优化 MemVid 的推理能力和端到端性能，该方法设计了一个课程学习框架，首先通过有监督学习进行预热，然后利用生成反馈的强化学习进一步提升推理结果的质量。
3. 🚀 在 MLVU、VideoMME 和 LVBench 等主流长视频理解基准测试中，MemVid 展现出卓越的效率和有效性，显著超越了现有的 LVLM 和其他 RAG 方法，实现了更优的性能与成本效益。

### 关键词

- Long-Video Understanding: 长期视频理解（Long-Video Understanding, LVU）是指在计算机视觉领域，对包含大量时间信息的长视频内容进行分析和理解的任务。这对于视频分析、自动驾驶和具身AI等现实世界应用至关重要。与处理短视频或静态图像不同，长视频理解面临着如何有效处理冗长信息、捕捉关键事件和理解时序关系等挑战。现有方法常因信息丢失（如压缩或降采样）和计算成本高昂而难以应对。
- Long-Context Vision-Language Models: 长上下文视觉语言模型（Long-Context Vision-Language Models, LVLMs）是现有视觉语言模型（VLMs）的延伸，旨在扩展其处理能力以理解更长的视频序列。尽管它们通过扩大上下文窗口来减少信息损失，但仍然受限于原始视频序列的压缩、强制降采样带来的信息丢失以及高昂的计算成本。
- Retrieval-Augmented Generation: 检索增强生成（Retrieval-Augmented Generation, RAG）是一种通过检索相关信息来辅助生成任务的技术。在处理长序列问题时，RAG通过从海量数据中检索出有用的信息，然后基于高度简化的输入进行内容生成，从而实现成本效益。在视频领域，它通过检索视频中的相关时刻或帧来辅助回答问题。然而，标准RAG在处理需要隐含推理或复杂信息需求的问题时能力受限，因为它依赖于明确的查询，可能导致错过关键证据。
- MemVid: MemVid是本文提出的一种新颖的、受人类认知记忆启发的记忆增强检索增强（memory-enhanced retrieval augmentation）框架，专门用于解决长期视频理解（LVU）的挑战。它通过模拟人类记忆和推理过程，能够更有效地处理长视频中的信息。
- Memory-enhanced Retrieval Augmentation: 记忆增强检索增强（Memory-enhanced Retrieval Augmentation）是MemVid框架的核心概念。它在传统的检索增强（RAG）基础上，引入了一个“记忆”模块，用于存储视频的整体信息，并在此基础上进行“推理”以生成任务相关的“线索”（clues）。这些线索指导检索器更精准地定位视频中的关键时刻（evidentiary moments），从而克服了标准RAG在处理复杂或隐含查询时的不足，减少了信息丢失和证据遗漏。
- Memorizer: 记忆器（Memorizer）是MemVid框架中的一个核心模块，负责扫描整个长视频并将其整体信息存储到内存（Memory M）中，以形成对视频的全面理解。它通过编码视频的稀疏采样帧，并将其转化为一种可供推理的“记忆”表示（例如，基于Transformer的KV缓存）。记忆器是MemVid进行后续推理和生成检索线索的基础。
- Reasoning: 推理（Reasoning）在MemVid框架中扮演着至关重要的角色，特别是在“记忆器”模块之后。它接收整体视频记忆和用户问题，旨在推断出完成任务（如回答问题）所需的信息需求，并生成一系列“任务导向的检索线索”（task-oriented retrieval clues）。这些线索比原始问题更具体、更利于指导检索器去定位视频中的关键证据片段，从而实现了更深层次、更智能的视频内容理解。
- Curriculum Learning: 课程学习（Curriculum Learning）是MemVid用于训练其“记忆器”模块的策略。该策略通过分阶段的方式逐步提升模型的学习效率和性能。首先，采用监督式微调（SFT）阶段，使用高质量的、由强大教师模型生成的标注数据来预训练记忆器，使其能够生成结构化的推理输出。接着，引入带有生成反馈的强化学习（RLGFNTP）阶段，利用下游生成器对答案的正确性来优化检索线索，使其与端到端的最佳性能对齐。这种方法确保了记忆器能更好地服务于检索和最终的答案生成。

### 摘要

本文介绍了一种名为 MemVid 的新型记忆增强型检索增强生成（RAG）方法，旨在解决长视频理解（LVU）中的挑战。当前的长上下文视觉-语言模型（LVLMs）因压缩和暴力下采样导致信息丢失，而现有 RAG 方法则受限于显式查询依赖性，无法有效处理隐式且复杂的查询。受人类认知记忆过程的启发，MemVid 提出了一个四步工作流：记忆视频整体信息、根据记忆推理任务所需信息、检索关键时刻、以及聚焦检索结果以生成最终答案。

#### 核心方法论

MemVid 的工作流程包含四个基本步骤，由记忆器（memorizer）、检索器（retriever）和生成器（generator）三个关键模块驱动。本文主要关注优化记忆器模块，同时固定其他模块。

1.  **记忆（Memorizing）**：
    首先，通过一个记忆模型 $R$ 将视频 $V$ 的整体信息存储到记忆 $M$ 中。具体而言，原始视频 $V$ 被均匀下采样为 $T$ 帧的 $V' \in \mathbb{R}^{T \times H \times W \times 3}$。预训练的视觉编码器 $E_v$ 将 $V'$ 压缩为 token 状的视觉特征 $F = E_v(V') \in \mathbb{R}^{(T \times K) \times d_v}$，其中 $K$ 是每帧的 token 数量，$d_v$ 是特征深度。
    为了使视觉特征具备推理能力，MemVid 使用一个因果 Transformer 基础的语言模型 $\Theta$ 将 $F$ 进一步处理成面向推理的 Key-Value (KV) 缓存。推理指令通过嵌入器 $E_q$ 转换为 token 嵌入 $\{x_1, \ldots, x_p\}$，视觉特征表示为 $\{x_{p+1}, \ldots, x_{p+T \times K+1}\}$。对于每个时间步 $t$，Key 和 Value 计算为 $K_t = W_k X_t$ 和 $V_t = W_v X_t$，并与之前的 KV 对连接，形成最终的记忆 $M = \{K, V\}$。这一步可用公式表示为 $M = R (V' | \phi)$，其中 $\phi$ 是记忆模型的参数。

2.  **推理（Reasoning）**：
    给定一个具体的问题 $Q$，记忆模型 $R$ 利用其记忆 $M$ 推理出任务导向的检索线索 $C$。这些线索 $C$ 包含了多个子线索 $\{c_1, \ldots, c_m\}$ 以及一个草稿答案 $c_a$。通过将记忆 $M$ 与问题嵌入 $E_q(Q)$ 动态拼接，记忆模型 $R$ 能够进行整体推理，以定位与问题相关的证据时刻。
    $C = R (\text{Concat}(M; E_q(Q)))$

3.  **检索（Retrieving）**：
    长视频被分割成固定时长、不重叠的时刻集合 $\mathcal{C} = \{s_1, \ldots, s_M\}$。对于 $C$ 中的每个检索线索 $c$，使用预训练的视频检索器计算其文本嵌入和每个时刻 $s_j$ 的嵌入。根据余弦相似度对时刻进行排名，并聚合前 $k$ 个最相关的时刻作为初步检索结果。
    $S'' = \bigcup_{c \in C} \text{Top-}k(c |\mathcal{C}, \omega)$
    其中 $\omega$ 表示检索器参数。

4.  **聚焦（Focusing）**：
    最终，基于检索到的信息丰富的证据时刻 $S''$ 和原始问题 $Q$，生成最终答案 $A''$。检索到的时刻会进行时间上的重新排序并均匀采样，以满足下游生成模型的上下文限制。
    $A'' = G(S'', Q | \theta)$
    其中 $G$ 是生成模型，$\theta$ 是其参数。

#### 课程学习框架

为了优化记忆器并在没有中间检索线索真值标签的情况下进行训练，MemVid 引入了一个课程学习框架：

1.  **监督微调（Supervised Fine-Tuning, SFT）热身**：
    首先，利用一个强大的 72B 参数的 VLM 作为教师模型，生成结构化的检索线索和草稿答案作为高质量的监督数据。这些数据经过筛选，只保留那些能导致下游正确答案的样本。记忆器通过在这些精炼数据集上进行下一词元预测（next-token prediction）来优化。
    $L_{NTP}(\theta) = -\sum_{t=1}^T \log P_{\theta}(w_t | w_{<t})$

2.  **生成反馈强化学习（Reinforcement Learning with Generation Feedback, RLGFN）**：
    在 SFT 预热之后，通过直接偏好优化（Direct Preference Optimization, DPO）进一步精炼线索质量。对于每个查询，模型会采样多个线索，并根据生成器对正确答案的置信度对其进行排名。构建偏好对 $(y_i^+, y_i^-)$，并确保它们之间存在最小得分裕度 $\tau$ 以保证质量区分度。
    $L_{dpo} = -\sum_i \log \sigma \left( \beta \cdot \left[ \log \frac{\pi_{\theta}(y_i^+)}{\pi_S(y_i^+)} - \log \frac{\pi_{\theta}(y_i^-)}{\pi_S(y_i^-)} \right] \right)$
    其中 $\pi_{\theta}$ 是可学习策略，$\pi_S$ 是冻结的 SFT 参考模型，$\beta$ 控制策略分歧。这种双阶段方法加速了收敛，并通过下游目标进一步完善线索。

#### 贡献

1.  引入了 MemVid，一个针对长视频理解的新型检索增强框架，强调推理和记忆在理解长视频中的关键作用。
2.  设计了一个有效的课程学习框架，通过利用多种训练信号来增强记忆器改进端到端 RAG 性能的能力。
3.  通过广泛实验证明 MemVid 在长视频理解中实现了高质量结果，并显著提高了成本效益。

#### 实验结果

MemVid 在 MLVU、VideoMME 和 LVBench 等流行 LVU 基准上进行了综合评估。

*   **性能提升**：与暴力下采样方法相比，MemVid 在相同上下文限制下显著提高了准确性，例如在 MLVU 上比 Qwen2VL 提升 8.7%，在 VideoMME (long) 上分别提升 6.2%（无字幕）和 3.7%（有字幕）。
*   **超越专业 LVLMs**：MemVid 在使用更少输入帧的情况下，超越了专注于长视频理解的 LVLMs，例如在 VideoMME 上分别比 Video-XL 高出 8.2%（无字幕）和 4.7%（有字幕）。
*   **优于 RAG 基线**：与最佳 RAG 基线 RAGsimple 相比，MemVid 在 MLVU 上提升 5.8%，在 VideoMME 上分别提升 3.4%（无字幕）和 3.0%（有字幕），验证了其记忆-推理机制的优越性。
*   **效率优势**：MemVid 显著降低了计算成本。与 VideoXL 相比，MemVid 处理的帧数减少了 93.8%（从 1024 帧降至 64 帧），推理延迟减少了 35.1%，内存使用减少了 34.9%，同时性能提高了 17.3%。
*   **泛化能力**：MemVid 在不同任务（显式和隐式查询、单细节和多细节问答）、不同上下文限制（16-128 帧）以及不同下游模型架构（3B-72B VLMs）上均表现出强大的泛化能力。

#### 结论

MemVid 通过记忆增强的推理机制有效识别关键帧，从而在长视频理解方面取得了显著进展，验证了记忆增强检索的强大潜力。

## Abstract


本节介绍了MemVid这一新型的基于记忆增强的检索增强生成（RAG）方法，旨在解决长视频理解（LVU）中信息丢失的问题。当前的长上下文视觉-语言模型（LVLMs）由于采用压缩或直接下采样策略，导致信息损失；虽然检索增强生成（RAG）方法在一定程度上缓解了这一问题，但其依赖显式查询，适用性受限。

MemVid受人类认知记忆机制启发，提出四步处理流程：
1. **整体记忆视频信息**；
2. **基于记忆推理任务所需信息**；
3. **根据信息需求检索关键帧或片段**；
4. **聚焦于检索结果生成最终答案**。

为了提升模型基于记忆的推理能力并实现端到端最优性能，作者还提出了一种**课程学习策略**：先通过监督学习训练模型理解已标注的推理结果，再逐步通过强化学习探索和强化更合理的推理路径。

实验结果显示，MemVid在多个主流LVU基准数据集（如MLVU、VideoMME、LVBench）上均表现出优于现有LVLMs和RAG方法的效率与效果。

---

**重点内容强调**：
- MemVid的核心创新在于结合**记忆机制**与**检索增强生成**，避免了传统方法的信息丢失问题。
- 提出的**课程学习策略**（监督学习 + 强化学习）显著提升了模型的推理能力和整体性能。
- 实验结果验证了MemVid在多个LVU任务上的**优越性**。

**非重点内容简要说明**：
- 图1展示了MemVid与其他方法的框架对比，强调其通过整体记忆生成有用线索，从而更准确地定位细节进行问答。


## 1. Introduction



## 1. 引言（Introduction）总结

本节介绍了**长视频理解**（Long-video understanding）在现实应用中的重要性，如视频分析、自动驾驶和具身智能（embodied AI），并指出当前主流视觉-语言模型（VLMs）在处理长视频时的局限性。传统模型如 [InternVL]、[Qwen2VL]、[Bai2023QwenVL] 主要针对图像或短视频设计，难以有效处理长序列视频。虽然一些扩展模型（如 [LongVA]、[LongVILA]、[Shu2024VideoXL]）尝试扩大上下文窗口，但仍面临**信息丢失**和**计算成本高**的问题。

接着，文章讨论了**检索增强生成**（RAG）在长视频理解中的潜力与局限。标准RAG方法在处理明确问题（如“橘猫什么时候躺在主人怀里？”）时表现良好，但在处理**隐含复杂信息**的问题（如“第五分钟时，为什么橘猫会躺在主人怀里？”）时效果不佳。这类问题需要模型具备**推理能力**，识别事件之间的隐含关系，而不仅仅是直接检索关键词。

文章通过图示对比了**人类处理长视频理解的方式**：人类会先通看视频形成整体记忆，在回答问题时进行推理并有选择地检索关键片段。这种结构化流程启发作者提出了一种新的RAG框架——**MemVid**（Memory-enhanced retrieval augmentation for long Video understanding）。

MemVid 的核心流程包括四个步骤：
1. 生成视频整体信息的记忆；
2. 基于记忆进行问题推理；
3. 检索关键视频片段；
4. 基于检索结果生成答案。

该流程由三个模块驱动：**记忆模块**（memorizer）、**检索模块**（retriever）和**生成模块**（generator）。作者重点优化**记忆模块**，并引入**课程学习框架**（curriculum learning）以提升其推理能力。训练过程分为两个阶段：
- 第一阶段：使用监督学习，基于高质量标注数据训练记忆模块生成结构化推理输出；
- 第二阶段：探索多种推理路径，强化能生成高质量答案的推理轨迹。

最后，作者在多个长视频理解基准（如 VideoMME、MLVU、LVBench）上进行了大量实验，验证了 MemVid 的有效性。实验结果显示，MemVid 在性能上优于现有 RAG 方法，并在成本效益方面显著优于主流长视频 VLMs。

### 贡献总结：
1. 提出首个结合**记忆与推理**的 RAG 框架 MemVid，专为长视频理解设计；
2. 设计基于**课程学习**的训练框架，提升记忆模块的推理能力；
3. 在多个基准上验证了 MemVid 的**高性能与高成本效益**。


## 2. Related Work



以下是对你提供的论文“Related Work”章节的总结，按照原文结构进行组织，重点内容进行了详细讲解，次要内容进行了精简。

---

## 2. Related Work

### 2.1. 大型视觉-语言模型（Large Vision-language Models）

本节回顾了当前大型视觉-语言模型（LVLMs）的发展情况。随着大语言模型（LLMs）的突破，多模态人工智能系统的研究迅速发展，多个代表性工作（如 InternVL、QwenLM、LLaVA、MiniGPT-4）在架构设计和训练方法上进行了探索。

- **InternVL** 是一个具有代表性的模型，其视觉模型参数达到60亿，并通过大规模多模态预训练实现了与LLMs的良好对齐。
- **QwenVL** 在 QwenLM 的基础上引入了专用视觉编码器、统一的输入输出接口、三阶段训练策略以及多语言多模态数据集，提升了模型能力。
- 随着研究的深入，视觉-语言模型的应用从静态图像扩展到视频理解。当前主流方法是将视频帧编码为视觉表示，再转换为LLMs可处理的token形式。
  - **ST-LLM** 提出直接输入原始时空token的方法，有效建模视频序列。
  - **VideoChat** 和其升级版 **VideoChat2** 通过可训练的神经适配器提升时间推理能力，并通过多场景指令微调进一步优化。
  - 早期模型多采用 **BLIP-2 的 Q-Former** 进行特征融合，但近期更倾向于简化设计。
    - **VideoLLaVA** 和 **MiniGPT4-Video** 使用线性变换将视觉特征映射到语言空间。
    - **Video-ChatGPT** 和 **Valley** 则采用token池化机制提取关键视觉信息。
- 数据质量的重要性日益凸显：
  - **LLaVA-Video** 强调高质量合成数据对视频指令理解的显著提升。
  - **Qwen2VL** 通过大规模训练数据和自适应分辨率技术实现了跨任务的广泛适用性。

**挑战**：当前多模态系统在处理长视频时受限于上下文长度（通常最多128帧），这是研究与应用之间的重要瓶颈。

---

### 2.2. 长视频视觉-语言模型（Long Large Vision-language Models）

本节聚焦于处理长视频的视觉-语言模型。长视频通常包含大量冗余信息，只有部分片段对任务有意义。现有方法主要通过**记忆机制**或**压缩模块**来应对这一挑战。

- **MovieChat** 和 **MA-LMM** 使用记忆库存储历史视觉特征，并通过整合策略进行压缩。
- **LLAMA-VID** 采用上下文注意力模块，将每帧压缩为仅两个token，增强长程理解能力。
- **LongVLM** 引入token合并模块，融合局部与全局特征。
- **Video-CCAM** 使用带有因果交叉注意力掩码的跨注意力机制。
- 一些模型（如 **LongVA**、**LongVILA**、**LongLLaVA**）尝试直接增强模型对长视频的理解能力。

**问题**：尽管这些方法扩展了视频处理长度，但往往在冗余处理和计算复杂度（通常是二次增长）之间做出妥协，效果并不理想。

---

### 2.3. 基于检索增强的视频理解（Retrieval-augmented Video Understanding）

本节介绍了**检索增强生成**（RAG）在视频理解中的应用。RAG是一种在文本领域广泛应用的技术，通过检索器获取相关信息以辅助生成，特别适合处理长视频中的冗余数据。

- **DrVideo** 和 **Goldfish** 分别对关键帧和视频片段生成文本描述，并基于这些描述构建RAG框架。
  - **问题**：文本描述与原始视频内容之间存在语义差距，影响检索效果。
- 由于缺乏上下文和深层语义信息，直接使用查询检索视频内容效果不佳。
- 为解决这一问题，作者提出了一种**生成式上下文感知查询扩展机制**，直接作用于视频内容。
  - **优势**：
    - 提升检索性能；
    - 在有限输入长度内聚焦关键内容；
    - 降低计算成本。

---

整体来看，本章系统梳理了视觉-语言模型的发展脉络，从基础模型到视频理解，再到长视频处理与检索增强机制，层层递进，为后续提出的方法（MemVid）提供了充分的背景支持。


## 3. Methodology



### 3. Methodology

#### 3.1. Overview of MemVid
**重点内容：**
MemVid 是一种用于长视频理解的增强型框架，旨在解决传统方法（如稀疏采样和标准RAG）在处理长视频时的信息丢失和检索不足问题。其核心思想是通过构建一个全局视频记忆模块，模拟人类认知过程，引导上下文感知的检索。

**结构与流程：**
1. **Memorizing（记忆）**：通过记忆模型将视频压缩为全局记忆 ℳ，保留整体理解。
2. **Reasoning（推理）**：基于问题 Q 和全局记忆 ℳ 推理出任务导向的检索线索 𝒞。
3. **Retrieving（检索）**：根据线索 𝒞 从视频中检索相关片段，并聚合结果。
4. **Focusing（聚焦）**：基于检索到的片段生成最终答案。

**优势：**
相比传统方法，MemVid 通过记忆和推理机制提升了检索的准确性和上下文理解能力，从而提高长视频问答的准确性。

---

#### 3.2. Reasoning-oriented Memory Module
**重点内容：**
设计了一个基于键值缓存（KV Cache）的记忆模块，具备以下特性：
- 支持对视频的全局理解
- 支持推理过程
- 灵活生成检索线索

**实现方式：**
- 使用预训练视觉编码器将视频压缩为特征 F
- 利用因果Transformer语言模型将特征转换为推理导向的KV缓存
- 当问题 Q 到达时，结合记忆 ℳ 和问题嵌入推理出检索线索 𝒞

**关键公式：**
- 视觉特征提取：F = Ev(𝒱′)
- KV缓存更新：K ← Concat(K, Kt), V ← Concat(V, Vt)
- 线索生成：𝒞 = ℛ(Concat(ℳ; Eq(Q)))

**优势：**
该模块通过动态推理线索，提升了检索的针对性和上下文适应性。

---

#### 3.3. Clue-guided Retrieval
**主要内容：**
- 将长视频划分为非重叠的固定时长片段作为候选检索池
- 使用预训练视频检索器为每个线索和片段生成文本嵌入
- 通过余弦相似度排序，选取 top-k 片段
- 结合局部（检索片段）和全局（全视频）采样策略，提升上下文覆盖

**精简说明：**
该部分描述了如何基于推理线索进行高效检索，并通过采样策略平衡局部与全局信息。

---

#### 3.4. Curriculum Learning Framework
**重点内容：**
提出了一种课程学习框架，用于优化记忆模块（Memorizer），解决其训练中缺乏监督信号的问题。

**两个阶段：**
1. **监督微调预热（Supervised Fine-Tuning Warmup）**
   - 使用强大的教师模型生成高质量推理线索
   - 通过 next-token prediction 优化记忆模块
   - 损失函数：ℒ_NTP(θ) = -∑ log Pθ(w_t | w_<t)

2. **基于生成反馈的强化学习（Reinforcement Learning with Generation Feedback）**
   - 使用 DPO（Direct Preference Optimization）优化线索质量
   - 构建偏好对 (yi+, yi−)，基于生成器的正确性进行优化
   - 损失函数：ℒ_dpo = -∑ log σ(β·[log πθ(y+)/πS(y+) - log πθ(y−)/πS(y−)])

**优势：**
双阶段训练策略提升了模型的泛化能力和端到端性能。

---

### 总结
MemVid 通过引入记忆增强机制，结合推理、检索与生成，有效解决了长视频理解中的信息丢失和检索不准确问题。其核心创新包括：
- 基于KV缓存的记忆模块，支持全局理解与动态推理
- 线索引导的检索机制，提升检索相关性
- 课程学习框架，优化记忆模块的训练过程

该方法在多个视频理解基准上取得了优于现有模型的性能表现。


## 4. Experiments



## 4. 实验总结

### 4.1. 实验设置

#### 4.1.1. 基准与评估指标
本研究在三个具有不同特征的长视频基准上进行了全面实验，以提供多维度评估：

- **VideoMME**：包含2700个专家整理的问题，对应900个不同长度的视频（短、中、长），提供有字幕和无字幕两个版本。
- **MLVU**：涵盖3分钟到2小时的视频，包含9个任务（如动作识别、事件定位、计数等），评估全局和局部理解能力。
- **LVBench**：专为超长视频设计，平均时长4101秒，任务包括关键信息检索、事件理解、时间定位等。

这三个基准共同构建了全面的评估框架：VideoMME 提供不同视频长度的细粒度分析，MLVU 测试多样任务能力，LVBench 专注于超长视频处理。

#### 4.1.2. 基线模型
比较了三类基线模型：

1. **闭源模型**：如 GPT-4V、GPT-4o、Gemini-1.5-Pro，性能强但架构不可比。
2. **开源视觉语言模型（VLMs）**：包括通用模型和长上下文模型（如 LongVA、LongVILA、VideoCCAM 等），扩展了传统VLM的上下文长度。
3. **基于RAG的VLMs**：如 Goldfish、SALOVA-Qwen、Video-RAG，通过检索关键片段或文本增强理解。其中，RAGsimple 作为无记忆模块的对照模型。

#### 4.1.3. 实现细节
- 使用 **LanguageBind-Large** 进行10秒片段划分，生成4个查询相关线索和草稿答案。
- 输入截断为128帧，α=0.6，与 Qwen2VL-7B 对齐。
- 训练数据来自 TVQA-Long、NExT-QA、ActivityNet-QA，生成10,000条合成线索和答案。
- DPO训练使用 CinePile 数据集，筛选1000对高质量训练样本。
- 实验在单节点8×A800 GPU上运行。

---

### 4.2. 总体结果

MemVid 在三个长视频基准上均表现优异，尤其在7B模型中达到SOTA：

1. **优于传统下采样方法**：相比 Qwen2VL，在MLVU上提升+8.7%，在VideoMME（长）上提升+6.2%（无字幕）和+3.7%（有字幕）。
2. **优于长视频VLMs**：尽管输入帧数更少，MemVid 在VideoMME上超越 Video-XL +8.2%（无字幕）和+4.7%（有字幕）。
3. **优于闭源模型**：在LVBench上，MemVid 以30倍更少帧数超越 Gemini 1.5 Pro，提升11.3%。
4. **优于RAG方法**：相比 RAGsimple，在MLVU和VideoMME上分别提升+5.8%、+3.4%、+3.0%。

---

### 4.3. 消融实验

评估MemVid各模块的有效性：

- **无推理模块（MemVid w/o reasoning）**：性能下降2.4%（MLVU）、1.6%/0.4%（VideoMME）。
- **无记忆模块（MemVid w/o memory）**：性能下降1.9%（MLVU）、1.2%/0.3%（VideoMME）。
- **训练策略**：零样本 → SFT → DPO，逐步提升约3%。

结果验证了记忆增强机制和课程学习框架的有效性。

---

### 4.4. 泛化性分析

#### 4.4.1. 任务性能
MemVid 在NeedleQA、Count、Order、TutorialQA等多个任务上均优于 RAGsimple，尤其在Count（+8%）、TutorialQA（+6%）、Order（+3%）上表现突出，即使在训练数据未覆盖的任务上也具备良好泛化能力。

#### 4.4.2. 帧数分析
MemVid 在16~128帧范围内均优于均匀采样和标准检索策略，且帧数越多优势越明显（如128帧提升2.9%）。

#### 4.4.3. 不同下游架构
MemVid 可适配不同规模和架构的VLM（3B~72B），在VILA-1.5（+10.1%）、LongVA（+3.5%）、Qwen2VL（+2.4%）上均提升性能，尤其在小模型和低帧数下效果显著。

---

### 4.5. 效率分析

MemVid 在效率上显著优于长视频VLM（如VideoXL）：

- **帧数减少93.8%**（1024→64）
- **延迟降低35.1%**
- **内存减少34.9%**
- **性能提升17.3%**

表明MemVid能高效提取关键帧，兼顾性能与成本。

---

### 4.6. 案例分析

图示案例显示，MemVid 能通过上下文推理生成细粒度线索，准确检索关键片段（如家庭关系、教堂哀悼），从而生成正确答案，而 RAGsimple 无法捕捉关键信息。

---

## 总结

MemVid 通过**记忆增强的检索机制**，在多个长视频基准上实现SOTA，具备以下优势：

- **高效提取关键帧**，减少输入帧数和计算成本。
- **强泛化能力**，适配不同任务、模型架构和帧数限制。
- **优于闭源模型和RAG方法**，尤其在复杂任务和低资源条件下表现突出。
- **模块化设计**，可灵活集成到不同VLM中。

该方法为长视频理解提供了高效、通用的解决方案。


## 5. Conclusion



## 5. 结论

本节总结了论文的核心贡献与实验成果。作者提出了一种基于RAG的新型长视频理解方法——MemVid，其设计灵感来源于人类的认知记忆机制。MemVid的工作流程包含四个关键步骤：**整体视频信息的记忆、任务相关信息的推理、关键帧的检索以及聚焦生成最终答案**。为了进一步提升模型在记忆引导下的推理能力并优化整体表现，作者还引入了**课程学习策略**。

在实验部分，作者在多个LVU基准数据集（如MLVU、VideoMME、LVBench）上进行了大量测试。结果表明，MemVid在性能上**显著优于现有的RAG方法和主流的LVU模型**，验证了该方法的有效性和优越性。

> **重点内容强调**：  
- MemVid是首个将RAG与记忆机制结合用于长视频理解的工作。  
- 提出的四步流程有效提升了模型对长视频中关键信息的捕捉与推理能力。  
- 课程学习策略有助于模型逐步学习复杂任务，提升整体表现。  
- 实验结果显示MemVid在多个权威LVU任务中具有领先优势。
