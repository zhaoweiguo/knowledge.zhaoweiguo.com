# 2510.29xxx.NL: Nested Learning: The Illusion of Deep Learning Architecture

* 首页: <https://openreview.net/forum?id=nbMeRvNb7A>
* PDF: <https://abehrouz.github.io/files/NL.pdf>
* 引用: 
* 组织: Google Research



## 总结 From Zhihu

### 认为

* From: https://www.zhihu.com/question/1970631480595190298/answer/1971247090358261614
* 认为: 包装Kazuki Irie et al的各种idea
* 实验部分： 完全没有验证文章argue的东西
* 结论: 这种文章发发blog就可以了，结果却非要发会议




## 总结 From Moonlight

### 关键词

- Nested Learning: 嵌套学习（Nested Learning, NL）是一种新的机器学习范式，它将机器学习模型及其训练过程视为一系列嵌套的、多层级的、/或并行的优化问题。在这种范式下，模型的每个组件（包括架构和优化器）都可以有自己的“上下文流”（context flow）和内部梯度流（internal gradient flow），它们以不同的更新频率在不同层级上进行优化。NL 旨在通过提供计算深度的全新维度，来增强模型的表达能力、优化能力以及持续学习能力，并统一和推广了诸如元学习、上下文学习、循环神经网络和超网络等概念。
- Associative Memory: 联想记忆（Associative Memory）是指能够形成和检索事件之间联系的能力，是人类学习的基本过程。在本文的框架中，它被定义为一个映射器（operator），能够将一组“键”（keys）映射到一组“值”（values）。模型通过一个目标函数来学习这种映射，并通过优化过程（如梯度下降）来找到最优的映射器。联想记忆不仅限于记忆数据点到其输出误差的映射，还可以是其他各种形式的映射，如梯度到预期的参数更新方向，或输入数据到其内部状态的表示。通过将联想记忆的概念应用于模型的各个组件，包括训练过程和架构本身，NL 试图揭示更深层次的统一性。
- Neural Learning Module: 神经学习模块（Neural Learning Module）是根据嵌套学习（NL）范式构建的计算模型。它将架构和优化过程视为一个相互连接的整体系统，共同决定模型的行为和输出。在这种视角下，模型不再被简单地分解为固定的架构和独立的训练阶段，而是被看作一个不断学习和适应的动态系统。神经学习模块的每个组成部分（如权重、动量项、记忆状态）都被视为具有不同更新频率和上下文的优化问题。
- In-context Learning: 上下文学习（In-context Learning, ICL）是指模型仅凭其接收到的上下文（例如，几个示例或自然语言指令）来推断并执行新任务的能力。在本文中，ICL 被更广泛地定义为模型适应和学习给定上下文的能力。NL 视角认为，ICL 是模型具有“多个嵌套层级”的内在特征。Transformer 模型中的 ICL 源于其作为处理 tokens 的一种特定回归问题的非参数化解决方案，而其他循环模型则在其较低层级使用参数化学习过程。NL 框架表明，所有层级都在以各自的上下文流和学习更新机制执行着某种形式的上下文学习。
- Continuum Memory System: 连续体记忆系统（Continuum Memory System, CMS）是一种对传统“长时/短时记忆”（LSM）概念的泛化。它将记忆视为一个分布式、相互连接的系统，其中包含不同频率更新的神经元或参数。高频率的神经元负责快速适应并存储短期记忆，而低频率的神经元则负责更持久的知识存储。CMS 通过将 MLP 块组织成具有不同更新频率（通过“块大小”或“更新周期”定义）的链式结构，实现了记忆的连续体。这种多频率设计有助于实现更鲁棒的记忆管理，并可能缓解灾难性遗忘。
- Delta Gradient Descent: Delta Gradient Descent（DGD）是一种由论文提出的、更具表现力的梯度下降变体。它基于 Delta 规则（delta-rule），与标准梯度下降（其基本形式可以看作是基于点积相似性的联想记忆）不同，DGD 使用 L2 回归损失作为内部目标。其更新规则不仅考虑了当前输入（或梯度），还考虑了模型权重的当前状态，从而生成一个自参考过程。这使得 DGD 能够根据当前数据样本自适应地调整其权重更新，从而在处理高度依赖的数据序列（如文本）时具有更好的适应性。
- Hope: Hope 是论文提出的一个名为“希望”的持续学习模块，它结合了自修改学习模块和连续体记忆系统（CMS）。Hope 旨在解决现有大型语言模型（LLMs）在预训练后学习能力受限、无法持续获取新能力的问题。它通过利用 NL 的见解，设计了一个能够自我修改的模型，学习自身的更新算法，并结合 CMS 来实现多时间尺度的记忆处理和知识整合，从而在语言建模、知识纳入、少样本泛化和长上下文推理等任务中展现出持续学习的能力。


### 三句摘要

1. 💡 本文提出了一种名为 Nested Learning (NL) 的新学习范式，将机器学习模型视为一套嵌套、多层次、并行的优化问题，每个问题都有其“上下文流”，旨在解决现有深度学习模型在持续学习和自改进方面的挑战。
2. 🔄 通过 NL 的视角，传统优化器如 Adam 被重新解释为关联记忆模块，旨在压缩梯度信息；现有的 Transformer 架构则被视为通过多层线性或 MLP 单元实现的分层学习过程。
3. 🧠 NL 倡导设计具有更多“层次”的表达性学习算法，并提出自修改学习模块 (Hope) 和统一的 Continuum Memory System (CMS)，以期实现更高阶的 In-Context Learning 和有效的持续学习能力。


### 摘要

Nested Learning (NL) 提出了一种新的学习范式，旨在解决现有深度学习模型在持续学习、自我改进和有效问题解决方面的根本性挑战。该范式将机器学习模型表示为一组嵌套的、多层次的或并行的优化问题，每个问题都拥有自己的“上下文流”（context flow）。

**核心思想 (Nested Learning Paradigm)**

NL 认为，现有深度学习方法通过压缩其自身的上下文流来从数据中学习，而 In-context Learning (ICL) 自然地出现在大型模型中。NL 倡导设计具有更多“层次”的表达性学习算法，以实现更高阶的 In-context Learning 和潜在的持续学习能力。其神经科学动机源于人脑的统一和可复用结构以及多时间尺度更新机制，例如不同脑电波频率对应不同的信息处理时间尺度。

NL 将机器学习模型及其训练过程（即优化）视为一个由嵌套的、多层次的、并行的优化问题组成的互联系统。每个组件，无论是参数化的（如可学习权重、动量项）还是非参数化的（如注意力块），都有其自身的更新频率（Update Frequency）。频率越高，表示更新越快。通过这种频率排序，模型组件被组织成 K 个有序的“层次”，其中同一层次的组件具有相同的更新频率，且更高层次的频率更低。

形式上，一个嵌套系统包含 $K$ 个层次，每个层次 $k$ 包含一组优化问题 $\{(L^{(k)}_i, C^{(k)}_i, \Theta^{(k)}_i)\}_{i=1}^{N_k}$。每个参数 $\theta_i^{(k)}$ 通过梯度下降进行优化：
$$ \theta_i^{(k)}_{t+1} = \arg \min_{\Phi^{(k)}_i} \left\langle \Phi^{(k)}_i x_{t+1}, -\nabla L^{(k)}_i(\theta_i^{(k)}_t; x_{t+1}) \right\rangle + \frac{1}{2\eta_i^{(k)}_{t+1}} \| \Phi^{(k)}_i - \theta_i^{(k)}_t \|_2^2 $$
其中 $x_{t+1} \sim C^{(k)}_i$ 是该层次的上下文数据，$L^{(k)}_i$ 是优化目标，$\Theta^{(k)}_i$ 是参数的可行集。当每个优化过程都是一个联想记忆（Associative Memory）时，系统被称为 Nested System of Associative Memories (NSAM)。联想记忆 M($\cdot$) 将一组键 K 映射到一组值 V，通过优化目标 $\tilde{L}(\cdot; \cdot)$ 来学习这种映射：
$$ M^* = \arg \min_M \tilde{L}(M(K); V) $$

知识可以在不同层次之间进行传递，方式包括：
1.  **直接连接（参数化/非参数化）**: 低频（高层）记忆的输出或参数受高频（低层）记忆的输出或参数影响，但无反向传播。
2.  **反向传播（Backpropagation）**: 不同层次的块之间存在梯度流，使得它们的状态在同一梯度流中被优化，但更新频率不同。
3.  **通过初始化**: 如 MAML (Model-Agnostic Meta-Learning)，高层问题学习内层问题的最佳初始点。
4.  **通过生成**: 一个低频块生成高频块的权重（超网络 Hypernetworks）或上下文（优化器生成梯度）。

**关键贡献与技术细节**

1.  **表达性优化器（Expressive Optimizers）**:
    *   **反向传播作为联想记忆**: 反向传播可以被视为一个联想记忆，它学习将层的输入（$\hat{x}_{\ell-1}$）映射到其局部误差信号（$\delta_\ell$），即层对预测的“意外”程度。对于一个线性层，权重更新可表示为：
        $$ W_{\ell, t+1} = \arg \min_W \left\langle W \hat{x}_{\ell-1}, \delta_\ell \right\rangle + \frac{1}{2\eta_{\ell, t+1}} \| W - W_{\ell, t} \|_F^2 $$
        其中 $\delta_\ell = J_{\phi_\ell}(z_\ell)^\top W_{\ell+1}^\top \delta_{\ell+1}$。这意味着模型通过记忆每个数据点对其预测的误差来学习。
    *   **基于动量的优化器作为联想记忆**: 动量项可以被视为一种联想记忆，用于压缩过去梯度信息。例如，SGD with Momentum 的更新规则：
        $$ W_{\ell, t+1} = W_{\ell, t} - m_{\ell, t+1} $$
        $$ m_{\ell, t+1} = \alpha_{\ell, t+1} m_{\ell, t} - \eta_{\ell, t+1} \nabla_{W_\ell, t} L(W_{\ell, t}; x_{t+1}) $$
        这构成一个两层优化过程：内层学习动量，外层使用动量更新权重。Adam 等流行优化器也被重新解释为一种最优联想记忆，旨在预测梯度的方差。
    *   **Delta Gradient Descent (DGD)**: 一种更通用的梯度下降形式，其更新不仅依赖于当前输入，还依赖于神经网络权重的状态，从而捕获数据样本间的依赖性。其更新规则基于 $L_2$ 回归损失：
        $$ W_{t+1} = \arg \min_W \frac{1}{2} \| W x_t - u_t \|_2^2 + \frac{1}{2\eta_t} \| W - W_t \|_2^2 $$
        其中 $u_t = -\nabla_{y_t} L(W_t; x_t)$。这使得更新规则自适应地衰减，而非仅仅基于点积相似度。
    *   **广义梯度下降 (Generalized Gradient Descent, GGD)**: GGD 是一种自指联想记忆，旨在压缩数据样本并将其映射到一组自生成的键。
    *   **Multi-scale Momentum Muon (M3) 优化器**: 作为一种概念验证，M3 结合了 Adam、Muon 和 Continuum Memory System (CMS) 的思想。它使用多尺度的动量项，其中一些负责短期梯度记忆，另一些负责长期梯度记忆，并通过 Newton-Schulz 正交化处理，以提高长期上下文理解能力，从而在持续学习场景中表现更好。

2.  **持续记忆系统 (Continuum Memory System, CMS)**:
    *   **概念**: 推广了传统的“长时/短时记忆”观点，将记忆视为一个具有不同更新频率谱的分布式互联系统。高频神经元负责快速适应，但记忆时间短；低频神经元负责更持久的知识。
    *   **实现**: CMS 可以实现为一系列 MLP 块 $MLP^{(f_1)}(\cdot), \dots, MLP^{(f_k)}(\cdot)$，每个块 $MLP^{(f_\ell)}$ 的参数 $\theta^{(f_\ell)}$ 每 $C^{(\ell)}$ 步更新一次：
        $$ \theta_{i+1}^{(f_\ell)} = \begin{cases} \theta_{i}^{(f_\ell)} - \left( \sum_{t=i-C^{(\ell)}}^{i} \eta_{t}^{(\ell)} f(\theta_{t}^{(f_\ell)}; x_t) \right) & \text{if } i \equiv 0 \pmod{C^{(\ell)}} \\ 0 & \text{otherwise} \end{cases} $$
        这种设计意味着高频知识可能在低频记忆中被保留，从而缓解灾难性遗忘。
    *   **变体**: 可以是完全嵌套的（高层初始化低层）、顺序的（MLP 块依次连接）或独立的（各 MLP 块独立处理输入并聚合输出）。
    *   **效率**: CMS 通过在每个时间点只更新少量参数，并允许高频层次的序列并行化，从而在实践中可以高效运行。

3.  **自修改学习模块 (Hope)**:
    *   Hope 是一个自我参照的学习模块，所有组件都可以进行 In-context Learning 和自我修改。
    *   **自指 Titans**: 模型的投影层、学习率、保留门等所有组件都由其自身生成并更新，例如：
        $$ k_t = M_{k, t-1}(x_t), v_t = M_{v, t-1}(x_t), q_t = M_{q, t-1}(x_t), \eta_t = M_{\eta, t-1}(x_t), \alpha_t = M_{\alpha, t-1}(x_t) $$
        $$ \hat{v}_{\square, t} = M_{\square, t-1}(v_t) $$
        $$ \min_{M_\square} L(M_\square; k_t, \hat{v}_{\square, t}) $$
        其中 $M_{\square,0}$ 等记忆的初始状态通过元学习（meta-learned）获得。这种机制使得模型能够根据新数据改变自身的参数或学习过程，从而在持续学习中至关重要。

**NL 对现有概念的重新解释**

*   **记忆与学习**: 记忆是输入引起的神经更新，学习是获取有效和有用记忆的过程。模型中的任何参数更新都被视为一种记忆形式，且记忆分布式存在于所有参数中。
*   **模型参数**: 模型的参数不仅限于预训练阶段被优化的那些，还包括优化器中存储损失函数景观信息的动量参数等，它们都对模型性能和表达力有贡献。
*   **In-context Learning (ICL)**: ICL 是模型 NSAM 中多层次的直接结果，而非简单的“涌现”特性。每个层次都在其自身的上下文流上执行 ICL。
*   **预训练**: 预训练被视为一种 In-context Learning，其上下文是整个预训练数据集，其区别在于上下文长度极大。
*   **训练/测试时间**: 在 NL 范式下，训练和测试之间没有明确界限。模型持续接收信息并执行内部计算。
*   **架构的统一性**: 现有深度学习架构（如 Transformer、RNNs）在 NL 视角下是统一的，由前馈网络组成，差异在于它们所在的层次、目标函数和学习规则。循环模型被视为 MLP 块，只是增加了新的计算层次以实现 In-context Learning。

NL 强调了神经学习模块作为一个互联系统的重要性，架构生成优化器所需的上下文（梯度），因此需要设计特定于架构的优化器，以实现系统的和谐运作。









