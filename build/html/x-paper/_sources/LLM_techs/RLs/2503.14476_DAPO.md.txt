# ❇️2503.14476_DAPO: An Open-Source LLM Reinforcement Learning System at Scale

* 首页: <https://arxiv.org/abs/2503.14476>
* PDF: <https://arxiv.org/pdf/2503.14476>
* 引用: 663(2025-11-19)
* 组织: 
    * 1ByteDance Seed 
    * 2Tsinghua University
    * 3The University of Hong Kong 
    * 4SIA-Lab of Tsinghua AIR and ByteDance Seed
* 项目
    * GitHub: <https://github.com/volcengine/verl>
    * 首页: <https://dapo-sia.github.io/>


## 总结

**总结**
* DAPO: Decoupled Clip and Dynamic sAmpling Policy Optimization

**Preliminary**
* PPO
    * 一种用于策略优化的强化学习方法，其核心在于通过引入 裁剪的代理目标函数（clipped surrogate objective）来稳定训练过程，并提高样本效率
    * PPO 通过限制策略更新的范围，避免了策略突变，从而提升了训练的稳定性和效率。
* GRPO
    * 去除了价值函数，并采用 基于组的相对优势估计 来更新策略。
    * GRPO 在 样本级别 进行目标函数的平均，而 PPO 通常在 时间步级别 进行计算，这一区别可能对算法性能产生影响。
* Removing KL Divergence
    * 在 RLHF（人类反馈强化学习）中，KL 散度惩罚项用于限制策略与初始模型的偏离
    * 在训练长链推理（long-CoT）模型时，模型分布可能与初始模型显著不同，因此该限制并不必要
* Rule-based Reward Modeling
    * 使用 基于规则的奖励建模，直接使用任务的最终准确率作为奖励信号



**DAPO**
* 四项关键技术，解决长链式推理（long-CoT）场景下的RL训练问题：
    * Clip-Higher：提升系统多样性，防止熵坍塌；
    * Dynamic Sampling：提升训练效率和稳定性；
    * Token-Level Policy Gradient Loss：对长链式推理场景至关重要；
    * Overlong Reward Shaping：减少奖励噪声，稳定训练。
* Clip-Higher(提升上限)
    * 问题：
        * 在使用传统的 PPO 或 GRPO 算法时，我们发现策略的熵（entropy）会迅速下降，导致生成的响应变得相似，限制了探索能力
    * 为了解决这个问题，DAPO 引入了 Clip-Higher 策略，将上界和下界的剪切范围（clip range）分别设置为 $ \varepsilon_{\text{low}} $ 和 $ \varepsilon_{\text{high}} $
    * 实验结果：策略的熵显著增加，生成的样本更加多样
* Dynamic Sampling(多多益善)
    * 现有的 RL 算法在某些提示（prompt）的准确率等于 1 时会出现“梯度下降”问题
    * DAPO 提出动态采样策略：
        * 在训练前持续采样，直到每批次中不包含准确率为 0 或 1 的样本。
        * 保证每个批次中所有样本都有有效梯度，并保持样本数量一致。
* Token-Level Policy Gradient Loss(Rebalancing Act)
    * 问题：
        * 原始GRPO使用样本级损失：先对一个回答的所有词元损失求平均，再对所有回答的损失求平均。
        * 这带来了两个问题：
            * 长样本惩罚： 一个高质量的长推理回答，因其词元多，每个词元的平均贡献被稀释，模型从中学到的信号变弱
            * 低质量长样本泛滥： 模型可能生成冗长、重复或无意义的废话（gibberish）
      * 解决
          * 改变损失计算方式，直接对所有生成的所有词元的损失进行平均
          * 惩罚低质量 token，促进高质量推理
* Overlong Reward Shaping()
    * 问题：训练时必须设置一个生成令牌的最大长度（L_max）。超过此长度的回答会被截断。通常，系统会直接给这些被截断的样本一个惩罚性奖励（如-1）。
        *  奖励噪声： 一个推理过程完全正确、仅仅因为篇幅过长而被截断的样本，会收到与完全错误的样本相同的惩罚。这会混淆模型，让它不知道自己的推理过程其实是好的，只是写得太长。
    * 解决方案
        * Overlong Filtering：忽略被截断样本的损失，提升训练稳定性。
        * Soft Overlong Punishment：设计长度相关的惩罚机制
            * 动态惩罚：长度越接近最大值，惩罚越小；超过最大值后统一惩罚为 -1。


## Abstract

本研究提出了一种名为**Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)** 的算法，并开源了一个先进的大规模强化学习（RL）系统。该系统在 Qwen2.5-32B 的基础模型上，达到了 **AIME 2024 考试 50 分** 的成绩，表现优于此前的 SOTA 模型（如 DeepSeek-R1-Zero-Qwen-32B），且仅用了该模型 50% 的训练步数。

研究指出，尽管推理扩展大大增强了大语言模型（LLMs）的推理能力，但最先进的推理模型（如 OpenAI o1 和 DeepSeek R1）的关键技术细节并未公开，导致社区难以复现其强化学习训练结果。

为此，本文公开了 DAPO 的**四个关键技术**，这些技术是实现大规模 LLM 强化学习成功的核心。此外，还开源了训练代码（基于 verl 框架），并提供了经过精心整理和处理的数据集，从而提升了研究的**可复现性**，并支持未来在大规模 LLM 强化学习领域的研究。

---


### 重点内容总结

1. **研究亮点**：
   - 提出了 DAPO 算法，并开源了训练系统。
   - 在 AIME 2024 上取得 50 分，表现优异。
   - 仅用 50% 的训练步数即超过现有 SOTA 模型。

2. **研究意义**：
   - 针对 SOTA 推理模型技术细节不透明的问题，提供了可复现的实现和关键算法设计。
   - 开源了训练代码和数据集，推动了 LLM 强化学习领域的进一步研究。

3. **核心贡献**：
   - 提出四个关键 RL 技术（将在后文详细介绍）。
   - 全面开源训练流程和数据，提高研究透明度。


## 1 Introduction


**引言部分主要介绍了**测试时扩展（test-time scaling）在大语言模型（LLMs）中的重要性，以及当前大规模强化学习（RL）训练中的挑战，并提出了本文的研究目标和主要贡献。

---

### **1.1 测试时扩展的意义**

- **测试时扩展**（如OpenAI的o1和DeepSeek的R1）**带来了LLMs范式的重大转变**，使得模型在AIME、Codeforces等需要复杂推理的数学和编程任务中表现优异。
- 其核心是通过**更长的链式思维（Chain-of-Thought, CoT）**和**复杂的推理行为**（如自我验证、迭代优化）来提升性能。

---

### **1.2 大规模强化学习（RL）的作用与挑战**

- **关键驱动力**是**大规模强化学习（RL）**，它能够激发模型复杂的行为模式。
- 但目前**RL训练的具体算法和关键技术仍未公开**，导致许多研究难以复现已有成果（如DeepSeek R1）。
- 本文的目标是**揭示大规模RL训练中的主要障碍**，并**开源一个完整的RL系统**，包括算法、训练代码和数据集。

---

### **1.3 实验与挑战**

- 作者基于**Qwen2.5-32B**模型进行初步实验，使用**GRPO**（一种基础RL方法）仅取得**AIME 30分**，远低于DeepSeek RL的47分。
- 分析发现，**基础GRPO方法存在多个关键问题**：
  - **熵坍塌（entropy collapse）**
  - **奖励噪声（reward noise）**
  - **训练不稳定（training instability）**
- **社区在复现DeepSeek-R1结果时也遇到类似挑战**，表明其论文中可能省略了关键训练细节。

---

### **1.4 本文的贡献**

为解决上述问题，本文**开源了一个最先进的大规模LLM RL系统**，其主要贡献包括：

- 基于Qwen2.5-32B模型，在**AIME 2024上达到50分**，**优于DeepSeek-R1的47分**，且仅用了其**50%的训练步数**；
- 提出**DAPO（Decoupled Clip and Dynamic sAmpling Policy Optimization）算法**；
- 引入**四项关键技术**，解决长链式推理（long-CoT）场景下的RL训练问题：

  1. **Clip-Higher**：提升系统多样性，防止熵坍塌；
  2. **Dynamic Sampling**：提升训练效率和稳定性；
  3. **Token-Level Policy Gradient Loss**：对长链式推理场景至关重要；
  4. **Overlong Reward Shaping**：减少奖励噪声，稳定训练。

- **代码和数据均开源**，使用**verl框架**实现，旨在为社区提供**可复现、可扩展的大规模LLM RL训练方案**。

---

### **总结**

本节强调了测试时扩展和强化学习在提升LLM推理能力中的关键作用，指出现有系统的复现困难，提出了本文的解决方案：开源DAPO系统及其核心优化技术，以推动大规模LLM RL训练的普及与发展。


## 2 Preliminary


本节介绍了论文中使用到的一些关键方法和概念，主要包括 **PPO（Proximal Policy Optimization）**、**GRPO（Group Relative Policy Optimization）**、**KL 散度的去除原因** 以及 **基于规则的奖励建模**。以下是各部分内容总结：

---

### 2.1 Proximal Policy Optimization (PPO)

PPO 是一种用于策略优化的强化学习方法，其核心在于通过引入 **裁剪的代理目标函数**（clipped surrogate objective）来稳定训练过程，并提高样本效率。

#### 重点内容：
- **目标函数**：
  PPO 通过最大化以下目标函数来更新策略：

  $$
  \mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},o_{\leq t}\sim\pi_{\theta_{\text{old}}}(\cdot\mid q)}\Bigg{[} \min\Bigg{(} \frac{\pi_{\theta}(o_t\mid q,o_{<t})}{\pi_{\theta_{\text{old}}}(o_t\mid q,o_{<t})} \hat{A}_t, \ \text{clip}\Bigg{(} \frac{\pi_{\theta}(o_t\mid q,o_{<t})}{\pi_{\theta_{\text{old}}}(o_t\mid q,o_{<t})}, 1-\varepsilon, 1+\varepsilon \Bigg{)} \hat{A}_t \Bigg{)} \Bigg{]}
  $$
  其中：
  - $ \hat{A}_t $ 是时间步 $ t $ 的优势估计。
  - $ \varepsilon $ 是裁剪的范围。
  - $ \pi_{\theta_{\text{old}}}(o_t\mid q,o_{<t}) $ 是旧策略的概率密度。

- **优势函数计算**：
  使用 **GAE（Generalized Advantage Estimation）** 来计算优势函数：
  $$
  \hat{A}_t^{\text{GAE}(\gamma,\lambda)} = \sum_{l=0}^{\infty} (\gamma\lambda)^l \delta_{t+l}
  $$
  其中：
  $$
  \delta_l = R_l + \gamma V(s_{l+1}) - V(s_l)
  $$
  - $ R_l $ 是奖励。
  - $ V(s_l) $ 是状态 $ s_l $ 的价值函数估计。

#### 总结：
PPO 通过限制策略更新的范围，避免了策略突变，从而提升了训练的稳定性和效率。

---

### 2.2 Group Relative Policy Optimization (GRPO)

GRPO 是论文中提出的一种改进方法，与传统的 PPO 不同，它 **去除了价值函数**，并采用 **基于组的相对优势估计** 来更新策略。

#### 重点内容：
- **优势计算方式**：
  GRPO 对于每个问题-答案对 $ (q,a) $，生成 $ G $ 个响应，并计算每个响应的相对优势：
  $$
  \hat{A}_{i,t} = \frac{r_i - \text{mean}(\{R_i\})}{\text{std}(\{R_i\})}
  $$
  其中 $ \{R_i\} $ 是组内 $ G $ 个响应的奖励集合。

- **目标函数**：
  GRPO 采用与 PPO 类似的裁剪目标函数，并加入了 KL 散度惩罚项：
  $$
  \mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E} \Bigg{[} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \Big( \min(r_{i,t}(\theta)\hat{A}_{i,t}, \text{clip}(r_{i,t}(\theta), 1-\varepsilon, 1+\varepsilon)\hat{A}_{i,t}) - \beta D_{\text{KL}}(\pi_\theta||\pi_{\text{ref}}) \Big) \Bigg{]}
  $$
  其中：
  - $ r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t}|q,o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q,o_{i,<t})} $ 是重要性采样比。
  - $ \beta $ 是 KL 散度的系数。

- **与 PPO 的不同**：
  GRPO 在 **样本级别** 进行目标函数的平均，而 PPO 通常在 **时间步级别** 进行计算，这一区别可能对算法性能产生影响。

#### 总结：
GRPO 通过组内奖励的标准化和相对优势估计，无需价值函数即可更新策略，是一种更适用于生成式任务的策略优化方法。

---

### 2.3 Removing KL Divergence

在 RLHF（人类反馈强化学习）中，KL 散度惩罚项用于限制策略与初始模型的偏离。但论文指出，在训练**长链推理（long-CoT）模型**时，模型分布可能与初始模型显著不同，因此该限制并不必要。

#### 总结：
在论文中，作者决定 **去除 KL 散度惩罚项**，以提高训练效率和模型性能。

---

### 2.4 Rule-based Reward Modeling

传统的奖励建模方法容易受到“奖励劫持”问题的影响。因此，论文提出使用 **基于规则的奖励建模**，直接使用任务的最终准确率作为奖励信号。

#### 重点内容：
- **奖励函数定义**：
  $$
  R(\hat{y}, y) = 
  \begin{cases} 
  1, & \texttt{is\_equivalent}(\hat{y}, y) \\
  -1, & \text{otherwise}
  \end{cases}
  $$
  其中：
  - $ \hat{y} $ 是模型的预测答案。
  - $ y $ 是真实答案。
  - 如果两者等价，则奖励为 1，否则为 -1。

- **应用领域**：
  该方法已被证明在多个任务中有效，例如自动定理证明、计算机编程和数学竞赛。

#### 总结：
基于规则的奖励建模方法避免了复杂的奖励模型训练，同时在多个领域中表现出色，是一种高效且实用的替代方案。

---

### 本节总结

本节系统介绍了论文中所用的关键方法和技术：
1. **PPO**：通过裁剪目标函数实现稳定策略更新。
2. **GRPO**：基于组的相对优势估计，无需价值函数。
3. **去除 KL 散度**：适应生成式任务的训练需求。
4. **基于规则的奖励建模**：避免奖励建模中的偏差问题，提升模型性能。

这些方法为后续的算法设计和实验奠定了理论基础。


## 3 DAPO

我们提出了一种新的算法：Decouple Clip and Dynamic sAmpling Policy Optimization（**DAPO**）。DAPO 对于每个问题 $ q $ 及其答案 $ a $，采样一组输出 $ \{o_i\}_{i=1}^G $，并通过以下目标函数优化策略：

$$
\mathcal{J}_{\text{DAPO}}(\theta) = \mathbb{E}_{(q,a)\sim\mathcal{D},\{o_i\}_{i=1}^G\sim\pi_{\theta_{\text{old}}}(\cdot|q)} \left[ \frac{1}{\sum_{i=1}^{G}|o_i|} \sum_{i=1}^G \sum_{t=1}^{|o_i|} \min\left(r_{i,t}(\theta)\hat{A}_{i,t},\ \text{clip}\left(r_{i,t}(\theta),1-\varepsilon_{\text{low}},1+\varepsilon_{\text{high}}\right)\hat{A}_{i,t}\right) \right]
$$

**约束条件为**：$ 0<|\{o_i \mid \texttt{is\_equivalent}(a, o_i)\}|<G $，即确保每组输出中不能全部是正确答案，也不能完全没有正确答案。

其中：

- $ r_{i,t}(\theta) = \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t}|q, o_{i,<t})} $：重要性采样比。
- $ \hat{A}_{i,t} = \frac{R_i - \text{mean}(\{R_i\}_{i=1}^G)}{\text{std}(\{R_i\}_{i=1}^G)} $：标准化优势函数，用于调整奖励的分布。

完整算法见表 1，我们将介绍 DAPO 的关键技术。

---

### 3.1 Raise the Ceiling: Clip-Higher（重点）

在使用传统的 PPO 或 GRPO 算法时，我们发现策略的熵（entropy）会迅速下降，导致生成的响应变得相似，限制了探索能力。

**为了解决这个问题，DAPO 引入了 Clip-Higher 策略**，将上界和下界的剪切范围（clip range）分别设置为 $ \varepsilon_{\text{low}} $ 和 $ \varepsilon_{\text{high}} $。

- **具体来说**，当 $ \varepsilon=0.2 $ 时，若 $ \hat{A}_{i,t} > 0 $，概率高的“exploitation”词（如 0.9）不容易被剪切，而概率低的“exploration”词（如 0.01）则难以提升，导致探索受限。
- **解决方案**：提高 $ \varepsilon_{\text{high}} $ 的值，使得低概率词的提升有更大空间，从而**增强策略的探索能力**。

**实验结果**：策略的熵显著增加，生成的样本更加多样。

---

### 3.2 The More the Merrier: Dynamic Sampling（重点）

现有的 RL 算法在某些提示（prompt）的准确率等于 1 时会出现“梯度下降”问题：

- 如果所有输出 $ \{o_i\}_{i=1}^G $ 都正确且奖励相同，则优势函数为 0，导致梯度为 0，样本效率下降。
- **随着训练进行，这类样本比例不断增加**（见图 3(b)），导致每批次有效样本减少，梯度方差增加。

**为了解决这个问题，DAPO 提出动态采样策略**：

- **在训练前持续采样**，直到每批次中不包含准确率为 0 或 1 的样本。
- 保证每个批次中所有样本都有有效梯度，并保持样本数量一致。

**优点**：虽然增加了采样成本，但不降低训练效率，反而**加快收敛速度**（见图 6）。

---

### 3.3 Rebalancing Act: Token-Level Policy Gradient Loss（重点）

传统 GRPO 算法使用“样本级损失计算”（average loss per sample），即每个样本权重相同：

- **在长链推理（long-CoT）任务中存在问题**：
  1. 长文本中每个 token 的权重被平均，导致重要 token 被稀释。
  2. 长文本中可能存在无意义的重复内容，但这些内容对损失影响小，导致模型熵增加，响应变长。

**DAPO 引入“token 级损失”策略**：

- 损失计算基于每个 token，而非整个样本。
- **优点**：
  - 长文本中的每个 token 对梯度更新有同等影响。
  - 惩罚低质量 token，促进高质量推理（见图 4）。

---

### 3.4 Hide and Seek: Overlong Reward Shaping（重点）

在 RL 训练中，通常会设置最大长度，超出长度的样本会被截断，但默认的惩罚机制会导致训练不稳定：

- **问题**：即使一个样本的推理是正确的，也可能因长度过长而被惩罚，导致模型难以判断正确性。
  
**解决方案**：

- **Overlong Filtering**：忽略被截断样本的损失，提升训练稳定性。
- **Soft Overlong Punishment**：设计长度相关的惩罚机制：
  $$
  R_{\text{length}}(y) = \begin{cases}
  0, & |y| \leq L_{\text{max}} - L_{\text{cache}} \\
  \frac{(L_{\text{max}} - L_{\text{cache}}) - |y|}{L_{\text{cache}}}, & L_{\text{max}} - L_{\text{cache}} < |y| \leq L_{\text{max}} \\
  -1, & |y| > L_{\text{max}}
  \end{cases}
  $$
  - 动态惩罚：长度越接近最大值，惩罚越小；超过最大值后统一惩罚为 -1。

**效果**：提升 AIME 等任务中的模型性能（见图 5）。

---

### 3.5 Dataset Transformation（次要）

- **数据来源**：网络爬取和人工标注。
- **数学答案格式多样**（表达式、公式、数字），难以统一解析。
- **解决方案**：将答案转换为整数形式，例如将 $ a + \sqrt{b} + c $ 转换为 $ a + b + c $，便于规则处理。
- **最终数据集**：DAPO-Math-17K，包含 17,000 个问题，每个问题答案为整数。

---

### 总结

本节重点介绍了 DAPO 算法的核心技术，包括：

1. **Clip-Higher**：增强探索能力，通过解耦上下界剪切范围。
2. **Dynamic Sampling**：防止梯度消失，保证每批次样本的多样性与有效性。
3. **Token-Level Loss**：提升长链推理任务中的 token 层面学习能力。
4. **Overlong Reward Shaping**：通过动态惩罚机制稳定训练过程。

这些策略共同提升了 DAPO 的训练效率和生成质量，适合作为大规模 LLM 的强化学习系统。


## 4 Experiments

### 4.1 训练细节

本研究主要集中在数学任务上评估所提出的算法，该算法可有效迁移至其他任务。我们使用 **verl 框架** 进行训练，并以 **GRPO** 为基线算法，采用**分组奖励归一化**进行优势估计。

在超参数设置方面，我们使用 **AdamW** 优化器，学习率固定为 $1 \times 10^{-6}$，并加入了在前 20 步进行的线性预热（warm-up）。对于 rollout 的设置，提示批大小为 512，每个提示生成 16 个响应；训练时的 mini-batch 大小也是 512，即每一步进行 16 次梯度更新。

在**过长奖励塑形（Overlong Reward Shaping）**中，我们将最大期望长度设为 16,384 tokens，并额外预留 4,096 tokens 作为“软惩罚缓存”，因此生成的最大 token 数为 20,480。

在 **Clip-Higher 机制**中，我们设置剪切参数 $\varepsilon_{\text{low}} = 0.2$ 和 $\varepsilon_{\text{high}} = 0.28$，以在探索与利用之间取得平衡。

对于 AIME 测试集的评估，我们重复 32 次，报告 **avg@32** 结果以确保稳定性。推理时设置温度参数为 1.0，top-p 为 0.7。

图 6 展示了使用动态采样前后的训练进展曲线。

---

### 4.2 主要实验结果

在 AIME 2024 数据集上的实验表明，**DAPO 算法成功将 Qwen-32B Base 模型训练为一个强大的推理模型**，其性能优于 DeepSeek 使用 R1 方法在 Qwen2.5-32B 上的实验结果。

根据图 1 所示，AIME 2024 的准确率从接近 0% 提升到 50%，且仅使用了 DeepSeek-R1 所需训练步数的 50%。

通过表 1 可以看到，各种训练技术的逐步引入显著提升了模型性能。例如，在原始 GRPO 设置下，Qwen2.5-32B 只能达到 30% 的准确率，而使用 DAPO 后提升至 50%。

尽管**token 级别的损失**在性能提升方面贡献较小，但其提升了训练的稳定性，并有助于生成长度的健康增长。

**动态采样（Dynamic Sampling）**虽然需要采样更多数据（因部分零梯度数据被过滤），但整体训练时间并没有显著增加。图 6 显示，尽管采样次数增加，模型收敛时间却减少了，因为训练步数减少了。

---

### 4.3 训练动态

大规模语言模型的强化学习训练不仅是一项前沿研究，也是一项复杂的系统工程挑战。系统中各个子系统相互依赖，单一子系统的修改可能引发连锁反应，带来不可预见的后果。即使输入数据或超参数发生轻微变化，也可能通过迭代过程被放大，导致结果出现显著偏差。

因此，在训练过程中**监控关键中间结果**是至关重要的，有助于快速识别问题并优化系统。

图 7 展示了四个关键指标的动态变化曲线：

- **(a) 响应长度**：与训练稳定性密切相关，长度适当增加有助于探索更复杂的推理行为，但也会出现停滞甚至下降的情况。
- **(b) 奖励得分**：奖励的增长趋势通常稳定，但在验证集上与准确率的相关性较低，可能存在过拟合问题。
- **(c) 生成熵**与**(d) 平均概率**：这些指标反映了模型的探索能力。通过 **Clip-Higher** 技术有效防止了熵崩溃，并发现熵值缓慢上升有助于模型性能提升。

---

### 4.4 案例研究

我们展示了一个具体的数学问题（见表 2），在训练过程中，模型逐步展现出**反思性行为**，即不仅强化已有推理路径，还逐渐发展出新的推理模式，例如对前期推理步骤的检查与回溯。

这种现象揭示了强化学习算法在模型训练中的**适应性和探索能力**，为理解模型推理能力的形成机制提供了新视角。我们将在未来的工作中进一步研究这一现象。


## 5 Conclusion


在本文中，我们发布了一个**全开源的大型语言模型强化学习（LLM RL）系统**，涵盖了算法、代码基础设施和数据集。该系统在大规模LLM RL任务中达到了**最先进的性能**（使用Qwen-32B预训练模型，在AIME 50任务中表现优异）。

我们提出了一个关键算法——**Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO)**，并引入了**四项关键技术**，使强化学习在长链推理（long-CoT RL）场景下具有强大的**高效性和有效性**。

此外，通过开源训练代码和数据集，我们为更广泛的研究社区和社会提供了一个**可扩展的强化学习解决方案**，使得更多人能够从这一进展中受益。


## Contributions

本部分详细列出了在项目中不同角色的贡献者，按类别划分如下：

---

### Project Lead 项目负责人

- **Qiying Yu**（余启英）  
  - 担任项目负责人（Project Lead），是本研究的主要推动者和领导核心。

---

### Algorithm 算法设计

- **Qiying Yu**（余启英）  
- **Zheng Zhang**（张正）  
- **Ruofei Zhu**（朱若飞）  
- **Yufeng Yuan**（袁宇峰）  
- **Xiaochen Zuo**（左晓晨）  
- **Yu Yue**（乐宇）  

  - 该部分列出了参与算法设计与实现的主要成员。
  - **重点**：余启英在本研究中不仅担任项目负责人，同时也参与了核心算法的设计。

---

### Infrastructure 基础设施构建

- **Weinan Dai**（戴文楠）  
- **Tiantian Fan**（樊甜甜）  
- **Gaohong Liu**（刘高红）  
- **Juncai Liu**（刘俊才）  
- **Lingjun Liu**（刘灵君）  
- **Xin Liu**（刘欣）  
- **Haibin Lin**（林海彬）  
- **Zhiqi Lin**（林志强）  
- **Bole Ma**（马泊乐）  
- **Guangming Sheng**（盛光明）  
- **Yuxuan Tong**（童雨轩）  
- **Qiying Yu**（余启英）  
- **Chi Zhang**（张驰）  
- **Mofan Zhang**（张墨帆）  
- **Ru Zhang**（张儒）  
- **Wang Zhang**（张旺）  
- **Hang Zhu**（朱航）  
- **Jinhua Zhu**（朱金华）  

  - **重点**：这是最长的一组名单，表明基础设施建设是一个涉及大量人员的系统性工作。
  - 注明“**Last-Name in Alphabetical Order**”，说明该部分按姓氏字母顺序排列，无主次之分。
  - **余启英**再次出现在此部分，说明其在算法和基础设施方面均有重要贡献。

---

### Dataset 数据集建设

- **Jiaze Chen**（陈佳泽）  
- **Jiangjie Chen**（陈江洁）  
- **Chengyi Wang**（王诚毅）  
- **Hongli Yu**（余红丽）  
- **Yuxuan Song**（宋雨轩）  
- **Xiangpeng Wei**（魏翔鹏）  
- **Qiying Yu**（余启英）  

  - 该部分列出了参与数据集构建的成员。
  - **余启英**再次出现，表明其参与了多个关键环节，从项目管理到算法、基础设施和数据集均有贡献。

---

### Supervision 项目指导

- **Hao Zhou**（周浩）  
- **Jingjing Liu**（刘静静）  
- **Wei-Ying Ma**（马维英）  
- **Ya-Qin Zhang**（张亚勤）  
- **Lin Yan**（颜林）  
- **Mu Qiao**（乔木）  
- **Yonghui Wu**（吴勇晖）  
- **Mingxuan Wang**（王明轩）  

  - 该部分为高级研究人员或专家，提供项目指导与学术支持。
  - **重点**：其中包括来自**清华大学人工智能产业研究院（AIR）**和**字节跳动种子实验室（ByteDance Seed）**的多位资深研究人员，显示项目具有跨机构、跨领域的合作性质。

---

### Affiliation 机构归属

- **1 ByteDance Seed**（字节跳动种子实验室）  
- **2 Institute for AI Industry Research (AIR), Tsinghua University**（清华大学人工智能产业研究院）  
- **3 The University of Hong Kong**（香港大学）  
- **4 SIA-Lab of Tsinghua AIR and ByteDance Seed**（清华大学AIR与字节跳动种子实验室联合实验室）

  - **重点**：本研究由字节跳动与清华大学人工智能产业研究院合作主导，同时联合了香港大学。
  - 机构“**4**”为联合实验室，显示了产学研结合的科研合作模式。

---

### 总结

该部分系统性地列出了本研究的各个参与人员及其角色，展现了跨机构、多学科的协作体系。**余启英**作为项目负责人，贯穿于算法、基础设施和数据集等多个核心环节，是本项目的核心人物之一。此次合作体现出字节跳动与清华大学在AI领域的深度协同。


## Acknowledgments

本节主要对在DAPO项目中提供支持的个人和团队表达了感谢。作者特别提到了Zhengyin Du、Shengding Hu、Kai Shen、Tianyang Zhan、Zhen Xiao、Renjie Zheng、Li Han、Kaihua Jiang等同事，以及ByteDance的其他同仁。重点在于强调这些人在项目推进过程中所做出的贡献和支持。


## 6 Dataset Transformation



以下是该章节内容的总结，按照原文结构进行讲解，重点内容予以强调，不重要内容适当精简：
x

本节介绍了一种**数据转换（Data Transformation）**的示例，展示了原始问题、转换后的问题以及模型在转换过程中的输出。目的是让模型**在保持高准确率的同时进行综合性推理，并避免生成虚假信息（hallucinations）**。

为实现这一目标，文中采用了**Chain of Thought（CoT）推理方法**，并定义了四个清晰的步骤：

1. **提取答案格式（Extract the answer format）**  
2. **重写问题描述（Rewrite the problem statement）**  
3. **解决修改后的问题并提供整数答案（Solve the modified problem and provide an integer as the final answer）**  

在每个步骤中，作者提供了**少量示例（few-shot solutions）或详细指导（detailed guidelines）**，以引导模型的推理过程。

通过这种方法，作者观察到在大多数情况下，**大型语言模型（LLM）可以生成在格式和质量上都令人满意的重写版本。**

---

### **Original Problem**

原始问题要求：  
给定实数 $ x $ 和 $ y $，满足方程：

$$
x^2 + y^2 - 22x - 16y + 113 = 0
$$

求 $ x $ 的**最小可能值**。  
**答案为：** $ 11 - 2\sqrt{6} $

---

### **Transformed Problem**

转换后的问题要求：  
给定实数 $ x $ 和 $ y $，满足相同的方程：

$$
x^2 + y^2 - 22x - 16y + 113 = 0
$$

但问题不再是求 $ x $ 的最小值，而是**将原答案形式 $ k - m\sqrt{n} $ 中的整数部分 $ k $、$ m $、$ n $ 提取出来，求和 $ k + m + n $**。  
**答案为：** $ 19 $

---

### **Model’s output (Markdown Text)**

模型输出部分展示了转换过程中的几个关键步骤和中间结果。

---

#### **Transformation Thought Process（转换的思考过程）**

模型首先识别出原始答案是一个代数表达式 $ 11 - 2\sqrt{6} $，包含一个整数和一个平方根项。  
然后根据规则将其转换为未知系数的形式 $ k - m\sqrt{n} $，并要求求出 $ k + m + n $。

---

#### **Answer Format（答案格式）**

答案格式为：
$$
k - m\sqrt{n}
$$

---

#### **Transformed Problem（转换后的问题）**

与上文一致，要求根据原答案形式计算 $ k + m + n $，答案为 19。

---

#### **Answer Calculation for the Transformed Problem（转换后问题的答案计算）**

- 原始答案：$ 11 - 2\sqrt{6} $
- 匹配格式 $ k - m\sqrt{n} $，得到：
  - $ k = 11 $
  - $ m = 2 $
  - $ n = 6 $
- 最终结果：$ k + m + n = 11 + 2 + 6 = 19 $

---

#### **Final Answer（最终答案）**

$$
19
$$

---

### 总结

本节展示了如何通过**结构化的问题转换和清晰的推理步骤**，引导模型将一个数学答案从**代数表达式转换为整数形式**，从而验证模型在处理复杂任务时的准确性与逻辑性。该方法强调了**避免幻觉（hallucinations）**，并通过**CoT推理与格式引导**来提升输出质量。


## 7 Supplementary Case



### 问题描述

在Aimeville市的900名居民中，有195人拥有钻石戒指，367人拥有一套高尔夫球杆，562人拥有一把园艺铲。此外，所有900名居民都拥有一袋糖果心。已知有437人恰好拥有这四种物品中的两样，234人恰好拥有这四种物品中的三样。问题要求我们找出拥有全部四种物品的居民人数。

---

### 解题思路

1. **变量定义**  
   用集合表示拥有不同物品的居民：
   - A：拥有钻石戒指的居民集合
   - B：拥有高尔夫球杆的居民集合
   - C：拥有园艺铲的居民集合
   - D：拥有糖果心的居民集合。由于所有居民都拥有糖果心，所以集合D的大小为900。

   令 $ a_i $ 表示恰好拥有 $ i $ 项物品的居民人数。

2. **集合关系与问题转化**  
   由于所有居民都拥有糖果心（即D集合包含所有人），可以将问题简化为只考虑A、B、C这三项物品的组合情况。因此，可以重新定义变量 $ x_i $ 表示恰好拥有这三项物品中 $ i $ 项的居民人数。

3. **组合分析与人数划分**  
   根据题意：
   - 有437人恰好拥有两项物品
   - 有234人恰好拥有三项物品
   - 要求找出恰好拥有四项物品的人数（即 $ a_4 $）

   此时采用组合分析方法，将拥有D物品（糖果心）纳入统计，重新计算拥有不同数量物品的居民总数。

---

### 解题过程

1. **使用组合分析法**  
   由于每个人至少拥有一项物品（D），问题简化为统计A、B、C三者之间的交集人数。再考虑D的全包含性，逐级统计拥有1项、2项、3项、4项物品的人数。

2. **计算公式**  
   通过统计每个组合的交集人数，结合已知条件（437人拥有两项，234人拥有三项），列出方程组求解拥有四项物品的人数。

3. **关键计算**  
   根据上述方法，最终得出：
   $$
   a_4 = \frac{219}{4} = 54.75
   $$

   由于人数必须为整数，说明计算过程中可能出现了错误或方法选择不当。因此，需要重新考虑统计方法，确保所有组合关系准确无误。

4. **结论**  
   本题的关键在于正确理解D集合的全包含性，并合理划分其他集合之间的交集关系。由于初步计算产生非整数结果，说明需进一步修正公式或采用更合适的组合分析方法。

---

### 重点总结

- **问题简化**：因为所有居民都拥有糖果心，可将问题简化为统计A、B、C三项物品的交集。
- **关键数据**：已知恰好拥有两项或三项物品的人数，是求解拥有四项物品人数的关键条件。
- **方法选择**：直接应用容斥原理可能导致非整数解，因此需采用组合分析法，并确保统计逻辑准确。
- **最终目标**：找出恰好拥有四项物品的居民人数 $ a_4 $。
