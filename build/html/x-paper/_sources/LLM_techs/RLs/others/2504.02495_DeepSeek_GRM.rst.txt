2504.02495_DeepSeek-GRM: Inference-Time Scaling for Generalist Reward Modeling
##############################################################################

* <https://arxiv.org/abs/2504.02495>
* ç»„ç»‡: DeepSeek
* å¼•ç”¨: 67


* RM: Reward Modeling
* GRM: Generative Reward Modeling
* SPCT: Self-Principled Critique Tuning

Abstract
========

* è¿™æ®µå†…å®¹ä¸»è¦è®²çš„æ˜¯å¦‚ä½•é€šè¿‡æ”¹è¿›å¥–åŠ±å»ºæ¨¡ï¼ˆReward Modeling, RMï¼‰å’Œå­¦ä¹ æ–¹æ³•æ¥æå‡å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨æ¨ç†é˜¶æ®µçš„å¯æ‰©å±•æ€§ï¼ˆinference-time scalabilityï¼‰ï¼Œç‰¹åˆ«æ˜¯åœ¨æ³›åŒ–ä»»åŠ¡ï¼ˆgeneral queriesï¼‰ä¸­ä¹Ÿèƒ½æœ‰æ•ˆè¿›è¡Œå¥–åŠ±åé¦ˆã€‚
* ã€æ€»ç»“ã€‘è¿™ç¯‡å·¥ä½œæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ï¼ˆSPCTï¼‰ç»“åˆç”Ÿæˆå¼å¥–åŠ±å»ºæ¨¡ï¼ˆGRMï¼‰ï¼Œè®©å¤§è¯­è¨€æ¨¡å‹åœ¨æ¨ç†é˜¶æ®µä¹Ÿèƒ½æ›´æ™ºèƒ½åœ°è¯„ä¼°å’Œé€‰æ‹©é«˜è´¨é‡è¾“å‡ºï¼Œå®ç°äº†æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›å’Œå¯æ‰©å±•æ€§ï¼Œå°¤å…¶é€‚åˆå¤æ‚ä»»åŠ¡å’Œç¼ºä¹æ˜ç¡®å¥–åŠ±ä¿¡å·çš„åœºæ™¯ã€‚


æˆ‘æ¥å¸®ä½ æ‹†è§£ç†è§£è¿™æ®µè¯çš„æ ¸å¿ƒå†…å®¹å’Œå…³é”®æœ¯è¯­ï¼š

ğŸŒŸ æ€»ä½“ç›®æ ‡
    - å½“å‰åœ¨è®­ç»ƒ LLM æ—¶ï¼Œç»å¸¸ä½¿ç”¨ å¼ºåŒ–å­¦ä¹ ï¼ˆReinforcement Learning, RLï¼‰ æ¥å¾®è°ƒï¼Œä½¿æ¨¡å‹æ›´ç¬¦åˆäººç±»åå¥½ã€‚
    - ç„¶è€Œï¼Œåœ¨å¾ˆå¤šæ— æ³•ç›´æ¥éªŒè¯çš„ä»»åŠ¡ä¸­ï¼ˆæ¯”å¦‚å¼€æ”¾å¼é—®ç­”ã€å¤æ‚æ¨ç†ï¼‰ï¼Œå¾ˆéš¾å®šä¹‰å‡†ç¡®çš„å¥–åŠ±ä¿¡å·ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚
    - æœ¬æ–‡çš„ç›®æ ‡æ˜¯ï¼šæ¢ç´¢åœ¨æ¨ç†é˜¶æ®µï¼ˆinference-timeï¼‰ï¼Œå¦‚ä½•é€šè¿‡æ”¹è¿› RM å’Œ RL æ–¹æ³•æ¥æå‡æ³›åŒ–èƒ½åŠ›å’Œæ‰©å±•æ€§ã€‚

ğŸ§  æ ¸å¿ƒè´¡çŒ®
    1. ä½¿ç”¨ Pointwise Generative Reward Modelingï¼ˆGRMï¼‰
        - ä¼ ç»Ÿçš„ RM æ–¹æ³•ç»å¸¸åŸºäºæ‰“åˆ†æˆ–è€…æ’åºï¼ˆrankï¼‰ï¼Œä¸å¤Ÿçµæ´»ã€‚
        - GRMï¼šæ˜¯ä¸€ç§åŸºäºç”Ÿæˆçš„æ–¹å¼ç»™å¥–åŠ±è¯„åˆ†ï¼Œå¯ä»¥é€‚åº”ä¸åŒç±»å‹çš„è¾“å…¥ï¼ˆå¦‚å¯¹è¯ã€ä»£ç ã€é—®ç­”ç­‰ï¼‰ã€‚
        - Pointwiseï¼šæ„æ€æ˜¯å¯¹æ¯ä¸ªå€™é€‰å›ç­”å•ç‹¬è¯„åˆ†ï¼Œè€Œä¸æ˜¯æ¯”è¾ƒå¤šä¸ªå›ç­”ã€‚
    2. æå‡º Self-Principled Critique Tuningï¼ˆSPCTï¼‰
        - ä¸ºäº†è®© GRM æ›´æœ‰æ•ˆåœ°äº§ç”Ÿå¥–åŠ±ä¿¡å·ï¼Œä»–ä»¬è®¾è®¡äº†ä¸€ç§æ–°çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼š
        - æ¨¡å‹è‡ªå·±ç”Ÿæˆè¯„ä»·æ ‡å‡†ï¼ˆprinciplesï¼‰ï¼Œæ¯”å¦‚å›ç­”æ˜¯å¦é€»è¾‘æ¸…æ™°ã€æœ‰äº‹å®æ”¯æŒã€‚
        - ç„¶ååŸºäºè¿™äº›æ ‡å‡†æ¥è¿›è¡Œæ‰¹åˆ¤æ€§è¯„ä»·ï¼ˆcritiquesï¼‰ï¼Œè¯„ä¼°å€™é€‰ç­”æ¡ˆçš„å¥½åã€‚
        - è¿™ç§æ–¹æ³•èƒ½è®©æ¨¡å‹æ›´ç¨³å®šåœ°ç”Ÿæˆæ›´é«˜è´¨é‡ã€æ›´å¯æ‰©å±•çš„å¥–åŠ±åé¦ˆï¼Œç”Ÿæˆçš„æ–°æ¨¡å‹å« DeepSeek-GRMã€‚
    3. æ¨ç†é˜¶æ®µçš„æ‰©å±•ç­–ç•¥ï¼ˆInference-time Scalingï¼‰
        - ä¸ºäº†åœ¨æ¨ç†é˜¶æ®µæå‡æ€§èƒ½ï¼Œæå‡ºäº†ä¸¤ä¸ªç­–ç•¥ï¼š
        - å¹¶è¡Œé‡‡æ ·ï¼ˆParallel Samplingï¼‰ï¼šç”¨æ›´å¤šè®¡ç®—èµ„æºç”Ÿæˆå¤šä¸ªç»“æœï¼Œæé«˜æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚
        - å¼•å…¥ Meta RMï¼ˆå…ƒå¥–åŠ±æ¨¡å‹ï¼‰ï¼šå¯¹å¤šä¸ªå€™é€‰ç­”æ¡ˆçš„â€œæŠ•ç¥¨â€ç»“æœè¿›è¡ŒæŒ‡å¯¼ï¼Œæå‡æœ€ç»ˆé€‰æ‹©è´¨é‡ã€‚

ğŸ“Š å®éªŒæ•ˆæœ
    - SPCT æ–¹æ³•æ˜¾è‘—æå‡äº† GRM çš„è´¨é‡å’Œæ‰©å±•æ€§ï¼Œåœ¨å¤šä¸ª RM åŸºå‡†æµ‹è¯•ä¸­è¶…è¿‡ç°æœ‰æ¨¡å‹å’Œæ–¹æ³•ã€‚
    - æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨æ¨ç†é˜¶æ®µæ‰©å±•ï¼ˆinference-time scalingï¼‰è¡¨ç°ä¼˜äºè®­ç»ƒé˜¶æ®µæ‰©å±•ï¼ˆtraining-time scalingï¼‰ã€‚
    - å½“ç„¶ï¼ŒDeepSeek-GRM ä»åœ¨æŸäº›ä»»åŠ¡ä¸Šå­˜åœ¨æŒ‘æˆ˜ï¼Œè¿™ä¹Ÿæ˜¯æœªæ¥é€šç”¨å¥–åŠ±ç³»ç»Ÿéœ€è¦è§£å†³çš„æ–¹å‘ã€‚


1. Introduction
===============

* æœ¬ç« ä¸»è¦å›´ç»•å¥–åŠ±å»ºæ¨¡ï¼ˆReward Modeling, RMï¼‰åœ¨å¤§è¯­è¨€æ¨¡å‹æ¨ç†é˜¶æ®µæ‰©å±•æ€§ä¸­çš„æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆå±•å¼€
* ã€æ€»ç»“ã€‘æœ¬æ–‡æå‡ºäº†ä¸€ç§èƒ½åœ¨æ¨ç†é˜¶æ®µç”Ÿæˆé«˜è´¨é‡å¥–åŠ±çš„æ–°æ–¹æ³•ï¼ˆSPCTï¼‰ï¼Œé€šè¿‡ç»„åˆåŸåˆ™ç”Ÿæˆã€æ‰¹åˆ¤æ€§è¯„ä¼°å’Œå¹¶è¡Œé‡‡æ ·ï¼Œæ„å»ºå‡ºå¯æ‰©å±•ã€é€šç”¨çš„å¥–åŠ±ç³»ç»Ÿï¼Œæ˜¾è‘—æå‡äº† LLM åœ¨æ³›åŒ–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

* èƒŒæ™¯ä»‹ç»
    - å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿‘å¹´æ¥å–å¾—å·¨å¤§è¿›å±•ï¼Œå·²ç»å…·å¤‡ç†è§£ã€ç”Ÿæˆã€æ¨ç†ã€å†³ç­–ç­‰å¤æ‚èƒ½åŠ›ã€‚
    - LLM å·²æˆä¸º AI ç ”ç©¶çš„é‡è¦æ”¯æŸ±ï¼Œæ¨åŠ¨äº†æ¨ç†èƒ½åŠ›å’Œä»»åŠ¡è¡¨ç°çš„æå‡ã€‚
* å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸äººç±»åå¥½å¯¹é½
    - å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä½œä¸º LLM çš„åè®­ç»ƒæ–¹æ³•ï¼ˆpost-trainingï¼‰è¢«å¤§è§„æ¨¡ä½¿ç”¨ã€‚
    - å®ƒåœ¨ä¸‰ä¸ªæ–¹é¢æ•ˆæœæ˜¾è‘—ï¼š
        1. äººç±»ä»·å€¼å¯¹é½ï¼ˆalignment with human preferencesï¼‰
        2. é•¿ç¨‹æ¨ç†èƒ½åŠ›ï¼ˆlong-term reasoningï¼‰
        3. é€‚åº”ç¯å¢ƒå˜åŒ–ï¼ˆadaptation to different tasks and contextsï¼‰

* å¥–åŠ±å»ºæ¨¡ï¼ˆReward Modelingï¼‰æ˜¯ RL çš„å…³é”®
    - åœ¨ RL ä¸­ï¼Œå¥–åŠ±å»ºæ¨¡ï¼ˆRMï¼‰æ˜¯å…³é”®ï¼Œå®ƒè´Ÿè´£åˆ¤æ–­ LLM è¾“å‡ºçš„å¥½åã€‚
    - é«˜è´¨é‡çš„å¥–åŠ±èƒ½æ˜¾è‘—æå‡æ¨¡å‹è¡¨ç°ï¼Œä¸ç®¡æ˜¯åœ¨è®­ç»ƒä¸­è¿˜æ˜¯æ¨ç†ä¸­ã€‚

* ç°æœ‰ RM çš„å±€é™æ€§
    - å½“å‰é«˜è´¨é‡çš„å¥–åŠ±ä¿¡å·ï¼Œä¸»è¦ä¾èµ–ï¼š
        - æ˜ç¡®è®¾è®¡çš„ç¯å¢ƒï¼ˆå¦‚æ¸¸æˆã€ä»¿çœŸï¼‰
        - å¯éªŒè¯çš„é—®é¢˜ï¼ˆå¦‚æ•°å­¦ã€ä»£ç ï¼‰
    - ä½†å¯¹å¼€æ”¾ä»»åŠ¡ï¼ˆgeneral domainsï¼‰æ¥è¯´ï¼Œåˆ¤æ–­â€œå¥½å›ç­”â€å¾€å¾€ä¸»è§‚å¤æ‚ã€ç¼ºä¹æ˜ç¡®æ ‡å‡†ï¼Œå› æ­¤æ›´éš¾å»ºæ¨¡ã€‚

* é€šç”¨å¥–åŠ±å»ºæ¨¡çš„æ„ä¹‰
    - æ‰€ä»¥æå‡ºäº†â€œé€šç”¨å¥–åŠ±å»ºæ¨¡ï¼ˆGeneralist RMï¼‰â€ï¼š
    - å®ƒèƒ½æ”¯æŒæ›´å¹¿æ³›ä»»åŠ¡
    - æ— éœ€æ‰‹å·¥è§„åˆ™
    - é€‚ç”¨äºæ¨ç†é˜¶æ®µï¼ˆå¦‚é‡‡æ ·é€‰æ‹©ï¼‰æˆ–è®­ç»ƒé˜¶æ®µï¼ˆRLå¾®è°ƒï¼‰

* RM é¢ä¸´çš„æŒ‘æˆ˜
    1. è¾“å…¥çµæ´»æ€§ï¼šèƒ½å¦æ”¯æŒå•ä¸ª/å¤šä¸ªå›ç­”çš„è¯„ä¼°
    2. è·¨ä»»åŠ¡å‡†ç¡®æ€§ï¼šèƒ½å¦ç»™å‡ºåˆç†å¥–åŠ±
    3. æ¨ç†é˜¶æ®µè®¡ç®—æ‰©å±•æ€§ï¼šèƒ½å¦é€šè¿‡æ›´å¤šæ¨ç†è®¡ç®—æå‡å¥–åŠ±è´¨é‡
    4. å­¦ä¹ ç­–ç•¥çš„æ‰©å±•èƒ½åŠ›ï¼šèƒ½å¦å­¦å‡ºé€‚åˆâ€œæ‰©å±•è®¡ç®—â€çš„è¡Œä¸ºæ¨¡å¼

* ç°æœ‰ RM ç±»å‹å’Œæ‰“åˆ†æ¨¡å¼åˆ†æ

    - æŒ‰ç”Ÿæˆæ–¹å¼åˆ†ï¼š
        - Scalarï¼ˆæ ‡é‡æ‰“åˆ†ï¼‰ï¼šåªè¾“å‡ºä¸€ä¸ªåˆ†æ•°
        - Semi-scalarï¼šä»‹äºæ ‡é‡ä¸ç”Ÿæˆä¹‹é—´
        - Generativeï¼ˆç”Ÿæˆå¼ï¼‰ï¼šç”Ÿæˆæ–‡å­—è¯„ä»·
    - æŒ‰æ‰“åˆ†æ–¹å¼(scoring patterns)åˆ†ï¼š
        - Pointwiseï¼ˆå•ä¸ªè¯„åˆ†ï¼‰
        - Pairwiseï¼ˆæ¯”è¾ƒä¸¤ä¸ªå›ç­”ï¼‰
    - è¯´æ˜ï¼š
        - Pairwise è™½ç„¶æœ‰ç”¨ï¼Œä½†ä¸æ”¯æŒå•ç‹¬æ‰“åˆ†ï¼Œè¾“å…¥ä¸çµæ´»ï¼›
        - Scalar è™½ç®€å•ï¼Œä½†å¾ˆéš¾ä½“ç°å¤æ‚å·®å¼‚ï¼Œæ‰©å±•æ€§å·®ã€‚



.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/ACcM4g.png

    Figure 2: Different paradigms for reward generation, including (a) scalar, (b) semi-scalar, and (c) generative approaches, and different scoring patterns, including (i) pointwise and (ii) pairwise approaches. We list the representative methods for each approach, and corresponding inference-time scalability (whether better rewards could be obtained from multiple sampling) and input flexibility (whether supports rating single and multiple responses).



* ä½œè€…çš„æ´å¯Ÿä¸æ–¹æ³•æ„æ€
    - ä»–ä»¬å‘ç°ï¼šç”Ÿæˆå¼+Pointwiseçš„GRMæ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç»Ÿä¸€æ–¹æ³•ï¼Œèƒ½æ”¯æŒå¤šç§è¾“å…¥ç±»å‹ã€‚
    - å¦‚æœå¼•å…¥â€œåŸåˆ™ï¼ˆprinciplesï¼‰â€å’Œâ€œæ‰¹åˆ¤æ€§åˆ†æï¼ˆcritiquesï¼‰â€ï¼Œå¯ä»¥æå‡å¥–åŠ±è´¨é‡ã€‚
    - è¿™å¯å‘äº†ä»–ä»¬æå‡ºæ–°çš„è®­ç»ƒæ–¹æ³•ï¼šSelf-Principled Critique Tuningï¼ˆSPCTï¼‰ã€‚


* æ–¹æ³•è®¾è®¡ & å®ç°ç»†èŠ‚
    - SPCT æ ¸å¿ƒç‚¹ï¼š
        - åˆ©ç”¨è§„åˆ™å¼•å¯¼ RLï¼ˆrule-based online RLï¼‰
        - å­¦ä¹ ç”Ÿæˆç¬¦åˆä»»åŠ¡çš„â€œåŸåˆ™â€å’Œâ€œæ‰¹åˆ¤æ€§åˆ†æâ€
    - æ–°æ¨¡å‹å«åš DeepSeek-GRM-27Bï¼ˆåŸºäº Gemma-2-27B åè®­ç»ƒè€Œæˆï¼‰
    - åœ¨æ¨ç†é˜¶æ®µï¼š
        - ä½¿ç”¨å¹¶è¡Œé‡‡æ ·ç”Ÿæˆå¤šä¸ªè¯„ä¼°ç»“æœ
        - ç”Ÿæˆå¤šä¸ªä¸åŒåŸåˆ™+åˆ†æï¼Œè¿›è¡ŒæŠ•ç¥¨
        - å¼•å…¥Meta RMæå‡æœ€ç»ˆè¯„ä¼°è¡¨ç°



2. Preliminaries
================

* æœ¬ç« ä¸»è¦è§£é‡Šäº† å„ç§ Reward Modelï¼ˆå¥–åŠ±æ¨¡å‹, RMï¼‰æ–¹æ³•çš„æ¯”è¾ƒ ä»¥åŠ å¦‚ä½•ç”¨â€œåŸåˆ™ï¼ˆprinciplesï¼‰â€æå‡å¥–åŠ±è´¨é‡ã€‚

2.1 Comparisons of Different RM approaches
------------------------------------------


âœ… ä¸¤ä¸ªå…³é”®èƒ½åŠ›ï¼š
    1. è¾“å…¥çµæ´»æ€§ï¼ˆInput Flexibilityï¼‰ï¼šæ˜¯å¦æ”¯æŒæ‰“åˆ†å•ä¸ªã€å¤šç»„æˆ–æˆå¯¹çš„å›ç­”ã€‚
    2. æ¨ç†é˜¶æ®µå¯æ‰©å±•æ€§ï¼ˆInference-Time Scalabilityï¼‰ï¼šåœ¨æ¨ç†æ—¶æ˜¯å¦å¯ä»¥é€šè¿‡é‡‡æ ·ï¼ˆsamplingï¼‰å¤šæ¬¡æ¥æå‡è¯„åˆ†æ•ˆæœã€‚

âœ… ä¸¤ä¸ªç»´åº¦çš„åˆ†ç±»æ–¹å¼ï¼š
    1. ç”Ÿæˆæ–¹å¼ï¼ˆReward Generation Paradigmsï¼‰ï¼š
        - Scalarï¼ˆæ ‡é‡ï¼‰ï¼šç›´æ¥ç”Ÿæˆä¸€ä¸ªåˆ†æ•°ã€‚
        - Semi-Scalarï¼ˆåŠæ ‡é‡ï¼‰ï¼šå…ˆç”Ÿæˆä¸€æ®µæ–‡å­—è¯„ä»·ï¼ˆcritiqueï¼‰ï¼Œå†ç”Ÿæˆä¸€ä¸ªåˆ†æ•°ã€‚
        - Generativeï¼ˆç”Ÿæˆå¼ï¼‰ï¼šåªç”Ÿæˆæ–‡å­—è¯„ä»·ï¼Œé€šè¿‡åˆ†ææ–‡å­—æå–å¥–åŠ±ï¼ˆrewardï¼‰å€¼ã€‚
    2. è¯„åˆ†æ¨¡å¼ï¼ˆScoring Patternsï¼‰ï¼š
        - Pointwiseï¼ˆé€æ¡è¯„åˆ†ï¼‰ï¼šæ¯ä¸ªå›ç­”å•ç‹¬æ‰“åˆ†ã€‚
        - Pairwiseï¼ˆä¸¤ä¸¤æ¯”è¾ƒï¼‰ï¼šä»å¤šä¸ªå›ç­”ä¸­æŒ‘å‡ºæœ€å¥½ä¸€ä¸ªã€‚

âœ… æ¨ç†æ—¶æ‰©å±•çš„æŒ‘æˆ˜ï¼š
    - åœ¨æ¨ç†é˜¶æ®µæ‰©å±•ç®—åŠ›çš„æ–¹æ³•æ˜¯â€œé‡‡æ ·å¤šæ¬¡â€ï¼ˆæ¯”å¦‚ç”Ÿæˆå¤šä¸ªå¥–åŠ±ç»“æœï¼Œå†æ±‡æ€»æŠ•ç¥¨ï¼‰ï¼Œ
    - Scalar æ¨¡å‹çš„é—®é¢˜ï¼šæ¯æ¬¡éƒ½ç”ŸæˆåŒä¸€ä¸ªåˆ†æ•°ï¼Œæ²¡æ³•ä»å¤šæ¬¡é‡‡æ ·ä¸­å—ç›Šã€‚
    - Pairwise æ¨¡å‹çš„é—®é¢˜ï¼šåªçœ‹æˆå¯¹çš„æ¯”è¾ƒï¼Œä¸æ”¯æŒå•ä¸ªæˆ–å¤šä¸ªå›ç­”çš„è¯„ä¼°ï¼Œçµæ´»æ€§å·®ã€‚

âœ… å…¬å¼è§£é‡Šï¼š
    - ä½œè€…ç»™å‡ºä¸€ä¸ª Pointwise Generative Reward Modelï¼ˆGRMï¼‰çš„æ•°å­¦è¡¨è¾¾å¼ï¼š

.. math::

    {S_i\}_{i=1}^{n} = f_{\text{extract}}(C),\quad C \sim r_\theta(x, \{y_i\}_{i=1}^{n})

* æ„æ€æ˜¯ï¼š
    - å¯¹è¾“å…¥ query x å’Œå¤šä¸ªå›åº” :math:`\{y_i\}` ï¼Œç”Ÿæˆä¸€ä¸ªæ–‡æœ¬è¯„ä»· Cï¼ˆCritiqueï¼‰ï¼›
    - ç„¶åç”¨æå–å‡½æ•° :math:`f_{\text{extract}}` æŠŠè¯„ä»· C å˜æˆæ¯ä¸ªå›ç­”çš„å¾—åˆ† :math:`\{S_i\}`
    - é»˜è®¤æ‰“åˆ†æ˜¯ç¦»æ•£æ•´æ•° 1-10 åˆ†ã€‚



2.2 Boosting Reward Quality with Principles
-------------------------------------------

âœ… ä¸ºä»€ä¹ˆè¦ç”¨â€œåŸåˆ™â€ï¼Ÿ
    - åœ¨ç‰¹å®šé¢†åŸŸï¼ˆæ¯”å¦‚æ•°å­¦ã€ç¼–ç¨‹ï¼‰ï¼Œå¯ä»¥ç”¨è§„åˆ™è‡ªåŠ¨åˆ¤æ–­å¯¹é”™ï¼›
    - ä½†åœ¨é€šç”¨é¢†åŸŸï¼ˆä¾‹å¦‚å¯¹è¯è´¨é‡ã€è§‚ç‚¹åˆç†æ€§ç­‰ï¼‰ï¼Œæ²¡é‚£ä¹ˆæ¸…æ™°çš„æ ‡å‡†ï¼›
    - æ‰€ä»¥å¯ä»¥ç”¨â€œåŸåˆ™ï¼ˆprinciplesï¼‰â€æ¥è¾…åŠ©æ‰“åˆ†ï¼ŒåŸåˆ™å¯ä»¥æ˜¯ï¼šé€»è¾‘æ¸…æ™°æ€§ã€ç¤¼è²Œã€äº‹å®å‡†ç¡®æ€§ã€ä»»åŠ¡å®Œæˆåº¦ç­‰ï¼›
    - è¿™ç±»ä¼¼äº Constitutional AI çš„æ€æƒ³ï¼Œå³ç”¨è§„åˆ™æˆ–é“å¾·æ¡†æ¶è®­ç»ƒæ¨¡å‹ã€‚

âœ… å¥—ç”¨åŸåˆ™åçš„å…¬å¼ï¼š
    - æ„æ€æ˜¯ï¼šç°åœ¨æ¨¡å‹çš„ reward ä¸ä»…å–å†³äº query å’Œå›ç­”ï¼Œè¿˜åŒ…æ‹¬åŸåˆ™é›† :math:`\{p_j\}`

.. math::

    \mathcal{R} = C \sim r_\theta(x, \{y_i\}, \{p_j\})



3. Self-Principled Critique Tuning (SPCT)
=========================================

ğŸŒŸæ€»è§ˆï¼šä»€ä¹ˆæ˜¯ SPCTï¼Ÿ
    - SPCT æ˜¯ä¸€ç§æ–°çš„è®­ç»ƒæ–¹æ³•ï¼Œç›®æ ‡æ˜¯è®© ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ èƒ½å¤Ÿï¼š
        1. è‡ªå·±ç”Ÿæˆè¯„åˆ¤åŸåˆ™ï¼ˆprinciplesï¼‰
        2. æ ¹æ®è¿™äº›åŸåˆ™ç”Ÿæˆæ‰¹åˆ¤ï¼ˆcritiquesï¼‰
        3. ä»æ‰¹åˆ¤ä¸­æå–å¥–åŠ±åˆ†æ•°ï¼ˆrewardï¼‰
        4. åœ¨æ¨ç†æ—¶ï¼ˆinference timeï¼‰æ›´å¥½æ‰©å±•ï¼Œæ›´é«˜è´¨é‡
    - æ–¹æ³•åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š
        - Rejective Fine-Tuningï¼ˆæ‹’ç»å¼å¾®è°ƒï¼‰ï¼šç”¨äºâ€œå†·å¯åŠ¨â€ï¼Œè®©æ¨¡å‹å­¦ä¼šç”Ÿæˆç»“æ„æ­£ç¡®ã€åŸºæœ¬åˆç†çš„åŸåˆ™ä¸æ‰¹åˆ¤ã€‚
        - Rule-based RLï¼ˆåŸºäºè§„åˆ™çš„å¼ºåŒ–å­¦ä¹ ï¼‰ï¼šè¿›ä¸€æ­¥ä¼˜åŒ–ç”Ÿæˆè´¨é‡ã€‚


3.1 Unpinning Principles from Understanding to Generation
---------------------------------------------------------

* è¿‡å»çš„åšæ³•ï¼šäººå·¥å®šä¹‰å¥½â€œåŸåˆ™â€ï¼Œæ¨¡å‹æ ¹æ®è¿™äº›åŸåˆ™æ¥æ‰“åˆ†ã€‚
* SPCT çš„åšæ³•ï¼šæ¨¡å‹è‡ªå·±ç”ŸæˆåŸåˆ™ï¼Œå†ç”Ÿæˆæ‰¹åˆ¤ â†’ æå–å‡º rewardã€‚

* å…·ä½“å…¬å¼ï¼š
    - åŸåˆ™: :math:`{p_i} ~ p_Î¸(x, {y_i})`
    - å¥–åŠ±: :math:`ğ“¡ = C ~ r_Î¸(x, {y_i}, {p_i})`
    - è¾“å…¥æ˜¯ query `x` å’Œè‹¥å¹²å“åº” `y_i`
    - æ¨¡å‹å…ˆç”Ÿæˆä¸€ç»„åŸåˆ™ `p_i`
    - å†æ ¹æ®è¿™äº›åŸåˆ™ç”Ÿæˆæ‰¹åˆ¤ `C`
    - ä»æ‰¹åˆ¤ä¸­æå–å¥–åŠ± `S_i`

ğŸ”‘ æ„ä¹‰ï¼šè®©æ¨¡å‹æ ¹æ®å…·ä½“åœºæ™¯ã€é—®é¢˜å’Œå›ç­”ï¼Œè‡ªé€‚åº”åœ°ç”Ÿæˆä¸åŒè¯„åˆ¤æ ‡å‡†ã€‚æœ€ç»ˆèƒ½ç”Ÿæˆæ›´åŠ ç»†ç²’åº¦ã€åˆç†çš„ rewardï¼Œæœ‰åˆ©äºå¤§è§„æ¨¡æ¨ç†ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/2oqszA.png

    Figure 3: Illustration of SPCT, including rejective fine-tuning, rule-based RL, and corresponding scalable behaviors during inference. The inference-time scaling is achieved via naive voting or meta RM guided voting with principles generated at scale, resulting in finer-grained outcome rewards within a expanded value space.



3.2 Rule-Based Reinforcement Learning
-------------------------------------

* To optimize principle and critique generation in GRMs simultaneously, we propose SPCT, which integrates rejective fine-tuning and rule-based RL.
* Rejective Fine-Tuningï¼ˆå†·å¯åŠ¨ï¼‰
    - è¿™ä¸ªé˜¶æ®µçš„ç›®çš„æ˜¯ï¼š
        - è®© GRM ç†Ÿæ‚‰å¤šç§è¾“å…¥æ ¼å¼ï¼ˆå•ä¸ªå“åº”ã€æˆå¯¹ã€å¤šå“åº”ï¼‰
        - ç”Ÿæˆç»“æ„æ­£ç¡®çš„åŸåˆ™å’Œæ‰¹åˆ¤
        - æä¾›ä¸€ä¸ªåŸºç¡€æ¨¡å‹ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µå¼ºåŒ–å­¦ä¹ 
    - åšæ³•ï¼š
        - ä½¿ç”¨è®­ç»ƒå¥½çš„ GRM å¯¹åŒä¸€ä¸ª query + response ç»„é‡‡æ ·å¤šæ¬¡ï¼ˆ`ğ‘Ì„_{RFT}` æ¬¡ï¼‰
        - ä½¿ç”¨ç»Ÿä¸€çš„ rejection ç­–ç•¥ï¼š
            - è‹¥ reward è·Ÿ ground truth ä¸ä¸€è‡´ â†’ æ‹’ç»
            - è‹¥æ‰€æœ‰é‡‡æ ·éƒ½ä¸€è‡´æ­£ç¡® â†’ å¤ªç®€å• â†’ ä¹Ÿæ‹’ç»
    - âš ï¸æ³¨æ„ç‚¹ï¼š
        - å¦‚æœå¤šä¸ªå“åº”ï¼Œè¦æ±‚æ¨¡å‹èƒ½æŒ‘å‡ºæœ€å¥½é‚£ä¸ªï¼Œå¹¶ç»™å‡ºæœ€é«˜åˆ†ã€‚
        - æœ‰äº›éš¾ä¾‹æ¨¡å‹é‡‡æ ·å¤šæ¬¡ä¹Ÿä¸å¯¹ï¼Œè¿™æ—¶å€™å¯ä»¥ç»™å‡ºæç¤ºï¼ˆhintï¼‰ï¼šå‘Šè¯‰å®ƒ ground truth ä¸­å“ªä¸ªæ˜¯æœ€ä¼˜ï¼Œå†è®©å®ƒå°è¯•ã€‚
        - ä½†è¿™å¯èƒ½å¯¼è‡´ å·æ‡’çš„æ‰¹åˆ¤ï¼Œç‰¹åˆ«æ˜¯å¯¹å¤æ‚ä»»åŠ¡ï¼ˆå¦‚æ¨ç†ï¼‰ï¼Œæ‰€ä»¥è¿˜è¦ç»§ç»­ç”¨ RL å¾®è°ƒã€‚

* Rule-Based Reinforcement Learningï¼ˆå¼ºåŒ–å­¦ä¹ é˜¶æ®µï¼‰
    - è¿™ä¸ªé˜¶æ®µçš„ç›®æ ‡ï¼š
        - ç”¨ è§„åˆ™å®šä¹‰çš„ reward signal æ¥ä¼˜åŒ–æ¨¡å‹
        - å¼ºåŒ–åŸåˆ™å’Œæ‰¹åˆ¤çš„ç”Ÿæˆè´¨é‡
    - æ–¹æ³•åŸºäº GRPOï¼ˆGeneralized Reward Policy Optimizationï¼‰ï¼š
        - ä¸å†ç”¨â€œæ ¼å¼å¥–åŠ±â€å»å¥–åŠ±æ¨¡å‹å†™å¾—å¥½ä¸å¥½çœ‹
        - ç”¨ è§„åˆ™åˆ¤æ–­é¢„æµ‹çš„ reward æ˜¯å¦åˆç†
        - ç”¨ KL æƒ©ç½šçº¦æŸæ¨¡å‹ä¸è¦åç¦»å¤ªè¿œ


å°ç»“: SPCT æœ‰å•¥äº®ç‚¹
---------------------

+----------+--------------+----------------------+
| èƒ½åŠ›     | ä¼ ç»Ÿ GRM     | SPCT åŠ å¼ºç‚¹          |
+==========+==============+======================+
| åŸåˆ™ç”Ÿæˆ | äººå·¥è®¾å®š     | æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆ         |
+----------+--------------+----------------------+
| çµæ´»æ€§   | è¾“å…¥å›ºå®šæ ¼å¼ | ä»»æ„æ•°é‡å“åº”ç»Ÿä¸€æ ¼å¼ |
+----------+--------------+----------------------+
| æ¨ç†æ‰©å±• | é›£ä»¥æ‰©å±•     | å¯é€šè¿‡é‡‡æ ·+æŠ•ç¥¨æ‰©å±•  |
+----------+--------------+----------------------+
| æ‰¹åˆ¤è´¨é‡ | ä¸ç¨³å®š       | åŸåˆ™å¼•å¯¼+RLä¼˜åŒ–æ›´å¥½  |
+----------+--------------+----------------------+
| å¥–åŠ±ç²’åº¦ | ç²—ç•¥         | æ›´ç»†è‡´ã€é€‚åº”æ€§å¼º     |
+----------+--------------+----------------------+

* SPCT çš„ç›®æ ‡æ˜¯æ„å»ºä¸€ä¸ªçœŸæ­£â€œé€šæ‰å‹çš„å¥–åŠ±æ¨¡å‹â€ï¼Œèƒ½åœ¨å¤šç§ä»»åŠ¡ã€å¤šç§æ ¼å¼ä¸‹è¾“å‡ºé«˜è´¨é‡ rewardï¼Œå¹¶ä¿æŒé«˜å¯æ‰©å±•æ€§ã€‚


4. Inference-Time Scaling with SPCT
===================================

æ ¸å¿ƒèƒŒæ™¯
----------

* ä½œè€…æå‡ºäº†ä¸€ç§å« SPCTï¼ˆSelf-Principled Critique Tuningï¼‰ çš„æ–¹æ³•ï¼Œå‰é¢ç« èŠ‚ä¸»è¦è®²çš„æ˜¯è®­ç»ƒï¼ˆtrainingï¼‰è¿‡ç¨‹ä¸­å¦‚ä½•è®©æ¨¡å‹ç”Ÿæˆæ›´å¥½ã€æ›´è‡ªé€‚åº”çš„åŸåˆ™ï¼ˆprinciplesï¼‰å’Œè¯„ä»·ï¼ˆcritiquesï¼‰ï¼Œä»¥å¸®åŠ©å¥–åŠ±æ¨¡å‹ï¼ˆReward Model, GRMï¼‰æ›´å‡†ç¡®åœ°è¯„ä¼°ç­”æ¡ˆçš„è´¨é‡ã€‚
* è€Œç¬¬ 4 ç« åˆ™è®²çš„æ˜¯ï¼š > åœ¨æ¨ç†ï¼ˆinferenceï¼‰é˜¶æ®µï¼Œæ€ä¹ˆè¿›ä¸€æ­¥æå‡è¿™ä¸ªæ¨¡å‹çš„æ•ˆæœï¼Œå°¤å…¶æ˜¯é€šè¿‡ å¤šè½®é‡‡æ · + æŠ•ç¥¨ æ¥å®ç°â€œç®—åŠ›æ¢æ•ˆæœâ€çš„ç­–ç•¥ã€‚

4.1 Voting with Generated Rewards
---------------------------------

âœ… åšæ³•ï¼š
    - å¯¹äºæ¯ä¸€ä¸ªè¾“å…¥ï¼ˆé—®é¢˜å’Œå›ç­”ä»¬ï¼‰ï¼Œä½ å¯ä»¥ é‡‡æ · $k$ æ¬¡ä¸åŒçš„åŸåˆ™å’Œè¯„ä»·ã€‚
    - æ¯æ¬¡é‡‡æ ·ä¼šç”Ÿæˆä¸€å¥—åŸåˆ™ $\{p_{i,j}\}$ï¼Œç”¨äºè®¡ç®—å¯¹åº”çš„å¥–åŠ± $S_{i,j}$ã€‚
    - æœ€ç»ˆï¼ŒæŠŠæ¯ä¸ªå›ç­” $y_i$ çš„å¤šä¸ªå¥–åŠ± åŠ æ€»èµ·æ¥å½¢æˆæœ€ç»ˆå¾—åˆ† $S_i^* = \sum_j S_{i,j}$ï¼Œå¾—å‡ºå“ªä¸ªå›ç­”æœ€å¥½ã€‚

ğŸ¤” è¿™æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ
    - è™½ç„¶æ¯æ¬¡è¯„åˆ†æ˜¯ 1-10 ä¹‹é—´çš„æ•´æ•°ï¼Œä½†å¤šè½®æŠ•ç¥¨åï¼Œç›¸å½“äºæŠŠè¯„åˆ†ç©ºé—´ æ‰©å¤§äº† $k$ å€ï¼Œæ¯”å¦‚ä» 1-10 æ‰©å±•åˆ° 1-320ã€‚
    - å¤šç”Ÿæˆä¸€äº›â€œåŸåˆ™â€ï¼Œç›¸å½“äºå¤šè§’åº¦å®¡é¢˜ï¼Œâ€œæ¨¡æ‹Ÿæ›´å¤šè¯„å®¡å‘˜è§†è§’â€ï¼Œæ‰€ä»¥æ›´å‡†ç¡®ã€‚
    - æ¨¡å‹æ¨ç†æ—¶ä¼šæ‰“ä¹±ç­”æ¡ˆé¡ºåºï¼Œä»¥é¿å…ä½ç½®åå·®ã€æå‡å¤šæ ·æ€§ã€‚

4.2 Meta Reward Modeling Guided Voting
--------------------------------------

â“ä¸ºä»€ä¹ˆè¿˜éœ€è¦â€œæŒ‡å¯¼æŠ•ç¥¨â€ï¼Ÿ
    - å› ä¸ºæœ‰äº›é‡‡æ ·å‡ºæ¥çš„åŸåˆ™/è¯„ä»·è´¨é‡ä¸é«˜ï¼Œç”šè‡³æ˜¯â€œåºŸè¯â€æˆ–â€œèƒ¡è¯´å…«é“â€ï¼Œå¦‚æœæŠŠè¿™äº›ä¹ŸåŠ è¿›å»æŠ•ç¥¨å°±ä¼šâ€œæ‹‰ä½æ™ºå•†â€ã€‚
    - æ‰€ä»¥ï¼Œå¼•å…¥ä¸€ä¸ª Meta RMï¼ˆå…ƒå¥–åŠ±æ¨¡å‹ï¼‰ æ¥è¿‡æ»¤è¿™äº›é‡‡æ ·ã€‚

âœ… Meta RM æ€ä¹ˆåšï¼Ÿ
    - æ˜¯ä¸€ä¸ªç®€å•çš„åˆ†ç±»å™¨ï¼Œåˆ¤æ–­æŸæ¡é‡‡æ ·å‡ºæ¥çš„åŸåˆ™+è¯„ä»·æ˜¯ä¸æ˜¯é è°±ï¼ˆå¯¹é”™ç”¨äºŒå…ƒæ ‡ç­¾ï¼‰ã€‚
    - ç”¨ä¹‹å‰ rejective fine-tuning é˜¶æ®µçš„é hinted é‡‡æ ·æ•°æ®è®­ç»ƒï¼ˆåŒ…å«å¥½åéƒ½æœ‰ï¼Œå¢å¼ºé²æ£’æ€§ï¼‰ã€‚

ğŸ” æ¨ç†é˜¶æ®µæ€ä¹ˆç”¨ï¼Ÿ
    1. å¯¹äº $k$ æ¡é‡‡æ ·å‡ºæ¥çš„å¥–åŠ±ï¼ŒMeta RM å¯¹æ¯ä¸€æ¡æ‰“åˆ†ã€‚
    2. åªå–å¾—åˆ†æœ€é«˜çš„å‰ :math:`k_{\text{meta}} \leq k` æ¡æŠ•ç¥¨ï¼Œè¿‡æ»¤æ‰ä¸é è°±çš„æ ·æœ¬ï¼Œæå‡æœ€ç»ˆç»“æœã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/DIEF41.png

    Table 2: Overall results of different methods and models on RM benchmarks. Underlined numbers indicate the best performance, bold numbers indicate the best performance among baseline and our methods, and italicized font denotes scalar or semi-scalar RMs. For meta RM guided voting (MetaRM), :math:`k_{meta} = 1/2 k`


æ€»ç»“ç†è§£
----------

* æ¨ç†æ—¶ï¼ŒDeepSeek-GRM ä¼šï¼š
    1. å¤šæ¬¡ç”Ÿæˆä¸åŒè§†è§’çš„â€œåŸåˆ™+è¯„ä»·â€ï¼Œå°±åƒå¤šä¸ªè¯„å§”ç»™å‡ºæ„è§ï¼›
    2. æŠŠè¿™äº›æ„è§ç»¼åˆæŠ•ç¥¨ å¾—å‡ºæ›´ç»†è‡´çš„æ‰“åˆ†ï¼›
    3. å†ç”¨ Meta å¥–åŠ±æ¨¡å‹ç­›é€‰æ‰ä¸é è°±çš„æ„è§ï¼Œè¿›ä¸€æ­¥æå‡æŠ•ç¥¨è´¨é‡ã€‚

* è¿™å°±å®ç°äº†â€œæ¨ç†é˜¶æ®µçš„å¯æ‰©å±•æ€§â€ï¼šç”¨æ›´å¤šè®¡ç®—ï¼ˆæ¯”å¦‚ $k=32$ æ¬¡é‡‡æ ·ï¼‰æ¢å–æ›´å¥½çš„è¯„ä¼°è¡¨ç°ã€‚



5. Results on Reward Modeling Benchmarks
========================================


5.1 Experiment Settings
-----------------------

ğŸ“Š Benchmark å’Œè¯„ä¼°æŒ‡æ ‡ï¼š
    - åœ¨å¤šä¸ªé¢†åŸŸçš„ Reward Modelingï¼ˆRMï¼‰ åŸºå‡†æµ‹è¯•ä¸­å¯¹æ¨¡å‹è¡¨ç°è¿›è¡Œè¯„ä¼°
        - Reward Benchï¼ˆ2024ï¼‰
        - PPEï¼ˆ2025ï¼‰
        - RMBï¼ˆ2025ï¼‰
        - ReaLMistakeï¼ˆ2024ï¼‰
    - è¯„ä¼°æŒ‡æ ‡åŒ…æ‹¬ï¼š
        - Accuracyï¼šåœ¨ Reward Benchã€PPE å’Œ RMB ä¸­ç”¨äºè¡¡é‡æ¨¡å‹æ˜¯å¦èƒ½ä»å¤šä¸ªå€™é€‰å›å¤ä¸­é€‰å‡ºæœ€ä¼˜å›å¤ã€‚
        - ROC-AUCï¼šç”¨äº ReaLMistake æ•°æ®é›†ï¼Œè¯„ä¼°æ¨¡å‹åœ¨åˆ¤æ–­â€œé”™è¯¯â€è½¨è¿¹ä¸Šçš„èƒ½åŠ›ã€‚
    - ä¸ºé¿å…è¯„åˆ†ç›¸åŒï¼ˆtieï¼‰çš„é—®é¢˜ï¼Œä»–ä»¬ä¼šå…ˆæ‰“ä¹±å€™é€‰é¡ºåºï¼Œç„¶åç”¨ `argmax_i(S_i)` æ¥é€‰å‡ºå¾—åˆ†æœ€é«˜çš„å›å¤ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/pC3wtt.png

    Table 3: Inference-time scalability results of different methods on RM benchmarks. Settings are the same as Table 2.

âš™ï¸ æ–¹æ³•å®ç°ï¼ˆMethod Implementationï¼‰ï¼š
    - å¤ç°äº†ä»¥ä¸‹åŸºçº¿æ–¹æ³•ï¼ˆå…¨éƒ¨åŸºäº Gemma-2-27Bï¼‰ï¼š
        - LLM-as-a-Judge
        - DeepSeek-BTRM-27B
        - CLoud-Gemma2-27B
        - DeepSeek-PairRM-27B
    - è‡ªç ”çš„æ–¹æ³•æœ‰ï¼š
        - DeepSeek-GRM-27B-RFTï¼ˆç²¾è°ƒç‰ˆï¼‰
        - DeepSeek-GRMï¼ˆå¤šç§æ¨¡å‹è§„æ¨¡ç‰ˆæœ¬ï¼‰ï¼š
            - DeepSeek-V2-Lite (16B MoE)
            - Gemma-2-27B
            - DeepSeek-V2.5 (236B MoE)
            - DeepSeek-V3 (671B MoE)
    - è¿˜æœ‰ä¸€ä¸ª Meta RMï¼ˆå…ƒå¥–åŠ±æ¨¡å‹ï¼‰ï¼Œä¹Ÿæ˜¯ç”¨ Gemma-2-27B è®­ç»ƒçš„ã€‚

.. figure:: https://img.zhaoweiguo.com/uPic/2025/04/XIbG7p.png

    Table 4: Ablation studies for different components of the proposed SPCT. Bold numbers indicate the best performance.



5.2 Results and Analysis
------------------------

ğŸ“Š Reward Modeling æ€§èƒ½æ€»ç»“ï¼š
    - DeepSeek-GRM-27B è¡¨ç°æ•´ä½“ä¼˜äºåŸºçº¿æ–¹æ³•ï¼Œç”šè‡³æ¥è¿‘ GPT-4o å’Œ Nemotronã€‚
    - æ ‡é‡æ¨¡å‹ï¼ˆscalar RMsï¼‰æ¯”å¦‚ DeepSeek-BTRM æ›´æ“…é•¿å¤„ç†â€œå¯éªŒè¯ä»»åŠ¡â€ï¼ˆå¦‚ PPEï¼‰ï¼Œä½†å¯¹ç”Ÿæˆç±»ä»»åŠ¡æ³›åŒ–æ€§å·®ã€‚
    - è€Œ SPCT è®­ç»ƒçš„ GRM æ¨¡å‹æ›´å°‘åè§ï¼Œæ›´é€šç”¨ã€‚
    - LLM-as-a-Judge è¡¨ç°è¾ƒå·®ï¼Œå¯èƒ½å› ä¸ºç¼ºä¹â€œåŸåˆ™æŒ‡å¼•ï¼ˆprinciple guidanceï¼‰â€ã€‚

ğŸ’§ æ¨ç†æ—¶æ‰©å±•æ€§ï¼ˆInference-Time Scalabilityï¼‰ï¼š
    - ç”¨æ›´å¤šé‡‡æ ·æ•°ï¼ˆä¾‹å¦‚ Voting@8 æˆ– Voting@32ï¼‰ï¼ŒDeepSeek-GRM çš„æ•ˆæœå¤§å¹…æå‡ã€‚
    - Meta RM å¸®åŠ©è¿‡æ»¤ä½è´¨é‡çš„é‡‡æ ·ç»“æœï¼Œè®© Voting æ›´æœ‰æ•ˆã€‚
    - LLM-as-a-Judge åŠ ä¸Š Token Probability æŠ•ç¥¨æ–¹å¼ä¹Ÿæœ‰æå‡ã€‚
    - CLoud-Gemma2 çš„æå‡æœ‰é™ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ ‡é‡ reward ç¼ºä¹å¤šæ ·æ€§ã€‚

* æ€»ä½“ä¸Šï¼ŒSPCT æå‡äº† GRM çš„æ¨ç†æ‰©å±•èƒ½åŠ›ï¼ŒMetaRM åˆè¿›ä¸€æ­¥å¼ºåŒ–äº†å®ƒçš„æ•ˆæœã€‚

ğŸ§ª æ¶ˆèå®éªŒè¡¥å……å‘ç°ï¼š
    - General Instruction Data å¯¹ GRM çš„æ•ˆæœå¾ˆå…³é”®ã€‚
    - Principle Generation æ˜¯æ€§èƒ½æå‡çš„å…³é”®ç»„ä»¶ï¼Œæ— è®ºæ˜¯è´ªå©ªè§£ç è¿˜æ˜¯æ¨ç†æŠ•ç¥¨ã€‚
    - Voting ä¸­å¼•å…¥ MetaRM + k å€¼é€‰æ‹©éå¸¸æœ‰æ•ˆã€‚

.. note:: é€šè¿‡æ¨ç†ä¼˜åŒ–ï¼ˆè€Œéå¢åŠ æ¨¡å‹ä½“é‡ï¼‰ä¹Ÿå¯ä»¥è¾¾åˆ°éå¸¸å¼ºçš„æ•ˆæœï¼Œæˆæœ¬æ›´ä½ã€‚åŒæ—¶æµ‹è¯•å‘ç°ï¼ŒDeepSeek-R1 å³ä¾¿å…·å¤‡é•¿é“¾å¼æ€ç»´èƒ½åŠ›ï¼ˆchain-of-thoughtï¼‰ï¼Œä½†åœ¨ Reward Modeling ä¸Šå¹¶æ²¡æœ‰æ˜¾è‘—æå‡ã€‚


æ€»ç»“
----

1. DeepSeek-GRM-27B + SPCT æ¡†æ¶ + MetaRM æ˜¯å½“å‰åœ¨å¤šä¸ª Reward Modeling åŸºå‡†ä»»åŠ¡ä¸Šè¡¨ç°æœ€å¼ºçš„æ–¹æ¡ˆä¹‹ä¸€ã€‚
2. å®ƒé€šè¿‡åˆç†é‡‡æ ·ã€æœ‰æ•ˆæŠ•ç¥¨ç­–ç•¥å’Œé€šç”¨æŒ‡ä»¤æ•°æ®è®­ç»ƒï¼Œæå‡äº†æ¨¡å‹çš„é€šç”¨æ€§ä¸æ¨ç†æ‰©å±•æ€§ã€‚
3. ç›¸è¾ƒäºæ‰©å¤§æ¨¡å‹ä½“é‡ï¼Œä¼˜åŒ–æ¨ç†æ–¹å¼çš„æ€§ä»·æ¯”æ›´é«˜ã€‚



6. Related Work
===============

Generative Reward Models, GRMs
------------------------------

* ä¼ ç»Ÿå¥–åŠ±æ¨¡å‹ï¼ˆScalar RMsï¼‰çš„é—®é¢˜ï¼š  
    - ä¼ ç»Ÿçš„å¥–åŠ±æ¨¡å‹åªè¾“å‡ºä¸€ä¸ªâ€œæ ‡é‡åˆ†æ•°â€ï¼ˆæ¯”å¦‚æ‰“åˆ†ä¸º0~1ä¹‹é—´çš„ä¸€ä¸ªå€¼ï¼‰ï¼Œä»£è¡¨æŸä¸ªå›å¤çš„å¥½åï¼Œæ¯”å¦‚ Ouyang et al., 2022 çš„æ–¹æ³•ã€‚
    - ä½†è¿™ç§æ–¹å¼è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œä¸è¶³ä»¥å…¨é¢è¯„ä¼°å¤æ‚çš„è¯­è¨€è¾“å‡ºã€‚
* GRMs çš„æ–°æ€è·¯ï¼š  
    - GRMsï¼ˆç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼‰æŠŠâ€œå¥–åŠ±â€è¡¨ç¤ºæˆæ–‡æœ¬å½¢å¼çš„åé¦ˆæˆ–è¯„åˆ†ï¼Œå¯ä»¥æä¾›æ›´ç»†è‡´ã€è¯­ä¹‰æ›´ä¸°å¯Œçš„å¥–åŠ±ä¿¡å·ã€‚  
    - ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹ä¸åªæ˜¯â€œç»™åˆ†â€ï¼Œè¿˜èƒ½è§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”æ›´å¥½ã€‚
* GRMs çš„èƒ½åŠ›æå‡ï¼š
    - æ—¢èƒ½åˆ¤æ–­ä¸€ä¸ªå›ç­”ï¼ˆå•å“åº”ï¼‰ï¼Œä¹Ÿèƒ½æ¯”è¾ƒå¤šä¸ªå›ç­”ï¼ˆå¤šå“åº”ï¼‰ã€‚
    - LLM-as-a-Judge æ˜¯ä¸€ç§å…¸å‹æ–¹æ³•ï¼Œæ”¯æŒæœ‰å‚è€ƒï¼ˆreference-basedï¼‰å’Œæ— å‚è€ƒï¼ˆreference-freeï¼‰çš„è¯„ä¼°ã€‚
    - è¿˜æœ‰ç ”ç©¶å°è¯•ç»“åˆï¼š
        - ç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰æ–¹æ³•è®­ç»ƒ GRMsï¼Œä¾‹å¦‚ DPOï¼ˆDirect Preference Optimizationï¼‰ï¼›
        - å·¥å…·å’Œå¤–éƒ¨çŸ¥è¯†æ¥å¢å¼º GRM çš„åˆ¤æ–­ï¼›
        - ä½œä¸ºæ¥å£è¿æ¥å¤–éƒ¨ç¯å¢ƒï¼ŒåŠ¨æ€è°ƒæ•´å¥–åŠ±ã€‚

* æŒ‘æˆ˜ï¼š å°½ç®¡è¿™äº›æ–¹æ³•åœ¨æ•ˆç‡ä¸Šæœ‰æŒ‘æˆ˜ï¼ˆå¦‚æˆæœ¬é«˜ã€æ¨ç†æ…¢ï¼‰ï¼Œä½†å®ƒä»¬å±•ç¤ºäº†æ„å»ºâ€œé€šç”¨å¥–åŠ±ç³»ç»Ÿâ€ï¼ˆGeneralist Reward Systemï¼‰çš„æ½œåŠ›ã€‚

Inference-Time Scaling for LLMs
-------------------------------

* è¿™éƒ¨åˆ†è®¨è®ºçš„æ˜¯ï¼šåœ¨æ¨ç†é˜¶æ®µå¢å¼ºæ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸æ˜¯é€šè¿‡æ›´å¤§æ¨¡å‹æˆ–æ›´å¤šè®­ç»ƒã€‚
* ç ”ç©¶æ–¹å‘åŒ…æ‹¬ï¼š
    - å¤šæ ·é‡‡æ ·ä¸å¥–åŠ±æ¨¡å‹å¼•å¯¼çš„èšåˆï¼ˆsampling + RM-guided aggregationï¼‰ï¼Œç”¨å¤šä¸ªå›ç­”æŠ•ç¥¨/è¯„åˆ†é€‰æ‹©æ›´ä¼˜è¾“å‡ºï¼›
    - é•¿é“¾æ€ç»´ï¼ˆChain-of-Thought, CoTï¼‰æ¨ç†ç­–ç•¥ï¼šé¼“åŠ±æ¨¡å‹åœ¨æ¨ç†æ—¶ç”Ÿæˆè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹ï¼Œæé«˜é€»è¾‘å’Œå¤æ‚ä»»åŠ¡çš„è¡¨ç°ï¼›
    - ä½¿ç”¨ å¯æ‰©å±•çš„å¥–åŠ±æˆ–éªŒè¯å™¨ï¼ˆscalable rewards/verifiersï¼‰ æ¥æå‡å¦‚ç¼–ç¨‹ã€æ¨ç†ç­‰ç‰¹å®šé¢†åŸŸä¸­çš„æ¨¡å‹æ•ˆæœã€‚

* ä½œè€…å¼ºè°ƒï¼Œä»–ä»¬æå‡ºçš„ inference-time scalable generalist RMsï¼ˆæ¨ç†æ—¶åˆ»å¯æ‰©å±•çš„é€šç”¨å¥–åŠ±æ¨¡å‹ï¼‰ ä¸ä»…æå‡äº†è¯„ä¼°/å¥–åŠ±æ¨¡å‹æœ¬èº«çš„èƒ½åŠ›ï¼Œä¹Ÿæœ‰æœ›é€šè¿‡â€œæ¨ç†æ—¶åˆ»å…±æ‰©å±•ï¼ˆco-scalingï¼‰â€çš„æ–¹å¼æå‡ç­–ç•¥æ¨¡å‹ï¼ˆpolicy modelsï¼‰çš„æ•´ä½“æ€§èƒ½ã€‚

æ€»ç»“
----

* æœ¬æ–‡ç«™åœ¨ä¸¤ä¸ªå‰æ²¿é¢†åŸŸçš„äº¤å‰ç‚¹ä¸Š â€”â€” ç”¨æ›´å¼ºå¤§ã€é€šç”¨ã€å¯æ‰©å±•çš„å¥–åŠ±æ¨¡å‹ï¼Œé…åˆ æ¨ç†æ—¶åˆ»çš„å¢å¼ºç­–ç•¥ï¼Œæ¨åŠ¨ AI ç³»ç»Ÿåœ¨å¤æ‚ä»»åŠ¡ä¸Šè·å¾—æ›´é«˜è´¨é‡çš„åˆ¤æ–­å’Œè¾“å‡ºã€‚


7. Conclusion and Future Work
=============================

* æœ¬ç« æ˜¯æ•´ç¯‡è®ºæ–‡çš„æ€»ç»“å’Œæœªæ¥å±•æœ›éƒ¨åˆ†
* SPCT æ–¹æ³•çš„æ ¸å¿ƒè´¡çŒ®
    - SPCT å…¨ç§°ï¼šSelf-Principled Critique Tuningï¼ˆè‡ªæˆ‘åŸåˆ™æ‰¹è¯„è°ƒä¼˜ï¼‰
    - å®ƒè§£å†³çš„é—®é¢˜ï¼š
        - åœ¨â€œæ¨ç†é˜¶æ®µâ€ï¼ˆinference timeï¼‰å¦‚ä½•è®©é€šç”¨å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰è¡¨ç°æ›´å¼ºã€‚
    - SPCT æ˜¯æ€ä¹ˆåšåˆ°çš„ï¼Ÿ
        - ä½¿ç”¨è§„åˆ™é©±åŠ¨çš„åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆrule-based online RLï¼‰ï¼š
        - æ¨¡å‹å¯ä»¥è‡ªå·±ç”Ÿæˆè¯„ä»·åŸåˆ™ï¼ˆprinciplesï¼‰å’Œæ‰¹è¯„æ„è§ï¼ˆcritiquesï¼‰ï¼Œæ¥åˆ¤æ–­å…¶ä»–æ¨¡å‹çš„è¾“å‡ºè´¨é‡ã€‚
        - ç›¸å½“äºå¥–åŠ±æ¨¡å‹è‡ªå·±å­¦ä¼šâ€œæ€ä¹ˆè¯„ä»·å¥½åâ€ï¼Œè€Œä¸æ˜¯å®Œå…¨é äººå·¥æ ‡æ³¨ã€‚
* å®éªŒæ•ˆæœ
    - æå‡ºçš„ DeepSeek-GRMï¼ˆåŸºäº SPCT çš„ GRMï¼‰ï¼š
    - è¶…è¶Šäº†å¤šä¸ªåŸºçº¿æ–¹æ³•ï¼ˆbaseline methodsï¼‰ã€‚
    - åœ¨å¤šä¸ªå…¬å¼€çš„å¼ºå¤§æ¨¡å‹ï¼ˆpublic RMsï¼‰ä¸­ä¹Ÿè¡¨ç°å¾—å¾ˆå¥½ã€‚
    - åœ¨æ¨ç†é˜¶æ®µè¿›è¡Œæ‰©å±•ï¼ˆinference-time scalingï¼‰åï¼Œè¡¨ç°è¿›ä¸€æ­¥æå‡ã€‚
    - ç‰¹åˆ«æ˜¯åœ¨æœ‰ meta RM æŒ‡å¯¼ä¸‹æ•ˆæœæœ€å¥½ï¼šmeta RM è´Ÿè´£ç­›é€‰è´¨é‡æ›´é«˜çš„æ¨ç†è½¨è¿¹ï¼Œèµ·åˆ°äº†â€œæŠ•ç¥¨ä¸“å®¶â€çš„ä½œç”¨ã€‚

* æœªæ¥æ–¹å‘ï¼ˆFuture Workï¼‰
    ğŸ”¸ a)å°† GRM èå…¥ RL æµæ°´çº¿ï¼š
        - è®© GRM æˆä¸º åœ¨çº¿å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿä¸­çš„é€šç”¨å¥–åŠ±æ¥å£ã€‚
        - ä¸åªæ˜¯â€œç¦»çº¿è¯„ä¼°â€ï¼Œè¿˜èƒ½å®æ—¶åœ°æŒ‡å¯¼è®­ç»ƒã€‚
    ğŸ”¸ b)ä¸ç­–ç•¥æ¨¡å‹ï¼ˆPolicy Modelï¼‰åš inference-time co-scalingï¼š
        - ä¸åªæ˜¯å¥–åŠ±æ¨¡å‹æœ¬èº«å¢å¼ºï¼Œåœ¨æ¨ç†é˜¶æ®µè¿˜å¯ä»¥ä¸ç­–ç•¥æ¨¡å‹ä¸€èµ·æ‰©å±•ï¼ŒååŒæå‡æ€§èƒ½ã€‚
    ğŸ”¸ c)ä½œä¸ºç¨³å¥çš„ç¦»çº¿è¯„ä¼°å™¨ï¼ˆOffline Evaluatorsï¼‰ï¼š
        - GRMs å¯ä»¥ç”¨äºè¯„ä¼°å¤§å‹åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰çš„è¾“å‡ºè´¨é‡ï¼Œå‡å°‘äººåŠ›æ ‡æ³¨ã€‚
* æ€»ç»“ä¸€å¥è¯ï¼š
    > æœ¬æ–‡æå‡ºçš„ SPCT æ–¹æ³•è®©é€šç”¨å¥–åŠ±æ¨¡å‹åœ¨â€œæ¨ç†é˜¶æ®µâ€å˜å¾—æ›´èªæ˜ã€æ›´çµæ´»ï¼Œä¸ä»…æå‡äº†è¯„ä¼°èƒ½åŠ›ï¼Œä¹Ÿä¸ºæœªæ¥å°†å…¶åµŒå…¥ RL ç³»ç»Ÿã€ååŠ©è®­ç»ƒå’Œè¯„ä¼°å¤§æ¨¡å‹æ‰“å¼€äº†æ–°è·¯å¾„ã€‚


A. Additional Related Work
==========================

* ä¸‰ç§ä¸»æµçš„å¥–åŠ±æ¨¡å‹ï¼ˆReward Models, RMsï¼‰æ–¹å‘ï¼Œç”¨äºè®­ç»ƒæˆ–å¯¹é½å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼ŒåŒ…æ‹¬:
    1. å®ªæ³•å¼AI(Constitutional AI)
    2. æ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆScalar Reward Modelsï¼‰  
    3. åŠæ ‡é‡å¥–åŠ±æ¨¡å‹ï¼ˆSemi-Scalar Reward Modelsï¼‰

* Constitutional AI
    - æ ¸å¿ƒç†å¿µï¼š  
        - æ˜¯ä¸€ç§ä¸å†ä¾èµ–äººç±»åé¦ˆçš„æ›¿ä»£æ–¹æ³•ï¼Œé€šè¿‡â€œå®ªæ³•â€è§„åˆ™ï¼ˆconstitutionï¼‰æ¥å¯¹é½ LLM ä¸äººç±»ä»·å€¼ã€‚
    - ä¸»è¦åšæ³•ï¼š
        - ä¸ç”¨äººç±»ç‚¹è¯„ï¼ˆhuman critiquesï¼‰ï¼Œè€Œç”¨ AI ç”Ÿæˆçš„åé¦ˆï¼ˆAI feedbackï¼‰æˆ–è€…åˆ†ç±»å™¨ï¼ˆclassifiersï¼‰æ¥åˆ¤æ–­è¾“å‡ºæ˜¯å¦ç¬¦åˆâ€œå®ªæ³•â€è§„åˆ™ã€‚
        - è¿™äº›â€œå®ªæ³•â€æ˜¯äººå·¥ç¼–å†™çš„è§„åˆ™é›†ï¼ˆä¾‹å¦‚é“å¾·ã€å®‰å…¨ã€ç¤¼è²Œç­‰æ–¹é¢çš„å‡†åˆ™ï¼‰ã€‚
    - ä¼˜ç‚¹ï¼š
        - å¯ä»¥è‡ªåŠ¨ç›‘ç£æ¨¡å‹ï¼Œä¸å†ä¾èµ–å¤§é‡äººç±»åé¦ˆã€‚
        - æ¯”ä¼ ç»Ÿçš„ RLHF æ›´æ˜“æ‰©å±•ï¼ˆscalableï¼‰ã€‚
    - å±€é™ï¼š
        - èŒƒå›´æœ‰é™ï¼ˆlimited scopeï¼‰
        - æœ‰åæ€§ï¼ˆpotential biasï¼‰
        - ç¼ºä¹çµæ´»æ€§ï¼ˆinflexibilityï¼‰
    - è¿™ç¯‡è®ºæ–‡çš„å…³è”ï¼š
        > ä½œè€…ä¹Ÿè¯•å›¾è§£å†³è¿™äº›é—®é¢˜ï¼Œå³è‡ªåŠ¨ç”Ÿæˆæˆ–åŠ¨æ€è°ƒæ•´â€œåŸåˆ™ï¼ˆprinciplesï¼‰â€ï¼Œè¿™å’Œå®ªæ³•å¼AIçš„ç›®æ ‡æ˜¯ä¸€è‡´çš„ã€‚

* Scalar Reward Models
    - æ ¸å¿ƒç†å¿µï¼š
        - æœ€æ—©æœŸçš„å¯¹é½æ–¹æ³•ï¼šç”¨ä¸€ä¸ªç®€å•çš„åˆ†æ•°ï¼ˆæ ‡é‡ï¼Œscalarï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªè¾“å‡ºçš„å¥½åç¨‹åº¦ï¼Œé€šå¸¸æ˜¯äººç±»åé¦ˆçš„å›å½’ç»“æœã€‚
    - ä»£è¡¨æ–¹æ³•ï¼š
        - Stiennon et al., 2020 æ˜¯å¼€åˆ›è€…ï¼ˆOpenAI çš„æœ€æ—©å¥–åŠ±æ¨¡å‹ï¼‰
        - åç»­å¼•å…¥ Bradley-Terry æ¨¡å‹ï¼ˆä¸€ç§ç”¨äºå»ºæ¨¡åå¥½æ’åºçš„ç»Ÿè®¡æ¨¡å‹ï¼‰
        - è¿˜åŒ…æ‹¬å¾ˆå¤šå›å½’æ¨¡å‹ï¼ˆGao et al., 2023ï¼›Wang et al., 2024/2025ï¼‰
    - å»¶ä¼¸ï¼š
        - å‡ºç°äº†â€œè¿‡ç¨‹å¥–åŠ±æ¨¡å‹ï¼ˆProcess RMï¼‰â€ï¼šä¸æ˜¯åªçœ‹æœ€ç»ˆç­”æ¡ˆå¯¹é”™ï¼Œè€Œæ˜¯æ£€æŸ¥ä¸­é—´æ¯ä¸€æ­¥æ¨ç†æ˜¯å¦åˆç†ï¼ˆæ¯”å¦‚æ•°å­¦é¢˜ï¼‰ã€‚
    - ä¼˜ç‚¹ï¼š
        - ç®€å•é«˜æ•ˆï¼Œé€‚åˆå¤§è§„æ¨¡è®­ç»ƒã€‚
    - ç¼ºç‚¹ï¼š
        - è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼šåªèƒ½æä¾›ä¸€ä¸ªæ•°å­—ï¼Œæ²¡æ³•è§£é‡Šä¸ºä»€ä¹ˆï¼›
        - æ³›åŒ–æ€§å·®ï¼šä¸åŒä»»åŠ¡ã€è¾“å…¥ç±»å‹éš¾ä»¥ç»Ÿä¸€ï¼›
        - æ¨ç†æ—¶æ— æ³•ç»†è‡´è°ƒèŠ‚å¥–åŠ±ï¼ˆinference-time refinementï¼‰

* Semi-Scalar Reward Models
    - æ ¸å¿ƒç†å¿µï¼š
        - ä»‹äºæ ‡é‡å’Œç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰ä¹‹é—´ï¼šç”¨æ–‡æœ¬ä¸­é—´ç»“æœä¸°å¯Œæ ‡é‡å¥–åŠ±çš„è¡¨è¾¾åŠ›ã€‚
    - ä¼˜ç‚¹ï¼š
        - ä¿¡æ¯æ›´ä¸°å¯Œï¼Œæ¯”çº¯æ ‡é‡èƒ½è¡¨è¾¾æ›´å¤šç»†èŠ‚ï¼›
        - ä¿ç•™äº†éƒ¨åˆ†æ•ˆç‡ä¼˜åŠ¿ã€‚
    - ç¼ºç‚¹ï¼š
        - æ¨ç†æ—¶æ‰©å±•èƒ½åŠ›å¼±ï¼ˆinference-time scalingï¼‰ï¼š
        - ä¸»è¦è¿˜æ˜¯é  sampling å’Œ votingï¼Œä½†æå‡æœ‰é™ï¼›
        - åœ¨æ•ˆç‡å’Œæ•ˆæœä¸Šæ˜¯æŠ˜ä¸­æ–¹æ¡ˆï¼Œä¸¤å¤´éƒ½æ²¡å®Œå…¨å åˆ°ä¾¿å®œã€‚


B. Limitations and Future Directions
====================================


Limitations
-----------

* æ•ˆç‡é—®é¢˜ï¼ˆGRM vs Scalar RMï¼‰
    - ç°çŠ¶ï¼š
        - ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹ï¼ˆGRMï¼‰å¤©ç„¶æ¯”æ ‡é‡æ¨¡å‹æ…¢å¾—å¤šï¼Œå°¤å…¶æ˜¯åœ¨å¤§æ¨¡å‹ä¸Šï¼Œè¿™ä¼šé™åˆ¶å®ƒä»¬åœ¨å®æ—¶å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤§è§„æ¨¡åº”ç”¨ã€‚
    - ç¼“è§£æ–¹æ³•ï¼š  
        - è®ºæ–‡ä¸­é‡‡ç”¨äº† å¹¶è¡Œé‡‡æ ·ï¼ˆparallel samplingï¼‰ çš„æ–¹å¼æ¥é™ä½æ¨ç†æ—¶é—´ï¼ˆä¾‹å¦‚ç”¨ 8 ä¸ªæ ·æœ¬ï¼Œå»¶è¿Ÿä¸ä¼šæ˜¾è‘—å¢åŠ ï¼‰ã€‚  
        - ä½œè€…è®¤ä¸ºæœªæ¥å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•è¿›ä¸€æ­¥è§£å†³è¿™ä¸€é—®é¢˜ï¼š
            - æé«˜ LLM çš„æ¨ç†æ•ˆç‡  
            - ä¼˜åŒ– RM æ¶æ„  

* åœ¨å¯éªŒè¯ä»»åŠ¡ä¸Šçš„æ€§èƒ½è½å
    - ç°è±¡ï¼š  
        - åœ¨ä¸€äº›â€œå¯éªŒè¯æ€§å¼ºçš„ä»»åŠ¡â€ï¼ˆæ¯”å¦‚æ•°å­¦é¢˜æˆ–éœ€è¦é€»è¾‘éªŒè¯çš„æ¨ç†ä»»åŠ¡ï¼‰ä¸Šï¼ŒDeepSeek-GRM ä»è½åäº scalar RMsã€‚
    - åŸå› åˆ†æï¼š  
        - æ ‡é‡ RM æ›´å®¹æ˜“æ•æ‰â€œéšè—ç‰¹å¾â€ï¼ˆæ¯”å¦‚ç­”æ¡ˆç»“æ„ã€æ ¼å¼ï¼‰ï¼›
        - è€Œ GRM è¦é æ›´å¼ºçš„æ¨ç†èƒ½åŠ›æ¥ç»™å‡ºè¯„ä»·ï¼Œè¿™æ–¹é¢å°šä¸è¶³ã€‚
    - ç¼“è§£æ‰‹æ®µï¼š  
        - ä½¿ç”¨ å‚è€ƒç­”æ¡ˆç”Ÿæˆå¼è¯„åˆ†ï¼ˆreference-based rewardï¼‰ï¼ˆé™„å½• E.1.3ï¼‰ï¼›
        - ä½¿ç”¨ é•¿é“¾æ¨ç†ï¼ˆlong-horizon reasoningï¼‰ï¼ˆé™„å½• D.3ï¼‰ã€‚

* æœªå……åˆ†æ¢ç´¢çš„â€œè¿‡ç¨‹å¥–åŠ±â€æ½œåŠ›
    - ç°çŠ¶ï¼š  
        - DeepSeek-GRM ç›®å‰æ˜¯æŒ‰â€œç‚¹â€æ‰“åˆ†ï¼ˆpointwiseï¼‰ç”¨äºç»“æœè¯„ä»·ï¼ˆoutcome RMï¼‰ã€‚  
        - ä½†å®ƒä¹Ÿæœ‰æ½œåŠ›ç”¨äºè¿‡ç¨‹ç›‘ç£ï¼ˆprocess RMï¼‰ï¼Œä¾‹å¦‚é€æ­¥éªŒè¯æ¨ç†è¿‡ç¨‹ã€‚
    - å½“å‰è¿›å±•ï¼š  
        - è™½ç„¶æ–‡ç« æ²¡è¯¦ç»†ç ”ç©¶è¿™ä¸€ç‚¹ï¼Œä½†å®ƒåœ¨â€œReward Benchâ€ä¸­è¡¨ç°ä¸é”™ï¼ˆå°¤å…¶æ˜¯åœ¨ MATH-prm æ•°æ®ä¸Šï¼‰ï¼Œæš—ç¤ºè¿™ç§å¯èƒ½æ€§ã€‚

Future Directions
-----------------

1. ç»“åˆå·¥å…·ï¼ˆTool-Augmented RMï¼‰
    - å¯å‘ï¼š  
        - å‚è€ƒ Li et al., 2024b çš„å·¥ä½œï¼Œå¯ä»¥å°†å¤–éƒ¨å·¥å…·ï¼ˆå¦‚ä»£ç è§£é‡Šå™¨ã€æœç´¢å¼•æ“ï¼‰ç»“åˆè¿›æ¥ï¼Œç”¨æ¥å¢å¼º DeepSeek-GRMã€‚
    - ä¼˜åŠ¿ï¼š  
        - æ‰§è¡Œéœ€è¦ç²¾ç¡®æ­¥éª¤çš„ä»»åŠ¡ï¼›
        - å¤„ç†å¤æ‚çš„çŸ¥è¯†æ€§é—®é¢˜ï¼›
        - é¿å…å‡ºé”™ï¼Œæ¯”å¦‚æ•°å­—è®¡ç®—ã€æ¨¡å¼åŒ¹é…é”™è¯¯ç­‰ã€‚

2. åŸåˆ™-ç‚¹è¯„é˜¶æ®µè§£è€¦ï¼ˆPrinciple-Critique Decouplingï¼‰
    - ç°çŠ¶ï¼š
        - å½“å‰åŸåˆ™ï¼ˆPrinciplesï¼‰å’Œç‚¹è¯„ï¼ˆCritiquesï¼‰æ˜¯ä¸€èµ·ç”Ÿæˆçš„ã€‚
    - æ”¹è¿›å»ºè®®ï¼š  
        - å¯ä»¥å…ˆä¸ºæ¯ä¸ª query å•ç‹¬ç”ŸæˆåŸåˆ™ï¼ˆæå‰ç”Ÿæˆï¼‰ï¼Œå†åŸºäºè¿™äº›åŸåˆ™å¯¹å“åº”è¿›è¡Œç‚¹è¯„ã€‚  
        - åŸåˆ™ç”Ÿæˆå¯ä»¥è§†ä¸ºä¸€ä¸ªâ€œæ¥å£â€ï¼Œä¾›ç‚¹è¯„æ¨¡å—ä½¿ç”¨ã€‚
    - å¥½å¤„ï¼š  
        - ä¼šæ˜¾è‘—æå‡æ•ˆç‡ï¼Œæ›´æ–¹ä¾¿ç”¨äºå¼ºåŒ–å­¦ä¹ ç®¡é“ã€‚

3. ç”¨äºç¦»çº¿è¯„ä¼°ï¼ˆOffline LLM Evaluationï¼‰
    - æ–¹æ³•ï¼š  
        - æ¯ä¸ªâ€œåŸåˆ™â€å¯ä»¥çœ‹ä½œä¸€ç§â€œè¯„ä»·æ ‡å‡†ï¼ˆcriteriaï¼‰â€ã€‚
    - ç”¨æ³•ï¼š  
        - æ¯”å¦‚ï¼šä¸€ä¸ª LLM åœ¨æŸä¸ªæ ·æœ¬ä¸Šæ¯”å¦ä¸€ä¸ªå¼±ï¼Œå°±å¯ä»¥ä»ç‚¹è¯„ä¸­æŠ½å–å‡ºâ€œå¤±è´¥çš„åŸåˆ™â€ï¼Œä½œä¸ºæ¨¡å‹å¼±ç‚¹çš„è§£é‡Šæ€§ä¿¡å·ã€‚
    - ä»·å€¼ï¼š  
        - å®ç°æ›´é€æ˜ã€æ›´å¯è§£é‡Šçš„ LLM å¯¹æ¯”ä¸è¯„ä¼°ã€‚

4. åˆ©ç”¨é•¿é“¾æ¨ç†ï¼ˆLong-Horizon Reasoningï¼‰
    - å¥½å¤„ï¼š  
        - æœ‰æ½œåŠ›æå‡ DeepSeek-GRM çš„è¯„ä¼°æ·±åº¦å’Œå‡†ç¡®æ€§ã€‚
    - æŒ‘æˆ˜ï¼š
        - ä¼šè¿›ä¸€æ­¥æ‹‰é«˜è®¡ç®—æˆæœ¬ï¼Œå¯¼è‡´æ•ˆç‡é—®é¢˜ï¼Œå› æ­¤æœªæ¥éœ€è¦æ›´å¤šç ”ç©¶æ¥å¹³è¡¡ã€‚

æ€»ç»“
----

ğŸ” Limitations:
    - â— GRMs æ•ˆç‡ä½ï¼Œéš¾ä»¥ç”¨äºå®æ—¶å¼ºåŒ–å­¦ä¹ ï¼›
    - â— åœ¨æ•°å­¦/æ¨ç†ç±»ä»»åŠ¡ä¸­ï¼ŒGRM è½åäº scalar RMï¼›
    - â— DeepSeek-GRM æ½œåŠ›æœªå®Œå…¨å¼€å‘æˆ Process RMã€‚

ğŸš€ Future Directions:
    1. å¼•å…¥å¤–éƒ¨å·¥å…·ï¼ˆä»£ç æ‰§è¡Œå™¨ã€æœç´¢ç­‰ï¼‰â†’ æå‡ç²¾ç¡®æ€§ï¼›
    2. åŸåˆ™ä¸ç‚¹è¯„è§£è€¦ â†’ æé«˜æ•ˆç‡ï¼Œé€‚é… RL ç®¡é“ï¼›
    3. ç”¨äº LLM ç¦»çº¿è¯„ä¼° â†’ æŠ½å–å¤±è´¥åŸå› ä½œä¸ºæ¨¡å‹å¼±ç‚¹è¯„ä»·ï¼›
    4. åˆ©ç”¨é•¿é“¾æ¨ç†èƒ½åŠ› â†’ æå‡å¤æ‚ä»»åŠ¡è¡¨ç°ï¼Œä½†éœ€å¹³è¡¡æ•ˆç‡ã€‚



G. Prompt Templates
===================

DeepSeek-GRM (Default)::

    You are a skilled little expert at scoring responses. 
    You should evaluate given responses based on the given judging criteria.
    Given the context of the conversation (the last round is the Userâ€™s query) and multiple responses from the Assistant, you need to refer to the [General Evaluation Criteria] to score the responses. 
    Based on the general evaluation criteria, state potential other specific criteria to the query, the weights of different criteria, and then provide an overall comprehensive score upon them.
    Each score is an integer between 1 and 10, with a higher score indicating that the response meets the relevant criteria more closely. 
    For example, a score of 1 means the response does not meet the criteria at all, a score of 6 means the response meets only some parts, and a score of 10 means the response perfectly meets the evaluation criteria.
    Before scoring, please analyze step by step. 
    Your scoring needs to be as strict as possible.

    #### Evaluation Criteria ####
    1. Instruction Adherence:
        - Fully Adhered (9-10 points): 
            The response fully complies with all instructions and requirements of the question.
        - Partially Adhered (6-8 points): 
            The response meets most of the instructions but has some omissions or misunderstandings.
        - Basically Adhered (3-5 points): 
            The response meets some instructions, but the main requirements are not fulfilled.
        - Not Adhered (1-2 points): 
            The response does not meet any instructions.
        Example: If the question requires three examples and the response provides only one, it falls under â€œPartially Adhered.â€
    2. Usefulness:
        - Highly Useful (9-10 points): 
            - The response provides comprehensive and accurate information, fully addressing the issue.
        - Useful but Incomplete (6-8 points): 
            - The response provides some useful information, but lacks details or accuracy.
        - Limited Usefulness (3-5 points): 
            - The response offers little useful information, with most content being irrelevant or incorrect.
        - Useless or Incorrect (1-2 points): 
            The response is completely irrelevant or incorrect.
        Example: If there are factual errors in the response but the overall direction is correct, it falls under â€œUseful but Incomplete.â€
    3. Level of Detail:
        - Very Detailed (9-10 points):
            - The response includes ample details covering all aspects of the issue.
        - Detailed but Slightly Lacking (6-8 points): 
            - The response is fairly detailed but misses some important details.
        - Basically Detailed (3-5 points): 
            - The response provides some details but is not thorough enough overall.
        - Not Detailed (1-2 points): 
            - The response is very brief and lacks necessary details.
        Example: If the response provides only a simple conclusion without an explanation, it falls under â€œNot Detailed.â€
    4. Relevance:
        - Highly Relevant (9-10 points):
            - The response is highly relevant to the question, with information closely aligned with the topic.
        - Generally Relevant (6-8 points):
            - The response is generally relevant but includes some unnecessary information.
        - Partially Relevant (3-5 points):
            - The response has a lot of content that deviates from the topic.
        - Not Relevant (1-2 points):
            - The response is completely irrelevant.
        Example: If the response strays from the topic but still provides some relevant information, it falls under â€œPartially Relevant.â€

    #### Conversation Context ####
    {conversation context & query}

    #### Responses to be Scored ####
    [The Begin of Response i]
    {the i-th response}
    [The End of Response i]

    #### Output Format Requirements ####
    Output with three lines Specific Criteria: <Other potential criteria specific to the query and the context, and the weights of each criteria>.
    Analysis: <Compare different responses based on given Criteria>.
    Scores: <the overall comprehensive score of all responses in order, separate by comma in the boxed, e.g., \boxed{x, x} if there exists 2 responeses>.




DeepSeek-GRM (Training on Rating Single Response)::

    You are a skilled little expert at scoring responses. 
    You should evaluate given responses based on the given judging criteria.
    Given the context of the conversation (the last round is the Userâ€™s query) and multiple responses from the Assistant, you need to refer to the [General Evaluation Criteria] to score the responses. 
    Based on the general evaluation criteria, state potential other specific criteria to the query, the weights of different criteria, and then provide an overall comprehensive score upon them. The score is 0 or 1, with 1 indicating that the response is correct.
    Before scoring, please analyze step by step. 
    Your scoring needs to be as strict as possible.

    #### Evaluation Criteria ####
    1. Instruction Adherence:\n - Fully Adhered: The response fully complies with all instructions and requirements of the question.\n - Partially Adhered: The response meets most of the instructions but has some omissions or misunderstandings.\n - Basically Adhered: The response meets some instructions, but the main requirements are not fulfilled.\n - Not Adhered: The response does not meet any instructions.\n Example: If the question requires three examples and the response provides only one, it falls under â€œPartially Adhered.â€
    2. Clarity:\n - Very Clear: The response is fluent, well-structured, and logically clear.\n - Clear but Minor Issues: The response is mostly clear but has some minor language or structural issues.\n - Basically Clear: The response has noticeable language or logic issues but is still understandable.\n - Not Clear: The response is disjointed, illogical, and hard to understand.\n Example: If the response has complex sentence structures and lacks punctuation, it falls under â€œBasically Clearâ€ or â€œNot Clear.â€
    3. Accuracy:\n - Completely Accurate: All information and data are completely accurate.\n - Mostly Accurate: Most information is accurate, with minor errors.\n - Some Errors: There are some noticeable errors affecting comprehension.\n - Mostly Incorrect: There are numerous errors seriously affecting the credibility of the information.\n Example: If a specific data point is incorrectly cited but doesnâ€™t affect the overall conclusion, it falls under â€œMostly Accurate.â€ 

    #### Conversation Context ####
    {conversation context & query}

    #### Responses to be Scored ####
    [The Begin of Response]
    {the response}
    [The End of Response]

    #### Output Format Requirements ####
    Output with three lines
    Specific Criteria: <Other potential criteria specific to the query and the context, and the weights of each criteria>.
    Analysis: <Compare different responses based on given Criteria>.
    Scores: <the overall comprehensive score of the response, e.g., \boxed{x}>


Meta RM::

    Prompt:
    Please score the responses.

    #### Conversation Context ####
    {conversation context & query}

    #### Responses to be Scored ####
    [The Begin of Response i]
    {the i-th response}
    [The End of Response i]

    Response:
    {principle & critique}



LLM-as-a-Judge::

    You are a skilled little expert at scoring responses. 
    You should evaluate given responses based on the given judging criteria.
    Given the context of the conversation (the last round is the Userâ€™s query) and multiple responses from the Assistant, you need to refer to the [General Evaluation Criteria] to score the responses. 
    Based on the general evaluation criteria, state potential other specific criteria to the query, the weights of different criteria, and then select the best response among all candidates.
    Before judging, please analyze step by step. Your judgement needs to be as strict as possible.

    #### Evaluation Criteria ####
    ... // ä¸ä¸Šé¢çš„(DeepSeek-GRM (Default))ç›¸åŒ

    #### Conversation Context ####
    {conversation context & query}

    #### Responses to be Scored ####
    [The Begin of Response]
    {the response}
    [The End of Response]


    #### Output Format Requirements ####
    Output with three lines
    Specific Criteria: <Other potential criteria specific to the query and the context, and the
    weights of each criteria>.
    Analysis: <Compare different responses based on given Criteria>.
    Scores: <the index of the best response based on the judgement, in the format of \boxed{x}>.






