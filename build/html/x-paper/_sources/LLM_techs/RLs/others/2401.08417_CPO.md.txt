# 2401.08417_CPO: Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation

* 首页: <https://arxiv.org/abs/2401.08417>
* PDF: <https://arxiv.org/pdf/2401.08417>
* 引用: 360(2025-11-19)


Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation



以下是论文章节内容的总结：

---

# Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation  
（对比偏好优化：推动大语言模型在机器翻译中的性能边界）

## 摘要（Abstract）  
本研究提出了一种新的训练方法——**对比偏好优化**（Contrastive Preference Optimization, CPO），用于提升大语言模型（LLM）在机器翻译任务中的表现。该方法通过利用**人类偏好数据**，在不依赖强化学习的前提下优化模型，取得了优于传统方法（如监督微调和强化学习）的效果，尤其在低资源语言对上表现突出。

---

## 引言（Introduction）  
作者指出，尽管大语言模型在机器翻译中已取得显著进展，但如何进一步提升其生成质量、流畅性和语义准确性仍是挑战。传统方法如监督微调（SFT）和强化学习（RL）存在局限性，例如RL需要设计奖励函数且训练不稳定。  
为此，本文提出CPO，一种基于人类偏好的对比学习方法，直接优化模型以生成更符合人类偏好的翻译结果。

---

## 相关工作（Related Work）  
- **监督微调**（SFT）：使用平行语料进行训练，但受限于数据质量和覆盖范围。  
- **强化学习**（RL）：通过奖励模型引导生成，但训练过程复杂且易不稳定。  
- **偏好学习**（Preference Learning）：近年来在文本生成中广泛应用，如DPO（Direct Preference Optimization），但多用于对话任务，较少应用于翻译。  
- **对比学习**（Contrastive Learning）：用于表示学习，但在生成任务中应用较少。

> 本节简要回顾了相关方法，为CPO的提出提供背景支持。

---

## 方法（Methodology）  
### Contrastive Preference Optimization (CPO)  
CPO的核心思想是：**通过对比正负样本对，优化模型以生成更符合人类偏好的翻译结果**。

- **输入**：源语言句子 + 两个人类标注的翻译候选（一个更优，一个次优）。
- **目标**：让模型更倾向于生成“被人类偏好的”翻译结果。
- **实现方式**：
  - 构建损失函数，鼓励模型对偏好翻译分配更高的概率。
  - 使用对比学习的思想，将偏好翻译与非偏好翻译进行对比。
- **优势**：
  - 无需显式建模奖励函数。
  - 不依赖强化学习的复杂训练流程。
  - 可直接端到端训练。

> 本节是论文的核心，详细介绍了CPO的设计原理和数学推导。

---

## 实验（Experiments）  
### 数据集与设置  
- 使用WMT数据集，涵盖多个语言对（如英德、英法、英中等）。
- 对比方法包括：SFT、RL、DPO等。
- 评估指标：BLEU、COMET、人工评估（流畅性、准确性、连贯性）。

### 主要结果  
- **自动评估指标**：CPO在BLEU和COMET上均优于SFT和RL，尤其在低资源语言对（如英中）上提升显著。
- **人工评估**：CPO生成的翻译在流畅性和准确性方面更受人类偏好。
- **训练效率**：CPO比RL更稳定，收敛更快。

> 实验部分充分验证了CPO的有效性和优越性。

---

## 消融实验与分析（Ablation Study and Analysis）  
- **样本对比方式的影响**：不同对比策略对结果有显著影响，CPO采用的“偏好 vs 非偏好”对比效果最佳。
- **损失函数设计**：不同损失权重和形式的对比实验表明，CPO的损失函数设计合理且鲁棒。
- **对低资源语言的适应性**：CPO在数据量较少的情况下仍能保持良好性能，说明其泛化能力强。

> 本节通过细致的实验分析，进一步验证了CPO的稳定性和泛化能力。

---

## 结论（Conclusion）  
本文提出了一种新的机器翻译优化方法——CPO，通过对比学习和人类偏好数据，有效提升了大语言模型的翻译质量。实验表明，CPO在多个语言对和评估指标上均优于传统方法，尤其适合低资源场景。未来工作可探索将CPO扩展到其他生成任务中。

---

## 总结  
- **核心贡献**：提出CPO方法，结合对比学习与偏好数据，提升机器翻译质量。
- **关键优势**：无需强化学习、训练稳定、效果优于SFT和RL。
- **适用场景**：尤其适合低资源语言翻译任务。
- **未来方向**：扩展到其他文本生成任务，如摘要、对话生成等。

--- 

如需进一步精简或扩展某部分内容，请告知。


## Abstract



## 摘要（Abstract）总结：

本研究聚焦于中等规模的大语言模型（如7B或13B参数）在机器翻译（MT）任务中的表现。虽然这类模型已有不错的表现，但其性能仍不及当前最先进的编码器-解码器翻译模型或更大规模的语言模型（如GPT-4）。为缩小这一差距，作者提出了一种新的训练方法。

**重点内容：**

- **监督微调的局限性**：尽管参考翻译是人工生成的，但监督微调方法在训练中仍存在质量问题，无法有效提升模型翻译的准确性。
- **提出新方法：对比偏好优化（CPO）**：不同于传统模仿参考翻译的监督训练，CPO通过训练模型避免生成“基本正确但不完美”的翻译，从而提升整体质量。
- **高效训练策略**：将CPO应用于ALMA模型，仅使用22K平行语料，并只微调0.1%的参数，就取得了显著提升。
- **显著成果**：优化后的模型ALMA-R在WMT’21、WMT’22和WMT’23测试集上表现优异，能够媲美甚至超越WMT比赛优胜模型和GPT-4。

**关键词**：机器翻译、大语言模型


## 1 Introduction



## 1 引言（Introduction）

本节介绍了机器翻译（MT）领域的发展现状，以及作者提出的新方法 Contrastive Preference Optimization（CPO）的背景和目标。

### 1.1 传统模型与解码器-only 大模型的对比

- **传统机器翻译模型**：目前主流的机器翻译模型（如 NLLB-200、M2M100、BiBERT、MT5）基于 Transformer 的编码器-解码器结构。
- **解码器-only 大语言模型（LLMs）的兴起**：近年来，如 GPT 系列、LLaMA、Mistral、Falcon 等解码器-only 的大语言模型在多个 NLP 任务中表现出色，也引发了人们对其在机器翻译中应用的兴趣。
- **大模型 vs 小模型表现差异**：研究表明，GPT-3.5 和 GPT-4 等大模型在翻译任务中表现优异，但较小的模型（如 7B 或 13B）表现仍不如传统翻译模型。

### 1.2 提升小模型翻译能力的努力

- **已有研究尝试**：为提升小模型的翻译性能，已有研究尝试通过各种方法进行优化（如 Yang et al., 2023; Zhu et al., 2023b 等），但效果有限。
- **主要瓶颈**：这些模型主要在以英语为中心的数据上预训练，导致多语言能力受限。
- **ALMA 模型的提出**：Xu et al. (2023) 首次通过在大量非英语单语数据上微调 LLaMA-2，再使用高质量平行语料进行监督微调（SFT），训练出 ALMA 模型。该模型在翻译任务中优于其他中等规模模型，甚至超过 GPT-3.5，但仍落后于 GPT-4 和 WMT 比赛优胜模型。

### 1.3 本文提出的方法与成果（ALMA-R）

- **改进方法**：本文提出通过一种新的训练方法 Contrastive Preference Optimization（CPO）对 ALMA 模型进一步微调。
- **资源消耗极低**：仅使用 12M 可学习参数（占原模型的 0.1%）和 22K 的数据（涵盖 10 个翻译方向）。
- **模型名称**：微调后的模型称为 ALMA-R。
- **性能表现**：如图1所示，ALMA-R 在 WMT’22 测试集上的表现已接近或超过 GPT-4 和 WMT 比赛优胜模型。

### 1.4 CPO 方法的核心动机

CPO 的提出旨在解决监督微调（SFT）的两个根本缺陷：

1. **SFT 的性能上限问题**：
   - SFT 通过最小化预测输出与参考译文之间的差异进行训练，这使得模型性能受限于训练数据的质量。
   - 作者指出，即使是人工翻译的“高质量”参考译文，也可能存在质量问题（详见第2节）。
   - 一些强模型的翻译质量甚至优于参考译文。

2. **缺乏对“近似正确”翻译的抑制机制**：
   - 即使是强模型，也可能生成“几乎正确但有缺陷”的翻译（如遗漏部分内容）。
   - SFT 无法有效防止这类错误的生成。

**CPO 的作用**：通过使用精心构建的偏好数据进行训练，CPO 能有效突破 SFT 的性能瓶颈，提升模型翻译质量。

### 1.5 主要贡献总结

1. **“参考译文是否真的高质量？”分析**：
   - 对 ALMA 使用的训练数据（FLORES-200）进行了深入分析。
   - 发现许多人工参考译文的质量不如系统生成的翻译。
   - 表明单纯模仿参考译文可能不是最优训练策略。

2. **推动 SFT 性能边界**：
   - 提出 CPO 方法，在内存效率、训练速度和翻译质量提升方面均优于 SFT。
   - 使已接近性能饱和的模型进一步提升。

3. **构建并发布高质量偏好数据集**：
   - 为机器翻译领域构建并公开了一个高质量的偏好数据集。

---

**总结**：本节从当前机器翻译模型的发展现状出发，指出了小规模解码器-only 模型在翻译任务中的局限性，介绍了 ALMA 模型及其改进版 ALMA-R 的训练方法 CPO，并通过实验验证其性能优势。同时，作者强调了参考译文质量对模型训练的影响，并提出了新的训练范式和数据资源，为未来研究提供了重要参考。


## 2 Gold or Gilded? Scrutinizing Gold Reference Quality



### 2. Gold or Gilded? Scrutinizing Gold Reference Quality（黄金还是镀金？审视黄金参考的质量）

#### 核心内容总结：

本节主要探讨了机器翻译任务中**黄金参考（gold reference）质量的重要性与局限性**，并提出使用**无参考评估框架**来重新审视黄金参考和模型输出的质量。

---

#### 1. 黄金参考的重要性（The significance of target references）

- 黄金参考是训练和评估机器翻译模型的关键。
- 模型通常通过最小化预测输出与黄金参考之间的差异来优化，使用如负对数似然损失（NLL）：
  $$
  \mathcal{L}_{\text{NLL}} = -\mathbb{E}_{(x,y)\sim\mathcal{D}}[\log \pi_{\theta}(y|x)]
  $$
- 高质量的翻译对（x, y）对于模型性能至关重要。

> **重点**：黄金参考质量直接影响模型训练效果和评估结果。

---

#### 2. 参考质量的质疑（Are references truly equivalent to gold standards?）

- 研究发现，即使是人工编写的黄金参考，也可能存在质量问题。
- 举例：FLORES-200数据集中一个黄金参考遗漏了信息（如未展开缩写“CEP”），而GPT-4和ALMA模型的输出更完整。
- 这引发了对黄金参考是否真正“黄金”的质疑。

> **重点**：黄金参考并非完美，模型输出有时更优。

---

#### 3. 模型与数据（Models & Data）

- **模型**：
  - ALMA-13B-LoRA：基于LoRA微调的高性能翻译模型。
  - GPT-4：零样本翻译模型。
  - 评估模型：KIWI-XXL 和 XCOMET（均为10B参数，无参考评估模型，与人类判断高度相关）。

- **数据**：
  - 使用FLORES-200数据集（高质量人工翻译）。
  - 涉及5种英为中心的语言对（de, cs, is, zh, ru）。

> **重点**：使用当前最先进的模型和高质量数据进行对比。

---

#### 4. 模型输出可作为更优参考（Model Outputs Can Be Better References）

- 表1展示了KIWI-XXL和XCOMET对黄金参考、ALMA和GPT-4输出的评分及“胜率”（Win Ratio）。
- **xx→en（非英到英）翻译中**：
  - 模型输出显著优于黄金参考：
    - KIWI-XXL评分：ALMA高出约3分，GPT-4高出约4分。
    - XCOMET评分：ALMA高出约4分，GPT-4高出约6分。
    - 胜率：ALMA在KIWI-XXL中胜率73.24%，GPT-4为79.43%。

- **en→xx（英到非英）翻译中**：
  - 模型与黄金参考表现接近，但仍有约40%的输出被评估为更优。

> **重点**：模型输出在某些方向上明显优于黄金参考，尤其在xx→en任务中。

---

#### 5. 动机：帮助模型学会拒绝（Motivation: Help The Model Learn Rejection）

- 模型输出有时优于黄金参考，但传统训练方式仅优化与参考的匹配，**无法教会模型识别并拒绝低质量输出**。
- 提出新的训练目标：**对比学习（Contrastive Learning）**，通过引入“难负例（hard negatives）”来训练模型优先生成高质量翻译，拒绝低质量翻译。
- 目标是超越传统的交叉熵最小化，提升模型的“判断力”。

> **重点**：提出新的训练目标，使模型不仅能生成好翻译，还能识别并拒绝差翻译。

---

### 总结

本节揭示了黄金参考并非绝对可靠，并通过实验证明当前翻译模型（如ALMA、GPT-4）在某些任务中已能超越黄金参考。这为未来模型训练和评估提供了新思路：**利用模型输出作为更优参考，并通过对比学习提升模型的翻译判断能力**。


## 3 Contrastive Preference Optimization



## 3 对比偏好优化（Contrastive Preference Optimization）

### 3.1 三元组偏好数据（Triplet Preference Data）

本节详细介绍了偏好数据集 𝒟 的构建方法。该数据集基于 FLORES-200 的开发集和测试集，涵盖与第2节相同的语言对。每对语言包含2009个平行句子。

对于每个源句子 x，无论其是从英语翻译还是翻译成英语，都使用 GPT-4 和 ALMA-13B-LoRA 生成各自的翻译结果，分别记为 ygpt-4 和 yalma。结合原始目标参考 yref，形成一个三元组 𝐲 = (yref, ygpt-4, yalma)，表示对输入 x 的三种不同翻译输出。

使用参考无关的评估模型 KIWI-XXL 和 XCOMET 对这些翻译进行评分，平均得分记为 𝐬 = (sref, sgpt-4, salma)。得分最高的翻译被标记为优选翻译 yw，得分最低的为非优选翻译 yl，即 yw = 𝐲 的 argmax(i)(𝐬)，yl = 𝐲 的 argmin(i)(𝐬)。中间得分的翻译不被考虑。

需要注意的是，即使非优选翻译也可能具有高质量，只是仍有改进空间。这种使用高质量但不完美的翻译作为非优选数据的方法，有助于训练模型进一步优化细节，实现更完美的翻译。

### 3.2 推导 CPO 目标函数（Deriving the CPO Objective）

本节讨论了对比偏好优化（CPO）目标函数的推导过程，从直接偏好优化（DPO）开始分析。DPO 是一种用于基于人类反馈的强化学习（RLHF）中的更直接的优化目标。给定一组源句子 x，以及对应的优选翻译 yw 和非优选翻译 yl，可以访问一个静态的比较数据集 𝒟。

DPO 的损失函数是一个最大似然目标函数，用于参数化策略 πθ：

$$
\mathcal{L}(\pi_{\theta}; \pi_{\text{ref}}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]
$$

其中 πref 是预训练的语言（翻译）模型，σ 是 Sigmoid 函数，β 是超参数。

然而，DPO 存在内存和速度上的低效率问题。为了解决这些问题，引入了对比偏好优化（CPO），通过将 πref 设置为均匀先验 U，消除了对额外计算和存储的需求。

通过将 DPO 损失近似为使用均匀参考模型 U 的形式，可以有效减少 DPO 损失的上界：

$$
\mathcal{L}(\pi_{\theta}; U) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \pi_{\theta}(y_w|x) - \beta \log \pi_{\theta}(yl|x) \right) \right]
$$

此外，引入行为克隆（BC）正则化项，以确保 πθ 不偏离优选数据分布：

$$
\min_{\theta} \mathcal{L}(\pi_{\theta}, U) \quad \text{s.t.} \quad \mathbb{E}_{(x, y_w) \sim \mathcal{D}} \left[ \mathbb{KL}(\pi_w(y_w|x) || \pi_{\theta}(y_w|x)) \right] < \epsilon
$$

最终，CPO 损失包括一个偏好学习项 ℒprefer 和一个负对数似然项 ℒNLL：

$$
\min_{\theta} \underbrace{\mathcal{L}(\pi_{\theta}, U)}_{\mathcal{L}_{\text{prefer}}} \underbrace{- \mathbb{E}_{(x, y_w) \sim \mathcal{D}} [\log \pi_{\theta}(y_w|x)]}_{\mathcal{L}_{\text{NLL}}}
$$


## 4 Experiments



### 4. 实验

#### 4.1 数据
本研究基于ALMA模型的研究成果，即少量高质量数据可以产生出色的翻译效果，因此训练数据集非常紧凑。偏好训练数据来源于FLORES-200数据集，其中一部分也用于ALMA模型的训练。最终得到20K对句子，涵盖10个翻译方向。此外，数据集还包含1K内部人工标注的偏好数据，但仅限于en→zh和en→de两个翻译方向。人工标注数据的影响在附录D中进行了详细探讨，结果显示其影响较小。

测试集主要来自WMT’21（is）和WMT’22（其他语言），并进行了WMT’23的辅助实验，涵盖六个方向。

#### 表2：偏好数据中每种语言对的来源分布
- ALMA-13B-LoRA、GPT-4和参考数据的分布比例在不同语言对中有所不同，例如en↔de中ALMA-13B-LoRA占46%，GPT-4占37%，参考数据占17%。

#### 表3：en→xx在WMT’21和WMT’22上的总体结果
- CPO方法在ALMA-13B-LoRA模型上的微调显著提升了性能，等于或超过了WMT竞赛优胜者和GPT-4的表现。深蓝色表示改进显著，浅蓝色表示改进较小，黄色表示性能下降。

#### 表4：xx→en在WMT’21和WMT’22上的总体结果
- 使用的颜色和加粗方式与表3相同。CPO方法在多个方向上显著提升了性能。

#### 4.2 训练设置
模型以多对多的方式进行多语言机器翻译训练，初始检查点为ALMA-13B-LoRA。训练过程中仅更新LoRA参数，其秩为16，增加了12M参数。使用默认的β值0.1，批量大小为128，预热比为0.01，训练一个epoch，最大序列长度为512。使用deepspeed工具优化训练效率，并使用与Xu等人相同的提示，不计算提示的损失。

#### 4.3 基线

##### 最新模型（SoTA Models）
- 与ALMA-13B-LoRA、TowerInstruct和GPT-4进行比较，GPT-4在零样本翻译中表现最佳。还包括WMT竞赛优胜者作为比较基准。

##### SFT和DPO
- 比较不同训练目标，CPO与直接SFT和DPO进行比较。

#### 4.4 WMT’21和WMT’22结果
- 主要结果见表3和表4，强调无参考评估模型，如KIWI-XXL、XCOMET和KIWI-22。CPO显著提升了ALMA的性能，使其与GPT-4和WMT优胜者相当或更优。SFT和DPO在某些方向上略有改进或下降，而CPO在所有方向上均显著提升。

#### 4.5 WMT’23结果
- 表5展示了WMT’23上六个方向的平均结果，ALMA-13B-R在所有方向上均优于ALMA-13B-LoRA和TowerInstruct，并与WMT优胜者相当或更优。

总结：CPO方法在多个翻译任务中显著提升了ALMA模型的性能，使其在多个基准上达到或超过当前最先进的模型。


## 5 Analyses



## 5 分析

本节分析基于 WMT’21 和 WMT’22 测试集，报告了平均性能表现。

---

### 5.1 翻译真的更好了吗，还是只是更符合评估指标？

**核心问题**：训练数据是通过无参考指标（如 KIWI-XXL）筛选出的“偏好数据”，而最终评估也使用这些指标，是否存在“作弊”现象？即翻译是否真的更好，还是只是更符合评估模型的偏好？

#### 在指标层面：
- 使用 KIWI-XXL 或 XCOMET 构建偏好数据，并用 CPO 方法重新训练 ALMA-13B-LoRA 模型。
- 表6结果显示，无论使用哪种指标构建偏好数据，翻译性能在所有指标上都保持一致提升，没有明显偏向某一个指标。
- 进一步使用非 Comet 系列的 BLEURT 指标评估，也观察到显著提升，说明 ALMA-R 的翻译质量确实更好，而非仅因指标偏好。

#### 在方法层面：
- 尝试使用 DPO 或 SFT 方法训练模型，仅使用偏好数据，结果反而导致性能下降。
- 这表明，仅靠“迎合指标”并不能提升性能，CPO 的提升并非来自指标偏见。
- 因此，CPO 的效果是真实翻译质量的提升，而不是指标偏差的结果。

---

### 5.2 人工评估

为了进一步验证模型提升的真实性，进行了人工评估。

#### 评估设置：
- 选取 zh→en 方向的 400 个样本。
- 每个样本包含一个中文源句和两个英文翻译（分别来自 ALMA-13B-LoRA 和 ALMA-13B-R）。
- 四位中英双语者对翻译质量进行 0~6 分评分，并给出排名和胜率。

#### 评分标准：
- 0：完全无意义
- 2：部分保留原意，有重大错误
- 4：基本准确，有轻微错误
- 6：完美翻译

#### 结果（见表7）：
| 模型 | 平均得分 ↑ | 平均排名 ↓ | 胜率 (%) | 平局 (%) |
|------|-------------|-------------|-----------|-----------|
| ALMA-13B-LoRA | 4.86 | 1.60 | 62.50 | 40.30 |
| ALMA-13B-R | **5.16** | **1.40** | **77.80** | 40.30 |

- ALMA-13B-R 明显优于 ALMA-13B-LoRA。
- 说明使用无参考模型（如 KIWI-XXL、XCOMET）构建偏好数据和进行评估是有效且稳健的。

---

### 5.3 消融实验

#### CPO 损失函数的组成部分：
- CPO 损失由两部分组成：
  - **ℒprefer**：用于偏好学习
  - **ℒNLL**：确保模型不偏离偏好数据分布（相当于 SFT）

- 实验发现：同时使用两个损失项效果最佳，仅使用其中之一会导致性能下降。
- 附录中还显示，将 ℒNLL 加入 DPO 损失中也能显著提升性能。

#### 偏好数据的组成部分：
- 偏好数据来自三元组：GPT-4、ALMA 和黄金参考。
- 实验发现：
  - 移除 ALMA 生成的数据 → en→xx 翻译性能显著下降
  - 移除 GPT-4 生成的数据 → xx→en 翻译性能显著下降
- 说明两种模型生成的数据都对提升翻译质量有帮助。

---

### 5.4 不偏好数据的质量是否重要？

**实验设计**：
- 将原本高质量但得分最低的“不偏好翻译”替换为人工加噪的翻译（随机删除词、词交换）。
- 比较使用人工加噪 vs 原始自然生成的不偏好数据的性能差异。

#### 结果（见表8）：
| 不偏好数据类型 | KIWI-22 | KIWI-XXL | XCOMET |
|----------------|---------|----------|--------|
| 人工加噪 | 81.01 | 82.18 | 88.23 |
| 自然生成（本文方法） | **81.33** | **82.43** | **89.11** |

- 使用人工加噪的不偏好数据会导致性能显著下降。
- 说明高质量的不偏好数据对提升翻译质量至关重要。

---

### 总结

本章通过多个角度验证了 CPO 方法的有效性与稳健性：

1. **指标层面**：使用不同无参考模型构建偏好数据，性能提升一致，无明显指标偏见。
2. **方法层面**：仅使用偏好数据无法提升性能，CPO 的提升是真实翻译质量的提升。
3. **人工评估**：ALMA-13B-R 在人工评分、排名和胜率上均优于基线模型。
4. **消融实验**：
   - CPO 的两个损失项都对性能有贡献。
   - GPT-4 和 ALMA 生成的数据在不同翻译方向上各有重要作用。
5. **不偏好数据质量**：高质量的不偏好数据对模型训练至关重要，人工加噪会显著降低性能。

这些分析共同支持了 CPO 方法在机器翻译任务中的有效性与鲁棒性。


## 6 Conclusion



## 6 结论

本研究首先指出了机器翻译（MT）任务中**黄金参考译文**可能存在的质量问题，强调了在某些情况下，先进的翻译模型的表现可以超过这些“标准”参考。这一发现不仅对基于监督微调（SFT）的模型训练方式提出了挑战，也对依赖参考译文的评估方法（如BLEU、CHRF等指标）提出了质疑。

随后，作者提出了一种更高效的**DPO改进方法**——**对比偏好优化**（Contrastive Preference Optimization）。该方法结合了模型生成的翻译与参考译文，引导模型避免生成那些看似接近参考译文、但存在错误的翻译，从而学习更高质量的翻译结果。

最终，作者开发的模型 **ALMA-13B-R** 成为首个在性能上**媲美甚至超越 GPT-4 和 WMT 竞赛优胜模型**的中等规模基于大语言模型（LLM）的翻译模型，标志着机器翻译领域的重要进展。

---

**重点内容总结：**

- **黄金参考质量存疑**：先进模型可以超过“标准译文”，这对训练和评估方法都提出了挑战。
- **提出新训练方法**：对比偏好优化（CPO）比传统DPO更高效，结合模型生成与参考译文进行优化。
- **模型成果显著**：ALMA-13B-R在翻译质量上达到甚至超过当前顶级模型（如GPT-4），是中等规模模型中的首次突破。


## Impact Statement



## Impact Statement（影响声明）

本节内容简要说明了本文的研究目标是推动机器翻译和大语言模型领域的发展。作者指出，这项工作可能带来多种潜在的社会影响，但认为这些影响无需在本文中特别强调。该声明较为中性，未具体展开对社会正面或负面影响的讨论。


## Appendix Contents



以下是论文附录内容的总结，按照原文结构进行讲解：

---

### **Appendix A: Comprehensive Results of WMT’21 and WMT’22**  
本节提供了WMT 2021和2022年翻译任务的完整实验结果，包括多个语言对和不同模型的性能对比。重点在于展示CPO（Contrastive Preference Optimization）方法在这些数据集上的全面表现，验证其在多个基准上的优越性。

---

### **Appendix B: Prompts Used for Translations**  
介绍了在翻译任务中使用的提示（prompt）设计细节。这部分内容属于方法实现的技术细节，用于确保翻译任务的可复现性，但对核心理论影响较小，因此可作为补充材料简要了解。

---

### **Appendix C: Theory of The CPO Loss**  
详细推导了CPO损失函数的理论基础，包括其数学形式、优化目标以及与现有方法（如DPO）的联系。这是论文的核心理论部分，解释了CPO如何通过对比偏好优化提升翻译质量，属于重点内容。

---

### **Appendix D: Details and Influence of Human-Labeled Data**  
分析了人工标注偏好数据的构建过程及其对模型性能的影响。重点讨论了数据质量、标注一致性以及其在训练中的作用，是理解CPO训练数据来源和可靠性的关键部分。

---

### **Appendix E: Information of WMT Winners**  
列出了WMT 2021至2023年各语言对翻译任务的冠军系统信息，作为对比实验的参考基准。这部分内容主要用于背景介绍，帮助理解CPO在行业领先系统中的位置。

---

### **Appendix F: Estimated Accuracy with Human Agreements**  
探讨了模型预测结果与人类判断之间的一致性，并估计了模型在翻译质量评估上的准确率。这部分内容强调了模型输出与人类偏好的对齐程度，是评估模型实用性的关键指标。

---

### **Appendix G: Experimental Results on WMT’23**  
展示了CPO在WMT 2023年任务上的实验结果，进一步验证了该方法在最新数据集上的有效性。作为实证结果的一部分，具有较强的说服力。

---

### **Appendix H: Evaluation on Non-Comet Metrics**  
除了常用的COMET指标外，本节还使用其他翻译评估指标（如BLEU、METEOR等）对模型进行评估，以验证CPO方法的泛化能力。属于补充性评估内容。

---

### **Appendix I: Effectiveness of The BC Regularizer for DPO**  
分析了在DPO（Direct Preference Optimization）中引入行为克隆（Behavior Cloning）正则项的效果，探讨其对训练稳定性和性能的影响。这部分内容是对CPO相关方法的扩展研究，属于辅助性分析。

---

如需进一步了解某一部分的具体内容或技术细节，可以继续提问。


## Appendix A Comprehensive Results of WMT’21 and WMT’22



## 附录A WMT’21和WMT’22的全面结果

本节展示了在WMT’21和WMT’22数据集上英语到其他语言（en→xx）和其它语言到英语（xx→en）的全面翻译结果，包括多个基于大语言模型（LLM）的翻译系统，如Bayling-13B、BigTranslate、ALMA-13B-LoRA，以及LLaMA系列模型的零样本表现。同时，还与当前最先进的翻译模型（如GPT-4、Google Translate、NLLB等）进行了对比。除了传统的BLEU指标，还报告了sacreBLEU、COMET-22、KIWI、XCOMET等参考依赖和参考无关的评估指标。

### 引入ALMA-7B-R

本研究将ALMA-13B-R的训练方法扩展到7B模型规模，即在ALMA-7B-LoRA基础上使用CPO方法进行微调。结果显示，CPO方法在7B模型上同样显著提升了翻译性能，与13B模型的提升趋势一致。

**重点内容：**
- ALMA-7B-R是ALMA-13B-R方法在更小模型上的成功应用。
- 使用相同的偏好数据进行训练，验证了CPO方法的有效性。

---

### 与先进翻译模型的比较

ALMA-13B-R在多个翻译任务中表现优异，其性能与GPT-4和WMT比赛冠军模型相当，甚至在某些任务上超越了Google Translate、NLLB、MADLAD-10B和GPT-3.5等主流模型。

**重点内容：**
- ALMA-13B-R在多个语言对上表现接近或超过GPT-4。
- 在多语言翻译任务中优于NLLB、MADLAD-10B等模型。
- 表明CPO方法在提升翻译质量方面具有显著优势。

---

### 停止使用BLEU

BLEU作为传统评估指标，与基于神经网络的参考无关指标（如XCOMET）存在较大偏差。例如，WMT冠军模型在BLEU得分上远超其他模型，但在参考无关评估中表现不佳。这可能是因为WMT模型在训练时使用了与测试集高度相关的领域数据，导致词汇匹配度高但语义质量不足。

**重点内容：**
- BLEU在评估先进模型时存在局限性。
- WMT冠军模型在BLEU上得分极高，但在参考无关评估中不如GPT-4和ALMA模型。
- 推荐使用更符合语义质量的评估方法。

---

### 迈向参考无关的评估方法

COMET-22等参考依赖的神经评估指标比BLEU更稳定，与参考无关模型（如XCOMET）的一致性更高。例如，ALMA模型在COMET-22上得分接近GPT-4（87.74 vs. 87.68），但在XCOMET上甚至略优。然而，参考本身的质量问题也可能影响COMET-22的结果，例如黄金参考在某些情况下不如系统生成的翻译。

**重点内容：**
- COMET-22比BLEU更可靠，与参考无关模型更一致。
- XCOMET显示ALMA模型优于WMT冠军模型，而COMET-22则相反，说明参考质量可能影响评估结果。
- 推荐采用参考无关的评估方法以避免参考质量偏差。

---

### 表9：en→xx翻译结果

表9展示了英语到德语（de）、捷克语（cs）、冰岛语（is）、中文（zh）、俄语（ru）等语言的翻译结果，包括BLEU、COMET-22、KIWI-22、KIWI-XXL、XCOMET等指标。ALMA-13B-R和ALMA-7B-R在多个指标上表现优异，尤其在XCOMET上接近或超过GPT-4。

**重点内容：**
- ALMA-13B-R在XCOMET上得分高达95.22（en→ru）。
- WMT冠军模型在BLEU上表现突出，但XCOMET得分低于ALMA和GPT-4。
- LLaMA系列模型表现较弱，尤其在零样本设置下。

---

### 表10：xx→en翻译结果

表10展示了从德语、捷克语、冰岛语、中文、俄语到英语的翻译结果。ALMA-13B-R在COMET-22和XCOMET上表现优异，尤其在xx→en方向上接近GPT-4。

**重点内容：**
- ALMA-13B-R在XCOMET上得分高达91.18（xx→ru）。
- GPT-4在部分语言对上略优于ALMA模型。
- LLaMA系列模型在xx→en任务中表现仍较弱。

---

### 总结

本节通过大量实验结果展示了ALMA系列模型在机器翻译任务中的竞争力，尤其是在使用CPO方法后，ALMA-13B-R和ALMA-7B-R在多个语言对和评估指标上均表现优异。同时，强调了BLEU等传统指标在评估先进模型时的局限性，并推荐使用参考无关的评估方法（如XCOMET）以更准确地反映翻译质量。


## Appendix B Prompts for Translations



## 附录B 翻译任务的提示词

本节内容主要介绍了在研究中为不同语言模型设计的翻译任务提示词（prompts）。

- **GPT-4模型的提示词**：作者遵循Hendy等人（2023）为GPT系列模型设计的翻译提示格式，并将其直接应用于GPT-4模型中。该提示词的结构旨在引导模型进行高质量的翻译任务。

- **ALMA模型的提示词**：对于ALMA模型，则采用了Xu等人（2023）所使用的翻译提示方式。这说明作者在设计提示词时参考了已有研究中被验证有效的方式。

- **提示词展示**：如图5所示，展示了GPT-4和ALMA模型各自使用的提示词具体内容（图略）。该图帮助读者直观了解不同模型在翻译任务中所使用的输入格式。

**重点内容**：  
本节的核心在于说明作者在翻译任务中采用了已有研究中验证有效的提示词格式，分别适用于GPT-4和ALMA模型，以确保实验的可比性和有效性。


## Appendix C Theory



## 附录 C 理论分析

### C.1 上界证明

#### 定理 1
当参考策略 π_ref 设置为 π_w（即一个与偏好数据真实分布完全对齐的理想策略）时，DPO 损失 ℒ(π_θ; π_w) + C 被 ℒ(π_θ; U) 上界约束，其中 C 是一个常数。

#### 证明概要
1. **理想策略 π_w 的性质**：
   - π_w(y_w|x) = 1（对偏好数据的预测无需参考模型加权）。
   - 0 ≤ π_w(y_l|x) ≤ 1（对非偏好数据的预测概率在 [0,1] 之间）。

2. **DPO 损失的重写**：
   - 原始 DPO 损失 ℒ(π_θ; π_w) 可以展开为：
     $$
     \mathcal{L}(\pi_\theta; \pi_w) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_w(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_w(y_l|x)} \right) \right]
     $$
   - 利用 π_w(y_w|x) = 1 的性质，进一步简化为：
     $$
     \mathcal{L}(\pi_\theta; \pi_w) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}} \left[ \log \sigma \left( \beta \log \pi_\theta(y_w|x) - \beta \log \pi_\theta(y_l|x) + \beta \log \pi_w(y_l|x) \right) \right]
     $$

3. **Sigmoid 函数展开**：
   - 将 Sigmoid 函数展开后，损失函数进一步转化为：
     $$
     \mathcal{L}(\pi_\theta; \pi_w) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}} \left[ \log \left( \frac{1}{1 + \frac{\pi_\theta(y_l|x)^\beta}{\pi_\theta(y_w|x)^\beta \cdot \pi_w(y_l|x)^\beta}} \right) \right]
     $$

4. **简化与等价变换**：
   - 由于 π_w 是固定模型，log π_w(y_l|x)^β 不参与梯度计算，可以将其从损失函数中移除，得到等价形式：
     $$
     \mathcal{L}'(\pi_\theta; \pi_w) = -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}} \left[ \log \pi_\theta(y_w|x)^\beta - \log \left( \pi_\theta(y_w|x)^\beta \cdot \pi_w(y_l|x)^\beta + \pi_\theta(y_l|x)^\beta \right) \right]
     $$

5. **上界推导**：
   - 利用 π_w(y_l|x) ≤ 1 的性质，可以得到：
     $$
     \mathcal{L}'(\pi_\theta; \pi_w) \leq -\mathbb{E}_{(x, y_w, y_l)\sim\mathcal{D}} \left[ \log \sigma \left( \beta \log \pi_\theta(y_w|x) - \beta \log \pi_\theta(y_l|x) \right) \right] = \mathcal{L}(\pi_\theta; U)
     $$

6. **结论**：
   - 因此，DPO 损失 ℒ(π_θ; π_w) + C 被 ℒ(π_θ; U) 上界约束，其中 C 是一个常数项：
     $$
     C = \mathbb{E}_{(x, y_l)\sim\mathcal{D}} \left[ \log \pi_w(y_l|x)^\beta \right]
     $$

---

### C.2 行为克隆正则项简化

#### 优化目标
对比偏好优化（CPO）最初定义为在最小化 ℒ(π_θ; U) 的同时，约束策略 π_θ 与理想策略 π_w 的 KL 散度不超过某个阈值 ε：

$$
\min_\theta \mathcal{L}(\pi_\theta, U) \quad \text{s.t.} \quad \mathbb{E}_{(x, y_w)\sim\mathcal{D}} \left[ \mathbb{KL}(\pi_w(y_w|x) || \pi_\theta(y_w|x)) \right] < \epsilon
$$

#### 拉格朗日对偶形式
通过拉格朗日乘子法，该约束优化问题可以转化为无约束形式：

$$
\min_\theta \mathcal{L}(\pi_\theta, U) + \lambda \cdot \mathbb{E}_{(x, y_w)\sim\mathcal{D}} \left[ \mathbb{KL}(\pi_w(y_w|x) || \pi_\theta(y_w|x)) \right]
$$

其中 λ 是超参数，通常设为 1。

#### KL 散度展开
将 KL 散度展开后，CPO 损失可以写为：

$$
\mathcal{L}_{\text{CPO}} = \mathcal{L}(\pi_\theta, U) + \mathbb{E}_{(x, y_w)\sim\mathcal{D}} \left[ \pi_w(y_w|x) \cdot \log \pi_w(y_w|x) - \pi_w(y_w|x) \cdot \log \pi_\theta(y_w|x) \right]
$$

由于 π_w(y_w|x) = 1，进一步简化为：

$$
\mathcal{L}_{\text{CPO}} = \mathcal{L}(\pi_\theta, U) - \mathbb{E}_{(x, y_w)\sim\mathcal{D}} \left[ \log \pi_\theta(y_w|x) \right]
$$

#### 最终形式
最终的 CPO 损失函数为：

$$
\mathcal{L}_{\text{CPO}} = \mathcal{L}(\pi_\theta, U) - \mathbb{E}_{(x, y_w)\sim\mathcal{D}} \left[ \log \pi_\theta(y_w|x) \right]
$$

这构成了我们最终的 CPO 损失函数形式。


## Appendix D Details And Influence of Human-Labeled Preference Data



## 附录 D 人工标注偏好数据的细节与影响  
**简要总结：** 我们的分析表明，人工标注数据对模型性能的影响相对较小，这可能是因为存在大量“平票”翻译以及评估过程中可能存在的人类偏见。

### D.1 数据构建细节  
我们使用的人工标注数据集是成对形式的，与主数据集的三元组格式不同。该数据集专注于两个语言方向：en→de 和 en→zh，共增加了约2000个句子。英文源句选自维基百科，并经过过滤以去除时间戳和URL。每个句子由Google Translate和GPT-4分别翻译，然后由人工评估者在两者之间选择更优的翻译。偏好分布（即Google或GPT-4被选中或打平的次数）见表11。

**表11：** 各系统在人工评估中获胜或打平的翻译数量统计。

|  | Google 获胜 | GPT-4 获胜 | 平票 |
| --- | --- | --- | --- |
| en→de | 418 | 435 | 203 |
| en→zh | 362 | 412 | 282 |

**重点：**
- GPT-4在两个方向上都略优于Google，但“平票”比例较高（en→de为203次，en→zh为282次），这可能影响后续模型训练的有效性。

### D.2 对性能的影响  
由于我们的模型支持多语言方向翻译，而人工标注数据仅涵盖de和zh方向，因此我们预期这些方向的性能会有所变化，而其他方向则不会。我们比较了仅使用三元组数据训练的模型与同时使用三元组和人工标注数据训练的模型之间的性能差异。训练方法保持一致，使用ALMA-13B-LoRA模型并通过CPO进行微调。**注意：分析中排除了“平票”数据，因为它们没有明确的偏好信息。**

#### 结果与分析  
**表12（en→xx方向）：** 加入人工标注数据后，翻译性能没有显著提升。en→zh方向有轻微改善，但幅度很小；en→de方向反而略有下降。

**表13（xx→en方向）：** 总体来看，加入人工标注数据后平均性能略有下降。

**总结：**
- 人工标注数据对en→xx方向的性能影响不大，对xx→en方向甚至略有负面影响。
- **可能原因：**
  1. **高比例的“平票”数据**：大量翻译在人工评估中被认为是质量相当的，导致训练信号不够明确。
  2. **人类评估的主观性**：有时评估者偏好Google翻译，尽管作者认为GPT-4的翻译更好，这可能引入了偏差。

**重点结论：**
- 人工标注数据在当前设置下对模型性能的提升有限，甚至可能因评估偏差和模糊偏好而带来负面影响。


## Appendix E WMT Winner Systems



## 附录E：WMT获奖系统总结

### E.1 WMT’21和WMT’22的系统
本节指出，WMT’21和WMT’22比赛中各翻译方向的优胜系统与Hendy等人（2023）所使用的系统一致。如果需要更详细的描述，建议读者参考Hendy等人的论文。

**重点说明：**
- 本节内容较为简略，主要作用是引导读者参考其他文献，没有提供额外的细节。

---

### E.2 WMT’23的系统
本节详细介绍了WMT’23比赛中不同语言对的翻译系统选择标准：
- 对于 **de↔en（德语↔英语）** 和 **zh↔en（中文↔英语）** 的语言方向，选择了在基于源语言的直接评估（Direct Assessment）和标量质量度量（Scalar Quality Metrics, SQM）中获得最高人类评分的系统。
- 对于 **de↔ru（德语↔俄语）** 的方向，由于Kocmi等人（2023）的研究中没有提供人类评分，因此选择了在COMET-22指标中得分最高的模型。

**重点内容：**
- 提供了具体的系统名称及其对应的语言方向，详见表14。

**表14：WMT’23各语言方向的优胜系统**

| 系统名称 | 语言对 |
| --- | --- |
| ONLINE-B | 英语→德语 |
| ONLINE-A | 德语→英语 |
| Lan-BridgeMT (Wu & Hu, 2023) | 英语→中文 |
| Lan-BridgeMT (Wu & Hu, 2023) | 中文→英语 |
| ONLINE-G | 英语→俄语 |
| ONLINE-Y | 俄语→英语 |

**总结：**
- 本节是附录的重点部分，详细列出了WMT’23比赛中的优胜系统及其对应的语言方向，并说明了选择依据，具有较高的参考价值。


## Appendix F Estimated Accuracy with Human Agreements



## 附录 F 与人类协议估计的准确性

本节介绍了论文中采用的一种新方法，用于在表格中突出显示改进效果。不同于以往设定静态改进阈值的做法，本研究采用了**动态阈值**，该阈值基于人类判断所能感知的两个系统之间差异的最小指标变化（Kocmi et al., 2024）。这种方法更贴近实际的人类感知一致性。

### 核心内容讲解：

- **动态阈值设定**：  
  阈值不是固定的，而是根据人类判断的**一致性水平**（concordance rate）进行调整。例如：
  - 在人类判断一致性达到80%时：
    - KIWI-XXL 和 COMET-XXL 的改进阈值为 ≥1.24
    - KIWI-22 的改进阈值为 ≥0.53

  这意味着，只有当系统之间的指标差异超过这些阈值时，人类才能明显感知到性能差异。

- **表15：各指标的阈值与估计准确性**  
  表格展示了不同一致性水平（从50%到95%）下，BLEU、Comet-22、KIWI-22、XCOMET-XXL 和 KIWI-XXL 的阈值变化情况。  
  - **重点指标**：
    - BLEU 的阈值较高（例如80%一致性下为2.34），说明其对人类感知的敏感度较低。
    - KIWI-22 在80%一致性下阈值仅为0.53，表明其更贴近人类判断。
    - XCOMET-XXL 和 KIWI-XXL 在高一致性水平下表现相近，阈值均为1.24（80%一致性）。

### 总结：

本节通过引入基于人类感知的动态阈值，提升了评估系统性能差异的准确性，并通过表15具体展示了不同一致性水平下的阈值变化，强调了KIWI和COMET系列指标在高一致性下更小的感知差异阈值。


## Appendix G Full Results of WMT’23



## 附录 G WMT’23 完整结果总结

本节展示了 WMT’23 的完整翻译评估结果，主要通过表格形式呈现，并对模型表现进行了对比分析。

### 原文结构与重点内容总结：

### **标题：Appendix G Full Results of WMT’23**

- **主要内容**：提供了 WMT’23 翻译任务的完整评估结果，表格（Table 16）中列出了多个模型在不同语言对上的表现，包括自动评估指标（KIWI-22、KIWI-XXL、XCOMET）得分。
- **重点内容**：
  - **ALMA-13B-R（CPO优化版本）表现突出**：在多个语言对（如 de→en、zh→en、ru→en 和反向）上均取得最高分，优于其他 SOTA 模型。
  - **颜色标注机制**：
    - **深蓝色背景**：表示与原始 ALMA 模型相比，改进显著（至少 80% 人类判断准确率）。
    - **浅蓝色背景**：表示提升较小。
  - **对比模型**：包括 WMT 历届优胜系统、TowerInstruct、MADLAD-10B、ALMA-13B-LoRA 等。
  - **参考标准**：
    - “Gold Reference” 表示人工参考翻译的得分。
    - “WMT Winners” 表示历届 WMT 比赛中表现最佳的系统。

### **表格内容简要说明（Table 16）**：

表格分为两个部分：源语言到英文（如 de→en、zh→en、ru→en）和英文到目标语言（en→de、en→zh、en→ru）。

#### 源语言到英文（de→en, zh→en, ru→en）

- **ALMA-13B-R（+CPO）** 在所有语言对中均取得最高分，例如：
  - de→en 上，XCOMET 得分为 86.62（最高）。
  - zh→en 上，KIWI-22 得分为 80.01（最高）。
  - ru→en 上，XCOMET 得分为 88.75（最高）。

#### 英文到目标语言（en→de, en→zh, en→ru）

- ALMA-13B-R 同样在多个指标中领先，例如：
  - en→zh 的 XCOMET 得分为 88.34。
  - en→ru 的 XCOMET 得分为 92.56（最高）。

### 总结：

- **ALMA-13B-R（CPO 版本）在 WMT’23 中表现最佳**，在多个语言方向和评估指标上均优于其他主流模型。
- 表格清晰展示了各模型在不同语言对上的性能差异，并通过颜色标注强调了 CPO 优化带来的显著提升。
- 本节为后续模型改进和对比提供了详实的数据支持。


## Appendix H Evaluation on ALMA-R with Non-Comet Metric



## 附录H 使用非COMET指标在ALMA-R上的评估

本节旨在回应一个潜在的质疑：由于COMET指标在训练过程上的相似性，不同COMET模型之间可能存在高度相关性，这可能削弱第5.1节分析结果的有效性。为了解决这一问题，作者引入了BLEURT-20（一种非COMET、基于神经网络的参考型评估指标），对ALMA-13B-LoRA和ALMA-13B-R模型进行评估。

### 核心内容：
- **使用BLEURT-20进行验证**：尽管偏好数据是基于COMET构建的，ALMA-R在BLEURT-20指标上仍表现出显著优于ALMA-13B-LoRA的结果，说明其翻译质量不仅在COMET指标下更优，在独立的非COMET指标下也具有优势。
- **实验结果见表17**：在多个语言对（如德语、捷克语、冰岛语、中文、俄语）的翻译任务中，ALMA-R在“翻译成英文”和“从英文翻译”任务中均取得更高的BLEURT-20分数，平均分均高于ALMA-13B-LoRA。

### 结论：
这一分析进一步支持了作者的主要发现：ALMA-R生成的翻译不仅在COMET指标上表现优异，在独立的BLEURT指标上也展现出更强的性能，说明其翻译质量具有更高的真实性和鲁棒性。


## Appendix I The Effectiveness of The BC Regularizer for DPO



## 附录 I：BC 正则化在 DPO 中的有效性

### 标题不变：**BC 正则化在 DPO 中的有效性**

#### 内容总结：

本节主要探讨了在 DPO（Direct Preference Optimization）目标中引入 BC（Behavior Cloning）正则化的效果。DPO 的原始损失函数为：

$$
\mathcal{L}_{\text{DPO}} = \mathcal{L}(\pi_{\theta}, \pi_{\text{ref}})
$$

作者提出可以在 DPO 损失中加入 BC 正则化项，即：

$$
\min_{\theta} \mathcal{L}(\pi_{\theta}, \pi_{\text{ref}}) - \mathbb{E}_{(x, y_w) \sim \mathcal{D}}[\log(\pi_{\theta}(y_w|x))]
$$

其中，第二项是负对数似然损失（NLL），即 BC 正则化项，用于引导模型更贴近偏好数据分布。

---

### 重点内容讲解：

#### 1. **BC 正则化对 DPO 的提升**
- 表 18 显示，将 $\mathcal{L}_{\text{NLL}}$ 加入 DPO 目标后，无论是英译其他语言（xx→en）还是其他语言译英（en→xx），翻译性能都有显著提升。
- 这表明，原始 DPO 损失可能缺乏对数据分布的约束，导致其在偏好学习中表现不佳。加入 BC 正则化后，模型能更好地学习偏好数据。

#### 2. **CPO 与 DPO+BC 的对比**
- CPO（Contrastive Preference Optimization）使用的是 $\mathcal{L}_{\text{prefer}} + \mathcal{L}_{\text{NLL}}$，是 DPO 的一种高效近似形式。
- 虽然 DPO + BC 与 CPO 性能接近，但前者的**内存消耗和计算成本是后者的两倍**（2× Memory Cost 和 FLOPs/tok）。
- 更重要的是，实验结果显示 CPO 甚至可以**略微优于 DPO + BC**，说明其在节省资源的同时还能保持甚至提升性能。

---

### 精简内容讲解：

- 原始 DPO 损失在没有 BC 正则化时表现较差，说明 BC 正则化在偏好学习中具有关键作用。
- 表格中的多个指标（KIWI-22、KIWI-XXL、XCOMET）均一致支持上述结论。
- 最终强调：**BC 正则化对于提升 DPO 类方法在机器翻译任务中的表现至关重要**，而 CPO 是一种更高效、更优的替代方案。

---

### 总结：

本节通过实验验证了 BC 正则化在 DPO 中的重要性，并指出 CPO 是一种更高效、性能更优的替代方法。核心观点是：**BC 正则化有助于模型更好地学习偏好数据分布，是提升 DPO 性能的关键因素**。
