# 2310.12036ΨPO: A General Theoretical Paradigm to Understand Learning from Human Preferences

* 首页: <https://arxiv.org/abs/2310.12036>
* PDF: <https://arxiv.org/pdf/2310.12036.pdf>
* 引用: 809(2025-12-12)
* 组织: 
    * Google DeepMind



## From Moonlight


### 三行摘要

1. 😟 现有的偏好学习方法，如 RLHF 和 DPO，依赖于将成对偏好转化为点奖励的近似，且 DPO 在处理确定性偏好时容易出现过拟合问题。
2. 📚 本文提出了一种名为 ΨPO 的通用理论范式，它直接基于成对偏好进行学习，从而避免了现有方法中的两种近似，并将 RLHF 和 DPO 统一为其特殊情况。
3. ✨ 基于 ΨPO 并将 Ψ 设置为恒等函数，本文推导了 IPO (Identity-PO)，该方法避免了 Bradley-Terry 模型假设，有效防止了过拟合，并在示例中展示出优于 DPO 的性能。


### 关键词

- Reinforcement Learning from Human Feedback (RLHF): 强化学习从人类反馈中学习（RLHF）是一种用于使模型（特别是大型语言模型）与人类期望对齐的范式。它通常涉及两个主要阶段：首先，训练一个奖励模型（reward model）来预测人类对给定上下文下生成文本的偏好；然后，使用强化学习算法（如 PPO）来优化一个策略（policy），使其最大化这个奖励模型预测的奖励，同时通过 KL 散度（KL divergence）等方式约束策略与一个参考策略（reference policy）的距离，以防止模型漂移。RLHF 已成功应用于训练指令遵循型语言模型，如 InstructGPT 和 GPT-4。
- Direct Preference Optimization (DPO): 直接偏好优化（DPO）是一种旨在简化 RLHF 过程的方法。与 RLHF 不同，DPO 不显式训练奖励模型。它直接从人类的偏好数据（通常是成对比较）中优化策略。DPO 的核心思想是，它推导出一个目标函数，该函数直接与人类偏好概率相关，并且在特定假设（如奖励模型符合 Bradley-Terry 模型）下，其最优策略与 RLHF 的最优策略相同。DPO 的优势在于实现更简单，计算资源需求更少，并且避免了训练和部署奖励模型的复杂性。
- Reward Model: 奖励模型（Reward Model）是在 RLHF 流程中用于量化人类对模型输出偏好的组件。它通常被训练成一个分类器，使用成对的偏好数据（即，人类更喜欢生成 A 而不是生成 B）来学习一个函数，该函数可以为给定上下文下的每个可能的生成（或更准确地说，是其表示，如 Elo 分数）分配一个分数。这个分数被解释为该生成被人类偏好的程度。然后，强化学习算法使用这个学习到的奖励模型来指导策略的优化，目标是生成能够获得高奖励的输出。DPO 方法则尝试绕过显式训练奖励模型这一步。
- Pairwise Preferences: 成对偏好（Pairwise Preferences）是指人类在给定一个上下文（context）时，对两个不同生成（generations）进行比较，并选择其中一个更优的偏好信息。例如，对于一个提示 "写一首关于猫的诗"，模型可能生成两个不同的诗句，人类会从中选择一个更喜欢的。这种数据形式是训练奖励模型或直接优化策略（如 DPO 和本文提出的 IPO）的基础。论文中的数据集 D 通常由 (上下文 x, 更优生成 yw, 更劣生成 yl) 组成。
- Pointwise Rewards: 点位奖励（Pointwise Rewards）是将成对偏好数据近似转换为对单个生成（action/generation）的奖励值。在 RLHF 中，通常会训练一个奖励模型，使其输出一个标量值 r(x, y)，代表在上下文 x 下，生成 y 所能获得的“奖励”或“质量评分”。这种评分是通过将成对偏好数据拟合到 Bradley-Terry 模型等统计模型中来估计的，其中两个生成之间的偏好概率取决于它们各自奖励分数之差的 sigmoid 函数。因此，点位奖励是 RLHF 中用于强化学习优化的关键中间表示，而 DPO 旨在避免显式地学习这些点位奖励。
- Bradley-Terry Model: Bradley-Terry 模型（BT 模型）是一种用于对成对比较数据进行建模的统计模型。它假设对于两个选项 A 和 B，选项 A 被选中的概率取决于一个与 A 相关的参数（通常表示为奖励或分数 r(A)）和与 B 相关的参数（r(B)）之间的差值。具体来说，在本文的上下文中，BT 模型将生成 y 相对于生成 y' 的真实人类偏好概率 p*(y ≻ y'|x) 表示为 sigmoid 函数 σ(r(x, y) - r(x, y'))，其中 r(x, y) 是生成 y 在上下文 x 下的点位奖励（或 Elo 分数）。RLHF 依赖于 BT 模型来训练奖励模型，而 DPO 的理论有效性也建立在 BT 模型成立的假设之上。论文指出，当实际偏好接近确定性时，BT 模型可能导致 DPO 和 RLHF 过度拟合数据。
- Ψ-Preference Optimization (ΨPO): Ψ-偏好优化（ΨPO）是本文提出的一种更通用的理论框架，用于从人类偏好中学习。它将目标定义为最大化一个非线性函数 Ψ 的期望输出，其中 Ψ 是一个非减函数，作用于生成 y 相对于另一个生成 y'（通常来自参考策略 μ）的真实偏好概率 p*(y ≻ y'|x)，并结合 KL 正则化项来约束策略与参考策略 πref 的距离。其形式为：maxπ E[Ψ(p*(y ≻ y'|x))] - τ DKL(π || πref)。RLHF 和 DPO 被视为 ΨPO 在特定 Ψ 选择下的特例。ΨPO 的目的是提供一个统一的视角来分析现有的方法并探索新的优化目标。
- Identity-Preference Optimization (IPO): 身份偏好优化（IPO）是 ΨPO 框架下的一个特定实例，其中函数 Ψ 被设置为恒等函数（即 Ψ(q) = q）。这导致了一个更简单的目标函数：maxπ p*(π ≻ μ) - τ DKL(π || πref)，其中 p*(π ≻ μ) 表示策略 π 相对于参考策略 μ 的总期望偏好。IPO 的主要优点是它直接优化实际偏好（而不是通过 Bradley-Terry 模型推导的点位奖励），从而避免了因 BT 模型假设不成立或数据稀疏导致的过拟合问题，并且在理论和实验上证明了其在处理确定性或样本偏差偏好数据时比 DPO 更鲁棒。
- KL Regularization: KL 正则化（KL Regularization）是一种在策略优化中常用的技术，用于控制学习到的策略 π 与一个预定义的参考策略 πref 之间的距离。它通过在目标函数中减去策略 π 和参考策略 πref 之间的 KL 散度（Kullback-Leibler divergence）来实现，即 -τ DKL(π || πref)。KL 散度衡量了两个概率分布之间的差异。在此处，它鼓励学习到的策略 π 不要与参考策略 πref（通常是预训练模型或一个安全、保守的模型）偏离太远，这有助于防止模型漂移（model drift）、提高样本效率，并在泛化到未见过的数据时提供稳定性。τ 是一个超参数，控制正则化的强度。
- Overfitting: 过拟合（Overfitting）是指模型在训练数据上表现很好，但在未见过的新数据上表现不佳的现象。在本文的上下文中，过拟合主要体现在当使用的偏好数据（尤其是在样本量有限或偏好近乎确定时）未能充分代表真实分布时，模型（如 DPO）可能会过度拟合这些训练数据中的特定模式。例如，如果训练数据中有少量样本显示某个动作总是获胜，DPO 可能会将该动作的概率推向 1，即使这不符合 KL 正则化所期望的与参考策略保持一致的原则。论文指出，RLHF 通过训练奖励模型，并可能通过实践中的隐式或显式正则化（如奖励模型欠拟合）来缓解这个问题，而 DPO 因为不显式训练奖励模型，更容易受到过拟合的影响，尤其是在偏好近乎确定时，KL 正则化的效果会减弱。IPO 通过使用恒等函数作为 Ψ，避免了这种对 BT 模型的依赖，从而更有效地利用 KL 正则化来防止过拟合。


### 摘要

这篇论文提出了一种名为 ΨPO 的通用理论范式，用于理解从人类偏好中学习（Learning from Human Preferences）的算法，例如 RLHF 和 DPO。该范式旨在深入理解这些实际算法，并识别它们的潜在缺陷。

**现有方法的问题**

RLHF (Reinforcement Learning from Human Feedback) 和 DPO (Direct Preference Optimization) 是两种流行的从人类偏好中学习的方法。论文指出，这些方法依赖于两个重要的近似：
1.  **近似一：** 假设成对偏好（pairwise preferences）可以用点状奖励（pointwise rewards，例如 Elo score）来替代。这意味着将一个比较性的偏好信号转换为一个绝对的奖励分数。
2.  **近似二：** 假设在收集到的数据上训练的奖励模型（reward model）可以很好地泛化到策略采样的分布外数据（out-of-distribution data）。

RLHF 包含两个阶段：首先训练一个奖励模型，然后使用这个奖励模型通过强化学习来优化策略。DPO 的提出是为了绕过第二个近似，它直接从收集到的偏好数据中学习策略，而无需显式的奖励模型阶段。然而，DPO 仍然严重依赖于第一个近似。

**ΨPO 范式**

为了克服上述近似，论文提出了一种新的通用目标函数：Ψ-preference optimization (ΨPO)。
ΨPO 目标函数定义如下（Equation 6）：
$$
\max_{\pi} E_{x \sim \rho} E_{y \sim \pi(.|x)} E_{y' \sim \mu(.|x)}[\Psi(p^*(y \succ y'|x))] - \tau D_{KL}(\pi || \pi_{ref})
$$
其中：
*   $x$ 是上下文，$\rho$ 是上下文分布。
*   $y$ 和 $y'$ 是由当前策略 $\pi$ 和行为策略 $\mu$ 生成的动作（或生成内容）。
*   $p^*(y \succ y'|x)$ 是给定上下文 $x$ 时 $y$ 优于 $y'$ 的真实人类偏好概率。
*   $\Psi: [0, 1] \to R$ 是一个任意非递减函数。
*   $\tau$ 是一个正的正则化参数。
*   $D_{KL}(\pi || \pi_{ref})$ 是策略 $\pi$ 与参考策略 $\pi_{ref}$ 之间的 KL 散度，用于避免策略漂移（model drift）。

ΨPO 的核心思想是直接用成对偏好来表达目标，从而绕过了上述的两个近似。

**RLHF 和 DPO 作为 ΨPO 的特例**

论文证明了在 Bradley-Terry (BT) 模型假设成立的情况下（即存在奖励函数 $r$ 使得 $p^*(y \succ y') = \sigma(r(y) - r(y'))$，其中 $\sigma$ 是 sigmoid 函数），当 $\Psi(q) = \log(q/(1-q))$ 时，ΨPO 的最优策略与 RLHF 和 DPO 的最优策略是相同的（Proposition 1）。在这种特定情况下，最优策略 $\pi^*$ 的解析解为（Equation 7）：
$$
\pi^*(y) \propto \pi_{ref}(y) \exp\left(\tau^{-1} E_{y' \sim \mu}[\Psi(p^*(y \succ y'))]\right)
$$
这个结果表明，RLHF 和 DPO 可以被视为 ΨPO 在特定 $\Psi$ 函数和 BT 模型假设下的实例化。

**弱正则化与过拟合问题**

论文指出，当 $\Psi(q) = \log(q/(1-q))$ 时，这个函数是无界的。这导致了 RLHF 和 DPO 可能存在的过拟合问题：
*   如果存在确定性偏好，例如 $p^*(y \succ y') = 1$，那么 BT 模型会要求 $r(y) - r(y')$ 趋向于 $+\infty$。这将导致最优策略 $\pi^*$ 完全倾向于偏好的动作 $y$，使得 $\pi^*(y') = 0$，而无论 KL 正则化系数 $\tau$ 有多大。这意味着 KL 正则化的作用被大大削弱。
*   在有限数据的情况下，即使真实偏好是 $p^*(y \succ y') = 0.8$，经验估计 $\hat{p}(y \succ y')$ 也可能为 $1$，从而导致相同的过拟合问题，使得策略过度自信地排除某些动作。
*   RLHF 在实践中可能显得更鲁棒，因为它显式训练奖励函数，并且奖励函数的正则化（或其有限容量导致的欠拟合）可以间接提供策略的正则化。DPO 由于跳过奖励模型训练，失去了这一隐性正则化来源。

**IPO：Identity-Preference Optimization**

为了解决过拟合问题，论文提出了一种 ΨPO 的特殊形式，称为 Identity-Preference Optimization (IPO)。
*   **核心思想：** 将 Ψ 设置为恒等函数，即 $\Psi(q) = q$。
*   **IPO 目标函数（Equation 8）：**
    $$
    \max_{\pi} p^*_\rho(\pi \succ \mu) - \tau D_{KL}(\pi || \pi_{ref})
    $$
    这意味着直接最大化策略 $\pi$ 相对于行为策略 $\mu$ 的总偏好，同时保持与参考策略 $\pi_{ref}$ 的接近。
*   **优势：** 由于 $\Psi(q) = q$ 是有界的，即使遇到确定性偏好，KL 正则化项也能保持其有效性，从而避免策略变得过于确定性。

**IPO 的推导与高效优化**

论文进一步推导了 IPO 的离线学习目标。类似于 DPO，它将最优策略的解析形式转换为一个零点问题（root-finding problem）。对于 IPO，零点问题简化为 $h_\pi(y, y') = \tau^{-1}(p^*(y \succ \mu) - p^*(y' \succ \mu))$，其中 $h_\pi(y, y') = \log(\frac{\pi(y)\pi_{ref}(y')}{\pi(y')\pi_{ref}(y)})$。

基于此，论文提出了 IPO 的损失函数（Equation 13），并证明了其唯一的最优解（Theorem 2）：
$$
L(\pi) = E_{y,y' \sim \mu}\left[\left(h_\pi(y, y') - \frac{p^*(y \succ \mu) - p^*(y' \succ \mu)}{\tau}\right)^2\right]
$$
为了在实践中优化，论文提供了一个采样损失函数（Sampled Loss），它可以通过从偏好数据集中采样一对 $(y_w, y_l)$ 来构建，并利用了偏好数据中固有的对称性。最终的采样损失函数简化为（Equation 17）：
$$
E_{(y_w, y_l) \sim D}\left[\left(h_\pi(y_w, y_l) - \frac{1}{2\tau}\right)^2\right]
$$
这个损失函数表明 IPO 通过回归对数似然比（log-likelihood ratios）的差距来学习，始终通过控制 $\log(\pi(y_w)/\pi(y_l))$ 与 $\log(\pi_{ref}(y_w)/\pi_{ref}(y_l))$ 之间的差距来正则化其解，从而避免了对偏好数据集的过拟合。

**实验验证**

论文通过几个简单的上下文无关 bandit 示例来定性说明 IPO 和 DPO 之间的差异：
*   **渐近设置：** 在两动作场景中，DPO 总是收敛到确定性策略，即使 $\tau$ 值很大也无法有效正则化。而 IPO 则能够根据 $\tau$ 的大小在确定性策略和均匀参考策略之间平滑过渡。
*   **采样偏好：** 论文展示了 DPO 在有限数据集上过拟合的两个具体案例：
    1.  当一个动作 $y$ 在数据集中始终优于所有其他动作时，DPO 会将 $\pi(y)$ 推向 $1$，无论 $\tau$ 如何。IPO 在强正则化下可以避免这种贪婪策略。
    2.  当一个动作 $y$ 在数据集中从未获胜时，DPO 会将其概率推向 $0$，无论 $\tau$ 如何。IPO 则可以使其概率保持接近 $\pi_{ref}$，提供了更安全的行为。

**结论**

论文提出了 ΨPO 这一通用框架，统一了 RLHF 和 DPO，并揭示了它们在特定条件下潜在的过拟合问题。通过引入 IPO (Ψ 为恒等函数)，论文提供了一种理论上更稳健、且在经验上能有效避免过拟合的替代方案。IPO 的采样损失函数简洁易实现，并被证明在简单示例上优于 DPO。未来的工作将把这些发现扩展到更复杂的场景，例如大型语言模型的偏好学习。



















