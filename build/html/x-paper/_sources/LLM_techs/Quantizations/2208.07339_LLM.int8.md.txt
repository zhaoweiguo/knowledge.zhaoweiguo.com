# 2208.07339_LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale

* [https://arxiv.org/abs/2208.07339](https://arxiv.org/abs/2208.07339)
* ç»„ç»‡: University of Washington, Facebook AI Research, Hugging Face, ENS Paris-Saclay
* GitHub: [https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)


## ç›¸å…³å‚è€ƒ

* ç›¸å…³åšå®¢
    * [å¤§è§„æ¨¡ Transformer æ¨¡å‹ 8 æ¯”ç‰¹çŸ©é˜µä¹˜ç®€ä»‹ - åŸºäº Hugging Face Transformersã€Accelerate ä»¥åŠ bitsandbytes](https://huggingface.co/blog/zh/hf-bitsandbytes-integration)


## Abstract

* å¤§è¯­è¨€æ¨¡å‹é€šå¸¸éœ€è¦å¤§é‡GPUå†…å­˜è¿›è¡Œæ¨ç†ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§å«åš **LLM.int8()** çš„æ–¹æ³•ï¼ŒæŠŠæ¨¡å‹ä¸­çš„éƒ¨åˆ†çŸ©é˜µè®¡ç®—ä»16/32ä½å‹ç¼©æˆ **8ä½æ•´æ•°**ï¼Œå†…å­˜å ç”¨å‡åŠï¼Œä½†ç²¾åº¦å‡ ä¹ä¸å˜ã€‚
* å…·ä½“åšæ³•æ˜¯ï¼š
    * å¤§å¤šæ•°ç‰¹å¾ç”¨ä¸€ç§â€œé€å‘é‡é‡åŒ–â€æ–¹å¼å¤„ç†ï¼›
    * å¯¹å°‘æ•°é‡è¦ä½†éš¾ä»¥å‹ç¼©çš„â€œå¼‚å¸¸ç‰¹å¾â€ï¼Œç”¨ä¸€ç§æ··åˆç²¾åº¦æ–¹æ³•ï¼ˆä¿ç•™16ä½ï¼‰æ¥å¤„ç†ã€‚
* è¿™ä¸ªæ–¹æ³•å¯ä»¥è®©åƒ175Bå‚æ•°çš„è¶…å¤§æ¨¡å‹ï¼ˆå¦‚OPT-175B/BLOOMï¼‰åœ¨ä¸€å°é…æœ‰æ¶ˆè´¹çº§GPUçš„æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œè€Œä¸ä¼šå½±å“æ¨¡å‹æ€§èƒ½ã€‚

* å…¶ä»–
    * æœ‰ä¸¤ä¸ª Pitchã€‚
        * ä¸€ä¸ª Pitch æ˜¯å…³äºæˆ‘å¦‚ä½•ä½¿ç”¨å…ˆè¿›çš„é‡åŒ–æ–¹æ³•ï¼Œå®ç°è§„æ¨¡åŒ–ã€æ— æ€§èƒ½æŸå¤±çš„ Transformer æ¨ç†ï¼Œä»è€Œä½¿å¤§å‹æ¨¡å‹æ›´å®¹æ˜“ä¸Šæ‰‹ã€‚
        * å¦ä¸€ä¸ª Pitch è®¨è®ºäº† Transformer ä¸­æ¶Œç°çš„å¼‚å¸¸å€¼ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•å½»åº•æ”¹å˜ Transformer çš„å­¦ä¹ å†…å®¹å’Œè¿ä½œæ–¹å¼ã€‚
    * é‡åŒ–ç ”ç©¶å°±åƒæ‰“å°æœºã€‚
        * æ²¡äººå…³å¿ƒæ‰“å°æœºï¼Œä¹Ÿæ²¡äººå–œæ¬¢æ‰“å°æœºã€‚
        * ä½†å¦‚æœæ‰“å°æœºèƒ½æ­£å¸¸å·¥ä½œï¼Œæ¯ä¸ªäººéƒ½ä¼šå¾ˆé«˜å…´ã€‚




## 1. Introduction

### ğŸŒŸèƒŒæ™¯é—®é¢˜ï¼š

* å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚GPT-3ï¼‰å‚æ•°é‡å·¨å¤§ï¼Œæ¨ç†æ—¶æ¶ˆè€—å¤§é‡å†…å­˜å’Œè®¡ç®—èµ„æºã€‚
* Transformer æ¨¡å‹ä¸­ **95% çš„å‚æ•°** å’Œ **65-85%è®¡ç®—é‡** æ¥è‡ªå‰é¦ˆå±‚å’Œæ³¨æ„åŠ›æŠ•å½±å±‚ã€‚
* ä¸ºäº†å‡å°å†…å­˜ä½¿ç”¨ï¼Œç ”ç©¶è€…å°è¯•ç”¨ **8ä½é‡åŒ–**ï¼ˆå³ç”¨æ›´å°‘çš„æ¯”ç‰¹è¡¨ç¤ºæƒé‡ï¼‰ï¼Œä½†ï¼š
    * å¾€å¾€ä¼š**é™ä½æ¨¡å‹æ€§èƒ½**ï¼›
    * ä»¥å‰çš„æ–¹æ³•åªèƒ½å¤„ç† **å°äº 3.5 äº¿å‚æ•°** çš„æ¨¡å‹ï¼›
    * å¯¹äºæ•°åäº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œé‡åŒ–ä»æ˜¯ä¸ªæŒ‘æˆ˜ã€‚

### ğŸ§ è¿™ç¯‡è®ºæ–‡çš„æ ¸å¿ƒè´¡çŒ®ï¼š

* æå‡ºäº†ä¸€ç§æ–°çš„é‡åŒ–æ–¹æ³• **LLM.int8()**ï¼Œå¯åœ¨ **ä¸æŸå¤±æ€§èƒ½çš„å‰æä¸‹**ï¼ŒæŠŠ **é«˜è¾¾175Bå‚æ•°çš„æ¨¡å‹** çš„ä¸»è¦è®¡ç®—éƒ¨åˆ†ï¼ˆå‰é¦ˆå±‚å’Œæ³¨æ„åŠ›å±‚ï¼‰è½¬æ¢ä¸º 8ä½ç²¾åº¦ï¼Œä¸”ä¸æŸå¤±æ¨¡å‹æ€§èƒ½ï¼Œç›´æ¥å¯ç”¨äºæ¨ç†ã€‚

* ğŸ§ ä¸¤å¤§æŠ€æœ¯éš¾ç‚¹ï¼š
    1. **å¤§æ¨¡å‹éœ€è¦æ›´é«˜çš„é‡åŒ–ç²¾åº¦**ï¼ˆ>1B å‚æ•°æ—¶å¸¸è§„ç²¾åº¦ä¸å¤Ÿï¼‰
    2. **å¤§æ¨¡å‹ä¸­ä¼šå‡ºç°æå°‘ä½†å½±å“æå¤§çš„â€œç¦»ç¾¤ç‰¹å¾â€**ï¼Œç‰¹åˆ«æ˜¯åœ¨å‚æ•°é‡è¶…è¿‡ 6.7B æ—¶ï¼Œè¿™äº›ç‰¹å¾ä¸¥é‡å¹²æ‰°é‡åŒ–ç²¾åº¦ã€‚
* ğŸ› è§£å†³æ–¹æ³•åˆ†ä¸ºä¸¤æ­¥ï¼š
    1. å‘é‡çº§é‡åŒ–ï¼ˆVector-wise Quantizationï¼‰ï¼š
         * çŸ©é˜µä¹˜æ³•å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç³»åˆ—ç‹¬ç«‹çš„è¡Œå‘é‡å’Œåˆ—å‘é‡çš„å†…ç§¯
         * åˆ†åˆ«å¯¹æ¯ä¸ªå†…ç§¯å•ç‹¬å½’ä¸€åŒ–ï¼Œæé«˜é‡åŒ–ç²¾åº¦ï¼›
         * åœ¨æ‰§è¡Œä¸‹ä¸€ä¸ªè¿ç®—ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨åˆ—å‘é‡å’Œè¡Œå‘é‡å½’ä¸€åŒ–å¸¸æ•°çš„å¤–ç§¯è¿›è¡Œåå½’ä¸€åŒ–ï¼Œä»è€Œæ¢å¤çŸ©é˜µä¹˜æ³•çš„è¾“å‡º
         * å¯åœ¨å‚æ•°é‡ **â‰¤2.7B æ—¶ä¿æŒæ¨¡å‹æ€§èƒ½**ã€‚
    2. æ··åˆç²¾åº¦å¤„ç†ç¦»ç¾¤ç‰¹å¾ï¼ˆMixed-Precision for Outliersï¼‰ï¼š
          * åˆ†æå‘ç°ï¼š6.7B å‚æ•°åï¼Œéšè—çŠ¶æ€ä¸­ä¼šå‡ºç°æå¤§å€¼ç¦»ç¾¤ç‰¹å¾ï¼Œé›†ä¸­åœ¨ 6 ä¸ªç‰¹å¾ç»´åº¦ï¼›
          * è§£å†³åŠæ³•ï¼š**å¯¹è¿™æå°‘çš„ç¦»ç¾¤ç»´åº¦ä½¿ç”¨ 16 ä½ä¹˜æ³•**ï¼Œå…¶ä½™ 99.9% çš„ç»´åº¦ä»ä½¿ç”¨ 8 ä½ä¹˜æ³•ï¼›
          * è¿™æ ·æ—¢ä¿ç•™ç²¾åº¦ï¼Œä¹ŸèŠ‚çœå†…å­˜å’Œè®¡ç®—ã€‚

![](https://img.zhaoweiguo.com/uPic/2025/06/rtDkl2.jpg)

Figure 2 illustrates the workflow of LLM.int8(). Given 16-bit floating-point inputs $\mathbf{X}{f16}$ and weights $\mathbf{W}{f16}$, the features and weights are first decomposed into sub-matrices: one for large-magnitude (outlier) features and another for all remaining values. The outlier matrices are processed using standard 16-bit matrix multiplication to preserve precision. For the remaining values, 8-bit vector-wise quantization is applied. This involves computing the row-wise and column-wise absolute maximums, denoted as $\mathbf{C}_x$ and $\mathbf{C}w$, which are used to scale (normalize) the input and weight matrices. These scaled values are then quantized to Int8 and used in Int8 matrix multiplication, resulting in Int32 outputs $\mathbf{Out}{i32}$. These outputs are then dequantized using the outer product of the normalization constants, $\mathbf{C}_x \otimes \mathbf{C}_w$. Finally, both the 16-bit results from the outlier paths and the dequantized results from the 8-bit paths are accumulated into 16-bit floating-point outputs.




### ğŸ”ç¦»ç¾¤ç‰¹å¾çš„å½±å“ï¼š

* è™½ç„¶åªå  0.1% çš„ç‰¹å¾ï¼Œä½†å¯¹æ¨¡å‹è¾“å‡ºå½±å“æå¤§ï¼›
* å»æ‰è¿™äº›ç‰¹å¾ä¼šä½¿æ³¨æ„åŠ›çš„ softmax ç»“æœä¸‹é™ 20%+ï¼ŒéªŒè¯é›†å›°æƒ‘åº¦ï¼ˆperplexityï¼‰æ¶åŒ– 600%-1000%ï¼›
* è€Œéšæœºå»æ‰åŒæ ·æ•°é‡çš„æ™®é€šç‰¹å¾ï¼Œå½±å“å‡ ä¹å¯ä»¥å¿½ç•¥ã€‚

```note
æ€»ç»“ï¼šè¿™é¡¹å·¥ä½œæˆåŠŸåœ¨ä¸æŸå¤±ç²¾åº¦çš„å‰æä¸‹ï¼Œå°†è¶…å¤§æ¨¡å‹é‡åŒ–ä¸º 8 ä½ç²¾åº¦ï¼Œå¯æ˜¾è‘—é™ä½éƒ¨ç½²æˆæœ¬ï¼Œå¹¶é¦–æ¬¡è®©å¦‚ OPT-175B ç­‰æ¨¡å‹å¯åœ¨å•ä¸ªæ¶ˆè´¹çº§ GPU æœåŠ¡å™¨ä¸Šè¿è¡Œã€‚
```


### ğŸ“Šå®éªŒè¯æ˜ï¼š

* LLM.int8() ä¿æŒäº†ä¸ 16ä½åŸæ¨¡å‹ç›¸åŒçš„ç²¾åº¦ï¼›
* åœ¨ OPTã€GPT-3ã€BLOOM ç­‰å¤§æ¨¡å‹ä¸Šæµ‹è¯•è¡¨ç°ä¼˜å¼‚ï¼›
* å¯åœ¨æ¶ˆè´¹çº§æ˜¾å¡ä¸Šéƒ¨ç½² 175B è§„æ¨¡æ¨¡å‹ï¼›
* æ¨ç†é€Ÿåº¦ä¹Ÿæ²¡æœ‰å˜æ…¢ã€‚


## 2. Background

### ğŸ“Œ **ç ”ç©¶ç›®çš„ï¼š**

* æœ¬æ–‡ç ”ç©¶åœ¨å¤§æ¨¡å‹ï¼ˆTransformerï¼‰ä¸­ï¼Œ**é‡åŒ–æŠ€æœ¯ï¼ˆquantizationï¼‰åœ¨ä»€ä¹ˆè§„æ¨¡ä¸‹ä¼šå¤±æ•ˆ**ï¼Œä»¥åŠ**å¤±è´¥çš„åŸå› ä¸ç²¾åº¦ä¹‹é—´çš„å…³ç³»**ã€‚
* å¯¹æ¯”ä¸¤ç§å¸¸ç”¨çš„ **8bit é‡åŒ–æ–¹æ³•**ï¼šå¯¹ç§°é‡åŒ–ï¼ˆAbsmaxï¼‰å’Œéå¯¹ç§°é‡åŒ–ï¼ˆZeropointï¼‰ã€‚


### ğŸ”¹ **1. absmax(å¯¹ç§°é‡åŒ–)**

* absmax: absolute maximum quantization
* å°†æ•´ä¸ªå¼ é‡çš„æœ€å¤§ç»å¯¹å€¼è®¾ä¸º127ï¼Œç¼©æ”¾æ‰€æœ‰å€¼è¿›8bitèŒƒå›´ \[-127, 127]ã€‚
* ç®—æ³•ç®€å•ï¼Œç¡¬ä»¶å‹å¥½ï¼Œæ˜¯ç›®å‰ä½¿ç”¨æœ€å¹¿çš„æ–¹æ³•ã€‚
* ç¼ºç‚¹ï¼šå¦‚æœè¾“å…¥æ•°æ®åˆ†å¸ƒåå‘ä¸€ä¾§ï¼ˆå¦‚ReLUè¾“å‡ºéƒ½æ˜¯æ­£æ•°ï¼‰ï¼Œä¼šæµªè´¹ä¸€åŠçš„æ•°å€¼èŒƒå›´ã€‚

### ğŸ”¹ **2. zeropoint(éå¯¹ç§°é‡åŒ–)**

* zeropoint: Zeropoint Quantization
* ç”¨ç¼©æ”¾å› å­ï¼ˆndxï¼‰åŠ ä¸Šåç§»å€¼ï¼ˆzeropoint zpï¼‰æ¥çº¿æ€§æ˜ å°„æ•´ä¸ªå¼ é‡ï¼Œä½¿å…¶**å……åˆ†åˆ©ç”¨ \[-127,127] æ•´ä¸ªèŒƒå›´**ã€‚
* ä¼˜ç‚¹ï¼šæ›´é«˜ç²¾åº¦ï¼Œç‰¹åˆ«é€‚åˆéå¯¹ç§°æ•°æ®ï¼ˆå¦‚ReLUè¾“å‡ºï¼‰ã€‚
* ç¼ºç‚¹ï¼šå®ç°å¤æ‚ï¼Œéœ€è¦é¢å¤–çš„è®¡ç®—ï¼Œæ¯”å¦‚åŠ ä¸Š zeropointï¼Œé€šå¸¸æ¯” absmax æ…¢ï¼Œå°¤å…¶æ˜¯åœ¨æ²¡æœ‰ä¸“ç”¨æŒ‡ä»¤çš„ç¡¬ä»¶ä¸Šï¼ˆå¦‚ GPU/TPUï¼‰ã€‚

* è¯´æ˜
    * é›¶ç‚¹é‡åŒ–åˆ†ä¸ºä¸¤æ­¥
        * ç¬¬ä¸€æ­¥å€¼åŸŸæ˜ å°„ï¼Œå³é€šè¿‡ç¼©æ”¾å°†åŸå§‹çš„æ•°å€¼èŒƒå›´æ˜ å°„ä¸ºé‡åŒ–åçš„æ•°å€¼èŒƒå›´
        * ç¬¬äºŒæ­¥é›¶ç‚¹è°ƒæ•´ï¼Œå³é€šè¿‡å¹³ç§»å°†æ˜ å°„åçš„æ•°æ®çš„æœ€å°å€¼å¯¹é½ä¸ºç›®æ ‡å€¼åŸŸçš„æœ€å°å€¼
    * å…·ä½“ zeropoint å‚è§ **0normal**

![](https://img.zhaoweiguo.com/uPic/2025/06/tsTILp.png)

from: [è¿™å„¿](https://intellabs.github.io/distiller/algo_quantization.html)


### ç¤ºä¾‹

* å‡è®¾ä½ è¦ç”¨ absmax å¯¹å‘é‡ [1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4] è¿›è¡Œé‡åŒ–ã€‚
    * é¦–å…ˆéœ€è¦è®¡ç®—è¯¥å‘é‡å…ƒç´ çš„æœ€å¤§ç»å¯¹å€¼ï¼Œåœ¨æœ¬ä¾‹ä¸­ä¸º 5.4ã€‚ 
    * Int8 çš„èŒƒå›´ä¸º [-127, 127]ï¼Œå› æ­¤æˆ‘ä»¬å°† 127 é™¤ä»¥ 5.4ï¼Œå¾—åˆ°ç¼©æ”¾å› å­ 23.5ã€‚
    * æœ€åï¼Œå°†åŸå§‹å‘é‡ä¹˜ä»¥ç¼©æ”¾å› å­å¾—åˆ°æœ€ç»ˆçš„é‡åŒ–å‘é‡ [28, -12, -101, 28, -73, 19, 56, 127]ã€‚
* è¦æ¢å¤åŸå‘é‡ï¼Œå¯ä»¥å°† int8 é‡åŒ–å€¼é™¤ä»¥ç¼©æ”¾å› å­ï¼Œä½†ç”±äºä¸Šé¢çš„è¿‡ç¨‹æ˜¯â€œå››èˆäº”å…¥â€çš„ï¼Œæˆ‘ä»¬å°†ä¸¢å¤±ä¸€äº›ç²¾åº¦ã€‚

![](https://img.zhaoweiguo.com/uPic/2025/06/zR47oa.png)

* å¯¹äºæ— ç¬¦å· Int8ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆå‡å»æœ€å°å€¼ç„¶åå†ç”¨æœ€å¤§ç»å¯¹å€¼æ¥ç¼©æ”¾ï¼Œè¿™ä¸é›¶ç‚¹é‡åŒ–çš„åšæ³•ç›¸ä¼¼ã€‚
    * å…¶åšæ³•ä¹Ÿä¸æœ€å° - æœ€å¤§ç¼©æ”¾ (min-max scaling) ç±»ä¼¼ï¼Œä½†åè€…åœ¨ç¼©æ”¾æ—¶ä¼šé¢å¤–ä¿è¯è¾“å…¥ä¸­çš„ 0 å§‹ç»ˆæ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°ï¼Œä»è€Œä¿è¯ 0 çš„é‡åŒ–æ˜¯æ— è¯¯å·®çš„ã€‚





### ğŸ§® **çŸ©é˜µä¹˜æ³•ä¸­çš„ä½¿ç”¨æ–¹å¼**

* å…ˆæŠŠè¾“å…¥ï¼ˆXï¼‰å’Œæƒé‡ï¼ˆWï¼‰ä» 16-bit æµ®ç‚¹é‡åŒ–ä¸º 8-bit æ•´æ•°ã€‚
* ç„¶ååœ¨ Int8/Int16/Int32 ä¸Šåšä¹˜æ³•å’ŒåŠ æ³•ã€‚
* æœ€åç”¨ç¼©æ”¾å› å­æŠŠç»“æœè¿˜åŸï¼ˆdequantizeï¼‰ä¸º 16-bit æµ®ç‚¹ã€‚

### ğŸ§  æ ¸å¿ƒç»“è®ºé¢„ç¤ºï¼š

* é‡åŒ–ç­–ç•¥çš„æ•ˆæœéšç€æ¨¡å‹è§„æ¨¡å¯èƒ½ä¼šå˜åŒ–ã€‚
* è™½ç„¶ absmax æ›´æ˜“éƒ¨ç½²ï¼Œä½†åœ¨å¤§æ¨¡å‹ä¸­å¯èƒ½éœ€è¦è€ƒè™‘ zeropoint ä»¥æé«˜ç²¾åº¦ã€‚
* ç ”ç©¶çš„é‡ç‚¹æ˜¯ç†è§£è¿™ä¸¤ç§é‡åŒ–åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸‹çš„é€‚ç”¨æ€§ã€‚


## 3. Int8 Matrix Multiplication at Scale

### ğŸŒŸæ ¸å¿ƒé—®é¢˜

* ä¼ ç»Ÿé‡åŒ–æ–¹æ³•ï¼ˆä¾‹å¦‚ä¸€ä¸ªå¼ é‡ç”¨ä¸€ä¸ªç¼©æ”¾å› å­ï¼‰åœ¨é‡åˆ°å°‘é‡**å¼‚å¸¸å€¼ï¼ˆoutliersï¼‰** æ—¶ï¼Œä¼šä¸¥é‡å½±å“æ•´ä½“ç²¾åº¦ã€‚

### âœ…è§£å†³æ–¹æ¡ˆï¼šLLM.int8() æ–¹æ³•

1. **å‘é‡çº§é‡åŒ–ï¼ˆVector-wise Quantizationï¼‰**
      * ä¸å†ä½¿ç”¨å•ä¸€ç¼©æ”¾å› å­ï¼Œè€Œæ˜¯ï¼š
          * æ¯è¡Œè¾“å…¥çŸ©é˜µç”¨ä¸€ä¸ªç¼©æ”¾å› å­
          * æ¯åˆ—æƒé‡çŸ©é˜µç”¨ä¸€ä¸ªç¼©æ”¾å› å­
      * è¿™æ ·å¯ä»¥æ›´ç»†ç²’åº¦åœ°æ§åˆ¶ç¼©æ”¾ï¼Œæå‡é‡åŒ–ç²¾åº¦ã€‚

2. **æ··åˆç²¾åº¦åˆ†è§£ï¼ˆMixed-precision Decompositionï¼‰**
      * å¯¹äºéå¸¸ç¨€æœ‰ä½†æ•°å€¼å¤§çš„ç‰¹å¾ç»´åº¦ï¼ˆ< 0.1%ï¼‰ï¼Œä»ä½¿ç”¨16-bitç²¾åº¦
      * å…¶ä½™99.9%çš„ç»´åº¦ä½¿ç”¨8-bitè¿›è¡Œä¹˜æ³•è®¡ç®—
      * è¿™æ ·æ—¢ä¿ç•™ç²¾åº¦ï¼ŒåˆèŠ‚çœå†…å­˜ï¼ˆçº¦å‡å°‘50%ï¼‰

### ğŸ“Šå®éªŒ

![](https://img.zhaoweiguo.com/uPic/2025/06/oyKbET.jpg)

Table 1: å¯¹ä¸åŒ Transformer æ¨¡å‹è§„æ¨¡ï¼ˆä» 125M åˆ° 13Bï¼‰ ä½¿ç”¨ä¸åŒ é‡åŒ–æ–¹æ³• åçš„æ•ˆæœæ¯”è¾ƒï¼Œä½¿ç”¨çš„æ˜¯ C4 éªŒè¯é›†çš„ perplexity ä½œä¸ºè¡¡é‡æŒ‡æ ‡

* åœ¨ Transformer æ¨¡å‹å‚æ•°è§„æ¨¡ä» 125M æ‰©å±•åˆ° 13B æ—¶ï¼Œæ™®é€šçš„ Int8 é‡åŒ–ç­–ç•¥ï¼ˆå¦‚ absmaxã€row-wiseã€zeropointï¼‰ä¼šå› æ— æ³•è¡¨ç¤ºå¤§å¹… outlier ç‰¹å¾è€Œå¯¼è‡´æ˜¾è‘—çš„æ€§èƒ½ä¸‹é™ã€‚å°¤å…¶åœ¨ 13B æ¨¡å‹ä¸­ï¼Œé‡åŒ–åçš„ perplexity ç”šè‡³æ¯”æ›´å°çš„ 6.7B æ¨¡å‹è¿˜å·®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLLM.int8() ä½¿ç”¨æ··åˆç²¾åº¦æ–¹æ³•ï¼Œä»…ç”¨æå°‘æ•° Float16 ç»´åº¦è¡¨è¾¾ outlierï¼Œå…¶ä½™ç»´åº¦é‡‡ç”¨ Int8ï¼Œä»è€Œåœ¨ä¿æŒè®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œå®Œå…¨æ¢å¤äº†åŸå§‹æ¨¡å‹çš„ perplexityã€‚è¿™è¯æ˜äº†è¯¥æ–¹æ³•åœ¨å¤§æ¨¡å‹é‡åŒ–ä¸­çš„å¯æ‰©å±•æ€§ä¸æœ‰æ•ˆæ€§ã€‚
* 125Mé‡åŒ–åæ€§èƒ½ä¸‹é™å¤§çš„åŸå› ï¼š125M å‚æ•°è§„æ¨¡çš„æ¨¡å‹è¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼Œæ¯ä¸€ä½ç²¾åº¦çš„æŸå¤±å¯¹æ•´ä½“æ€§èƒ½å½±å“æ›´å¤§ã€‚å¤§æ¨¡å‹ï¼ˆå¦‚ 6.7Bã€13Bï¼‰è™½ç„¶æœ‰æ›´å¤š outlierï¼Œä¹Ÿæ›´éš¾é‡åŒ–ï¼Œä½†å®ƒä»¬å†—ä½™åº¦é«˜ï¼Œé‡åŒ–æ—¶ä¸¢ç‚¹ç²¾åº¦ï¼Œæ¨¡å‹å¯ä»¥è‡ªæˆ‘å¼¥è¡¥ã€‚å°æ¨¡å‹ï¼ˆå¦‚125Mï¼‰æœ¬èº«å†—ä½™å°‘ã€å®¹é”™ä½ã€è¡¨è¾¾èƒ½åŠ›å¼±ï¼Œæ‰€ä»¥å³ä¾¿é‡åŒ–ä¸­ outlier ä¸å¤šï¼Œä¹Ÿå› æ•´ä½“é‡åŒ–è¯¯å·®æ¯”ä¾‹è¾ƒå¤§ã€æŠ—å¹²æ‰°èƒ½åŠ›å·®è€Œå‡ºç°æ›´å¤§çš„æ€§èƒ½ä¸‹é™ã€‚




* å®éªŒè®¾ç½®ï¼š
    * åœ¨å¤šä¸ªå¤§æ¨¡å‹ï¼ˆ125M ~ 175Bå‚æ•°ï¼‰ä¸Šè¯„ä¼°æ–¹æ³•æ•ˆæœ
    * æŒ‡æ ‡åŒ…æ‹¬è¯­è¨€å»ºæ¨¡å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰å’ŒZero-shotè¡¨ç°
* ç»“æœï¼š
    * **ä¼ ç»Ÿæ–¹æ³•æ•ˆæœéšæ¨¡å‹å˜å¤§è€Œæ€¥å‰§ä¸‹é™**
    * **LLM.int8()** æ˜¯**å”¯ä¸€èƒ½éšç€æ¨¡å‹è§„æ¨¡æ‰©å±•ä»ä¿æŒæ€§èƒ½**çš„æ–¹æ³•


### æ€»ç»“

* LLM.int8() = å‘é‡çº§é‡åŒ– + æ··åˆç²¾åº¦åˆ†è§£
* èƒ½å¤§å¹…**èŠ‚çœå†…å­˜**ï¼ˆæœ€å¤§çº¦1.96xï¼‰
* æ˜¯ç›®å‰å”¯ä¸€åœ¨å¤§æ¨¡å‹ä¸Šä»ä¿æŒå‡†ç¡®æ€§çš„8-bité‡åŒ–æ–¹æ³•
* æ¨ç†é€Ÿåº¦ï¼š
    * ä¸ FP16 åŸºå‡†ç›¸æ¯”
    * å¯¹äºå‚æ•°å°‘äº 67 äº¿çš„æ¨¡å‹ï¼Œé‡åŒ–å¼€é”€å¯èƒ½ä¼šé™ä½æ¨ç†é€Ÿåº¦ã€‚
    * å¯¹äºç›¸å½“äº 1750 äº¿æ¨¡å‹çš„å¤§å‹çŸ©é˜µä¹˜æ³•ï¼ŒLLM.int8() çš„è¿è¡Œæ—¶é—´å¤§çº¦å¿«ä¸¤å€ã€‚


## 4. Emergent Large Magnitude Features in Transformers at Scale

### å®šä¹‰-ä»€ä¹ˆæ˜¯â€œOutlier Featureâ€ï¼Ÿ

* Transformer æ¨¡å‹ä¸­çš„ä¸­é—´è¡¨ç¤ºï¼ˆhidden stateï¼‰æ˜¯ä¸€ä¸ªäºŒç»´å¼ é‡ï¼š $\mathbf{X} \in \mathbb{R}^{s \times h}$
* å…¶ä¸­ï¼š
    * $s$ï¼šåºåˆ—é•¿åº¦ï¼ˆtokens æ•°é‡ï¼‰
    * $h$ï¼šç‰¹å¾ç»´åº¦ï¼ˆæ¯ä¸ª token çš„éšè—å‘é‡ç»´åº¦ï¼‰
* â€œ**Outlier Feature**â€ æŒ‡çš„æ˜¯ï¼šåœ¨æŸä¸€ç»´åº¦ $h_i$ ä¸Šï¼Œå®ƒçš„å€¼éå¸¸å¤§ï¼ˆæ¯”å¦‚ç»å¯¹å€¼ â‰¥ 6ï¼‰ï¼Œè€Œä¸”ï¼š
    * å‡ºç°åœ¨è‡³å°‘ **25% çš„ Transformer å±‚**ï¼ˆæ¯”å¦‚æœ‰ 100 å±‚ï¼Œæœ‰ 25 å±‚è¯¥ç»´åº¦ä¸Šæœ‰è¿™ç§å¤§å€¼ï¼‰
    * å¹¶ä¸”åœ¨è‡³å°‘ **6% çš„ tokenï¼ˆåºåˆ—ç»´åº¦ï¼‰** ä¸Šéƒ½å‡ºç°è¿‡
* ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ª **éå¸¸æ˜¾è‘—ã€è·¨å±‚ä¸€è‡´å‡ºç°çš„å¼‚å¸¸ç‰¹å¾ç»´åº¦**ã€‚


### è¯´æ˜

* éšç€ Transformer æ¨¡å‹è§„æ¨¡å¢å¤§ï¼Œä¼šå‡ºç°ä¸€äº› **ç‰¹å¾ç»´åº¦ä¸Šçš„â€œå¼‚å¸¸å€¼â€**ï¼ˆmagnitude å¾ˆå¤§ï¼‰ï¼Œå®ƒä»¬ï¼š
    * **å½±å“èŒƒå›´å¹¿**ï¼šå‡ºç°åœ¨å‡ ä¹æ‰€æœ‰å±‚ã€å½±å“å¤§éƒ¨åˆ† tokenã€‚
    * **æ•°é‡è™½å¤šä½†é›†ä¸­**ï¼šä¾‹å¦‚ä¸€ä¸ª 13B å‚æ•°çš„æ¨¡å‹ä¸­ï¼Œ2048 token çš„åºåˆ—å¯èƒ½æœ‰ 15 ä¸‡ä¸ªå¼‚å¸¸å€¼ï¼Œä½†å®ƒä»¬åªé›†ä¸­åœ¨æœ€å¤š **7 ä¸ªç‰¹å¾ç»´åº¦**ã€‚


### è¿™äº› Outlier Feature æ˜¯æ€ä¹ˆå‡ºç°çš„ï¼Ÿ

* ç ”ç©¶å‘ç°ï¼š**æ¨¡å‹å‚æ•°è¶Šå¤§ï¼Œè¿™ç§ Outlier Feature å°±è¶Šå®¹æ˜“å‡ºç°ï¼Œå¹…åº¦ä¹Ÿè¶Šå¤§**ã€‚è¿™ç§ç°è±¡è¢«ç§°ä¸ºâ€œæ¶Œç°â€ï¼ˆemergenceï¼‰ã€‚
* æœ‰ä¸¤ç§åˆ†ææ–¹å¼ï¼š
    * 1. ä»æ¨¡å‹è§„æ¨¡æ¥çœ‹ï¼ˆå‚æ•°é‡ï¼‰ï¼š
        * 6B å‚æ•°å‰åï¼Œçªç„¶å¼€å§‹å‡ºç°å¼‚å¸¸å€¼
        * æ¯”å¦‚æ¨¡å‹ä» 6B åˆ° 6.7Bï¼Œå‡ºç° outlier çš„å±‚æ•°ä» 65% â†’ 100%
        * å¯¹åº” token çš„æ•°é‡ä» 35% â†’ 75%
        * è¿™æ˜¯ä¸€ä¸ª**ç›¸å½“å‰§çƒˆçš„è·ƒè¿** â†’ ç±»ä¼¼ç‰©ç†ä¸­çš„â€œç›¸å˜â€
    * 2. ä»æ¨¡å‹æ•ˆæœæ¥çœ‹ï¼ˆperplexityï¼‰ï¼š
        * **Perplexity è¶Šä½ï¼ˆæ¨¡å‹æ€§èƒ½è¶Šå¥½ï¼‰ï¼Œoutlier å‡ºç°å¾—è¶Šå¤š**
        * ä½†è¿™ä¸ªè¿‡ç¨‹æ˜¯**å¹³æ»‘å˜åŒ–**çš„ï¼Œä¸åƒå‚æ•°é‡é‚£æ ·çªç„¶
* ç»“è®ºï¼šoutlier çš„å‡ºç°æ—¢ä¸å‚æ•°é‡ç›¸å…³ï¼Œä¹Ÿä¸è®­ç»ƒè´¨é‡ï¼ˆå¦‚æ•°æ®é‡ã€æ•°æ®å¥½åï¼‰ç›¸å…³ã€‚

### ä¸ºä»€ä¹ˆ Outlier ä¼šå¸¦æ¥é—®é¢˜ï¼Ÿ

* å¯¹æ¨¡å‹æ€§èƒ½å½±å“ï¼š
    * è¿™äº› Outlier ç‰¹å¾ **æåº¦é‡è¦**ï¼Œå³ä½¿åªæœ‰ 6~7 ä¸ªç‰¹å¾ç»´åº¦ï¼š
      * è‹¥ä½ æŠŠè¿™äº›ç»´åº¦è®¾ä¸º 0ï¼Œtop-1 å‡†ç¡®ç‡ä» 40% é™åˆ° 20%
      * éªŒè¯é›† perplexity å¢åŠ  600%-1000%
      * è¯´æ˜æ¨¡å‹**å¼ºçƒˆä¾èµ–è¿™äº›ç‰¹å¾**
* å¯¹é‡åŒ–çš„å½±å“ï¼ˆå°¤å…¶ Int8ï¼‰ï¼š
    * å¸¸è§„çš„ Int8 é‡åŒ–ï¼ˆå¦‚ row-wise æˆ– vector-wiseï¼‰ï¼š
      * å‡è®¾æ¯ä¸€è¡Œå•ç‹¬ç¼©æ”¾ï¼Œä½† outlier æ˜¯åˆ—æ–¹å‘çš„ï¼ˆå³ç»´åº¦æ–¹å‘ hï¼‰
      * å¯¼è‡´å¾ˆå¤šå€¼éƒ½è¢«â€œæŒ¤å‹â€åœ¨ä¸€èµ·ï¼Œå°å€¼å˜ 0ï¼Œå¤§å€¼ä¿ä¸ä½ â†’ ä¸¢å¤±ä¿¡æ¯
    * **å°¤å…¶ Int8 çš„åŠ¨æ€èŒƒå›´æœ‰é™ï¼ˆâˆ’127,127ï¼‰**
      * outlier çš„å­˜åœ¨ä¼šæ’‘çˆ†åˆ†å¸ƒï¼Œé€ æˆå…¶ä»–æ­£å¸¸å€¼ç²¾åº¦ä¸‹é™
      * ä¾‹å¦‚å½“å€¼è¾¾åˆ° 60ï¼Œå†ä¹˜ä¸€ä¸ªçŸ©é˜µï¼Œå¾ˆå®¹æ˜“è¶…å‡º Int16/Float16 çš„æœ€å¤§è¡¨ç¤ºèŒƒå›´ï¼ˆ65535ï¼‰
      * å³ä½¿æ˜¯å¯¹ç§°é‡åŒ–ï¼ˆå¦‚ row-wise/vector-wiseï¼‰ä¹Ÿéš¾ä»¥å¤„ç†è¿™äº›å¼‚å¸¸å€¼ï¼Œå› ä¸ºå®ƒä»¬å‡ºç°åœ¨ feature ç»´åº¦ï¼Œä¸æ˜¯åºåˆ—ç»´åº¦ã€‚

### **åº”å¯¹æ–¹æ³•ï¼š**

1. **Zeropoint é‡åŒ–**ï¼ˆéå¯¹ç§°ï¼‰
      * ä¸€ç§**éå¯¹ç§°é‡åŒ–**æ–¹æ³•ï¼Œå¯ä»¥å¯¹åç§»å€¼åšè¡¥å¿
      * é€‚åˆ outlier çš„åˆ†å¸ƒï¼ˆoutlier å¾€å¾€æ˜¯æ­£æˆ–è´Ÿï¼Œä¸æ˜¯å¯¹ç§°åˆ†å¸ƒï¼‰
      * å¯ä»¥ç¼“è§£ä¸€äº›é—®é¢˜
2. **LLM.int8() æå‡ºçš„ Mixed-Precision Decomposition æ–¹æ³•**ï¼š
   * æŠŠéšè—çŠ¶æ€åˆ†æˆï¼š
       * ä¸»ä½“éƒ¨åˆ†ï¼ˆç”¨ Int8ï¼‰
       * Outlier ç‰¹å¾éƒ¨åˆ†ï¼ˆä¿ç•™ä¸º Float16ï¼‰
   * åªç”¨æå°‘é‡çš„æµ®ç‚¹æ•°ï¼ˆæ¯”å¦‚ä¿ç•™ 6 ä¸ªç»´åº¦ï¼‰ï¼Œå°±èƒ½æ¢å¤å…¨éƒ¨ç²¾åº¦
   * **æ•ˆæœå¥½å¾—å¤š**ï¼Œè€Œä¸”æå¤§èŠ‚çœå†…å­˜å’Œç®—åŠ›

### ç¤ºä¾‹

* å‡è®¾ä½ æœ‰ä¸€ä¸ª Transformerï¼Œæ¯ä¸€å±‚çš„æŸä¸€ç»´åº¦ï¼ˆå¦‚ç¬¬ 1234 ç»´ï¼‰çªç„¶åœ¨å¾ˆå¤š token ä¸Šå€¼å˜æˆäº† 60ï¼ˆæ­£å¸¸å€¼å¯èƒ½æ˜¯ -1 åˆ° 1 ä¹‹é—´ï¼‰ï¼Œè¿™æ—¶ï¼š
* å¦‚æœä½ ç›´æ¥åš Int8 é‡åŒ–ï¼š
    * ä½ å¾—ä¸ºè¿™ä¸ª 60 ç•™å‡ºç©ºé—´
    * å¯¼è‡´å…¶ä»–ç»å¤§å¤šæ•°çš„å°å€¼ï¼ˆæ¯”å¦‚ 0.2ï¼‰ä¼šè¢«é‡åŒ–æˆ 0ï¼Œä¿¡æ¯ä¸¢å¤±
* å¦‚æœä½ ç”¨æ··åˆç²¾åº¦å¤„ç†ï¼š
    * æŠŠç¬¬ 1234 ç»´å•ç‹¬æå‡ºæ¥ï¼Œç”¨ Float16 è¡¨ç¤º
    * å…¶ä»–ç»´åº¦è¿˜æ˜¯ Int8ï¼Œå°±èƒ½ä¿ä½ç²¾åº¦åˆèŠ‚çœè®¡ç®—

![](https://img.zhaoweiguo.com/uPic/2025/06/Mixed-int8.gif)

* LLM.int8() é€šè¿‡ä¸‰ä¸ªæ­¥éª¤å®ŒæˆçŸ©é˜µä¹˜æ³•è®¡ç®—:
    * ä»è¾“å…¥çš„éšå«çŠ¶æ€ä¸­ï¼ŒæŒ‰åˆ—æå–å¼‚å¸¸å€¼ (å³å¤§äºæŸä¸ªé˜ˆå€¼çš„å€¼)ã€‚
    * å¯¹ FP16 ç¦»ç¾¤å€¼çŸ©é˜µå’Œ Int8 éç¦»ç¾¤å€¼çŸ©é˜µåˆ†åˆ«ä½œçŸ©é˜µä¹˜æ³•ã€‚
    * åé‡åŒ–éç¦»ç¾¤å€¼çš„çŸ©é˜µä¹˜ç»“æœå¹¶å…¶ä¸ç¦»ç¾¤å€¼çŸ©é˜µä¹˜ç»“æœç›¸åŠ ï¼Œè·å¾—æœ€ç»ˆçš„ FP16 ç»“æœã€‚



## 5. Related Work

1. **8ä½æ•°æ®ç±»å‹ï¼ˆInt8 å’Œ FP8ï¼‰**
    * ç›®å‰GPUåªæ”¯æŒ Int8 ç±»å‹ã€‚
    * è™½ç„¶ FP8ï¼ˆæµ®ç‚¹8ä½ï¼‰ç†è®ºä¸Šæ›´é€‚åˆæŸäº›æ•°æ®åˆ†å¸ƒï¼Œä½†ç¡¬ä»¶è¿˜ä¸æ”¯æŒã€‚
    * FP8 å¯¹å°æ•°å€¼å¾ˆç²¾ç¡®ï¼Œä½†å¯¹å¤§æ•°å€¼è¯¯å·®å¤§ã€‚

2. **è¯­è¨€æ¨¡å‹ä¸­çš„å¼‚å¸¸å€¼ï¼ˆOutliersï¼‰**
    * æœ‰ç ”ç©¶æŒ‡å‡ºï¼Œå¼‚å¸¸å€¼ä¸ LayerNorm å’Œè¯é¢‘åˆ†å¸ƒæœ‰å…³ã€‚
    * æˆ‘ä»¬åœ¨æ­¤åŸºç¡€ä¸Šè¿›ä¸€æ­¥è¯´æ˜ï¼šæ¨¡å‹è¶Šå¤§ï¼Œå¼‚å¸¸å€¼è¶Šçªå‡ºï¼Œå› æ­¤åœ¨é‡åŒ–æ—¶è¦ç‰¹åˆ«å»ºæ¨¡è¿™äº›å¼‚å¸¸å€¼æ‰èƒ½ä¿æŒæ€§èƒ½ã€‚

3. **å¤§è§„æ¨¡æ¨¡å‹çš„é‡åŒ–æ–¹æ³•**
      * nuQmm å’Œ ZeroQuant æ˜¯ä¸æˆ‘ä»¬åŒæ—¶æœŸæå‡ºçš„é«˜ç²¾åº¦é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨æ›´ç»†ç²’åº¦çš„â€œgroup-wiseâ€é‡åŒ–ï¼Œéœ€è‡ªå®šä¹‰CUDAå†…æ ¸ã€‚
      * ZeroQuant æˆåŠŸå°† 20B æ¨¡å‹åšåˆ°äº†æ— æ€§èƒ½ä¸‹é™çš„8ä½é‡åŒ–ã€‚
      * æˆ‘ä»¬çš„æ–¹æ³•èƒ½æ”¯æŒåˆ° 176B å‚æ•°æ¨¡å‹çš„æ— æŸé‡åŒ–ï¼Œé‡ç‚¹åœ¨ä¿æŒæ¨¡å‹å‡†ç¡®æ€§ã€‚
      * GLM-130B åˆ™ç»“åˆæˆ‘ä»¬çš„æ€è·¯ï¼Œç”¨8ä½å­˜å‚¨+16ä½è®¡ç®—ä¹Ÿå®ç°äº†æ— æŸé‡åŒ–ã€‚


## 6. Discussion and Limitations

* æˆ‘ä»¬é¦–æ¬¡è¯æ˜äº†ï¼šè¶…å¤§å‚æ•°é‡ï¼ˆæ¯”å¦‚ 175Bï¼‰çš„ Transformer æ¨¡å‹å¯ä»¥é‡åŒ–ä¸º Int8ï¼Œå¹¶åœ¨æ¨ç†æ—¶å‡ ä¹ä¸æŸå¤±æ€§èƒ½ã€‚æ–¹æ³•æ˜¯ï¼š
    * **å…³é”®åšæ³•**ï¼šç”¨æ··åˆç²¾åº¦ï¼ˆæŠŠç‰¹å¼‚å€¼åˆ†ç¦»å‡ºæ¥ç”¨16ä½è®¡ç®—ï¼‰+ å‘é‡çº§é‡åŒ–ï¼Œå¾—åˆ°ä¸€ç§å« **LLM.int8()** çš„æ–¹æ³•ã€‚
    * **ç»“æœ**ï¼šè¿™ä¸ªæ–¹æ³•å¯ä»¥ä¿æŒæ¨¡å‹åŸæœ‰çš„æ¨ç†èƒ½åŠ›ã€‚

* **å±€é™æ€§æœ‰å››ç‚¹ï¼š**
    1. åªç ”ç©¶äº† Int8ï¼Œæ²¡æœ‰ç ”ç©¶ FP8ï¼ˆå› ä¸ºç¡¬ä»¶æ”¯æŒä¸è¶³ï¼‰ã€‚
    2. åªæµ‹è¯•äº†æœ€å¤š 175B å‚æ•°çš„æ¨¡å‹ï¼Œæ›´å¤§çš„æ¨¡å‹å¯èƒ½å‡ºç°æ— æ³•é¢„æ–™çš„æ–°é—®é¢˜ã€‚
    3. æ³¨æ„åŠ›æœºåˆ¶æ²¡ç”¨ Int8ï¼Œå› ä¸ºå®ƒä¸æ¶‰åŠæƒé‡å‚æ•°ï¼Œä¹Ÿæš‚æ—¶æ²¡å¿…è¦ã€‚
    4. åªç ”ç©¶äº†æ¨ç†ï¼Œæ²¡æœ‰æ·±å…¥ç ”ç©¶è®­ç»ƒæˆ–å¾®è°ƒï¼ˆè®­ç»ƒæ—¶çš„ç²¾åº¦ã€é€Ÿåº¦å’Œå¹³è¡¡é—®é¢˜æ›´å¤æ‚ï¼Œç•™å¾…ä»¥åæ¢ç´¢ï¼‰ã€‚


## 7. Broader Impacts

* æˆ‘ä»¬çš„å·¥ä½œè®©åŸæœ¬å› æ˜¾å­˜ä¸è¶³è€Œæ— æ³•ä½¿ç”¨çš„å¤§æ¨¡å‹ç°åœ¨å¯ä»¥åœ¨GPUä¸Šè¿è¡Œï¼Œè¿™å¯¹èµ„æºæœ‰é™çš„ç ”ç©¶è€…å°¤å…¶æœ‰å¸®åŠ©ã€‚åŒæ—¶ï¼Œä¹Ÿè®©èµ„æºä¸°å¯Œçš„æœºæ„èƒ½åœ¨ç›¸åŒGPUä¸Šè¿è¡Œæ›´å¤šæ¨¡å‹ï¼Œå¯èƒ½åŠ å¤§èµ„æºå·®è·ã€‚
* æˆ‘ä»¬è®¤ä¸ºï¼Œç»“åˆæˆ‘ä»¬æå‡ºçš„ Int8 æ¨ç†æ–¹æ³•å’Œä¸€äº›å…¬å¼€çš„å¤§æ¨¡å‹ï¼ˆå¦‚ OPTï¼‰ï¼Œå°†ä½¿å­¦æœ¯ç•Œåœ¨é›¶æ ·æœ¬/å°‘æ ·æœ¬ä»»åŠ¡ä¸Šå¼€å±•æ–°çš„ç ”ç©¶ã€‚è¿™ç§æ¨¡å‹æ™®åŠå¯¹ç¤¾ä¼šå¯èƒ½å¸¦æ¥æ­£é¢å’Œè´Ÿé¢å½±å“ï¼Œå°šéš¾é¢„æµ‹ã€‚



## å…¶ä»–

### å°†æ‰€æœ‰ nn.Linear æ¨¡å—æ›¿æ¢ä¸º bnb.nn.Linear8bitLt è€Œæ— éœ€å ç”¨å†…å­˜:

```python
# æ”¾å¼ƒäº†å¯¹æŸäº›æ¨¡å— (è¿™é‡Œæ—¶ lm_head) è¿›è¡Œæ›¿æ¢ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ä¿æŒè¾“å‡ºå±‚çš„åŸå§‹ç²¾åº¦ä»¥è·å¾—æ›´ç²¾ç¡®ã€æ›´ç¨³å®šçš„ç»“æœ
# å°† has_fp16_weights å±æ€§è®¾ç½®ä¸º Falseï¼Œä»¥ä¾¿ç›´æ¥å°†æƒé‡åŠ è½½ä¸º Int8ï¼Œå¹¶åŒæ—¶åŠ è½½å…¶é‡åŒ–ç»Ÿè®¡ä¿¡æ¯
def replace_8bit_linear(model, threshold=6.0, module_to_not_convert="lm_head"):
    for name, module in model.named_children():
        if len(list(module.children())) > 0:
            replace_8bit_linear(module, threshold, module_to_not_convert)

        if isinstance(module, nn.Linear) and name != module_to_not_convert:
            with init_empty_weights():
                model._modules[name] = bnb.nn.Linear8bitLt(
                    module.in_features,
                    module.out_features,
                    module.bias is not None,
                    has_fp16_weights=False,
                    threshold=threshold,
                )
    return model
```

### ç¤ºä¾‹

* å¯¼å…¥æ¨¡å—

```python
import torch
import torch.nn as nn

import bitsandbytes as bnb
from bnb.nn import Linear8bitLt
```

* å®šä¹‰è‡ªå·±çš„æ¨¡å‹&è®­ç»ƒ&ä¿å­˜æ¨¡å‹

```python
fp16_model = nn.Sequential(
    nn.Linear(64, 64),
    nn.Linear(64, 64)
)
# [... train the model ...]
torch.save(fp16_model.state_dict(), "model.pt")
```

* å®šä¹‰ä¸€ä¸ª int8 æ¨¡å‹
```python
int8_model = nn.Sequential(
    Linear8bitLt(64, 64, has_fp16_weights=False),
    Linear8bitLt(64, 64, has_fp16_weights=False)
)
```

* åŠ è½½ 8 ä½æ¨¡å‹(åœ¨è®¾è®¡ä¸Šé‡‡ç”¨ **å»¶è¿Ÿé‡åŒ–ï¼ˆdelayed quantizationï¼‰** æœºåˆ¶)

```python
int8_model.load_state_dict(torch.load("model.pt"))
int8_model = int8_model.to(0) # é‡åŒ–å‘ç”Ÿåœ¨æ­¤å¤„
```

* åœ¨è°ƒç”¨ .to å‡½æ•°ä¹‹å‰æ‰“å° int8_model[0].weight:
```
int8_model[0].weight
Parameter containing:
tensor([[ 0.0031, -0.0438, 0.0494, ..., -0.0046, -0.0410, 0.0436],
        [-0.1013, 0.0394, 0.0787, ..., 0.0986, 0.0595, 0.0162],
        [-0.0859, -0.1227, -0.1209, ..., 0.1158, 0.0186, -0.0530],
        ...,
        [ 0.0804, 0.0725, 0.0638, ..., -0.0487, -0.0524, -0.1076],
        [-0.0200, -0.0406, 0.0663, ..., 0.0123, 0.0551, -0.0121],
        [-0.0041, 0.0865, -0.0013, ..., -0.0427, -0.0764, 0.1189]],
       dtype=torch.float16)
```
* åœ¨è°ƒç”¨ .to å‡½æ•°ä¹‹åæ‰“å° int8_model[0].weight:
```
int8_model[0].weight
Parameter containing:
tensor([[ 3, -47, 54, ..., -5, -44, 47],
        [-104, 40, 81, ..., 101, 61, 17],
        [ -89, -127, -125, ..., 120, 19, -55],
        ...,
        [ 82, 74, 65, ..., -49, -53, -109],
        [ -21, -42, 68, ..., 13, 57, -12],
        [ -4, 88, -1, ..., -43, -78, 121]],
        device='cuda:0', dtype=torch.int8, requires_grad=True)
```

* åé‡åŒ–è·å– FP16 æƒé‡
```python
# CBï¼šåŸå§‹ int8 æƒé‡ï¼Œä¾‹å¦‚èŒƒå›´æ˜¯ [-127, 127] çš„æ•´æ•°çŸ©é˜µ
# SCBï¼šscale å› å­ï¼Œå³æ¯ä¸ªé€šé“/å—çš„ç¼©æ”¾æ¯”ä¾‹
(int8_model[0].weight.CB * int8_model[0].weight.SCB) / 127
```










