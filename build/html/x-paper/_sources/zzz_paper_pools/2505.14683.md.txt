# 2505.14683_Emerging Properties in Unified Multimodal Pretraining

* 首页: [https://arxiv.org/abs/2505.14683](https://arxiv.org/abs/2505.14683)
* PDF: [https://arxiv.org/pdf/2505.14683](https://arxiv.org/pdf/2505.14683)
* 引用:
* 组织:



## LLM 总结

主要探讨了在统一多模态预训练中出现的新兴属性。总结如下：

1. **研究背景**：随着多模态学习的发展，研究者们越来越多地关注如何将文本、图像、音频等多种模态的信息融合在一个统一的框架中进行预训练。这种统一的预训练方法能够提升模型在多种下游任务中的表现。

2. **研究问题**：本文关注的问题是，随着预训练模型规模的增大以及训练数据的丰富，统一多模态模型在训练过程中是否会出现“新兴属性”（emerging properties）。这些属性并不是显式设计或训练的目标，但它们对模型性能具有重要影响。

3. **主要发现**：
   - 随着模型规模的增大，统一多模态模型在多种任务中表现出“突然能力提升”（emergent capabilities），即在某些任务上，模型表现随着规模增加而突然跃升，而不是线性提升。
   - 这些新兴能力包括跨模态推理、零样本泛化、对复杂任务的适应能力等。
   - 论文通过一系列实验验证了这些新兴属性的存在，并分析了它们与模型规模和数据多样性之间的关系。

4. **实验方法**：
   - 作者在多个统一多模态预训练模型（如CLIP、ALIGN、Florence等）上进行了实验。
   - 评估了不同规模的模型在多个下游任务（如图像-文本检索、视觉问答、跨模态生成等）上的表现。
   - 通过对比实验，分析了模型的规模、训练数据的多样性如何影响其能力的“涌现”。

5. **结论与意义**：
   - 本文指出，统一多模态预训练模型中存在类似语言模型的“涌现能力”，这为多模态模型的设计和训练提供了新的视角。
   - 这些发现强调了模型规模和数据多样性在多模态学习中的重要性，并为未来研究提供了方向。

总之，这篇论文通过系统实验揭示了统一多模态预训练模型中新兴能力的出现机制，具有重要的理论和实践意义。


## Abstract

该论文介绍了BAGEL，一个开源的基础模型，能够原生支持多模态的理解与生成。BAGEL是一个统一的、仅解码器的模型，预训练数据包含数万亿个标记，数据来源包括大规模的文本、图像、视频和网络数据的交错数据。通过在多样化且交错的多模态数据上进行扩展，BAGEL在复杂的多模态推理任务中展现出新兴能力。实验结果显示，BAGEL在多模态生成和理解的标准基准测试中显著优于其他开源的统一模型，并具备高级多模态推理能力，如自由形式的图像操作、未来帧预测、三维操作和世界导航。为了推动多模态研究的发展，作者分享了关键发现、预训练细节、数据创建协议，并向社区发布了代码和模型检查点。


## 1 Introduction



本章主要介绍了多模态统一理解和生成领域的发展现状、研究动机、数据构建方式、模型架构设计以及提出的BAGEL模型及其性能表现。

### 核心内容总结：

1. **背景与动机**  
   - 近年来，统一多模态理解和生成的研究迅速发展，但大多数模型仍主要依赖图像-文本对数据进行训练。
   - 学术界模型与如GPT-4o和Gemini 2.0等闭源系统的差距显著，尤其是在复杂任务和多模态交互能力上。
   - 作者认为，解决这一差距的关键在于使用结构化的多模态交错数据（包括文本、图像、视频和网页信息）进行大规模预训练。

2. **数据构建方法**  
   - 提出了一种新的数据采集、过滤和构建协议，以生成高质量的多模态交错数据。
   - 引入视频数据，提供像素级、概念级、时间连续性和物理连续性，增强对“真实世界知识”的理解。
   - 交错数据形式支持多模态对话、文本生成图像/视频、图像编辑等多种任务，便于模型学习跨模态交互。
   - 引入推理导向内容（受DeepSeek-R1启发），以提升模型的多模态推理能力。

3. **模型架构设计**  
   - 采用Mixture-of-Transformer-Experts（MoT）架构，通过选择性激活模态特定参数，最大化模型容量。
   - 不引入传统的瓶颈连接器或任务特定约束，采用共享自注意力机制，实现多模态理解和生成之间的长上下文交互。
   - 该设计有助于模型在训练数据和训练步数上进行有效扩展，避免架构限制对模型性能的干扰。

4. **提出的模型：BAGEL**  
   - BAGEL是一个开源的多模态基础模型，具有7B个活跃参数（总参数14B），在大规模交错多模态数据上训练。
   - 在标准多模态理解排行榜上，BAGEL优于当前领先的开源VLM模型。
   - 其文本到图像生成质量可与SD3和FLUX.1-dev等公开模型竞争。
   - 在传统图像编辑任务中表现一致优于领先的开源模型，并展现出自由形式图像编辑、多视角合成和世界导航等“世界建模”能力。

5. **模型能力的涌现特性**  
   - 随着交错多模态预训练的扩展，BAGEL表现出能力逐渐增强的模式：
     1. 基础的多模态理解和高质量生成能力最先出现；
     2. 接着是复杂的图像编辑和自由形式视觉操作能力；
     3. 最后是长上下文推理能力，促进多模态理解和生成的协同。
   - 这些能力不仅在公开基准上得到验证，还在作者提出的IntelligentBench中被更清晰地体现。
   - 作者强调，理解与生成的优化空间虽不完全耦合，但通过共享自注意力机制在一个Transformer模型中进行联合优化，可实现丰富的多模态能力组合。

### 总结：
本章系统介绍了BAGEL模型的设计理念、数据构建方法、模型架构及其展现出的强大多模态能力。通过大规模交错多模态训练，BAGEL不仅在基础任务上表现出色，还在复杂推理和自由形式生成方面展现出“世界建模”的能力，为开源系统实现与闭源模型的性能接近提供了可行路径。


## 2 Model



本章节主要介绍了BAGEL模型的整体架构设计及其关键技术选择，旨在构建一个统一的多模态预训练模型，支持理解和生成任务。以下是章节内容的总结：

### 1. 模型架构概览
- **MoT 架构**：BAGEL采用“Transformer专家混合”（Mixture-of-Transformers, MoT）架构，包含两个专用的Transformer专家：
  - **理解专家**：处理多模态理解任务。
  - **生成专家**：处理多模态生成任务。
- **共享注意力机制**：两个专家在每一层通过共享的自注意力机制处理相同的token序列。
- **双视觉编码器**：
  - **理解导向编码器**（ViT）：用于提取图像的语义信息。
  - **生成导向编码器**（VAE）：用于处理图像的低级像素信息。

### 2. 生成策略
- **文本生成**：采用自回归的Next-Token-Prediction方法。
- **图像生成**：使用**Rectified Flow**方法，提高生成质量。
- **对比其他方法**：
  - **Quantized AR**：易于实现，但生成质量较低，推理延迟高。
  - **External Diffuser**：通过外部扩散模块生成图像，但存在信息压缩瓶颈。
  - **Integrated Transformer**：将LLM与扩散模型融合，消除信息瓶颈，适合大规模训练，是BAGEL的最终选择。

### 3. 模型细节
- **基础架构**：基于Qwen2.5 LLM，采用解码器结构，使用RMSNorm、SwiGLU、RoPE、GQA等组件。
- **视觉表示**：
  - **ViT路径**：用于理解任务，基于SigLIP2模型，支持原生宽高比。
  - **VAE路径**：用于生成任务，从像素空间转换到潜在空间，冻结训练。
- **位置编码**：对ViT和VAE token进行2D位置编码，并采用特定的时间步编码方式。
- **注意力机制**：根据不同模态采用因果或双向注意力，提升生成效率和质量。

### 4. 广义因果注意力（Generalized Causal Attention）
- **多图像生成支持**：采用**扩散强制**（Diffusion Forcing）策略和随机分组，增强生成一致性。
- **注意力设计**：在多模态生成中，后续token可以关注前序图像的干净VAE token和ViT token，但不关注噪声VAE token。
- **推理加速**：通过KV缓存机制实现多模态解码加速。
- **分类器自由引导**（Classifier-Free Guidance）：通过随机丢弃token实现引导生成。

### 5. Transformer结构选择与实验
- **对比实验**：在1.5B参数规模下，比较了Dense Transformer、MoE和MoT三种结构。
- **实验结果**：
  - **MoT表现最佳**，尤其在多模态生成任务中，MSE损失最低且收敛最快。
  - MoE和MoT的参数量为Dense的两倍，但计算量（FLOPs）相同。
  - 实验表明，将理解与生成任务分开处理，有助于缓解模态间学习目标的冲突。

### 总结
BAGEL通过MoT架构、双视觉编码器、广义因果注意力机制等创新设计，实现统一多模态理解和生成。实验表明，该架构在大规模训练中具有明显优势，尤其适合处理长上下文的多模态任务和强化学习场景。


## 3 Data



本文“3 Data”章节主要介绍了BAGEL模型的训练数据构建方法，目的是通过多模态预训练，增强模型在多模态推理、上下文预测、物理动力学建模和未来帧预测等方面的能力。以下是该章节内容的总结：

### 数据特点与目标
- BAGEL模型训练数据涵盖**语言、图像、视频和网页数据**，构建了一个统一的多模态接口。
- 除了标准的视觉-语言（VLM）、文本到图像（T2I）和大规模语言建模（LLM）数据集，还引入了**新的视觉-文本交错数据集**，以提升模型在**序列化多模态推理**方面的能力。
- 数据的多样性、质量和采样策略对模型性能有重要影响。

### 数据来源与统计
- 数据涵盖多种类型，包括纯文本、图像-文本配对、交错理解数据、交错生成数据（视频/网页）。
- 数据规模和统计如表1所示，交错数据以灰色突出显示，说明在训练中的重要性。
  - 纯文本数据：400M数据，0.4T token。
  - 图像-文本配对数据（理解和生成）：共2100M数据，3.1T token。
  - 视频和网页来源的交错数据：共65M数据，1.1T token。

### 数据构建方法
#### 3.1 纯文本数据
- 用于保持模型的语言建模能力。
- 数据精选来自高质量文本资源，支持广泛的语言任务。

#### 3.2 图像-文本配对数据
- **VLM图像-文本对**：用于视觉-语言模型预训练，数据来源包括网页描述和标题，经过滤波提升质量（如分辨率、文本长度、去重等）。
- **T2I图像-文本对**：用于文本到图像生成，数据包括艺术、文本、超现实等风格，图像质量经过筛选。

#### 3.3 视觉-文本交错数据
- 视频和网页数据因其能提供复杂的时空信息和真实世界的多模态结构，被选为主要来源。
- **视频数据**：
  - 包括Koala36M和MVImgNet2.0等开源数据集。
  - 通过时间分割、空间裁剪、质量过滤等方式处理，并使用CLIP进行重复检测。
- **网页数据**：
  - 基于OmniCorpus构建，涵盖百科、教程等结构化文档。
  - 使用LLM进行主题选择和规则过滤（如图像清晰度、文本密度、相关性等），提升数据质量。

#### 3.3.1 数据过滤
- 为提升数据质量，设计了多阶段过滤流程。
- 例如：移除用户界面（UI）图像、保证分辨率和清晰度、控制文档图像数量（3-8张）等。

#### 3.3.2 数据构造
- **视频来源的交错数据**：通过生成帧间描述（如物体动作、场景转换）构建，使用小型VLM模型生成文本描述，降低推理成本。
- **网页来源的交错数据**：采用“先描述后生成”策略，在图像前插入描述作为引导，提升图像生成的准确性和语义相关性。
- 构建的交错数据包括：4500万条视频数据和2000万条网页数据。

#### 3.3.3 推理增强数据
- 引入多阶段推理数据，增强模型对复杂视觉目标的理解和规划能力。
- 通过语言引导图像生成（如T2I生成、自由图像编辑、抽象编辑），构建50万条推理增强数据。
- 使用DeepSeek-R1等模型生成推理路径，并通过VLM评估质量。

### 数据训练策略
- 表3总结了BAGEL的训练策略，包括不同任务阶段（对齐、预训练、训练、微调）的超参数设置和数据采样比例。
- 交错数据在训练中占比高，尤其是在生成任务中（如视频和网页生成）。

### 总结
本章节详细介绍了BAGEL模型训练数据的来源、筛选、构造方法和训练策略，强调通过**多样化、高质量、结构化的多模态数据**提升模型在复杂推理任务中的能力。交错数据（尤其是来自视频和网页的结构化数据）在训练中扮演了核心角色，为模型提供了丰富的语义和时空信息。


## 4 Training



本章节详细介绍了统一多模态预训练模型的训练策略及关键超参数的调整方法，主要包括以下几个方面：

### 1. 多阶段训练策略
整体训练过程分为四个阶段，每个阶段具有特定的目标和训练配置：

- **对齐阶段（Alignment Stage）**：将视觉编码器（SigLIP2 ViT）与语言模型（Qwen2.5 LLM）进行对齐。仅训练多层感知机（MLP）连接器，保持视觉编码器和语言模型冻结。使用图像-文本对数据进行图像描述任务，图像尺寸固定为 378×378。

- **预训练阶段（Pre-training Stage, PT）**：引入QK-Norm模块，除VAE参数外，所有参数均可训练。训练语料总量为 2.5T tokens，包含文本、图像-文本对、多模态对话、网页交织和视频交织数据。采用原生分辨率策略进行多模态理解和生成训练。

- **持续训练阶段（Continued Training Stage, CT）**：在PT基础上提升视觉输入分辨率，从而增强模型的多模态生成和理解能力。同时增加交叉模态数据的采样比例以强化跨模态推理能力。该阶段训练总量为 2.6T tokens。

- **监督微调阶段（Supervised Fine-tuning Stage, SFT）**：使用高质量的图像-文本对和交织生成数据进行多模态生成训练，使用LLaVA-OV和Mammoth-VL数据集进行多模态理解微调。该阶段训练总量为 727 亿 tokens。

所有阶段均使用 AdamW 优化器（β1=0.9，β2=0.95），并设置极小的 ϵ=1.0×10⁻¹⁵ 以抑制损失波动。在提升生成分辨率时，扩散时间步数从 1.0 提高到 4.0 以确保合理的噪声分布。学习率保持恒定，以便于扩展训练数据而无需重启训练过程。各阶段中，序列被打包为相近长度（PT/Alignment 阶段为 32K–36K tokens，CT/SFT 阶段为 40K–45K tokens）以实现负载均衡。

---

### 2. 关键超参数调整

统一多模态预训练需要平衡理解和生成任务之间的信号，因此特别关注两个关键超参数：

#### 4.1 数据采样比例
通过在 1.5B Qwen2.5 LLM 上进行控制实验，研究不同生成与理解数据比例对模型训练效果的影响。结果显示：

- 增加生成数据的采样比例（从 50% 到 80%）可显著降低均方误差（MSE）损失，提升模型性能。
- 交叉熵（CE）损失在不同比例下变化不明显，对下游任务影响较小。
- 因此，训练中应优先增加生成类数据的采样频率，这一策略在训练协议中被采用。

#### 4.2 学习率设置
在相同实验设置下，研究不同学习率对模型训练的影响，得出以下结论：

- 较大的学习率有助于 MSE 损失的快速收敛，但对 CE 损失有负面影响。
- 较小的学习率则有利于 CE 损失的稳定。
- 为了平衡两者，采用不同权重来分别处理生成和理解任务的损失函数，具体设置见训练协议表。

---

### 总结
本章通过多阶段训练策略与关键超参数的优化调整，构建了一个高效的统一多模态预训练框架。实验证明，增加生成数据的采样比例和合理设置学习率可以显著提升模型在多模态生成和理解任务中的表现。这些经验总结为训练协议的制定提供了理论支持，并为后续模型优化提供了重要指导。


## 5 Evaluation



本章主要围绕模型评估展开，旨在全面评估统一多模态模型的各项能力。论文从多个方面进行了评估设计和实验：

1. **多模态理解评估**：采用六个广泛应用的基准（MME、MMBench、MM-Vet、MMMU、MathVista、MMVP），这些基准覆盖了感知、认知和多模态推理能力，具备良好的区分度，能够有效评估当前最先进的模型。

2. **文本到图像生成评估**：使用GenEval和新提出的WISE基准进行评估，分别测试模型在生成图像时的语义理解和世界知识整合能力。同时，论文还提供了与当前最先进模型在生成质量上的定性对比。

3. **图像编辑评估**：基于GEdit-Bench进行评估，该基准包含从网络抓取的真实用户请求，具有高度的现实意义。评估得分通过GPT-4.1自动生成，并辅以定性示例以增加评估的细致性。

4. **智能图像编辑评估**：论文提出一个新基准 **IntelligentBench**，用于评估模型在自由形式图像编辑任务中的能力，要求模型具备复杂的多模态推理和任务组合能力。每个样本包含问题图像、问题文本和参考答案图像，评估使用GPT-4o进行，评分标准涵盖任务完成度、视觉一致性与知识驱动的创造性。最终模型得分通过归一化总分为100分得出。此外，论文还展示了在该基准上的部分案例和定性结果。

综上所述，本章通过现有基准与新提出的评估方法相结合，系统地评估了模型在多模态理解、文本到图像生成、图像编辑以及复杂推理任务中的表现，为统一多模态模型的能力提供了全面的衡量标准。


## 6 Emerging Properties



本章主要探讨了统一多模态基础模型 BAGEL 在不同训练阶段中出现的“新兴属性”，并分析其在图像理解、生成、编辑等任务中的表现变化。以下是主要内容的总结：

---

### 1. **新兴属性的定义与研究背景**
- 新兴属性指在模型训练的早期阶段并不存在，但在后期预训练阶段出现的能力。
- 这种能力的出现通常表现为一种“相变”，即模型行为发生突然且显著的变化，无法通过训练损失曲线预测。
- BAGEL 的研究聚焦于统一多模态模型中的新兴属性，通过在不同训练阶段的历史模型检查点上评估任务表现，研究能力的演进。

---

### 2. **任务表现与训练阶段的关系**
- **图像理解**：在训练早期（约 0.18T token）便表现出较高水平，且收敛较快。
- **图像生成**：在约 0.68T token 时达到较高性能，性能增长较快但收敛也较早。
- **图像编辑**：需要同时具备理解和生成能力，因此在训练中后期（约 2.64T token）才达到较高水平。
- **智能编辑**（Intelligent Editing）：作为最复杂的任务，要求强大的多模态推理能力，直到约 3.61T token 才达到 85% 的最佳性能，表现出明显的“新兴行为”。

---

### 3. **多模态特征的重要性**
- 使用 ViT（视觉变换器）特征提取器与 VAE 特征结合，在图像编辑任务中效果优于仅使用 VAE。
- 尤其在智能编辑任务中，ViT 提供的语义上下文信息对生成能力有显著提升作用。
- 实验表明，移除 ViT 特征会显著降低智能编辑任务的性能（下降约 16%），说明视觉语义推理在复杂任务中的关键性。

---

### 4. **定性分析与生成质量提升**
- 在不同训练阶段的质量对比中，生成图像质量在 1.5T token 时已较高，3.0T token 后因高分辨率训练略有提升。
- 文字生成方面，如“hello” 和 “BAGEL” 的正确拼写能力是在训练中后期逐渐出现的新兴能力。
- 智能编辑任务中，模型在训练早期的编辑能力较弱，倾向于复制输入图像进行微小修改；但随着训练量增加，模型开始生成具有合理语义的、全新概念的图像编辑结果，表现出明显的推理能力提升。

---

### 5. **核心发现与结论**
- 模型能力的出现顺序与任务复杂度相关：理解能力 → 生成能力 → 基础编辑 → 智能编辑。
- 智能编辑任务是第一个在统一多模态模型中表现出“相变”式新兴行为的任务，其能力提升依赖于大量训练数据与多模态语义理解和生成能力。
- ViT 特征的引入显著提升了复杂任务的性能，表明视觉语义信息在多模态任务中的重要性。

---

### 总结
本章通过对 BAGEL 模型在不同训练阶段的系统评估，揭示了统一多模态预训练模型中新兴属性的演化过程。研究证明，随着训练数据量的增加和模型规模的扩展，模型在理解、生成和编辑任务中逐步展现出复杂的能力，尤其在智能编辑任务中表现出类似“相变”的新兴行为。这些发现为理解大规模多模态模型的能力演化提供了重要参考。


## 7 Main Results



本章总结了BAGEL模型在多模态能力上的主要研究成果，涵盖了图像理解、生成、编辑、带有推理的生成/编辑以及世界建模等方面，通过定量和定性评估展示了其在多个基准测试中的卓越表现。

### 7.1 图像理解
在图像理解方面，BAGEL在多个公开基准测试中表现出色，如MME-P、MMBench、MMMU等。在与现有最先进模型的对比中，BAGEL在7B参数规模下显著优于Janus-Pro和MetaQuery-XL等模型，在MMMU和MM-Vet两个任务上分别提升了14.3和17.1个百分点。此外，BAGEL在与专注图像理解的模型（如Qwen2.5-VL和InternVL2.5）对比中也表现优异，证明其MoT设计在缓解任务冲突的同时保持了强大的视觉理解能力。

### 7.2 图像生成
在图像生成方面，BAGEL在GenEval和WISE两个基准上均表现出色。在GenEval上，BAGEL在未使用LLM重写器的情况下，取得了82%的总体得分，超过了FLUX-1-dev、SD3-Medium等专用生成模型以及Janus-Pro、MetaQuery-XL等统一模型。在WISE上，BAGEL的得分也接近GPT-4o这一最先进的私有模型，显示出其在世界知识推理方面的能力。此外，BAGEL支持中英文混合提示和任意宽高比生成，图像质量也优于Janus-Pro和SD3-Medium等模型。

### 7.3 图像编辑
在图像编辑方面，BAGEL在GEdit-Bench和IntelligentBench两个基准上的表现优于多个开源模型，如Step1X-Edit和IC-Edit，并接近Gemini 2.0等私有模型。在IntelligentBench中，BAGEL的得分从44.9提升至55.3，这主要得益于引入了推理过程（Self-CoT）。定性评估也显示，BAGEL在复杂编辑任务中表现稳定，能够避免对原图的非预期修改。

### 7.4 带有推理的生成/编辑
本节验证了推理增强生成在多个基准上的有效性。在图像生成任务中，BAGEL结合推理链（CoT）后，在WISE上的得分提升了0.18，超越了MetaQuery-XL等模型。在图像编辑任务中，BAGEL通过引入推理过程，显著提高了IntelligentBench的得分，表明其能够利用世界知识并提供详细的编辑指导。

### 7.5 世界建模
为提升BAGEL在长序列视觉生成中的世界建模能力，研究团队增加了视频和导航数据在训练中的比例，并构建了基于真实世界街道导航的轨迹数据集。实验结果显示，BAGEL能够根据指令生成动态数量的图像，完成导航、旋转和多帧生成等任务，且具备良好的泛化能力，能够迁移到水墨画、卡通、游戏等不同领域。

### 总结
本章通过广泛的定量和定性评估，展示了BAGEL在图像理解、生成、编辑等多模态任务中的卓越表现。其基于MoT的设计在任务冲突缓解和性能提升方面表现出优势，而推理增强机制进一步提升了图像生成和编辑的准确性和合理性。此外，BAGEL在世界建模方面具备良好的理解和生成能力，适用于更广泛的应用场景。


### 7.6 More Qualitative Results



本节总结如下：

本节主要展示了BAGEL-1.5B在多模态任务中的定性性能表现及失败案例。

1. **BAGEL-1.5B的性能**：  
   BAGEL-1.5B虽然模型参数仅为15亿，但在文本到图像生成（T2I）和图像编辑任务中，其表现超越了参数量更大的JanusPro-7B和Step1X-Edit（12B）模型。此外，BAGEL-1.5B与BAGEL-7B之间的性能差距表明，随着模型规模的扩大，性能仍有显著提升空间。

2. **失败案例分析**：  
   当前最先进的文本到图像系统在处理一些复杂任务时仍存在困难，例如：特殊IP生成、复杂文本渲染、精细的人体姿态生成以及多个实例同时生成。在图像编辑方面，诸如物体位置互换或同时修改多个实例等操作也具有挑战性。在某些复杂场景中，BAGEL与Gemini 2.0在遵循指令方面存在类似的问题，而GPT-4o在所有示例中表现最为稳定和成功。

3. **改进方法**：  
   BAGEL的性能可以通过以下方式提升：增加包含文本的图像数据、扩大模型容量，或在最终的微调阶段应用强化学习与人类反馈（RLHF）。

总体而言，BAGEL在缩小模型规模的同时保持高性能，展示了其在多模态预训练中的潜力，但也表明在某些复杂任务上仍有改进空间。


## 8 Conclusion



本章总结了BAGEL这一统一的多模态理解和生成模型。BAGEL在统一预训练的扩展下展现出新兴能力，不仅在标准的多模态理解与生成基准测试中表现出色，还具备强大的世界建模和推理能力。为了推动多模态研究的进一步发展，作者已将BAGEL开源给研究社区。


## 9 Acknowledgement



本章节为致谢部分，作者对在BAGEL项目中作出贡献的多位研究人员和同事表示感谢，包括Wei Ziqian、Chen Haoli、Xu Shengyang等共计30余位学者，肯定了他们在项目推进过程中的支持与努力。
