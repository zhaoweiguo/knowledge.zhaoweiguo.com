# 2205.12035_RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder

* 首页: <https://arxiv.org/abs/2205.12035>
* PDF: <https://arxiv.org/pdf/2205.12035>


[2205.12035] RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder



文章《RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder》提出了一种新的预训练模型 RetroMAE，旨在提升语言模型在检索任务中的性能。文章的核心思想是通过掩码自编码器（Masked Auto-Encoder）的预训练方法，使模型能够更好地理解和生成与检索相关的语言内容。

---

## 1. Introduction（引言）

本节介绍当前自然语言处理中检索任务的重要性，例如问答系统、信息检索等，同时也指出现有语言模型在这些任务中存在理解深度和上下文关联性不足的问题。文章提出 RetroMAE，利用掩码自编码器的预训练方式，增强模型在检索任务中的表现能力。

**重点内容：**
- 检索任务在实际应用场景中的重要性。
- 当前模型在检索任务中的局限性。
- 提出 RetroMAE 的动机和目标。

---

## 2. Related Work（相关工作）

本节回顾了自然语言处理中与检索任务相关的研究，包括基于检索的问答系统、语言模型的预训练方法（如 BERT、MAE），以及检索增强生成（Retrieval-Augmented Generation, RAG）等。

**重点内容：**
- 预训练语言模型的发展历程。
- 检索增强生成方法的优缺点。
- 掩码自编码器（MAE）的原理及其在语言模型中的应用。

---

## 3. Method（方法）

RetroMAE 的核心是通过掩码自编码器的方式进行预训练，与以往的掩码语言模型（如 BERT）不同，RetroMAE 强调在掩码条件下恢复原始文本时，模型需要依赖外部检索信息，从而提升其对检索能力的理解。

**重点内容：**
- **掩码自编码器机制**：模型在训练过程中随机掩码部分输入，并通过检索模块获取相关信息以辅助解码。
- **检索模块设计**：模型在掩码区域生成时，结合检索模块提供的信息，增强对上下文的理解。
- **训练目标**：最大化模型在给定掩码输入和检索信息条件下的重建能力。

**精简内容：**
- 模型的结构图和训练流程图用于辅助理解模型如何整合检索信息。
- 数据来源和预处理方式。

---

## 4. Experiments（实验）

作者在多个检索相关任务上评估了 RetroMAE 的性能，包括问答、文档检索、检索增强生成等任务，并与 BERT、MAE、RAG 等模型进行了比较。

**重点内容：**
- **实验设置**：使用的数据集、评估指标（如 ROUGE、BLEU、准确率、召回率等）。
- **实验结果**：RetroMAE 在多个任务中均表现优于基线模型。
- **消融实验**：验证掩码机制、检索模块对模型性能的影响。

**精简内容：**
- 模型在不同参数规模下的表现。
- 模型训练和推理时间的对比。

---

## 5. Discussion（讨论）

本节讨论了 RetroMAE 的优势与局限性。优势包括对检索任务的适应性更强、在少样本场景下的表现更好。局限性包括对检索模块的依赖较强，以及在某些任务中仍存在优化空间。

**重点内容：**
- 模型在少样本学习和跨领域任务中的泛化能力。
- 检索模块对模型性能的决定性作用。
- 未来改进方向，如优化检索机制、减少对检索依赖等。

---

## 6. Conclusion（结论）

文章总结了 RetroMAE 的核心思想和实验结果，指出通过掩码自编码器的方式结合检索信息，能够有效提升语言模型在检索任务中的性能，并为未来研究提供了新的思路。

**重点内容：**
- RetroMAE 的创新点和贡献。
- 对未来在检索任务中使用预训练模型的研究方向的展望。

---

## 附录（Appendix）（如有）

附录中通常包括：
- 模型超参数设置。
- 数据集详细描述。
- 补充分析和实验结果。

---

**总结：**

《RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder》通过引入掩码自编码器和检索模块，提出了一种新的预训练方法，旨在提升语言模型在检索任务中的性能。文章结构清晰，从问题背景、相关工作、方法设计、实验验证到讨论与总结，全面展示了 RetroMAE 的研究动机、实现方式和实际效果。


## Abstract



### 摘要总结

**Abstract（摘要）**

尽管预训练在许多重要的自然语言处理（NLP）任务中取得了显著进展，但在**密集检索（dense retrieval）**方面，有效的预训练策略仍有待探索。本文提出了**RetroMAE**，这是一种**面向检索的新型预训练范式**，它基于**掩码自编码器（Masked Auto-Encoder, MAE）**。

RetroMAE 的设计有三个关键点，是其创新与性能提升的核心：

1. **新的 MAE 工作流程**（重点）：  
   与传统的 MAE 不同，RetroMAE 对输入句子在**编码器和解码器中使用不同的掩码**。编码器从**部分掩码的输入中生成句子嵌入（sentence embedding）**，然后解码器通过**这种句子嵌入和另一个掩码输入**，利用**掩码语言建模（Masked Language Modeling）**来恢复原始句子。这种设计使得模型在学习语言表示的同时，也学习了高效的检索能力。

2. **不对称的模型结构**（重点）：  
   RetroMAE 采用**不对称结构**，编码器是一个**大型的 BERT 类 Transformer**，而解码器则是一个**仅一层的 Transformer**。这样的结构设计降低了计算成本，同时保证了编码器的学习能力。

3. **不对称的掩码比例**（重点）：  
   编码器和解码器的掩码比例不同。编码器使用**中等比例的掩码**（15%~30%），解码器使用**较高的掩码比例**（50%~70%）。这种设置强化了模型对上下文的理解能力，并提升了在检索任务中的表现。

RetroMAE 的整体框架**实现简单、效果显著**。实验表明，它在多种**密集检索基准任务**（如 BEIR 和 MS MARCO）上显著提升了**当前最优（SOTA）性能**。源代码和预训练模型已公开在 [GitHub](https://github.com/staoxiao/RetroMAE)，以促进更多相关研究。

---

### 作者及机构

**RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder**

**作者**：
- **Shitao Xiao**（小石涛）¹  
- **Zheng Liu**（刘振）²  
- **Yingxia Shao**（邵颖霞）¹  
- **Zhao Cao**（曹钊）²  

**机构**：
- ¹ 北京邮电大学，北京，中国  
- ² 华为技术有限公司，深圳，中国

**邮箱**：
- 小石涛、邵颖霞：{stxiao, shaoyx}@bupt.edu.cn  
- 刘振、曹钊：{liuzheng107, caozhao1}@huawei.com

---

### 总结结构保持不变，重点内容突出，非重点内容精简。


## 1 Introduction



### 1 Introduction 总结

#### 背景介绍
**dense retrieval（密集检索）** 在许多网络应用中具有重要作用。通过将语义相关的问题和文档表示为在嵌入空间中彼此接近的向量，密集检索可以借助近似最近邻搜索（如 PQ、HNSW）高效实现。

近年来，**大规模语言模型** 被广泛用作密集检索的编码器。主流模型如 BERT、RoBERTa 和 T5 通常通过 token 级任务（如 MLM、Seq2Seq）进行预训练，但这些任务并没有充分提升句子级别的表示能力，这限制了它们在密集检索中的潜力。

#### 现有方法的局限性
为解决上述问题，研究者提出了**检索导向的预训练模型**。其中一种主流策略是**自对比学习**（self-contrastive learning），它依赖于数据增强，但这种方法受限于数据增强的质量，并且通常需要大量负样本，成本较高。

另一种策略是**自编码**（auto-encoding），如 MAE 等方法，无需依赖数据增强或负样本。当前的研究主要集中在编码-解码流程的设计上，但如何设计更有效的自编码框架仍是一个开放问题。

#### 本文提出：RetroMAE
作者认为，基于自编码的预训练关键在于两个方面：  
1. **重建任务必须对编码质量提出高要求**；  
2. **预训练数据必须被充分利用**。

为此，作者提出了 **RetroMAE**，其核心设计包括：

- **新的 MAE 流程**：输入句子被两次使用不同掩码污染。编码器使用中等掩码比例的输入生成句子嵌入，解码器使用高掩码比例的输入，结合句子嵌入进行 MLM 重建原句。
  
- **非对称结构**：编码器使用完整的 BERT，生成高质量的嵌入；解码器结构极度简化，仅使用一层 Transformer，专注于重建原句。

- **非对称掩码比例**：编码器输入掩码比例为 15%～30%，解码器输入掩码比例为 50%～70%，促使编码器学习更深层次的语义信息。

#### 优势分析
RetroMAE 的设计具有以下优点：
- **提升编码质量要求**：相比传统自回归和 MLM，RetroMAE 的解码输入被严重掩码，使得重建过程依赖于编码器生成的高质量句子嵌入。
- **充分利用输入信息**：相比传统 MLM 仅利用 15% 的 token，RetroMAE 可以在 100% 的 token 上生成训练信号。
- **增强解码机制**：采用双流注意力与位置特定掩码，进一步提升了重建效果。

#### 实验结果
RetroMAE 实现简单且效果显著。仅使用 Wikipedia、BookCorpus 和 MS MARCO 数据进行预训练，使用 BERT base 编码器：
- 在**零样本设置**中，平均 BEIR 得分为 45.2；
- 在**监督设置**中，通过知识蒸馏，MS MARCO 任务的 MRR@10 达到 41.6。

这些成绩在相同模型规模和预训练条件下是前所未有的。作者还对各组件的影响进行了详细评估，为未来研究提供了有价值的见解。


## 2 Related works



## 2 相关工作

本节总结了与**密集检索（dense retrieval）**相关的研究，重点分析了其在应用中的发展、大规模语言模型的作用以及针对检索任务的预训练方法。

---

### 2.1 密集检索的应用与编码模型的重要性

密集检索广泛应用于**搜索引擎**（Karpukhin et al., 2020）、**广告系统**（Lu et al., 2020; Zhang et al., 2022）和**推荐系统**（Xiao et al., 2022b）等Web级应用中。其核心思想是**将查询和文档编码到同一潜在空间中**，从而可以通过**近似最近邻搜索 (ANN)** 高效地检索相关文档。**编码模型的质量是决定检索性能的关键因素**。

得益于**大规模语言模型**（如 BERT、RoBERTa、T5）的快速发展，密集检索的性能有了显著提升（Karpukhin et al., 2020; Luan et al., 2021; Lin et al., 2021）。

---

### 2.2 大规模语言模型的预训练任务多样性

大规模语言模型在**预训练任务**上存在较大差异。常见的任务包括：

- **掩码语言建模（MLM）**：如 BERT 和 RoBERTa 采用的 MLM，通过上下文预测被掩码的词。该任务被扩展为**实体掩码、短语掩码和跨度掩码**，以更好支持序列标注任务（如实体识别和问答）。
- **自回归任务**（如 GPT 系列）和**序列到序列（Seq2Seq）任务**（如 T5），用于支持**自然语言生成**（NLG）相关场景。

但大多数通用语言模型基于**词级别任务**，**句子表示能力较弱**（Chang et al., 2020）。因此，为了使这些模型在密集检索中表现良好，通常需要**大量标注数据**（如 SNLI、SQuAD）和**复杂的微调方法**（如多任务学习、课程学习）。

---

### 2.3 面向检索的预训练模型

为了解决上述问题，近年来研究者提出了**面向检索的预训练模型**，主要分为两类：

#### (1) 基于**自对比学习（SCL）**的方法

- 代表方法包括：Chang et al. (2020)、Guu et al. (2020)、Xu et al. (2022)。
- 核心思想是**通过数据增强生成正样本**（例如使用逆Cloze任务ICT），然后在对比学习框架下训练模型**区分正负样本**。
- **缺点**：需要大量负样本，计算资源消耗大；效果依赖于数据增强质量。

#### (2) 基于**自动编码（AE）**的方法

- 无需大量负样本，模型直接学习**基于句子嵌入重建原始句子**。
- 常见的重建任务包括：
  - **MLM**（Gao and Callan, 2021）：利用句子嵌入和部分掩码上下文重建句子。
  - **自回归**（Lu et al., 2021; Wang et al., 2021; Li et al., 2020）：利用句子嵌入和前缀进行重建。
- **训练目标差异**：
  - 自回归任务对所有输入词进行建模；
  - MLM 仅建模被掩码的词（通常为15%）。
- **理想目标**：**解码操作应具有足够挑战性**，迫使编码器**充分捕捉输入语义**，以保证重建质量；同时追求**高数据效率**，即充分利用输入数据进行预训练。

---

### 总结

本节综述了密集检索在Web应用中的重要性，以及大规模语言模型对其性能的推动作用。重点分析了通用语言模型在句子表示方面的不足，并介绍了两种面向检索的预训练方法：**基于对比学习的方法**依赖高质量数据增强，但计算成本高；**基于自动编码的方法**更加灵活高效，但对重建任务的设计提出了更高要求。


## 3 Methodology



以下是论文章节 **“3 Methodology”** 的中文总结，按照原文结构进行讲解，重点部分着重说明，次要内容适当简化：

---

## 3 方法论

本章提出了一种新的**面向检索的预训练模型 RetroMAE**，它基于**掩码自编码器**（masked auto-encoder）架构。该模型由两个模块组成：

- 一个类似 BERT 的**编码器 Φ<sub>enc</sub>**，用于生成句子嵌入；
- 一个**单层 Transformer 解码器 Φ<sub>dec</sub>**，用于句子重建。

输入句子 X 首先被掩码为 X̃<sub>enc</sub>，输入编码器生成句子嵌入 h̃<sub>X</sub>；然后再次被掩码为 X̃<sub>dec</sub>，与 h̃<sub>X</sub> 一起输入解码器进行重建。整个预训练流程在图 2 中展示。

---

### 3.1 编码

在编码阶段，输入句子 X 被**轻度掩码**，即随机替换一小部分词为特殊标记 [M]（掩码比例为15%~30%）。这样可以保留大部分输入信息，同时引入噪声以增强模型的鲁棒性。

- 编码器 Φ<sub>enc</sub> 采用类似 BERT 的结构，包括12层Transformer，隐藏维度为768。
- 最终选择 [CLS] 位置的隐藏状态作为句子的嵌入表示 h̃<sub>X</sub>。

这个过程可以表示为：

$$
\mathbf{h}_{\tilde{X}} \leftarrow \Phi_{enc}(\tilde{X}_{enc})
$$

（式中，X̃<sub>enc</sub> 为轻度掩码后的输入）

---

### 3.2 解码

解码阶段的输入 X 被**高度掩码**（掩码比例为50%~70%），与编码器输出的句子嵌入 h̃<sub>X</sub> 一起输入解码器，用于重建原始句子。

- 解码器输入是一个组合序列：
  
  $$
  \mathbf{H}_{\tilde{X}_{dec}} \leftarrow [\mathbf{h}_{\tilde{X}}, \mathbf{e}_{x_1} + \mathbf{p}_1, ..., \mathbf{e}_{x_N} + \mathbf{p}_N]
  $$

  其中 e<sub>x<sub>i</sub></sub> 是词的嵌入，p<sub>i</sub> 是位置嵌入。

- 解码器的目标是通过以下损失函数重建原始句子：

  $$
  \mathcal{L}_{dec} = \sum_{x_i \in \text{masked}} \mathrm{CE}(x_i | \Phi_{dec}(\mathbf{H}_{\tilde{X}_{dec}}))
  $$

由于解码器结构简单（单层Transformer），掩码比例高，因此重建任务较为困难，这促使模型学习**高质量的句子嵌入**。

---

### 3.3 增强解码

解码阶段的一个**局限性**是：训练信号（即交叉熵损失）**仅来自被掩码的词**，且所有掩码词都基于相同的上下文进行重建。

为提升训练效果，提出**增强解码**（Enhanced Decoding），借鉴双流自注意力机制和位置特定注意力掩码，引入两种输入流：

- **查询流 H₁**，用于重建任务；
- **上下文流 H₂**，提供不同位置的上下文信息。

定义如下：

$$
\begin{aligned}
\mathbf{H}_1 &\leftarrow [\mathbf{h}_{\tilde{X}} + \mathbf{p}_0, ..., \mathbf{h}_{\tilde{X}} + \mathbf{p}_N] \\
\mathbf{H}_2 &\leftarrow [\mathbf{h}_{\tilde{X}}, \mathbf{e}_{x_1} + \mathbf{p}_1, ..., \mathbf{e}_{x_N} + \mathbf{p}_N]
\end{aligned}
$$

进一步引入**位置特定注意力掩码 M**，使得每个词的重建只能依赖其所在行的可见上下文，不能参考自身或被掩码的位置。

- 注意力计算如下：

  $$
  \mathbf{A} = \mathrm{softmax}(\frac{\mathbf{Q}^T \mathbf{K}}{\sqrt{d}} + \mathbf{M}) \mathbf{V}
  $$

- 解码损失更新为：

  $$
  \mathcal{L}_{dec} = \sum_{x_i \in X} \mathrm{CE}(x_i | \mathbf{A}, \mathbf{H}_1)
  $$

这样一来，**每个词都可以参与训练**，并基于多样化的上下文进行重建，从而提升预训练效果。

---

### 总结与特点

RetroMAE 预训练方法具有以下**显著特点**：

1. **重建任务具有挑战性**：由于掩码比例高、解码器结构简单，迫使模型学习高质量的句子嵌入。
2. **训练信号丰富**：通过增强解码，**每个词都可参与训练**，充分利用无监督语料。
3. **实现简单**：无需复杂的数据增强或负采样操作，**计算成本与传统 BERT 类方法相当**。
4. **综合优化**：编码器（BERT 风格）和解码器的损失共同优化，形成最终训练目标：

   $$
   \mathcal{L}_{\text{total}} = \mathcal{L}_{enc} + \mathcal{L}_{dec}
   $$

---

### 算法 1：RetroMAE 的预训练流程

该流程总结如下：

1. 对输入进行轻度掩码 → X̃<sub>enc</sub>
2. 编码器生成句子嵌入 → h̃<sub>X</sub>
3. 构建 H₁、H₂ 两个输入流
4. 生成位置特定注意力掩码 M
5. 基于 H₁、H₂ 和 M 计算输出 A
6. 优化增强解码损失 ℒ<sub>dec</sub>
7. 联合优化编码器和解码器损失：ℒ<sub>enc</sub> + ℒ<sub>dec</sub>

整个流程简洁高效，适合作为面向检索的语言模型预训练方法。

---

**总结：**  
本章提出并详细阐述了 RetroMAE 的方法论。通过结合编码与增强解码机制，该模型能够有效学习高质量的句子嵌入，并在预训练阶段充分利用上下文信息，为后续的检索任务提供坚实基础。


## 4 Experimental Studies



### 4 实验研究总结

本节主要评估了 RetroMAE 预训练编码器在句子嵌入检索任务中的性能，从两个关键方面进行了探索：一是 **RetroMAE 在零样本和监督检索任务中的表现**，并与通用预训练模型和检索专用预训练模型进行对比；二是 **RetroMAE 中四个技术因素的影响**，分别为增强解码、解码器尺寸、解码器掩码比例和编码器掩码比例。

---

### 4.1 实验设置总结

#### 数据集
- **预训练数据**：使用了 BERT 常用的英文维基百科和 BookCorpus，并在 MS MARCO 上进行了在域预训练（对 MS MARCO 的表现影响显著）。
- **评估数据**：
  - **监督检索**：MS MARCO 和 Natural Questions，用于评估模型在检索任务中的性能。
  - **零样本检索**：使用 BEIR 基准，在 18 个跨领域任务中评估模型的迁移到能力。

#### 基线模型
- **通用预训练模型**：BERT、RoBERTa、DeBERTa。
- **检索专用模型**：
  - 自对比学习类：SimCSE、LaPraDoR、DiffCSE。
  - 自编码类：SEED、Condenser。
- **RetroMAE 实现细节**：使用双向 Transformer 编码器（12 层，768 维），单层 Transformer 解码器，默认掩码比例编码器 0.3，解码器 0.5，使用 AdamW 优化器和 8×A100 GPU 训练。

#### 评估方法
- **零样本检索**：使用 BEIR 基准，采用 NDCG@10 指标。
- **监督检索**：使用 DPR 和 ANCE 进行微调，并结合知识蒸馏（distillation）方法进行评估。

---

### 4.2 主要实验结果总结

#### 零样本检索
- **RetroMAE 的优势显著**：在 BEIR 基准的 18 个任务中，RetroMAE 的平均 NDCG@10 指标为 0.452，比最强基线模型高 4.5%。
- **提升来源**：性能提升主要来自预训练算法的改进，而非模型规模或训练数据的增加。

#### 监督检索
- **RetroMAE 在 DPR 和 ANCE 微调后均表现优异**：
  - 在 MS MARCO 和 Natural Questions 两个任务中，均优于其他模型，提升幅度显著（+1.4% 到 +2.8%）。
- **与最新模型对比**：
  - 在 MS MARCO 上，RetroMAE 超过 coCondenser、AR2、ColBERTv2、RocketQAv2、ERNIE-Search 等模型，MRR@10 提升最高达 2.8%。

#### 实验观察
- **通用模型（如 RoBERTa、DeBERTa）在检索任务中不如 BERT**，这说明通用预训练未必适合检索任务。
- **自编码预训练模型（如 SEED、Condenser、RetroMAE）在检索任务中表现更优**。
- **自对比学习模型在微调后提升有限**，说明其在检索任务中对微调的依赖较大。

---

### 4.3 消融实验总结

RetroMAE 的性能受以下四个技术因素的影响：

#### 1. **增强解码 vs. 基本解码**
- **增强解码效果更好**：通过更充分和多样化的训练信号，显著提升检索性能。

#### 2. **解码器层数**
- **单层解码器表现最优**：增加解码器层数并未带来性能提升，且无法使用增强解码。

#### 3. **解码器掩码比例**
- **掩码比例影响显著**：
  - 在使用增强解码时，最优掩码比例为 0.5。
  - 未使用增强解码时，最优比例为 0.7。
  - 高掩码比例能提升训练信号的多样性，从而改善检索性能。

#### 4. **编码器掩码比例**
- **中等掩码比例（0.3）表现最佳**：掩码比例过高（如 0.9）会破坏句子嵌入质量，导致性能下降。

#### 总结
- **增强解码是提升性能的关键**。
- **单层解码器结构最优**。
- **解码器高掩码比例、编码器适度掩码比例有助于提升检索效果**。

---

### 总体结论
RetroMAE 作为一种基于掩码自编码的检索专用预训练模型，表现出以下优势：
1. **零样本能力强**：在跨领域任务中表现优于其他模型。
2. **监督微调后效果好**：在 MS MARCO 和 Natural Questions 任务中均达到 SOTA。
3. **结构设计合理**：增强解码、单层解码器、掩码比例等关键因素对其性能有显著影响。

这些实验结果证明了 RetroMAE 在预训练模型设计方面的有效性，特别是在提升检索性能方面具有显著优势。


## 5 Conclusion



## 5 结论

本文提出了 **RetroMAE**，这是一种用于**预训练面向检索的语言模型**的新型**掩码自编码**框架。其核心机制是：对输入句子在**编码器**和**解码器**中进行随机掩码处理，并将**句子嵌入**与解码器的掩码输入结合，以重建原始输入。这一设计旨在提高模型对语义信息的捕捉能力。

### 重点内容讲解：

1. **模型结构的非对称性**  
   - 采用**不对称模型架构**：使用**全规模编码器**（full-scale encoder）和**单层解码器**（single-layer decoder）。这种结构设计有助于在计算效率与性能之间取得平衡。
   - **掩码比例不对称**：对编码器采用**中等掩码比例**，而对解码器使用**较高掩码比例**。这种设计增加了重建任务的难度，从而提升了模型学习的深度。

2. **改进的解码机制**  
   - 引入了**增强解码**（enhanced decoding）技术，充分利用预训练数据。这有助于提升模型在实际任务中的泛化能力。

3. **实验验证**  
   - 在 **BEIR**、**MS MARCO** 和 **Natural Question** 等主流检索数据集上的实验结果表明，RetroMAE 在**零样本**（zero-shot）和**监督**（supervised）评估中均表现出显著优于现有方法的性能。

### 精简内容讲解：

- 所有设计目标都是为了提升语言模型在信息检索任务中的表现，特别是在输入不完整或需要进行密集语义匹配的场景下。
- 实验部分验证方法的有效性，强调了在多个基准任务中的优越性。

总结而言，RetroMAE 通过创新的掩码策略、非对称结构和增强解码机制，为预训练语言模型在检索任务中的应用提供了新的有效路径。


## 6 Limitations



## 6 限制（Limitations）

本节主要探讨了当前研究中存在的几个限制因素。

首先，**研究基于 BERT base 规模的 Transformer 模型**进行。虽然 BERT base 已经在许多任务中表现出色，但更大型的网络（如 BERT large 或更大）可能会带来性能的进一步提升。这一点在近期的工作中（如 Ni 等人，2021）也得到了验证，网络规模的扩大通常对模型效果有积极影响。因此，**扩大网络规模的影响仍有待探索**。

其次，**预训练数据量仅达到中等水平**，这主要是由于计算资源的限制。尽管当前的实验结果已经显示出模型的有效性，但已有研究表明，增加预训练数据量对模型性能有显著提升作用。因此，**预训练数据量增加对模型的影响也值得进一步研究**。

### 重点总结：
- **模型规模受限**：目前仅使用 BERT base 模型，未来可尝试更大规模的网络。
- **数据量有限**：由于计算资源限制，预训练数据量较少，需探索更大规模数据对性能的影响。
- **研究意义**：尽管当前结果有效，但扩大模型和数据规模可能是未来提升性能的关键方向。
