# 2501.12948_DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

* 首页: <https://arxiv.org/abs/2501.12948>
* PDF: <https://arxiv.org/pdf/2501.12948>


DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning



## Abstract

本研究介绍了第一代推理模型 DeepSeek-R1-Zero 和 DeepSeek-R1。

- **DeepSeek-R1-Zero** 是一个通过大规模强化学习（RL）训练的模型，且 **不依赖监督微调（SFT）** 作为前置步骤，展现出显著的推理能力。
- 通过强化学习，DeepSeek-R1-Zero **自然涌现出多种强大且有趣的推理行为**。
- 然而，它也面临一些挑战，例如 **可读性差** 和 **语言混杂** 等问题。
- 为解决这些问题并进一步提升推理性能，作者引入了 **DeepSeek-R1**，它在强化学习前加入了 **多阶段训练和冷启动数据**。
- DeepSeek-R1 的推理性能可与 OpenAI 的 o1-1217 模型相媲美。
- 为了支持研究社区，作者开源了 DeepSeek-R1-Zero 和 DeepSeek-R1，并发布了基于 Qwen 和 Llama 架构从 DeepSeek-R1 蒸馏出的 **六个密集模型（1.5B, 7B, 8B, 14B, 32B, 70B）**。

---

### 1. Introduction（引言）
#### 1.1 Contributions（贡献）
- 提出两个推理模型：**DeepSeek-R1-Zero** 和 **DeepSeek-R1**。
- 通过 **强化学习直接训练模型**，不依赖于监督微调。
- 提出 **冷启动机制** 和 **多阶段训练策略**，以提升模型的推理能力与语言质量。
- 开源多个模型版本和蒸馏模型，促进社区研究。

#### 1.2 Summary of Evaluation Results（评估结果总结）
- DeepSeek-R1 在多个推理基准测试中表现优异，与 OpenAI-o1-1217 相当。
- 蒸馏模型在保持高性能的同时，显著降低了模型规模。

---

### 2. Approach（方法）
#### 2.1 Overview（概述）
- 核心思想是通过 **强化学习** 激发大语言模型的 **推理能力**。
- 区别于传统训练方式，采用 **基于奖励的训练机制** 和 **多阶段流程** 来优化推理表现。

#### 2.2 DeepSeek-R1-Zero: 强化学习在基础模型上的应用
- **2.2.1 强化学习算法**：使用基于策略梯度的强化学习方法训练模型，以提高推理能力。
- **2.2.2 奖励建模**：设计了专门的奖励函数，用于评估模型推理过程的质量，如推理逻辑、结果准确性等。
- **2.2.3 训练模板**：提出了适用于强化学习训练的模板结构，强调推理过程的结构化生成。
- **2.2.4 性能与自进化过程**：DeepSeek-R1-Zero 在训练过程中表现出 **自我进化** 的能力，部分模型甚至出现了 “顿悟时刻”（Aha Moment）。

#### 2.3 DeepSeek-R1: 带冷启动的强化学习
- **2.3.1 冷启动（Cold Start）**：在强化学习前，引入了高质量的推理数据以帮助模型起步，避免早期训练的不稳定性。
- **2.3.2 推理导向的强化学习**：训练过程中特别关注推理过程的逻辑性和步骤清晰性。
- **2.3.3 拒绝采样与监督微调**：在强化学习前加入拒绝采样和 SFT（监督微调）以优化语言输出。
- **2.3.4 多场景强化学习**：模型在多种推理任务中进行强化训练，提升泛化能力和适应性。

#### 2.4 蒸馏：为小模型赋能推理能力
- 从 DeepSeek-R1 中蒸馏出多个小模型（如 1.5B、7B 等），以实现推理能力在小模型上的迁移。
- 蒸馏过程保留了推理核心能力，同时大幅降低了模型规模，便于部署。

---

### 3. Experiment（实验）
#### 3.1 DeepSeek-R1 评估
- 在多个推理基准测试（如数学、逻辑、编程等）中，DeepSeek-R1 表现出色，与 OpenAI-o1-1217 相比具有竞争力。
- 在复杂推理任务中，其性能尤为突出。

#### 3.2 蒸馏模型评估
- 蒸馏出的小模型在保持高性能的同时，显著降低了计算和存储开销。
- 评估结果显示，即使在小规模下，模型仍能保持较高的推理准确性。

---

### 4. Discussion（讨论）
#### 4.1 蒸馏 vs 强化学习
- 蒸馏有助于将大模型的推理能力迁移到小模型，而强化学习是提升推理能力的核心手段。
- 两者结合可以实现 **高性能、低资源消耗的模型部署**。

#### 4.2 未成功的尝试
- 作者列举了一些尝试失败的方法，例如某些奖励函数设计不够有效、训练不稳定等问题，强调了方法设计的重要性。

---

### 5. Conclusion, Limitations, and Future Work（结论、局限与未来工作）
- **结论**：强化学习是提升大语言模型推理能力的有效手段，DeepSeek-R1 和蒸馏模型展示了其巨大潜力。
- **局限性**：部分模型在语言表达和推理连贯性方面仍有改进空间。
- **未来工作**：探索更稳定的训练机制、更高效的奖励建模方式，以及如何进一步优化小模型的推理能力。

---

### 6. A Contributions and Acknowledgments（贡献与致谢）
- 作者对模型设计、训练流程、开源贡献等方面进行了总结，并感谢支持单位和团队成员。

---

### 总结重点
- 本文重点在于 **通过强化学习提升大语言模型的推理能力**，提出 **DeepSeek-R1-Zero** 和 **DeepSeek-R1** 两个系列模型。
- 强调 **无需监督微调**、**冷启动机制** 和 **多阶段训练流程** 的重要性。
- 通过 **蒸馏技术**，将推理能力迁移到小模型，提高模型在实际部署中的可行性。
- 研究开源了多个模型版本，推动了大模型推理能力的研究与应用。

--- 

如需更详细的某一部分（如方法设计或实验细节）讲解，请告知。


## 1 Introduction

近年来，**大型语言模型（LLMs）** 正在快速迭代和演进（OpenAI、Anthropic、Google 等公司相关成果），逐步缩小与**人工通用智能（AGI）**之间的差距。

其中，**后训练（post-training）** 作为完整训练流程中的重要组成部分，已显示出提升推理任务准确性、对齐社会价值观和适应用户偏好的能力，且相较于预训练所需的计算资源较少。在推理能力方面，OpenAI 的 **o1 系列模型** 通过在推理过程中延长**思维链（Chain-of-Thought, CoT）** 的长度，首次引入了**推理时扩展**（inference-time scaling），在数学、编程和科学推理任务中取得了显著提升。然而，**推理能力的测试时扩展**（test-time scaling）问题仍然是一个开放的研究挑战。

已有研究尝试使用**基于过程的奖励模型**（process-based reward models）、**强化学习（RL）** 和**搜索算法**（如 MCTS、Beam Search）等方法来解决这一问题，但均未达到 OpenAI-o1 系列模型的通用推理性能。

为了解决这一问题，本文首次尝试**纯强化学习（pure RL）** 来提升语言模型的推理能力。我们使用 **DeepSeek-V3-Base** 作为基础模型，结合 **GRPO**（一种强化学习框架）来优化模型的推理能力。在训练过程中，**DeepSeek-R1-Zero** 模型自然展现出许多强大的推理行为。经过数千次 RL 训练后，其在推理基准上表现出色，如在 AIME 2024 上，**pass@1 分数从 15.6% 提升至 71.0%**，通过多数投票进一步提升至 **86.7%**，接近 OpenAI-o1-0912 的性能。

然而，DeepSeek-R1-Zero 也存在一些问题，如**可读性差**和**语言混合**。为了解决这些问题并进一步提升推理性能，我们引入了 **DeepSeek-R1**，采用**少量冷启动数据（cold-start data）** 和**多阶段训练流程**。具体包括：  
1. 收集冷启动数据对 DeepSeek-V3-Base 模型进行微调；  
2. 使用与 DeepSeek-R1-Zero 类似的推理导向 RL；  
3. 接近收敛后，通过拒绝采样生成新的 SFT 数据，结合 DeepSeek-V3 在写作、事实问答、自我认知等领域的监督数据，重新训练模型；  
4. 最终通过进一步的 RL 训练，得到最终的 **DeepSeek-R1** 模型，其性能与 OpenAI-o1-1217 相当。

此外，我们还探索了从 DeepSeek-R1 中**蒸馏出更小的密集模型**（dense models）。通过使用 **Qwen2.5-32B** 作为基础模型，我们发现直接从 DeepSeek-R1 蒸馏的模型在性能上优于直接在小模型上应用 RL 的结果。这表明，**大模型中发现的推理模式对提升小模型推理能力至关重要**。我们开源了多个蒸馏版本的 Qwen 和 Llama 模型，其中 **14B 蒸馏模型在 AIME 2024 上显著优于 QwQ-32B-Preview**，而 32B 和 70B 蒸馏模型在推理基准上创造了新的记录。

---

## 1.1 Contributions（贡献）

### 后训练：基于基础模型的大规模强化学习

- **纯强化学习训练基础模型**：我们首次在不依赖监督微调（SFT）的前提下，直接对基础模型应用 RL，训练出 **DeepSeek-R1-Zero**，展现出**自我验证、反思和生成长思维链**的能力。这是首次通过 RL 纯粹激励 LLM 推理能力的研究，为未来研究开辟了新路径。
- **DeepSeek-R1 的训练流程**：我们提出了包含两个 RL 阶段和两个 SFT 阶段的训练流程，旨在发现更优的推理模式并对齐人类偏好。我们相信，这一流程将有助于工业界训练出更强大的模型。

### 蒸馏：小模型也能很强大

- **大模型推理能力可蒸馏到小模型**：我们证明了大模型的推理模式可以通过蒸馏转移到小模型中，其性能优于小模型通过 RL 自行发现的推理模式。我们开源了 DeepSeek-R1 及其 API，为社区未来研究小模型蒸馏提供基础。
- **蒸馏模型的优秀表现**：基于 DeepSeek-R1 生成的数据，我们微调了多个广泛使用的密集模型。例如：  
  - **DeepSeek-R1-Distill-Qwen-7B** 在 AIME 2024 上得分为 55.5%，超越 QwQ-32B-Preview。  
  - **DeepSeek-R1-Distill-Qwen-32B** 在 AIME 2024 上得分为 72.6%，在 MATH-500 上得分为 94.3%，在 LiveCodeBench 上得分为 57.2%，显著优于现有开源模型，接近 o1-mini 水平。  
  - 我们开源了基于 Qwen2.5 和 Llama3 系列的多个蒸馏模型（1.5B、7B、8B、14B、32B、70B）。

---

## 1.2 Summary of Evaluation Results（评估结果概要）

### 推理任务

1. **AIME 2024（数学竞赛）**：DeepSeek-R1 的 pass@1 得分为 79.8%，略高于 OpenAI-o1-1217。
2. **MATH-500（数学推理）**：得分 97.3%，与 OpenAI-o1-1217 表现相当，远超其他模型。
3. **代码相关任务**：在 Codeforces 上，DeepSeek-R1 获得 2,029 的 Elo 分，超过 96.3% 的人类选手。
4. **工程任务**：DeepSeek-R1 略优于 DeepSeek-V3，对实际开发任务有一定帮助。

### 知识任务

- **MMLU、MMLU-Pro、GPQA Diamond**：DeepSeek-R1 表现优异，分别得分为 90.8%、84.0%、71.5%，显著优于 DeepSeek-V3，接近 OpenAI-o1-1217，表现优于其他闭源模型。
- **SimpleQA（事实问答）**：DeepSeek-R1 表现优于 DeepSeek-V3，但仍然落后于 OpenAI-o1。

### 其他任务

- **创意写作、通用问答、编辑、摘要**：DeepSeek-R1 在 AlpacaEval 2.0 和 ArenaHard 上分别取得 87.6% 和 92.3% 的胜率，显示其在非考试导向任务上的强大能力。
- **长上下文理解**：DeepSeek-R1 在长上下文任务上的表现显著优于 DeepSeek-V3。

---

### 小结

本研究展示了通过**纯强化学习**提升语言模型推理能力的可行性，并通过**多阶段训练流程**与**知识蒸馏**，成功构建了性能接近甚至超越当前顶级闭源模型（如 OpenAI-o1）的开源模型。研究结果表明，**大模型中发现的推理模式可以有效迁移到小模型中**，这对构建高效、可部署的推理模型具有重要意义。


## 2 Approach

## 2.1 Overview（概述）

本研究旨在展示通过大规模强化学习（Reinforcement Learning, RL）显著提升语言模型的推理能力，且**无需依赖监督微调（SFT）作为冷启动**。进一步地，研究发现，引入少量冷启动数据可进一步提升性能。

研究提出以下三项主要内容：

1. **DeepSeek-R1-Zero**：在不使用任何SFT数据的前提下，对基础模型进行强化学习。
2. **DeepSeek-R1**：以数千个长链式推理（Chain-of-Thought, CoT）示例进行微调后，再进行强化学习。
3. **Distill**：将DeepSeek-R1的推理能力迁移到较小的密集模型中。

---

## 2.2 DeepSeek-R1-Zero: 强化学习应用于基础模型

本节探索了**不依赖任何监督数据**，仅通过纯强化学习过程，使大语言模型（LLM）自主发展推理能力的潜力。

### 2.2.1 强化学习算法

采用**Group Relative Policy Optimization（GRPO）**算法，该方法通过优化策略模型，避免了传统方法中使用与策略模型规模相同的critic模型，从而节省训练成本。

- GRPO通过从旧策略中采样一组输出，并基于这些输出的组内得分估计基线。
- 优化目标函数结合了剪枝操作（clip）与KL散度惩罚项（用于防止策略偏离过远）。
- 优势值 $ A_i $ 通过组内奖励的均值与标准差标准化得到。

### 2.2.2 奖励建模

强化学习中奖励机制决定了优化方向。本部分使用基于规则的奖励系统，主要包含两种奖励类型：

- **Accuracy rewards**：对数学问题、编程问题等具有确定答案的任务，使用规则验证模型输出的准确性。
- **Format rewards**：强制模型将推理过程放在特定标签（如 `【reasoning】` 与 `【answer】`）之间，以保证输出格式清晰。

不使用神经网络奖励模型，是因为其在大规模强化学习过程中容易出现“奖励欺骗”（reward hacking），且维护成本高。

### 2.2.3 训练模板

为引导模型遵循指定的推理流程，设计了一种简化的模板结构：

- 要求模型先生成推理过程，再输出答案。
- 不施加内容相关的偏见（如强制使用特定解题策略），以便观察模型在RL过程中的自然演化。

### 2.2.4 DeepSeek-R1-Zero的性能、自进化过程与“顿悟时刻”

#### 性能表现

- DeepSeek-R1-Zero在AIME 2024、MATH、GPQA等多个推理基准上表现优异，甚至在使用多数投票（majority voting）后，性能超过OpenAI o1-0912。
- 例如，在AIME 2024上，模型的pass@1评分从15.6%提升至71.0%。

#### 自进化过程

- DeepSeek-R1-Zero在训练过程中展现出“自进化”能力，能自主适应更复杂的推理任务。
- 模型在训练中自然地学会了增加“思考时间”（即生成更长的推理过程），从数百到数千个token不等。
- 通过长时间的计算，模型自发地发展出诸如“反思”（re-evaluation）和“探索替代方案”的行为。

#### 顿悟时刻（Aha Moment）

- 在训练过程中，DeepSeek-R1-Zero展现出一种“顿悟”行为——在中间版本中，模型会重新审视其解题步骤，甚至以人类语言风格进行自我对话。
- 这种“顿悟”不仅体现了模型推理能力的提升，也展示了强化学习在模型行为演化上的强大潜力。

#### 限制

- DeepSeek-R1-Zero在推理过程的**可读性**和**语言一致性**方面存在不足，例如语言混合（中英文混杂）。
- 为了解决这些问题，研究引入了DeepSeek-R1方法，结合少量冷启动数据进行训练。

---

## 2.3 DeepSeek-R1: 强化学习结合冷启动

基于DeepSeek-R1-Zero的成功，研究提出两个关键问题：

1. 是否可以通过引入少量高质量冷启动数据进一步提升推理性能或加速收敛？
2. 如何训练一个用户友好、具备清晰推理链（CoT）且通用性强的模型？

### 2.3.1 冷启动（Cold Start）

- 为避免DeepSeek-R1-Zero中早期训练不稳定的问题，研究收集了数千条长CoT数据，用于对基础模型进行微调，作为强化学习的起点。
- 冷启动数据的优势包括：
  - **可读性**：输出格式规范，包含总结部分，便于用户理解。
  - **潜力**：通过人类先验设计的格式，模型表现优于DeepSeek-R1-Zero。

### 2.3.2 推理导向的强化学习

- 在微调基础上继续进行大规模强化学习，增强模型在数学、编程、逻辑推理等任务上的能力。
- 为解决语言混合问题，引入语言一致性奖励，提升输出语言的统一性。
- 最终奖励由推理准确性与语言一致性奖励相加而成。

### 2.3.3 拒收采样与监督微调（SFT）

- 在推理导向的RL收敛后，利用该模型收集SFT数据。
- 数据包括推理类与非推理类任务（如写作、问答、自我认知等）。
- 通过筛选高质量数据，生成约800k训练样本，进一步微调DeepSeek-V3-Base。

### 2.3.4 面向所有场景的强化学习

- 为使模型更符合人类偏好，进行二次强化学习训练，提升模型的**有用性**与**无害性**。
- 对于推理任务，沿用基于规则的奖励；对于通用任务，使用奖励模型捕捉人类偏好。
- 最终训练出一个具备强推理能力、同时符合人类价值观的模型。

---

## 2.4 蒸馏：将推理能力赋予小型模型

为使更小、更高效的模型继承DeepSeek-R1的推理能力，研究使用DeepSeek-R1生成的数据对开源模型（如Qwen、Llama）进行微调。

- 使用800k样本对Qwen2.5（1.5B~32B）和Llama（8B~70B）等模型进行SFT训练。
- 尽管引入强化学习可进一步提升性能，但本阶段只进行SFT，以验证蒸馏方法的有效性。
- 实验表明，该方法可显著提升小型模型的推理能力。

---

### 总结

本方法章节重点展示了以下内容：

1. **DeepSeek-R1-Zero**：无需监督数据，通过纯强化学习实现推理能力的自进化，取得了与监督模型相当的性能。
2. **DeepSeek-R1**：通过引入少量高质量冷启动数据，进一步提升推理能力与输出可读性。
3. **推理能力蒸馏**：将推理能力迁移到更小模型中，提升其推理能力，为实际部署提供基础。

本研究证明了：**强化学习可作为大语言模型推理能力训练的有效手段，尤其在少监督或无监督环境下具有巨大潜力**。


## 3 Experiment



### 3 Experiment 实验部分总结

#### **Benchmarks 基准测试**
本部分展示了对模型性能评估的多个基准数据集，涵盖了**通用常识推理（如MMLU、MMLU-Pro、C-Eval）**、**数学（如GPQA Diamond、AIME 2024）**、**编程（如LiveCodeBench、Codeforces）**、**中文理解（如C-SimpleQA、CLUEWSC）**等多个领域。

- **重点内容**：除了标准的基准测试，作者还采用 **LLMs作为评判者**（如AlpacaEval 2.0 和 Arena-Hard），使用GPT-4-Turbo-1106进行成对生成内容的评估。对于蒸馏模型，主要关注 AIME 2024、MATH-500、GPQA Diamond、Codeforces、LiveCodeBench 等任务。

#### **Evaluation Prompts 评估提示词**
- 使用了**简单提示框架（simple-evals）** 对 MMLU、DROP、GPQA 和 SimpleQA 等任务进行评估。
- MMLU-Redux 使用 **Zero-Eval** 零样本提示格式。
- MMLU-Pro、C-Eval 等原本使用少量样本提示（few-shot）的，这里改为**零样本提示**以避免DeepSeek-R1输出重复性问题。
- **重点内容**：代码与数学任务评估中，采用**CoT（Chain-of-Thought）** 推理格式，如 LiveCodeBench 与 Codeforces。DeepSeek-R1 的输出限制在 32,768 tokens。

#### **Baselines 基线模型**
- 对比了多个强模型：**DeepSeek-V3、Claude-3.5-1022、GPT-4o-0513、OpenAI-o1-mini 和 o1-1217**。由于 o1-1217 在中国大陆难以访问，其性能基于官方报告。
- 对于蒸馏模型，对比了**QwQ-32B-Preview**。
  
#### **Evaluation Setup 评估设置**
- **生成长度限制**：所有模型的最大输出长度为 32,768 tokens。
- **重点内容**：为了避免输出重复和偏差，使用 **pass@k** 评估方法（以 k 个生成结果中至少一个正确作为指标），默认温度 0.6，top-p 0.95。对于 AIME 2024，还使用 **consensus @64**（多数投票）方法。

---

### 3.1 DeepSeek-R1 评估

#### **主要结论**
- **DeepSeek-R1 在多数任务上优于 DeepSeek-V3**，尤其是在 STEM 相关任务、文档分析（如 FRAMES）、格式指令执行（如 IF-Eval）和开放领域问答（如 AlpacaEval 2.0、ArenaHard）中。
- **数学和编程能力突出**：在 AIME 2024、MATH-500、LiveCodeBench 和 Codeforces 等任务中，DeepSeek-R1 表现接近甚至超越 OpenAI-o1-1217。
- **中文任务表现**：在 CLUEWSC 和 C-Eval 中表现优异，但在 C-SimpleQA 中因**安全强化学习（safety RL）拒绝回答部分问题**而略逊。
- **输出长度控制良好**：避免了在 GPT 评估中引入长度偏差，输出更简洁、质量更高。

#### **关键数据**
- 在 MMLU 任务中，DeepSeek-R1 达到 90.8%，显著高于 GPT-4o 的 85.2%。
- 在 Codeforces 上，DeepSeek-R1 的排名达到 **2029**，接近 OpenAI-o1-1217 的 2061。
- 在 AIME 2024 中，DeepSeek-R1 的 Pass@1 为 79.8%，远超 GPT-4o 的 9.3%。

---

### 3.2 蒸馏模型评估

#### **主要结论**
- **DeepSeek-R1 蒸馏模型（如 -7B、-14B、-32B）在多个任务上优于原始模型和非推理模型**，如 GPT-4o-0513、Claude-3.5 和 QwQ-32B。
- **重点内容**：在 AIME 2024、MATH-500、GPQA Diamond、LiveCodeBench 和 Codeforces 上，DeepSeek-R1 蒸馏模型表现突出，特别是 32B 和 70B 模型接近甚至超越 OpenAI-o1-mini。
- **蒸馏效果显著**：通过简单蒸馏即可获得与大模型相当的推理能力，显示出**模型压缩与效率优化的潜力**。

#### **关键数据**
- DeepSeek-R1-Distill-Qwen-32B 在 AIME 2024 上 Pass@1 到达 72.6%，接近 o1-mini 的 63.6%。
- 在 Codeforces 上，DeepSeek-R1-Distill-Llama-70B 的评分达到 1633，接近 o1-mini 的 1820。
- **进一步优化空间**：作者指出，蒸馏模型结合强化学习（RL）还有进一步提升空间，但暂未展示。

---

### 总体总结
本章通过系统的实验评估，验证了 **DeepSeek-R1** 模型在多领域推理任务中的优越性能，尤其是在**数学、编程、多语言理解和文档分析**任务中。蒸馏模型的引入进一步证明了该模型在**高效推理和压缩部署**方面的潜力。未来的研究方向包括探索蒸馏模型与 RL 的结合，以进一步提升推理能力和工程表现。


## 4 Discussion



## 4 讨论

在本节中，作者对使用**知识蒸馏（Distillation）**与**强化学习（Reinforcement Learning, RL）**训练大语言模型（LLM）的方法进行了比较，并探讨了前期失败的尝试，如**Process Reward Model (PRM)**与**Monte Carlo Tree Search (MCTS)**。作者旨在分析这些方法的优劣，为后续研究提供经验和指导。

---

### 4.1 知识蒸馏 与 强化学习

在第3.2节中，我们看到通过**知识蒸馏**，小型模型能够达到令人印象深刻的效果。然而，仍存在一个问题：**是否可以在不使用蒸馏的情况下，通过本文中讨论的强化学习方法训练出具有相似性能的模型？**

为了解决这一问题，作者在**Qwen-32B-Base**模型上进行了**大规模强化学习训练**，使用数学、编程和STEM数据集，训练超过10,000步，最终得到**DeepSeek-R1-Zero-Qwen-32B**模型。实验结果如表6所示，显示出该模型在多个基准测试（如AIME 2024、MATH-500、GPQA Diamond、LiveCodeBench）中，性能与**QwQ-32B-Preview**相当。但相比之下，通过蒸馏得到的**DeepSeek-R1-Distill-Qwen-32B**在所有测试中表现更优。

由此可得出两个重要结论：

1. **知识蒸馏**是一种高效且经济的方法，能够将较大模型的能力迁移到小型模型上；而**大规模强化学习训练小型模型**则需要巨大的计算资源，且可能无法达到蒸馏的效果。
2. 虽然知识蒸馏在性能和成本上具有优势，但若要进一步突破模型的智能边界，可能仍需依赖更强大的基础模型和更大规模的强化学习训练。

---

### 4.2 不成功的尝试

在开发**DeepSeek-R1**的早期阶段，作者也尝试了一些方法，但结果并不理想。尽管这些失败不代表这些方法无法构建有效的推理模型，但它们在当前实验中并未取得成功，因此作者在此分享经验教训。

#### Process Reward Model (PRM)

PRM是一种通过奖励中间推理步骤来引导模型生成更优推理路径的方法。虽然理论上有一定潜力，但在实际应用中存在三个主要问题：

1. **定义细粒度推理步骤**较为困难，尤其是在通用推理任务中。
2. **判断中间步骤是否正确**极具挑战，自动化标注结果不佳，人工标注又难以扩展。
3. 引入基于模型的PRM容易导致**奖励劫持（reward hacking）**，即模型为了获得高奖励而采取不合理的策略。这不仅需要额外训练资源，也使整个训练流程变得复杂。

因此，尽管PRM在某些辅助任务（如重排序生成的Top-N答案）中表现不错，但在大规模强化学习过程中引入PRM所带来的**计算开销**远远超过了其优势。

#### Monte Carlo Tree Search (MCTS)

受到AlphaGo和AlphaZero的启发，作者尝试将**蒙特卡洛树搜索（MCTS）**应用于模型推理过程，以提升推理的系统性和可扩展性。该方法通过将答案分解为多个推理步骤，并使用预训练的价值模型引导探索路径。

然而，MCTS在大规模训练中也面临挑战：

1. 与国际象棋等结构清晰的搜索空间不同，**自然语言生成的搜索空间过于庞大**，即使设置节点扩展限制，也容易陷入**局部最优**。
2. **价值模型的质量**直接影响生成结果，而训练一个细粒度的价值模型本身就非常困难，导致模型难以迭代优化。
3. 尽管AlphaGo通过MCTS与价值模型的迭代提升了性能，但在当前模型中，由于**token生成的复杂性**，这一方法难以复现。

综上所述，MCTS虽可在推理时辅助生成更优答案，但通过自搜索实现模型性能的持续提升仍是一个重大挑战。

---

### 总结

本节通过对比**知识蒸馏**与**强化学习**的训练效果，得出蒸馏在性能与效率上的显著优势。同时，作者也探讨了PRM与MCTS等方法的尝试与失败，指出它们在当前框架下难以替代大规模强化学习或蒸馏策略。这些经验为未来构建更高效、更强大的推理模型提供了有益的参考。


## 5 Conclusion, Limitations, and Future Work

### 总结  
本文分享了通过强化学习（RL）提升模型推理能力的探索过程。  
- **DeepSeek-R1-Zero** 是一个**纯粹的强化学习模型**，不依赖冷启动数据（cold-start data），在多种任务中表现出色。  
- **DeepSeek-R1** 在此基础上，结合了**冷启动数据**和**迭代式强化学习微调**，性能更加强大，最终在多个任务上的表现**接近 OpenAI-o1-1217** 的水平。  

进一步，我们还探索了**将推理能力蒸馏到小规模密集模型**中。  
- 使用 DeepSeek-R1 作为教师模型，生成了 800K 训练样本，并对多个小模型进行了微调。  
- 实验结果令人鼓舞：例如，**DeepSeek-R1-Distill-Qwen-1.5B** 在数学基准测试中超越了 **GPT-4o** 和 **Claude-3.5-Sonnet**，在 AIME 上达到 **28.9%**，在 MATH 上达到 **83.9%**。  
- 其他小模型也表现出色，显著优于基于相同基础模型的指令微调模型。

---

### 局限性与未来工作  

#### 一般能力  
- 当前，DeepSeek-R1 在 **函数调用、多轮对话、复杂角色扮演、JSON 输出** 等任务上的能力仍**不及 DeepSeek-V3**。  
- **未来方向**：探索如何利用 **CoT（Chain of Thought）** 来增强这些领域的表现。

#### 多语言混合  
- DeepSeek-R1 当前主要针对 **中文和英文** 进行优化，处理其他语言时可能出现 **语言混合问题**。  
- 例如，即使查询是其他语言，模型也可能使用英文进行推理和回答。  
- **改进计划**：在未来版本中解决这一问题，提升多语言任务的适应能力。

#### 提示工程（Prompt Engineering）  
- DeepSeek-R1 对提示（prompt）较为敏感，**少样本提示（few-shot prompting）** 会显著降低性能。  
- **建议**：用户在使用时应采用 **零样本（zero-shot）** 的方式，直接描述问题并明确输出格式，以获得最佳效果。

#### 软件工程任务  
- 由于 **评估时间较长**，影响了强化学习的效率，目前未在软件工程任务中大规模应用 RL。  
- 因此，DeepSeek-R1 在软件工程基准上的表现 **相较于 DeepSeek-V3 并无显著提升**。  
- **未来计划**：通过 **拒绝采样（rejection sampling）** 或 **异步评估** 等方法，提高 RL 在软件工程任务中的效率和应用范围。

---

### 总结重点  
- DeepSeek-R1 通过 RL 提升了推理能力，性能接近 OpenAI-o1-1217。  
- 小模型蒸馏实验表现优异，优于多个大模型。  
- 当前仍存在多语言支持、提示敏感、软件工程任务效率等限制。  
- 未来将从多个方向推动模型能力的持续提升。


## Appendix

由于您提供的内容中没有附录部分的具体内容，因此无法进行详细总结。通常，“**Appendix**（附录）”是论文或报告中用于补充主文的重要部分，常见结构和内容如下：

---

### 1. **附录的作用**
- **补充说明**：提供主文中未展开讨论的详细数据、图表、公式推导、代码、问卷调查、原始数据等。
- **便于查阅**：方便读者查阅相关资料，避免主文过于冗长。
- **增强可信度**：通过提供原始资料或计算过程，增强论文的学术严谨性和可信度。

---

### 2. **常见附录内容**
- **数据表格**：详细的数据统计表、实验原始数据等。
- **代码片段**：实验中使用的程序代码（如Python、MATLAB等）。
- **问卷调查表**：用于实证研究的问卷原文。
- **公式推导**：较为复杂的数学或理论推导过程。
- **图表补充**：对正文图表的扩展或补充说明。
- **参考文献补充**：部分文献的扩展信息、翻译或原文摘录。

---

### 3. **附录的编写规范**
- **编号与命名**：每个附录应有独立编号（如 Appendix A、Appendix B），并附上标题。
- **顺序合理**：附录内容应按逻辑顺序或出现顺序排列，方便读者查找。
- **简洁清晰**：虽然附录内容补充性强，但仍需保持逻辑清晰、格式统一。
- **引用说明**：在正文中引用附录时应标明附录编号，如“见附录A”。

---

### 4. **注意事项**
- **不重要的内容不宜放入附录**：附录应用于补充主文无法涵盖的重要信息，而非随意堆放无关材料。
- **附录不宜过多**：过多附录可能影响论文整体结构，建议控制在3-5个以内。
- **格式统一**：附录应与论文其他部分保持一致的字体、字号、页边距等格式。

---

如您有具体附录内容需要总结，请提供相关内容，我可以为您进行详细分析。


## Appendix A Contributions and Acknowledgments



## 附录 A 贡献与致谢

**重点内容**：核心贡献者对项目具有关键性的技术或理论贡献，通常在研究设计、实现、数据分析等方面起到主导作用。

---

### 贡献者

本节列出了对项目或论文有贡献，但贡献程度略低于核心贡献者的人员名单。这些作者同样按姓名的首字母顺序排列，具体名单包括：

- Aixin Liu
- Bing Xue
- Bingxuan Wang
- Bochao Wu
- Bei Feng
- （此处略去大量姓名，有兴趣可查阅完整原文）

**重点内容**：该部分列出的人员可能在项目中参与了部分工作，如实验支持、数据收集、模块实现、辅助分析等，但不是核心的主导角色。

---

### 特别说明

- **按姓名首字母排序**：所有作者名单均按照姓名的首字母顺序排列，确保公平与规范。
- **带星号的姓名**：文中注明的带星号（\*）的姓名表示这些人员目前已离开团队，但曾为项目做出贡献。

---

### 生成信息

本附录由 [LaTeXML](http://dlmf.nist.gov/LaTeXML/) 自动生成，生成时间为 **2025年10月13日 16:10:19（星期一）**。系统附带了一个小图标，显示了LaTeXML的吉祥物（Sammy的小logo）。

---

**总结**：

本附录主要列出了在项目或论文中做出贡献的所有人员，分为“核心贡献者”和“贡献者”两类。人员按首字母顺序排列，带星号的人员表示已离开团队。最后，附录信息由LaTeXML自动生成，并附带了生成时间和吉祥物图标。
