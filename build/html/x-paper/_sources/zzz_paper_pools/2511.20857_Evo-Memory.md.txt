# 2511.20857_Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory



* 首页: <https://arxiv.org/abs/2511.20857>
* PDF: <https://arxiv.org/pdf/2511.20857>
* 引用: 
* 组织: 
    * Google DeepMind
    * University of Illinois Urbana-Champaign

## 总结


## From Moonlight


### 三句摘要


### 关键词



### 摘要



## Abstract


本论文强调了**状态性（statefulness）**对于大语言模型（LLM）代理在进行长期规划和问题解决中的重要性，指出**记忆（memory）**是实现这一能力的关键组件。然而，当前对记忆机制的研究和评估主要集中在**静态对话场景**，即通过对话历史检索信息来回答问题，忽略了在连续任务流中**经验的积累与复用**这一动态能力。

在实际应用中，如交互式问题助手或具身智能体，LLM需要处理**连续的任务流（task streams）**，但目前模型往往无法从交互中持续学习，导致上下文信息丢失。为此，作者提出**测试时演化（test-time evolution）**的概念，即在部署过程中不断检索、整合和更新记忆。

为填补这一研究空白，作者提出了**Evo-Memory**，这是一个用于评估LLM代理**自演化记忆（self-evolving memory）**能力的**流式任务基准与框架**。Evo-Memory将数据集组织为**顺序任务流**，要求LLM在每次交互后搜索、适应并演化记忆。

研究亮点包括：
- 统一并实现了**超过10种代表性记忆模块**；
- 在**10个多样化数据集**上进行评估，涵盖多轮目标导向任务、单轮推理与问答任务；
- 提出了一种经验复用的基线方法**ExpRAG**；
- 进一步设计了**ReMem**框架，采用**行动–思考–记忆更新**的集成流程，实现持续性能提升。

关键词：LLMs，代理记忆（Agentic Memory），测试时学习（Test-time Learning），自演化代理（Self-evolving Agents），终身智能（Lifelong Intelligence）


## 1 Introduction


本节介绍了当前大型语言模型（LLMs）在推理、规划和工具使用方面取得的进展，但指出一个核心能力——**记忆（memory）**——仍被广泛忽视。记忆能力使LLMs能够在交互中保持状态、积累经验并随时间调整策略。尽管已有研究引入了通过压缩、索引或检索来跟踪对话历史的记忆模块，提升了**对话回忆（conversational recall）**和个性化能力，但这些系统大多仅重复使用静态对话上下文，而未能从经验中学习以改进未来的推理或决策。

### 核心问题：记忆系统的静态性

当前的LLM记忆系统普遍存在**被动检索信息**而非**通过使用进化记忆**的问题。评估主要集中在模型是否能回忆过去的内容，而很少测试其**重用经验（experience reuse）**的能力。作者区分了两个关键概念：

- **Conversational recall**：回忆过去的事实（如解方程的方法）。
- **Experience reuse**：抽象出推理策略用于未来任务（如使用公式解题）。

若缺乏经验重用，模型在面对类似问题时会重复解决，即使长期助手能回忆上下文，也难以跨会话适应。

### 现有基准的局限性

作者列举了几个现有基准：

- **StreamBench**：评估顺序学习，但主要测量事实保留，不涉及推理或路径重用。
- **LifelongBench**：研究跨环境和技能的终身学习，但侧重于保留而非记忆结构或更新。
- **其他研究**（如hu2025evaluating等）：评估长期对话一致性，但未测试代理在部署期间如何演化记忆。

这些基准共同揭示了一个关键问题：虽然在顺序推理方面取得进展，但仍缺乏一个统一的框架来评估不同记忆方法在真实流式场景中**检索、整合和演化历史策略**的能力。

### 解决方案：Evo-Memory

为填补这一空白，作者提出**Evo-Memory**，一个全面的流式基准和评估框架，用于测试LLM代理的**自演化记忆（self-evolving memory）**能力。该框架通过图示说明：

- 一个有状态代理在面对多轮任务（如具身操作）和单轮任务（如解方程）时，应能从过去经验中学习可重用的策略。
- 任务以**任务流（task streams）**形式组织，要求模型在每次交互后检索、适应并演化记忆。

### 覆盖任务类型与记忆模块

Evo-Memory涵盖：

- **多轮目标导向任务**
- **单轮推理或问题解决任务**

并统一实现了**超过十个代表性记忆模块**，包括：

- 基于检索的记忆
- 工作流记忆
- 分层记忆系统

用于研究其适应行为。

### 新方法：ExpRAG 与 ReMem

为了进一步研究经验重用，作者提出：

- **ExpRAG**：一种基于检索的简单基线方法，利用先前任务经验。
- **ReMem**：一种先进的**行动–思考–记忆精炼（action–think–memory refine）**流程，将推理、行动与记忆更新紧密结合，实现持续改进。

### 贡献总结

1. **Benchmark（基准）**：
   - 提出Evo-Memory，评估LLM代理在多轮与单轮任务中的**测试时演化（test-time evolution）**能力，填补“对话回忆”与“经验重用”之间的空白。

2. **Framework（框架）**：
   - 提供统一的评估框架，包含以记忆为中心的指标，分析适应性、效率和稳定性，并承诺开源代码与配置以确保可复现性。

3. **Analysis and Insights（分析与洞见）**：
   - 提出ExpRAG作为经验重用的基线方法；
   - 开发ReMem，整合推理、行动与记忆更新的流程，为未来记忆系统设计提供指导。

---

总结：本节通过分析当前LLM记忆系统的局限性，提出Evo-Memory这一新基准，旨在推动LLM在真实场景中实现从“记住内容”到“学习策略”的转变，并通过新方法ExpRAG与ReMem提供实践路径。


## 2 Related Work

### 2.1 测试时学习  
**测试时学习**（TTL）起源于**测试时适应**（TTA），其目标是在模型部署过程中应对数据分布变化，使模型能够动态调整自身行为。例如，Tent (Wang et al., 2021)、Efficient TTA (Niu et al., 2022) 和 MEMO (Zhang et al., 2023) 等方法通过在线优化实现模型适应。

近年来，TTL 的研究进一步扩展为**持续自我改进**（continuous self-improvement），如 T3A (Iwasawa et al., 2021) 和 TTT++ (Liu et al., 2023) 等方法，使模型能够通过在线学习不断优化自身表现。

更进一步地，**基于智能体**（agent-based）的研究通过反思、规划和自我演化机制实现持续学习。例如：
- Reflexion (Shinn et al., 2023)：通过错误反馈进行自我反思；
- Voyager (Wang et al., 2023)：在复杂环境中自主探索与学习；
- Eureka (Zhou et al., 2024)：生成高质量指令；
- Generative Agents (Park et al., 2023)：模拟长期行为演化；
- Self-evolving Agents (Zhao et al., 2025)：实现模型结构与功能的协同进化；
- 更新的方法如 LLM-as-Optimizer (Chen et al., 2025) 和 Self-Discovering (Huang et al., 2025) 等，展示了模型如何自主修订计划、合成反馈并协同演化。

这些研究标志着模型从静态适应向**动态自适应、持续学习的智能体**转变。本文在此基础上，提出从**自演化记忆**的角度来评估这类动态演化过程。

---

### 2.2 自演化记忆  
早期的大型语言模型（LLM）记忆系统主要是**被动存储**（passive storage），用于保存最近对话或检索事实，以弥补上下文窗口的限制。例如：
- Retrieval-Augmented LMs (Lewis et al., 2020)
- Memory Bank (Zhong et al., 2023)
- LlamaIndex (Liu et al., 2022)
- MemGPT (Packer et al., 2023)

后续研究引入了更复杂的记忆管理机制，如：
- 可微读写控制器（RETLLM, Modarressi et al., 2023；SCM, Liang et al., 2023）
- 在真实对话场景下的评估（Maharana et al., 2024；Wu et al., 2024）

进一步地，研究开始探索**策略驱动的记忆控制**（policy-driven control），即模型被显式优化以决定何时存储、检索或覆盖记忆内容，例如：
- MemAgent (Yu et al., 2025)
- Mem (Xu et al., 2025)
- Mem1 (Zhou et al., 2025)
- Memory-Driven LMs (Yan et al., 2025)
- MemOS (Li et al., 2025)

此外，一些研究提出了**结构化记忆表示**，将经验组织为关系型或过程型知识，例如：
- RepoGraph (Ouyang et al., 2024)
- MEM0 (mem0.ai)
- Zep (Rasmussen et al., 2025)
- Dynamic Cheatsheets (Suzgun et al., 2025)

然而，目前尚缺乏统一的评估框架来衡量**自演化记忆**（self-evolving memory）能力，即模型在不同任务间复用和适应经验的能力。

本文提出的 **Evo-Memory** 框架正是沿着这一方向发展，不仅评估模型的存储与回忆能力，还关注其在任务流中对记忆的**演化、重组与复用**能力。

---

### 图3：ReMem 智能体框架概述  
图3展示了 ReMem 智能体的总体架构，分为两个部分：
- **左侧**：测试时演化过程，智能体在多个任务中迭代地搜索、合成并演化其记忆；
- **右侧**：智能体的三大核心模块：
  1. **Think**：负责推理与任务分解；
  2. **Refine Memory**：负责记忆的检索、剪枝与组织；
  3. **Act**：负责执行动作并与环境交互。

该架构体现了记忆在任务执行中的动态演化过程，是实现自演化记忆的关键机制。


## 3 Evo-Memory: Evaluating Self-Evolving Memory in LLM Agents

### 概述
本节提出 **Evo-Memory**，一个统一的基准，用于评估大语言模型（LLM）代理在推理过程中如何通过记忆的**自我演化**（self-evolving memory）来持续适应新任务。与以往将记忆视为静态回忆不同，Evo-Memory强调记忆的动态演化过程，包括**检索、整合与更新**。

文中图3展示了测试阶段的演化流程（左侧）和 ReMem 代理的三个模块：**Think（思考）**、**Act（行动）** 和 **Refine Memory（精炼记忆）**（右侧）。

随后，作者形式化了问题设定，并介绍了两个代表性实现：**ExpRAG** 和 **ReMem**，用于构建该基准。

---

### 3.1 问题设定（Problem Formulation）

#### 核心定义
作者将一个增强记忆的代理形式化为四元组：
$$
(F, U, R, C)
$$
其中：
- $ F $：基础 LLM；
- $ U $：记忆更新机制；
- $ R $：检索模块；
- $ C $：上下文构造机制。

代理处理输入序列 $ \{x_1, x_2, \ldots, x_T\} $，记忆状态 $ M_t $ 随时间演化。

在时间步 $ t $，代理执行以下步骤：

1. **检索（Search）**：
   $$
   R_t = R(M_t, x_t)
   $$
   使用相似性搜索、索引查找或注意力机制从记忆中提取相关信息。

2. **合成（Synthesis）**：
   $$
   \tilde{C}_t = C(x_t, R_t)
   $$
   将检索结果与当前输入结合，构造上下文。

3. **生成输出（Output）**：
   $$
   \hat{y}_t = F(\tilde{C}_t)
   $$

4. **演化（Evolve）**：
   $$
   m_t = h(x_t, \hat{y}_t, f_t), \quad M_{t+1} = U(M_t, m_t)
   $$
   构建新记忆条目 $ m_t $，并更新记忆状态 $ M_{t+1} $。不同算法对 $ U $ 的实现方式不同，如追加、压缩、替换等。

#### 数据集构建
Evo-Memory 将传统静态数据集重构为**流式任务序列**：
$$
\tau = \{(x_1, y_1), \ldots, (x_T, y_T)\}
$$
形成一个真实轨迹，其中早期任务为后续任务提供关键信息。

代理在每一步生成预测轨迹：
$$
(x_1, \hat{y}_1, M_1) \rightarrow \cdots \rightarrow (x_T, \hat{y}_T, M_T)
$$
这种设计将静态基准转化为动态交互流，评估 LLM 在部署过程中积累、适应和精炼知识的能力。

---

### 3.2 ExpRAG: Experience Retrieval and Aggregation

#### 概述
ExpRAG 是一种任务级检索增强代理，作为简单基线方法。

#### 实现步骤
1. 每个记忆条目为结构化经验文本：
   $$
   m_i = S(x_i, \hat{y}_i, f_i)
   $$
   其中 $ S $ 是模板函数。

2. 在时间步 $ t $，代理根据检索评分函数 $ \phi $ 检索 $ k $ 个最相似的经验：
   $$
   R_t = \text{Top-}k_{m_i \in M_t} \phi(x_t, m_i)
   $$

3. 模型基于这些经验进行上下文学习：
   $$
   \hat{y}_t = F(x_t, R_t)
   $$

4. 将新经验追加到记忆中：
   $$
   M_{t+1} = M_t \cup \{(x_t, \hat{y}_t, f_t)\}
   $$

#### 特点
ExpRAG 通过检索和聚合实现**一次经验复用**，但缺乏推理迭代和记忆自适应优化。

---

### 3.3 ReMem: Synergizing Reasoning, Acting, and Memory

#### 概述
ReMem 是一个统一推理、行动与记忆精炼的框架，引入了**记忆推理**（memory reasoning）维度，使代理在解决问题过程中主动评估、重组和演化记忆。

#### 核心机制
在每一步 $ t $，代理根据当前输入 $ x_t $、记忆状态 $ M_t $ 和历史推理轨迹 $ o^{1:n-1}_t $，选择以下三种操作之一：
$$
a_t^n \in \{\text{Think}, \text{Act}, \text{Refine}\}
$$

执行操作后，输出为：
$$
o_t^n = \text{Agent}(x_t, M_t, a_t^n)
$$

#### 操作说明
- **Think**：生成内部推理轨迹，分解任务并指导后续动作；
- **Act**：执行环境操作或输出用户可见结果；
- **Refine**：对记忆进行元推理，提取有用经验、剪枝噪声、重组记忆 $ M_t $，以支持未来推理与行动。

#### 决策过程
- 每一步可多次执行 Think 和 Refine；
- 一旦选择 Act，该步结束；
- 整个过程构成一个马尔可夫决策过程（MDP）：
  - 状态：$ s_t^n = (x_t, M_t, o^{1:n-1}_t) $
  - 动作空间：$\{\text{Think}, \text{Act}, \text{Refine}\}$
  - 转移函数：由 Agent 操作和环境响应决定

#### 优势
ReMem 扩展了 ReAct 类代理的行动空间，引入了**显式记忆推理机制**，使记忆成为与推理实时交互的动态组件，而非被动上下文。

通过将反思与记忆演化结合，ReMem 建立了自适应、自我改进的 LLM 代理新标准。

---

### 总结

| 章节 | 内容重点 | 精简说明 |
|------|----------|----------|
| 3.1 问题设定 | 形式化记忆代理结构，定义检索、合成、演化流程，提出流式任务序列 | 提出统一框架，强调记忆动态演化 |
| 3.2 ExpRAG | 基于检索的经验复用，结构化记忆条目，Top-k 检索，上下文学习 | 简单基线，缺乏推理与记忆优化 |
| 3.3 ReMem | 引入记忆推理模块，支持 Think/Act/Refine 三种操作，形成马尔可夫决策过程 | 强调记忆与推理的协同演化，提升代理自适应能力 |

---

如需进一步分析具体实验结果或表格数据，请提供后续章节内容。


## 4 Experiments


本节在统一的测试时学习框架下，评估了主流大语言模型（LLMs）在Evo-Memory基准上的表现，聚焦于五个关键研究问题（RQs）：

- **RQ1**：LLM代理在Evo-Memory的多领域和任务类型中的表现如何？ReMem是否增强了它们的测试时学习能力？
- **RQ2**：哪些因素影响不同任务中记忆的有效性？经验重用如何提高任务效率？
- **RQ3**：任务序列难度（如简单 vs. 困难轨迹）如何影响记忆适应和泛化？
- **RQ4**：不同类型的反馈如何影响任务间的动态学习和记忆优化？
- **RQ5**：累积性能如何随任务序列和时间步演变，反映部署期间的持续适应？

### 4.1 实验设置

Evo-Memory在现实的流式多任务条件下评估记忆机制。以下描述了基准数据集、评估指标和对比方法。

#### 4.1.1 数据集

Evo-Memory评估涵盖事实知识、推理、数学、编程和目标导向交互的多样化数据集：

- **事实与推理**：MMLU-Pro 和 GPQA-Diamond
- **数学问题求解**：AIME-24 和 AIME-25
- **工具使用与API**：ToolBench
- **多轮与目标导向交互**：AgentBoard（包括AlfWorld、BabyAI、ScienceWorld、Jericho和PDDL任务）

所有方法在统一的 *search–predict–evolve* 循环中进行评估：

$$
(x_t, M_t) \xrightarrow{\text{search}} R_t \xrightarrow{\text{synthesis}} \hat{y}_t \xrightarrow{\text{evolve}} M_{t+1}
$$

反馈 $ f_t $ 被视为正确性信号。

#### 4.1.2 评估指标

Evo-Memory从四个维度评估任务表现和记忆质量：

1. **答案准确率**：单轮任务中模型输出是否正确。
2. **成功率和进度率**：多轮任务中目标完成情况。
3. **步骤效率**：完成目标所需的步骤数，反映推理简洁性。
4. **序列鲁棒性**：不同任务顺序下性能是否稳定。

这些指标评估代理如何学习、适应和随时间重用知识。

#### 4.1.3 方法

评估方法分为四类：

1. **无持久记忆的代理**：如 ReAct 和 Amem，依赖短期上下文或轻量缓存。
2. **自适应代理记忆方法**：如 SelfRAG、MemOS、Mem0 和 LangMem，支持动态检索和持续更新。
3. **程序性知识代理**：如 Dynamic Cheatsheet（DC）和 Agent Workflow Memory（AWM），强调可重用的工作流和任务策略。
4. **提出的演化记忆框架**：包括 ExpRecent、ExpRAG 和 ReMem，统一推理、行动和记忆优化。

所有方法在统一的 *search–predict–evolve* 协议下评估，以隔离记忆设计的影响。

| LLM 基座 | 方法 | AIME24 | AIME25 | GPQA | MMLU-Pro (Eco.) | MMLU-Pro (Eng.) | MMLU-Pro (Philo.) | ToolBench | 平均值 ↑ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Claude 3.7 Sonnet | ReMem | 0.13 | 0.13 | 0.67 | 0.86 | 0.65 | 0.80 | 0.87/0.71 | 0.58 |
| Gemini 2.5 Flash | ReMem | 0.60 | 0.53 | 0.51 | 0.85 | 0.46 | 0.79 | 0.85/0.71 | 0.65 |

表1：单轮推理和问答数据集上的跨数据集结果。

| LLM 基座 | 方法 | AlfWorld | BabyAI | PDDL | ScienceWorld | 平均 |
| --- | --- | --- | --- | --- | --- | --- |
| Gemini 2.5 Flash | ReMem | 0.66 | 0.81 | 0.22 | 0.33 | 0.50 |
| Claude 3.7 Sonnet | ReMem | 0.92 | 0.96 | 0.83 | 0.95 | 0.78 |

表2：多轮推理环境下的跨环境结果。

### 4.2 实验

为回答上述研究问题，进行了以下实验。

### 4.3 结果分析（RQ1）

表1和表2总结了单轮和多轮设置下的结果。总体来看，自我演化记忆架构提供了持续改进。

- **单轮推理和问答基准**：ReMem在Gemini-2.5 Flash下达到0.65的平均精确匹配和0.85/0.71的API准确率。
- **多轮推理环境**：ReMem和ExpRAG在Gemini-2.5和Claude基座上表现强劲，如在BabyAI上达到0.92/0.96。

图4显示ReMem的性能增益与数据集内任务相似性高度相关（Pearson r=0.717）。

图5比较了四个环境中的步骤效率，ReMem始终需要更少步骤完成任务。

### 4.4 记忆改进分析（RQ2）

图4显示ReMem的性能增益与任务相似性高度相关。任务相似性通过计算每个任务嵌入与其数据集聚类中心的平均余弦距离来衡量。较小的平均距离表示更高的数据集内一致性，从而促进记忆重用和泛化。

图5比较了步骤效率，ReMem在所有数据集中始终需要更少步骤完成任务。

| 方向 | 方法 | AlfWorld | ScienceWorld | 平均 |
| --- | --- | --- | --- | --- |
| Easy→Hard | ReMem | 0.91 | 0.63 | 0.77 |
| Hard→Easy | ReMem | 0.94 | 0.68 | 0.81 |

表3：不同序列难度方向下的记忆代理比较。

### 4.5 任务序列：简单 vs. 困难（RQ3）

表3显示，基线方法在从简单到困难的任务中表现明显下降，而ReMem在两种方向下保持强劲表现，达到0.94/0.97的成功率和进度率。

这表明持续反思使ReMem即使在任务复杂度变化时也能保留可迁移知识。

### 4.6 反馈分析（RQ4）

表4评估了代理在存储成功和失败任务经验时的表现。基线方法在暴露于未过滤的失败时性能明显下降，而ReMem通过主动优化存储经验保持稳健。

这表明选择性利用（从成功中学习，适当利用失败信息）对稳定测试时适应至关重要。

### 4.7 时间步性能（RQ5）

图6显示了四个交互环境中的累计成功率。ReMem在所有环境中始终实现更快的适应和更稳定的保留。

这些结果表明持续反思使ReMem在长时间任务序列中保持性能，展示了其在测试时学习中的鲁棒性。


## 5 Conclusion

**核心内容：**
- **研究背景与问题**：当前关于大语言模型（LLM）的研究主要集中在静态对话记忆（如对话历史的回顾），而忽视了模型在任务流演变过程中如何积累和复用经验。
- **本文贡献**：提出 **Evo-Memory** 框架，将静态数据集转化为流式任务轨迹，系统评估 LLM 在交互过程中如何检索、适应和优化记忆。
- **实验发现**：
  - 记忆机制显著提升模型性能；
  - 但记忆在**稳定性**和**过程复用**方面仍存在脆弱性。
- **方法创新**：
  - 提出 **ExpRAG**：用于经验检索与聚合；
  - 提出 **ReMem**：实现推理、行为与记忆更新的交错机制。
- **未来展望**：希望 Evo-Memory 成为构建具有**可靠且持续进化记忆能力**的 LLM 的统一平台。

---

## 附录（Appendix）

### 1. 2.1 Test-time Learning（测试时学习）
- 回顾了 LLM 在测试阶段通过交互进行学习的相关研究；
- 强调了当前方法在记忆演化方面的不足。

### 2. 2.2 Self-evolving Memory（自演化记忆）
- 梳理了记忆演化机制的研究现状；
- 指出现有方法多为静态记忆，缺乏动态更新与复用机制。

### 3.1 Problem Formulation（问题定义）
- 定义了 Evo-Memory 的任务设置：模型在连续任务流中通过交互不断演化记忆；
- 数学形式化为：  
  $$
  M_t = f(M_{t-1}, x_t, y_t)
  $$  
  其中 $M_t$ 表示第 $t$ 步的记忆状态，$x_t$ 和 $y_t$ 分别为输入与输出。

### 3.2 ExpRAG: Experience Retrieval and Aggregation（经验检索与聚合）
- 提出 ExpRAG 方法，用于从历史经验中检索相关信息并进行聚合；
- 包括以下步骤：
  1. **检索**：基于当前输入 $x_t$，从记忆库中检索相关经验；
  2. **聚合**：使用注意力机制对检索结果进行加权融合；
- 公式表示为：
  $$
  e_t = \text{Retrieve}(x_t, M), \quad \hat{e}_t = \text{Aggregate}(e_t)
  $$

### 3.3 ReMem: Synergizing Reasoning, Acting, and Memory（推理、行为与记忆的协同）
- 提出 ReMem 框架，将推理、行为执行与记忆更新三者结合；
- 实现模型在执行任务的同时更新记忆，提升长期任务表现；
- 框架流程图与伪代码在附录中展示。

### 4.1 Experimental Setup（实验设置）

#### 4.1.1 Datasets（数据集）
- 使用多个任务数据集，包括问答、对话、推理等；
- 数据被组织为任务流形式，模拟真实交互场景。

#### 4.1.2 Evaluation（评估指标）
- 主要指标包括：
  - 准确率（Accuracy）
  - 记忆一致性（Memory Consistency）
  - 任务完成效率（Task Efficiency）

#### 4.1.3 Methods（方法对比）
- 对比方法包括：
  - 基线模型（无记忆机制）
  - 基于RAG的模型
  - 基于强化学习的记忆更新方法

### 4.2 Experiments（实验设计）
- 设计了多个实验验证记忆演化机制的有效性；
- 包括不同任务顺序、反馈机制、时间步长等变量。

### 4.3 Analysis of Results (RQ1)（结果分析 - 研究问题1）
- **RQ1：记忆机制是否提升模型性能？**
  - 实验结果显示，使用记忆机制的模型在多个任务上显著优于无记忆模型；
  - 表格数据展示不同方法在各任务上的准确率对比。

### 4.4 Analysis of Memory Improvement (RQ2)（记忆改进分析 - 研究问题2）
- **RQ2：记忆是否随任务流演化而改进？**
  - ExpRAG 和 ReMem 显著提升了记忆的复用效率；
  - 图表显示记忆检索准确率随任务数增加而提升。

### 4.5 Task Sequence: Easy vs. Hard (RQ3)（任务顺序影响 - 研究问题3）
- **RQ3：任务顺序是否影响记忆演化？**
  - 先易后难的任务序列更有利于记忆积累；
  - 表格对比了不同任务顺序下的性能差异。

### 4.6 Analysis of Feedback (RQ4)（反馈机制分析 - 研究问题4）
- **RQ4：反馈信息如何影响记忆更新？**
  - 引入反馈机制（如奖励信号）可显著提升记忆更新效率；
  - 图表展示了有无反馈机制下的记忆一致性对比。

### 4.7 Performance w.r.t Time Steps (RQ5)（时间步长性能分析 - 研究问题5）
- **RQ5：模型性能如何随时间步长变化？**
  - 长时间步下，记忆机制表现出更强的稳定性；
  - 曲线图展示了不同方法在不同时间步下的性能变化。

---

## 6. A Experimental Details（实验细节）

### A.1 Datasets（数据集详情）
- 列出各数据集来源、任务类型与样本数量；
- 包括 SQuAD、HotpotQA、MultiWOZ 等。

### A.2 Configuration（配置参数）
- 模型参数、训练设置、检索策略等；
- 包括 LLM 类型（如 Llama-3、ChatGPT）、检索 Top-K 值等。

### A.3 Evaluation（评估细节）
- 详细说明评估指标计算方式；
- 如准确率、F1 分数、BLEU 等。

### A.4 Methods（方法实现细节）
- ExpRAG 与 ReMem 的具体实现步骤；
- 包括检索器、聚合器、记忆更新器的结构。

---

## 7. B Experiments（补充实验）

### B.1 Additional Experiments（附加实验）
- 包括不同模型规模、不同任务组合的实验；
- 展示方法的泛化能力。

### B.2 Additional Analysis of Memory Pruning（记忆剪枝分析）
- 探讨记忆库过大时的剪枝策略；
- 包括基于时间、重要性、相似度的剪枝方法。

### B.3 Additional Comparative Curves on Single-turn Tasks（单轮任务对比曲线）
- 展示单轮任务下不同方法的性能变化曲线；
- 用于验证记忆机制在简单任务中的有效性。

---

## 8. C Prompts（提示模板）
- 提供了用于实验的提示模板（Prompt Templates）；
- 包括任务提示、记忆检索提示、反馈提示等。

---

## 9. D Limitations（局限性）
- 当前方法在长序列记忆保持、跨任务泛化方面仍存在限制；
- 模型对噪声反馈敏感，需进一步优化鲁棒性。

---

## 10. E Use of Large Language Models（大语言模型使用说明）
- 说明实验中使用的 LLM 模型来源、使用方式；
- 包括是否调用 API、是否进行微调等。

---

**总结：**
Evo-Memory 提出了一种新的 LLM 记忆演化评估框架，强调记忆在任务流中的动态演化能力。通过 ExpRAG 和 ReMem 方法，显著提升了模型在复杂任务中的表现。论文附录提供了详尽的实验设置、方法实现与补充分析，为后续研究提供了坚实基础。


## Appendix A Experimental Details

本节详细介绍了 Evo-Memory 所使用的实验设置，包括数据集、配置、评估指标和对比方法，旨在在现实的流式多任务环境下评估记忆机制。

---

### **A.1 数据集（Datasets）**

本节介绍用于评估的多样化基准数据集，涵盖事实知识、推理、数学、编程和目标导向交互等任务。

#### **单轮问答数据集（Single-Turn Datasets）**
- **MMLU-Pro**：扩展自原始 MMLU，具有更强的鲁棒性和挑战性，包含工程、哲学、经济学等领域的难题，适合评估跨学科推理能力。
- **GPQA-Diamond**：研究生级别的物理科学题库，问题由专家编写，具有“防谷歌”特性，要求多步推理。
- **AIME-24 / AIME-25**：美国数学邀请赛题目，测试符号操作和精确匹配条件下的问题解决能力。
- **ToolBench**：评估模型调用和配置外部 API 的能力，反映实际工具使用能力。

#### **多轮目标导向数据集（Multi-Turn, Goal-Oriented Benchmarks）**
- **AlfWorld**：家庭任务指令执行。
- **BabyAI**：基于语言的导航与组合推理。
- **ScienceWorld**：开放式科学实验模拟。
- **Jericho**：基于文本的游戏探索。
- **PDDL 任务**：符号规划任务。

这些数据集共同构成了一个全面的评估体系，涵盖静态知识回忆、领域专长、数学推理和程序性记忆，适用于评估 LLM 在学术和现实场景中的学习、行动与适应能力。

---

### **A.2 配置（Configuration）**

为保证高效检索和方法间公平比较，使用 **BAAI/bge-base-en-v1.5** 编码器作为检索器，对查询和记忆项进行索引。

#### **检索机制**
- 当前问题编码为查询向量，与记忆嵌入进行相似度比较，取 top-k（默认 k=4）个最相关项用于上下文增强。
- 所有方法使用相同的检索预算（即相同 k 值）。
- 为提高效率，对检索结果和任务输入进行截断，以适应生成模型的提示长度限制。

#### **方法比较设置**
- 所有基线方法使用相同检索配置，但部分方法（如 Self-RAG、ReMem）引入了自适应检索模块，决定“是否检索”和“检索什么”。
- 所有模型在每个数据集中使用统一的任务顺序，确保记忆演化过程一致。
- 检索与生成在同一管道中进行，检索项按相关性从高到低附加在提示中。

#### **模型基础架构**
- 使用 **Gemini-2.5 系列**（Flash、Flash-Lite、Pro）和 **Claude 家族**（3.5-Haiku、3.7-Sonnet）作为主干模型。

---

### **A.3 评估（Evaluation）**

Evo-Memory 从以下四个维度评估任务表现与记忆质量：

1. **答案准确率（Answer Accuracy）**：衡量模型是否能正确回答问题，反映其整合过往经验的能力。
2. **成功率（Success Rate）**：衡量模型是否能完成目标导向任务，体现整体有效性。
3. **步骤效率（Step Efficiency）**：衡量完成任务所需的步骤数，评估记忆是否提升推理效率。
4. **序列鲁棒性（Sequence Robustness）**：衡量模型在不同任务顺序下是否保持稳定表现，反映其经验复用的稳定性。

---

### **A.4 方法（Methods）**

本节对比了多种代理与记忆架构，研究不同设计对“测试时记忆演化”的影响。所有方法均基于 Gemini-2.5 和 Claude-3.5/3.7，重点在于记忆架构与更新策略的比较，而非模型主干能力。

#### **1. 无持久记忆的代理管道（Agent Pipelines without Persistent Memory）**
- **ReAct**：结合推理与动作的典型流程，记忆仅限于当前上下文。
- **Amem**：在 ReAct 基础上加入轻量级代理记忆，缓存近期观察与反思，提供最基础的经验复用。

#### **2. 自适应代理记忆方法（Adaptive Agentic Memory Methods）**
- **SelfRAG**：结合动态检索与反思机制，使推理过程能自适应地依赖历史上下文。
- **MemOS、Mem0、LangMem**：结构化代理级记忆系统，支持读、写、更新操作，体现持续优化能力。

#### **3. 程序性记忆代理（Memory-Based Agents for Procedural Memory）**
- **Dynamic Cheatsheet (DC)** 和 **Agent Workflow Memory (AWM)**：强调“如何做”知识的复用，而非静态事实。
- **DC-RS（基于检索）** 与 **DC-Cu（人工整理）**：用于分析流程归纳与更新机制对稳定性与迁移能力的影响。

#### **4. 提出的方法：演化记忆框架（Proposed: Evolving Memory Framework）**
- **ExpRecent**：维护近期任务轨迹的压缩片段。
- **ExpRAG 家族**：结合检索增强推理与显式“测试时演化”机制。
- **ReMem**：通过迭代反思与合成机制优化记忆嵌入。

这些方法体现了 Evo-Memory 的设计理念：将推理、行动与记忆优化视为交织过程，在部署过程中协同演化，实现持续自我提升和更接近人类的适应能力。

---

### **总结**

本附录为 Evo-Memory 提供了完整的实验设置说明，涵盖数据集选择、模型配置、评估维度与对比方法。通过多样化的任务设置和统一的评估标准，系统地研究了不同记忆架构在流式多任务环境下的演化能力，尤其强调了记忆机制在推理、决策和经验复用中的作用。


## Appendix B Experiments


本节提供了更多实验内容，进一步验证所提出方法的有效性。

---

### B.1 附加实验  
作者在多个模型家族（如 Gemini-2.5-Flash-Lite、Claude-3.5-Haiku）和多种数据集上进行了广泛的基准测试，结果如表5和表6所示。实验结果显示，无论是在多轮具身推理任务（AlfWorld, BabyAI, PDDL, ScienceWorld）还是单轮推理任务（AIME-24/25, GPQA, MMLU-Pro, ToolBench）中，**ReMem** 方法始终优于传统基线方法和自适应检索方法。

#### 表5：跨环境实验结果  
- **任务类型**：多轮具身推理任务（AlfWorld, BabyAI, PDDL, ScienceWorld）  
- **指标**：成功率（S）和进度率（P）  
- **重点结果**：  
  - ReMem 在多个模型（Gemini、Claude）上均表现最优，平均成功率达到 0.50（Gemini 2.5 Pro）和 0.78（Claude 3.5 Haiku）。  
  - 相比于传统方法（如 Baseline、History、ReAct），ReMem 显著提升了任务完成率和进度。  
  - 表明 ReMem 的优势具有模型无关性，适用于不同架构的 LLM。

#### 表6：跨数据集实验结果  
- **任务类型**：单轮推理任务（AIME24/25, GPQA, MMLU-Pro, ToolBench）  
- **指标**：准确率（Exact Match）和 API 准确率  
- **重点结果**：  
  - ReMem 在多个模型（Claude 3.7 Sonnet、Gemini 2.5 Flash）上均取得最高平均准确率（如 0.58 和 0.65）。  
  - 在 GPQA 和 ToolBench 上表现尤为突出，说明其在复杂推理和工具调用方面具有优势。  
  - 表明 ReMem 在零样本和任务适应性方面优于现有记忆方法（如 Mem0、SelfRAG、ExpRAG）。

---

### B.2 记忆剪枝的附加分析  
图7展示了不同数据集上的记忆剪枝率，揭示了记忆保留与任务多样性的关系：

- **关键发现**：  
  - 数据集领域覆盖越广（如 GPQA，涵盖工程、物理等领域），剪枝率越高（36.8%），表明系统能有效识别并剔除冗余记忆。  
  - 任务类型较集中的数据集（如 AIME）剪枝率较低（10.8%~17.5%），说明记忆更相关。  
  - 剪枝机制能有效识别领域无关的经验，但任务多样性与记忆选择性之间的关系仍需进一步研究。

---

### B.3 单轮任务的附加对比曲线  
图8展示了 ReMem 与基线方法在单轮推理任务上的累计准确率曲线：

- **实验设置**：在 Gemini-2.5-Flash-Lite 和 Claude-3.7-Sonnet 模型上测试 GPQA、ToolBench 和 MMLU-Pro（工程类）任务。  
- **关键结果**：  
  - 随着任务实例的增加，ReMem 的准确率持续提升，尤其在 GPQA 和 ToolBench 上表现显著优于基线。  
  - 初期由于冷启动阶段，History 方法表现接近 ReMem，但随着任务积累，ReMem 明显超越，显示出持续任务级适应的优势。  
  - 曲线显示 ReMem 收敛更快，最终准确率更高。

---

### 总结  
本节通过多个实验验证了 ReMem 在不同模型和任务上的有效性：

- **核心优势**：ReMem 在多轮和单轮任务中均优于现有方法，具有模型无关性和任务适应性。  
- **关键机制**：通过持续任务级反思和记忆剪枝，有效保留关键经验，剔除冗余信息。  
- **未来方向**：进一步研究任务多样性与记忆选择性之间的关系，以及在更广泛任务中的泛化能力。


ENVIRONMENT INSTRUCTIONS



请提供具体的论文章节内容，我将根据您给出的“ENVIRONMENT INSTRUCTIONS”部分进行总结。目前该部分内容为空，仅包含标题和占位符[Detailed task environment description and rules]。如能提供详细段落文本，我将按照您的要求进行结构化总结。


EXAMPLE DEMONSTRATIONS


本节通过静态的少样本示例（static few-shot examples）来展示模型在特定任务中的推理与决策过程。每个示例包含目标（Goal）、动作（Action）和观察结果（Observation）三个部分，用于模拟任务执行的流程。
示例1  
- **目标（Goal）**：描述任务的初始目标，例如完成某个推理或操作任务。  
- **动作（Action）**：模型根据当前状态选择执行的操作，如调用某个工具、进行推理或输入指令。  
- **观察结果（Observation）**：执行动作后获得的反馈信息，用于模型进一步决策。

> **重点说明**：该示例展示了模型如何在有限的上下文信息下，通过逐步推理和外部工具调用完成任务。虽然示例中未涉及复杂的数学公式或算法步骤，但体现了模型在交互式环境中的决策逻辑。

---

总结：本节通过结构化的示例展示模型在任务驱动场景下的推理流程，强调了动作选择与反馈观察的交互机制，适用于少样本提示（few-shot prompting）下的任务执行模拟。


RELEVANT EXPERIENCE FROM SIMILAR TASKS


本节主要通过列举一个或多个与当前任务目标相似的历史任务经验，来辅助当前任务的决策或方法设计。每个经验包括目标、执行过程（动作序列）以及最终是否成功。

[Experience #1]

- **Goal: [similar goal]**  
  当前经验的目标与当前任务的目标相似，可能是解决同一类问题，例如路径规划、任务调度或控制系统设计等。该目标的设定为后续动作的选择提供了方向。

- **Trajectory: [action sequence]**  
  描述了在该经验中所采取的一系列动作或决策步骤。这些动作可能是基于某种策略或算法生成的，例如使用强化学习选择动作、基于规则的决策系统，或是人工设定的控制策略。这部分内容对于理解成功或失败的原因至关重要。

- **Correctness: [success/failure]**  
  该经验的最终结果是成功还是失败。这一信息可用于评估所采取策略的有效性，并为当前任务提供经验借鉴。如果该经验成功，可以考虑复用其策略；如果失败，则应避免类似的决策路径。

### 总结：
本节通过回顾一个与当前任务相似的历史经验，分析其目标、执行过程和结果，为当前任务提供参考。重点在于通过历史经验的成败来指导当前任务的策略设计或优化。若该经验成功，其动作序列可作为候选方案；若失败，则需调整策略以避免相同错误。

OUR CURRENT TASK



看起来您可能误操作了，当前没有可总结的论文章节内容。请提供具体的论文章节内容，我将根据您的要求进行总结。

RECENT HISTORY


**内容总结：**

本节标题为“近期历史”，但根据所给内容来看，它实际上展示了一个交互式环境中的**观察-行动序列**，可能是强化学习或智能体（agent）与环境交互的记录。结构上是重复的“Observation: ...”和“Action: ...”条目，表示智能体在某个环境状态（Observation）下采取了某个动作（Action），然后环境返回新的状态，智能体再次做出决策。

**重点内容：**

- **交互序列结构**：  
  展示了智能体在连续环境交互中的行为轨迹，包括初始环境状态、前一步动作、以及该动作带来的新环境状态。这种结构常见于强化学习任务中，用于记录**经验回放（experience replay）**或用于策略更新的历史轨迹。

- **格式说明**：  
  每一行都以“Observation:”或“Action:”开头，分别表示环境反馈和智能体决策。例如：
  ```
  Observation: [initial environment state]
  Action: [previous action]
  Observation: [result of previous action]
  Action: [previous action]
  ```

  这种格式有助于追踪智能体的决策过程和环境变化，便于后续分析其行为逻辑或训练模型。

**非重点内容：**

- 本节未涉及具体数学公式、算法步骤或表格数据，仅是交互过程的文本记录。

**总结：**  
“近期历史”节展示了一个智能体与环境交互的观察-动作序列，用于记录历史状态和动作，可能用于后续的学习或分析过程。结构清晰，但没有深入的理论或数据支撑。


## Appendix D Limitations

本节讨论了Evo-Memory研究中存在的实际限制。

**重点内容：**

- **模型选择受限**：由于预算和API限制，研究仅选取了一组较强的LLM（大语言模型），而非覆盖所有可用模型。这可能影响结论的广泛适用性。
- **任务类型有限**：当前的基准测试主要集中在**文本型和目标导向型任务**，未充分涵盖多模态或真实世界环境。未来扩展至这些领域将有助于更全面地评估持续记忆演化能力。

**次要内容（精简讲解）：**

- 作者指出，尽管存在上述限制，目前的研究已在多个领域、任务和架构上进行了广泛实验，为后续研究打下了坚实基础。
- 未来可进一步在**开源模型**和**多语言模型**上进行验证，以增强结论的普适性。

**注：本节未涉及数学公式、算法步骤或表格数据。**


## Appendix E Use of Large Language Models

本节总结了在论文撰写过程中对大语言模型（LLMs）的使用情况，具体如下：

### 1. 使用目的  
作者有限且受控地使用了大语言模型（如 ChatGPT），仅作为**辅助写作工具**。  
主要用途是**语言风格优化**，包括提升文本的清晰度、语法和可读性。  
所有内容的原始文本仍由作者撰写。

### 2. 使用范围  
- **未参与科学内容**：LLMs 没有参与论文的科学思想、分析、实验设计或结论推导过程。  
- **仅限语言编辑**：其作用被严格限制在语言润色层面，不涉及任何学术或科学内容的生成。

### 3. 结论  
LLMs 在本研究中仅作为**语言编辑工具**使用，确保了论文的学术原创性和科学严谨性。

---

**总结**：本节明确说明了大语言模型在论文写作中的辅助角色，强调其仅用于语言润色，不涉及科研内容的创作。
