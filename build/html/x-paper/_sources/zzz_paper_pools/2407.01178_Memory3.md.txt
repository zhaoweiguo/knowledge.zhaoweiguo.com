# 2407.01178_Memory3: Language Modeling with Explicit Memory

* [https://arxiv.org/abs/2407.01178](https://arxiv.org/abs/2407.01178)
* PDF: [https://arxiv.org/pdf/2407.01178](https://arxiv.org/pdf/2407.01178)
* 引用: 6(2025-07-29)
* 组织:
    * 1Center for LLM, Institute for Advanced Algorithms Research, Shanghai
    * 2Moqi Inc
    * 3Center for Machine Learning Research, Peking University



## Language Modeling with Explicit Memory



文章《Memory3: Language Modeling with Explicit Memory》主要探讨了一种名为 Memory3 的语言模型，其核心特点是引入了**显式记忆机制**（Explicit Memory），以增强模型在处理语言任务时的推理能力与信息存储能力。

### 研究背景与动机：
传统语言模型（如 GPT、Transformer 等）依赖于隐式学习（implicit learning）来理解语言，虽然在生成任务中表现出色，但在需要长期记忆或复杂推理的任务中存在局限。作者提出 Memory3，旨在通过引入显式记忆模块，使模型能够**主动存储和检索信息**，从而提升其在需要记忆的任务中的表现。

### 主要内容与方法：
1. **显式记忆模块**：
   - Memory3 引入了一个**显式的记忆存储器**（memory bank），用于存储模型在处理过程中主动选择的重要信息。
   - 模型可以根据当前输入内容，**主动决定哪些信息需要存储**，并在后续处理中**检索这些信息进行推理**。

2. **记忆的编码与检索**：
   - 记忆信息通过可学习的编码机制进行表示，便于后期检索。
   - 检索机制使用注意力机制（attention）从记忆中找到与当前输入最相关的条目，辅助模型生成更准确的输出。

3. **训练策略**：
   - 作者设计了合适的训练目标，使模型在训练过程中学会如何有效使用记忆模块，包括何时存储、何时检索等。

### 实验与结果：
- 在多个需要记忆和推理能力的自然语言处理任务上（如问答、对话、逻辑推理等），Memory3 显示出优于传统语言模型的性能。
- 实验表明，显式记忆机制可以显著提升模型在处理长文本、多轮对话和逻辑推理任务中的表现。

### 总结：
《Memory3: Language Modeling with Explicit Memory》提出了一种新的语言模型架构，通过引入显式记忆机制，增强了模型的记忆和推理能力。该方法在多个任务中取得了良好的效果，为未来构建更智能、更具推理能力的语言模型提供了新的研究方向。


## Abstract



该论文提出了一种名为 **Memory3** 的大语言模型（LLM），其核心创新在于引入**显式记忆（explicit memory）**，以降低训练和推理成本。论文主要观点和贡献如下：

### 核心思想
- **受人脑记忆层级启发**，作者提出为LLM引入显式记忆，作为比模型参数（隐式记忆）和上下文缓存（工作记忆）更经济的知识存储方式。
- 显式记忆可以被**检索利用**，从而减少对模型参数规模的依赖，降低训练和推理成本。
- 模型的知识可以分为“抽象知识”（由参数存储）和“显式知识”（由记忆存储），显式知识的外化使得模型规模可以更小，但性能仍可保持。

### Memory3 模型特点
- 从头训练了一个2.4B参数的模型，**在性能上超越了更大规模的LLM和RAG模型**，同时在推理速度上优于RAG。
- 采用了**两阶段预训练方案**和**记忆稀疏化机制**，帮助模型高效地形成和存储显式记忆。
- 提出**记忆电路理论**（memory circuitry theory），支持知识从模型参数转移到显式记忆的可行性。

### 实验结果
- **图2（左）**：展示了在多个基准测试中的模型性能与模型规模的关系，Memory3在较小规模下仍表现出色。
- **图2（右）**：展示了在专业任务上的性能与推理速度对比，Memory3在保持高性能的同时，推理速度优于RAG模型。
- 作者强调该实验为初步探索，尚未对预训练数据质量和推理效率进行优化，因此结果可能与最先进模型（SOTA）不完全可比。

### 总结
该研究通过引入**显式记忆机制**，探索了一种新的大语言模型架构，试图以更小的模型规模实现高性能和低资源消耗。这为未来LLM的效率优化提供了新的思路，特别是在知识存储与利用方式上。


## 1 _ Introduction



本文主要探讨了大规模语言模型（LLMs）在训练和推理过程中所面临的高昂成本问题，并提出了一种新的优化思路——通过优化“知识存储”来降低整体成本。传统的优化方法大多集中在模型结构、数据质量、并行计算、硬件等方面，而本文提出了一种全新的“显式记忆”（explicit memory）机制，旨在提升知识处理的效率和模型性能。

文章的核心思想是将LLM的训练和推理成本建模为“知识写入成本”与“知识读取成本”的总和。通过引入一种新的记忆形式——显式记忆，作者构建了一个多级记忆层次结构（RAG → 显式记忆 → 模型参数），显式记忆在写入和读取成本之间取得了良好平衡，适合存储中等频率使用的知识。

显式记忆通过将知识编码为稀疏的注意力键值对，并在推理过程中直接调用这些记忆，减少了对模型参数的依赖。这种设计使得现有大多数基于Transformer的模型在少量微调后即可支持显式记忆，具有良好的通用性和扩展性。作者将其方法命名为Memory3（第三种记忆形式），并展示了其在减少预训练成本、提升推理效率、增强事实性、减少幻觉、支持长文本处理等方面的优势。

此外，作者还类比人类记忆系统，指出显式记忆的引入可以类比于人类将知识从工作记忆转化为长期记忆的过程，从而提高知识处理效率。通过实验验证，Memory3模型在多种任务中表现出色，超越了更大规模的SOTA模型，并在事实性和推理速度方面取得了显著提升。

最后，文章概述了论文的结构，后续章节将分别介绍Memory3的理论基础、设计细节、训练过程和性能评估等内容。


## 1.1.1 _ Retrieval-augmented Training



本节对近年来与语言模型中知识表示和高效计算相关的研究进行了系统总结，主要分为三个部分：

---

### 1.1.1 | 基于检索的训练（Retrieval-augmented Training）

研究者在语言模型的预训练和微调阶段引入文本检索机制，以增强模型对知识的访问能力。

- **预训练阶段**：  
  - **REALM** 在 BERT 模型中引入单步检索，用于解决问答任务。  
  - **Retro** 在解码过程中每 64 个 token 进行一次检索，并通过一个两层编码器和跨注意力层将检索内容注入模型。  
  - **Retro++** 对 Retro 的扩展性进行了探索，实现了高达 9.5B 参数规模的模型。  
  - 与这些模型不同，Retro 采用**实时编码检索内容**，但为减少计算开销采用浅层编码器和少量检索结果，导致知识利用率受限。

- **微调阶段**：  
  - **WebGPT** 通过模仿学习在网页浏览环境中训练模型使用搜索引擎。  
  - **Toolformer** 利用多个工具（包括搜索引擎）进行解码，并通过模型自身生成微调数据。

- **长上下文建模**：  
  - **Memorizing Transformer** 通过 kNN 查找缓存中的键值对扩展上下文长度。  
  - **LongLlama** 和 **LONGMEM** 则通过对比学习和架构设计解决记忆过时问题。  
  - 这些方法受限于缓存空间，难以应用于大规模知识库。本文提出的方法通过**更激进的稀疏记忆方法**克服了这一问题。

---

### 1.1.2 | 稀疏计算（Sparse Computation）

为了解决知识检索与计算效率之间的矛盾，研究者探索了稀疏性架构，以减少每 token 的参数使用量。

- **Mixture-of-Experts (MoE)**：  
  - 使用稀疏路由机制，将每个 token 分配给少量专家模块，从而在参数规模和计算成本之间取得平衡。  
  - 常见设计中每个 Transformer 块包含多个 MLP 层，通过线性分类器选择最相关的专家。  
  - 进一步的优化如 **QMoE** 引入压缩技术以减少内存负担。  
  - 尽管参数效率有所提升（如 Arctic 模型的活跃参数比例约 3.5%），但提升上限通常在 4~32 倍之间。

- **深度混合架构（Mixture of Depth）**：  
  - 通过早期退出（early exit）或 top-k 路由机制，仅使用模型中部分层级进行计算，可将计算量降低至 12.5~50%。

- **更细粒度的稀疏性**：  
  - **Deja Vu** 为每层 MLP 或注意力层训练一个低成本网络，预测每个神经元或注意力头对当前 token 的相关性。  
  - 推理时保留每个 token 的前 5~15% MLP 神经元和 20~50% 注意力头，实现更细粒度的计算节省。

---

### 1.1.3 | 参数即记忆（Parameter as Memory）

部分研究将模型参数视为**隐式记忆**，认为某些神经元可以存储知识。

- **GPT 中的 MLP 神经元行为**：  
  - 神经元可被视为键值对（Key-Value），其中第一层权重作为“键”，第二层权重作为“值”。  
  - 激活的键通常对应某些人类可理解的模式，值则用于生成最可能的下一个 token。

- **基于注意力的 GPT 变体**：  
  - **[[108]]** 提出仅由注意力层构成的 GPT 变体，将 MLP 的键值对作为“持久记忆”嵌入注意力机制中。

- **知识神经元**：  
  - **[[29]]** 发现 BERT 中的事实知识往往集中在少数 MLP 神经元中，称为“知识神经元”，可通过操作这些神经元更新模型知识。

- **神经网络中的超合（Superposition）现象**：  
  - **[[38]]** 研究发现，一个神经元可以存储多个不相关的概念，这种多语义性机制是神经网络表达复杂知识的一种方式。

---

### 总结

本节综述了当前语言模型中提升知识访问效率和计算效率的主要技术路线，包括：
1. **检索增强训练**：在预训练和微调中引入检索机制，提升模型对知识的访问能力。
2. **稀疏计算架构**：通过稀疏路由、混合专家和细粒度剪枝等方法减少计算开销。
3. **参数作为隐式知识载体**：揭示模型参数中存储知识的机制，为知识更新和模型优化提供了理论基础。

这些研究为本文提出的高效知识访问与存储方法提供了背景与对比。


## 2 _ Memory Circuitry Theory



本节介绍了**记忆电路理论（Memory Circuitry Theory）**，旨在在大型语言模型（LLM）的背景下定义知识与记忆。该理论通过将LLM的计算过程分解为可重复的模块（称为“电路”），帮助我们理解哪些知识可以被提取为**显式记忆（explicit memory）**，并指导适合读写显式记忆的模型架构设计。

---

### 核心概念总结：

1. **知识与电路的定义**：
   - **知识**定义为输入输出关系及其对应的**内部计算路径（电路）**。
   - **电路**被看作是“时空现象”，其因果结构局部存在于特定的MLP神经元和注意力头，并作用于特定的token上。
   - 电路是多个计算图中相似子图的**等价类**，用于描述知识的实现机制。

2. **知识提取的动机**：
   - 通过识别和提取电路，可以将LLM中的知识和技能模块化，从而支持**显式记忆的外部化**，实现轻量化的模型骨干和记忆层级结构。

3. **行为主义与白盒方法对比**：
   - 行为主义方法仅关注输入输出关系，但不足以确定知识的内在机制。
   - 白盒方法通过分析模型内部的计算路径（如注意力头与MLP层的交互）来定义知识，更具解释力。

4. **知识示例与机制**：
   - **事实性知识（如“China的首都是Beijing”）**：
     - 由“移动头（mover head）”将上下文信息移动到最后的token，并通过MLP层中的“知识神经元”映射到答案。
   - **算术推理**：
     - 依赖模型内部的计算路径，如通过注意力头提取表达式特征，再由MLP计算结果。
   - **“搜索、复制、粘贴”机制**（induction head）：
     - 通过两个注意力头实现类如“...[a][b]...[a]→[b]”的模式识别与输出。

5. **计算图与子图定义**：
   - **计算图**：描述LLM内部所有token在所有层中的隐藏状态及其激活路径。
   - **子图**：代表一个知识的电路，其输入输出序列可通过梯度或因果干预识别。
   - **等价类定义电路**：多个任务中结构相似的子图被视为同一电路的实例。

---

### 总体贡献：

- 提出了一种基于**计算图与子图分析**的框架，用于识别和提取LLM中的知识。
- 定义了“记忆电路理论”作为理解LLM内部记忆机制的指导原则。
- 为后续设计具有**显式记忆能力的模型架构**提供了理论支持。

该理论对LLM的可解释性、模块化构建以及记忆机制研究具有重要意义。


## Definition 2.



这篇文章的章节内容总结如下：

---

### 1. **定义与核心概念：计算图、同构与知识（电路）**

- **计算图（Computation Graphs）**：用于描述大型语言模型（LLM）中信息处理路径的结构，包括注意力头（head）和 MLP 神经元（neuron）等节点。
- **同构映射（Homomorphism）**：定义了两个计算图子图之间的结构关系，包括深度、位置和边（注意力或神经元连接）的对应性。这种映射允许节点不唯一，但保持结构一致性。
- **知识（Circuit / Knowledge）**：
  - 一组相互同构的子图，构成一个“电路”。
  - 所有子图中的边权重非可忽略，表示有实际作用。
  - 输入输出对（tin, tout）在语义上具有一致性，即具有可解释的模式或意义。
  - 知识分为**具体知识（specific knowledge）**和**抽象知识（abstract knowledge）**：
    - **具体知识**：输入语义一致，输出固定或差异很小。
    - **抽象知识**：输出可能变化较大，但输入有明确模式。

---

### 2. **知识的实例**

- **具体知识的例子**：如“中国首都是北京”的知识神经元，输入是“中国”和“首都”，输出总是“北京”。
- **抽象知识的例子**：如诱导头（induction head），输入是类似“[a][b]…[a]”的模式，输出为“[b]”，虽然输入有规律，但输出可以不同。
- **更复杂的抽象知识**：已发现用于上下文学习（in-context learning）和间接对象识别（indirect object identification）。

---

### 3. **知识的外部化与记忆**

- **知识的实现（Realization）**：表示能够激活某个知识的文本，即在该文本的计算图中存在该知识的子图。
- **可分离性（Separability）**：一个知识如果可以从模型参数中分离并存储在外部记忆中，则称为可分离的。分离的条件包括：
  - 某模型M在没有该知识的情况下无法生成输出。
  - 通过提供一个示例文本（prefix），模型M可以生成该知识的输出。
- **可模仿性（Imitability）**：如果任何实现文本都可以作为示例文本来辅助模型生成输出，则该知识是可模仿的。可模仿性是可分离性的一个特例。

---

### 4. **结论与断言**

- **断言 1（Claim 1）**：所有**具体知识**都是可模仿的，因此也是可分离的。这表明许多知识可以从模型参数中移除，并通过外部示例来实现。
- **分离性与模仿性**：分离性更宽泛，例如可以使用抽象描述作为前缀（如指令提示），而不仅仅是具体示例。
- **知识的组织**：文章提出，LLM 的效率本质上来源于这些知识的有效组织，而问题的核心是将这些知识从庞大模型中分离，以构建更高效的带记忆结构的模型。

---

### 总结

本节从计算图的结构出发，定义了知识的结构和语义特性，并通过同构关系建立了知识的统一表示。随后通过具体与抽象知识的例子展示了知识在模型中的表现形式，并提出了知识外部化的概念，包括可分离性和可模仿性。最后，文章指出许多知识（尤其是具体知识）可以被提取并存储在外部，从而为构建更高效的语言模型提供了理论依据。


## Remark 1.



本节主要围绕**记忆增强语言模型（Memory-Augmented LLM）**的理论模型展开，重点包括以下几个方面：

---

### 1. **电路构造的关键性质**
作者在 Claim 1 的证明中总结了三个对后续设计有指导意义的细节：

1. **单一注意力头**：构建的电路只有一个注意力头，该头从当前文本 𝐭 中关注参考文本 𝐭′，其他计算则局限于各自文本。
2. **注意力边的稀疏性**：该注意力头只需关注参考文本中少数几个标记（从 [b] 到 [a’] 的边），说明其对参考内容的依赖较为有限。
3. **自注意力机制**：参考文本只需关注自身，不需要外部信息，简化了注意力结构。

这些性质为后续的架构设计提供了指导，特别是在设计高效记忆访问机制方面。

---

### 2. **记忆增强 LLM 的形式化定义**
作者提出了一种通用的记忆增强 LLM 模型定义：

- **输入**：前缀（t₀…tᵢ₋₁）、非可分离知识集合 {𝒦₁,…,𝒦ₙ} 以及不同格式的参考文本集合 X₁,…,Xₘ。
- **输出**：一个基于前缀的 token 分布。
- **记忆格式**：每个 Xⱼ 由函数 fⱼ 从参考文本 𝐭ⱼ,k 中编码得到，每个 fⱼ 被称为“写函数（write function）”，表示该记忆格式的数据如何被写入模型。

- 如果某个知识 𝒦 被写入格式 j，则表示它以该格式被模型读取和使用。

---

### 3. **写入代价与读取代价的权衡（记忆层次结构）**
作者提出了一个关键的理论假设：

- **写入代价**（costwrite）与**读取代价**（costread）存在**负相关关系**：写入越便宜，读取越昂贵；反之亦然。
- 这种关系被称为**记忆层次结构（memory hierarchy）**，类似于人类的记忆系统：常用表达（如母语）写入代价高（需长期学习），但读取代价低；而罕见知识（如书籍内容）写入代价低，读取代价高。

---

### 4. **知识使用频率与记忆分配**
- 不同频率使用的知识应分配到不同的记忆格式中：
  - **高频知识**应分配到**高写入代价、低读取代价**的格式（如隐式记忆）。
  - **低频知识**应分配到**低写入代价、高读取代价**的格式（如显式记忆或外部信息）。
- 添加新的记忆格式可以扩展搜索空间，从而在某些知识的使用频率落在特定区间时，降低总体代价。

---

### 5. **图示与结论**
- 图 4 和 图 8 显示了写入与读取代价之间的反比例关系，并展示了不同记忆格式的使用场景。
- 人类和 LLM 的记忆分布有相似性：高频知识倾向隐式记忆，低频知识可能需通过显式记忆或外部检索表示。

---

### 小结
本节构建了一个形式化的记忆增强 LLM 模型，提出了记忆格式的通用定义，并通过理论分析与图示展示了写入与读取代价之间的权衡关系。这一理论框架为设计具备高效显式记忆机制的语言模型提供了理论支持。


## 3 _ Design



本章主要介绍了 **Memory3** 的设计，包括其架构、训练方案以及推理过程。目标是通过引入一种显式的记忆机制，使 Transformer 大型语言模型（LLM）能够在保持较低写入和读取成本的前提下，提升模型对知识的利用效率。

---

### **3 | Design**

#### **3.1 架构设计**
Memory3 的目标是为 Transformer 架构设计一种显式记忆机制，具有较低的写入和读取成本，同时尽量减少对原有 Transformer 架构的改动，不引入新的可训练参数，从而使得大多数现有的 Transformer LLM 可以通过少量微调转换为 Memory3 模型。

- **写入成本（Write cost）**：
  - 在推理之前，LLM 会将每个参考文本写入显式记忆中，存储在磁盘上。
  - 显式记忆是从自注意力层的 key-value 向量中选取的，因此写入过程不需要训练。
  - 每个参考文本独立处理，避免了长上下文注意力的开销。

- **读取成本（Read cost）**：
  - 在推理时，显式记忆从磁盘中检索，并与常规上下文的 key-value 一起通过自注意力机制处理。
  - 每个记忆只包含少量注意力头的 key-value，从而大大减少了额外的计算、GPU 存储、磁盘存储和加载时间。
  - 使得 LLM 能够频繁检索大量参考文档，而对解码速度影响较小。

#### **3.2 训练目标**
- 希望通过显式记忆机制，使得 LLM 只需要学习抽象知识，而将具体知识外部化存储到记忆库中。
- 通过减少预训练的数据量，使训练成本与模型参数中存储的知识量成比例，从而更接近人类的学习效率。

---

### **3.1 | 推理过程**
- **参考文本定义**：将可分离的知识实例称为“参考”（References）。
- **知识库**：由 1.1×10⁸ 个长度不超过 128 个 token 的文本块组成。
- **显式记忆结构**：每个参考可转换为一个张量，形状为 (22,2,8,8,80)，其中 2 表示 key 和 value，其他为层数、头数、稀疏 token 数和维度。
- **推理阶段**：
  - 所有参考在推理前被转换为显式记忆并存储在磁盘中。
  - 推理时，当需要一个参考时，其对应的显式记忆被加载到 GPU，并用于计算。
  - 由于每个参考在编码时不需关注其他文本，因此可以独立编码，减少注意力计算。
  - 支持“冷启动”（首次检索即转换为记忆）和“热启动”（提前预处理）两种方式。
- **检索机制**：
  - 使用向量搜索和余弦相似度来检索参考。
  - 利用 BGE-M3 模型进行嵌入，FAISS 构建索引。
  - 每生成 64 个 token，模型会丢弃旧记忆，检索 5 个新记忆。
- **内存缓存**：
  - 使用固定大小的 RAM 缓存来存储最近使用的显式记忆，以减少磁盘加载时间。
  - 相邻 chunk 常用相同参考，缓存有助于提升效率。

#### **备注与优化**
- **备注2**：理想情况下，应使用 LLM 自身的隐藏特征进行检索，因为这样更接近人类的内部检索机制。一种方法是使用稀疏注意力查询直接搜索显式记忆，无需微调模型。
- **备注3**：传统 RAG 方法中，参考多为文本块而非完整文档，导致缺乏上下文。Memory3 通过将完整文档编码后分块处理，使得 key-value 能够关注其上下文，从而克服这一问题。

---

### **3.2 | 写入与读取记忆**
- **显式记忆构成**：每个记忆是参考文本编码后自注意力层中某些注意力头的 key-value 子集。
- **读取方式**：推理时，显式记忆通过自注意力层与常规上下文的 key-value 拼接后处理。
- **公式说明**：模型在读取记忆时，其注意力输出公式中引入了显式记忆的 key 和 value，与常规上下文 key-value 结合进行 softmax 计算。
- **BOS 标记调整**：在编码参考时，BOS 标记改为“⟨s⟩Reference:”，以帮助模型区分常规文本与参考文本。推理时该标记用于区分参考与上下文。
- **并行位置编码**：
  - 使用 RoPE（Rotary Positional Encoding）对所有显式记忆的 key 进行编码。
  - 位置信息被保留，以便模型理解参考中 token 的相对位置。
  - 通过并行位置编码避免“中间被忽略”现象（middle loss），并增强上下文学习和长上下文建模能力。

---

### **总结**
Memory3 的设计旨在通过将具体知识外部化为显式记忆，使得 LLM 能更专注于学习抽象知识，从而降低训练成本并提升推理效率。其核心特点包括：
- 显式记忆机制，支持高效检索和读取；
- 尽量最小化对 Transformer 架构的改动；
- 使用缓存和并行位置编码优化检索和注意力处理；
- 支持“冷启动”与“热启动”推理模式；
- 通过参考文档的完整编码解决上下文缺失问题。

该设计为实现“人脑式”学习提供了可行的模型结构基础。


## 3.3 _ Memory Sparsification and Storage



本节介绍了**Memory3** 模型中**显式记忆的稀疏化与存储**方法，旨在解决显式记忆占用存储和显存空间过大的问题。主要内容总结如下：

---

### 一、显式记忆的存储挑战
- 显式记忆的关键值对（attention keys-values）占用大量空间，不仅消耗磁盘存储成本，也在推理时占用 GPU 显存，影响批量大小和生成吞吐量。
- 需要对显式记忆进行**高强度压缩**，压缩维度包括：**层（layers）、头（heads）、token 数量、头维度（head dimension）**。

---

### 二、各维度的稀疏化策略

#### 1. 层（Layers）的稀疏化
- 仅将**前半部分注意力层**设置为**记忆层（memory layers）**，用于生成和访问显式记忆。
- 更合理的做法是将**中间层（如 25%~75% 深度范围）**设置为记忆层，因为这些层更可能关注远处的上下文。
- 支持这一选择的研究表明，中间层更常参与长距离注意力。

#### 2. 头（Heads）的稀疏化
- 将每个记忆层的**所有 key-value 头设为记忆头（memory heads）**，然后通过**分组查询注意力（GQA）**将其数量减少（如从 40 个头压缩到 8 个头）。
- 也可以通过选择**对读取记忆最有帮助的头部子集**（非均匀选择）来进一步压缩。
- 提供了多种选择记忆头的方法，详见备注 4。

#### 3. Token 的稀疏化
- 对每个 key-value 头，从 128 个 token 中**选择前 8 个 token**，因为参考文献显示注意力集中在少量 token 上。
- 选择依据是每个 token 被其他 token 注意力权重的总和。
- 排除 BOS token 和填充 token。
- 不使用因果掩码（causal mask）或位置编码，以保证所有位置 token 被公平考虑。
- 实现中使用了近似计算加快速度。

#### 4. 头维度（Head Dimension）的压缩
- 可选地使用**向量量化器（vector quantizer）**对每个 key 和 value 向量进行压缩。
- 采用 FAISS 实现的**残差量化**，压缩比约为 11.4。
- 压缩后的显式记忆在推理前需先解压。

---

### 三、压缩效果
- 原始显式记忆存储需求约 7.17PB（7340TB），压缩后为 45.9TB（或 4.02TB，含向量压缩），适合 GPU 集群存储。
- 评估表明，向量压缩对性能影响可忽略不计。

---

### 四、部署方式
- 在终端设备（如手机、笔记本）上部署时，可将**显式记忆库和向量索引放在云服务器**，设备端仅存储模型参数和向量解码器。
- 推理时设备将查询向量发送给服务器，服务器检索后返回压缩记忆。

---

### 五、补充说明与建议

#### 备注 4：记忆头的选择方法
- 可通过微调预训练模型来选择记忆头。
- 一种方法是通过验证集测试每个头对长距离上下文性能的影响，选择对长距离注意力贡献最大的头。
- 通常这类头只占所有头的不到 10%。

#### 备注 5：自适应稀疏化
- 每个参考文献可能只需一个头来处理长距离注意力，不同参考文献可能使用不同头。
- 可借鉴 MoE（Mixture of Experts）机制，为每个参考文献**动态分配 1-2 个记忆头**，以进一步提升稀疏度和节省空间。

---

### 总结
本节通过**多维度的稀疏化策略**（层、头、token、维度）大幅压缩显式记忆的存储需求，使其在实际应用中可行，并提出了多种有效且灵活的实现方式和优化思路。


## 3.4 _ Model Shape



本节内容主要围绕 **Memory3 模型的设计**，特别聚焦于 **模型结构（Model Shape）** 和 **训练设计（Training Designs）**，其核心思想是通过将具体知识外部化为显式记忆，来优化模型参数的使用，从而更高效地学习抽象知识。以下是总结：

---

### 3.4 | 模型结构（Model Shape）

1. **设计目标：**
   - 由于具体知识可以外部化存储为显式记忆，模型参数（即隐式记忆）只需负责存储抽象知识和高频使用的具体知识。
   - 由此，模型结构的设计目标是在固定的参数量（24 亿）下，最大化模型对抽象知识的存储能力与对具体知识存储能力的比值。

2. **模型结构选择：**
   - 选择 Transformer 的层数 $L$、头数 $H$、每头维度 $d_h$ 和 MLP 层宽度 $W$。
   - 参考相关研究：
     - 具体知识的存储容量与参数量成正比，约为每个参数存储 2 bit。
     - 抽象知识的学习能力则与模型的结构（尤其是层数和头数）相关。
   - 因此，模型应优先增加 $L$ 和 $H$。

3. **结构参数确定：**
   - 通过实验和控制变量（图10）发现，当 $L:H ≈ 1:1$ 时，损失下降更快，因此采用此比例。
   - 每头维度 $d_h$ 不能太小（小于 64 时训练不稳定），最终设置为 80。
   - MLP 宽度 $W$ 设置为 $H \times d_h = 40 \times 80 = 3200$。
   - 采用分组查询注意力（GQA），设置键值头数 $H_{kv} = 8$。
   - 最终模型结构为：$L=44, H=40, H_{kv}=8, d_h=80, W=3200$，总非嵌入参数为 2.4B。

---

### 3.5 | 训练设计（Training Designs）

1. **训练目标：**
   - 由于具体知识已外部化，模型训练目标应转向学习抽象知识，从而减少训练计算资源需求。
   - 这要求重新设计传统的预训练设置（如数据、初始化、正则化）。

2. **数据选择：**
   - 应优先选择富含抽象知识、少含具体知识的数据。
   - 研究表明，具体知识会抑制模型学习抽象能力。
   - 类似 Phi-3 模型，选择强调推理、避免过多具体信息的数据。

3. **参数初始化：**
   - 使用较小的标准差初始化参数，有助于模型学习组合推理而非记忆。
   - 实验表明，小初始化鼓励模型在稀疏解中学习抽象结构。

4. **权重衰减（Weight Decay）：**
   - 使用较大的权重衰减可以促进泛化而非记忆，有助于模型更快地掌握可泛化的解决方案。
   - 生成模型需要更强的正则化，以防止分布塌缩到训练数据。

5. **当前实践：**
   - 由于本工作是 Memory3 的初步版本，目前仍采用传统训练设置。
   - 未来版本将尝试上述提到的优化设计，以进一步提升模型性能。

---

### 总结

3.4 节通过理论分析和实验验证，确定了最优的模型结构，使模型在有限参数下最大化抽象知识学习能力。  
3.5 节则从训练角度提出了优化方向，包括数据选择、初始化策略和权重衰减设置，旨在提升模型对抽象知识的学习效率与泛化能力。  
尽管目前尚未完全应用这些优化策略，但它们为 Memory3 模型的进一步发展提供了明确方向。


## 3.6 _ Two-stage Pretrain



该章节“3.6 | Two-stage Pretrain”主要介绍了 Memory3 模型在预训练阶段采用的“两阶段预训练”方法，具体包括以下内容：

---

### 一、预训练的两个阶段

1. **Warmup 阶段**（预热阶段）  
   - 在该阶段，模型采用传统语言模型的预训练方式，**不引入显式记忆**。
   - 作者发现，如果在预训练初期就引入显式记忆，模型可能不会有效利用这些记忆，反而导致训练损失不再下降。
   - 图11a 显示，使用显式记忆的预训练模型（绿色曲线）在 warmup 阶段训练损失没有明显优势。

2. **Continual Train 阶段**（持续训练阶段）  
   - 在此阶段，模型开始使用显式记忆机制。
   - 从 warmup 阶段的检查点继续训练，加入显式记忆后，模型的训练损失明显下降，说明 warmup 是必要的。
   - 图11b 显示，在 warmup 检查点基础上启用显式记忆（红色曲线），训练损失显著降低。

> **结论**：Memory3 模型需要先进行一个不涉及显式记忆的 warmup 阶段，再进行带有显式记忆的 continual train 阶段，这种两阶段策略有助于模型逐步适应和有效利用显式记忆。

---

### 二、对 continual train 的优化

1. **内存共享机制**  
   - 为减少 continual train 的计算开销，采用**共享参考内容**的方式。
   - 每个 64-token 的 chunk 在训练时只检索一个参考（reference），但可以关注前四个 chunk 的 reference，这样每个 chunk 就可以利用多个参考，而不会显著增加计算量。
   - 一个训练序列长度为 2048 token，分为 32 个 chunk，每个 chunk 对应一个 128-token 的 reference，共 4096 token 的 reference 输入。reference 的隐藏状态在通过最后一个记忆层后会被丢弃，以节省内存。

2. **时间开销**  
   - continual train 每个步骤的时间大约是 warmup 的两倍。

---

### 三、防止信息泄露

- **问题**：如果训练序列与其检索到的 reference 内容高度重合，模型可能只是“记忆”而不会真正学习。
- **解决方案**：  
  - 不允许训练序列与 reference 有超过 90% 的重叠。
  - 重叠度通过最长公共子序列（LCS）的长度占 reference 长度的比例来衡量。
  - 公式（6）给出了具体计算 overlap 的方法。

---

### 总结

本节提出 Memory3 模型的预训练采用“两阶段”策略：  
- 第一阶段（warmup）：传统预训练，不使用显式记忆；
- 第二阶段（continual train）：引入显式记忆机制，使用共享参考以减少计算开销；
- 同时，通过限制训练序列与 reference 的重叠，防止信息泄露，确保模型真正学习而不是“记忆”。

这种结构化、分阶段的训练方式，有助于模型更有效地利用显式记忆，提高性能。


## 4 _ Pretraining Data



本节主要介绍了 **Memory3 模型** 的 **预训练数据（Pretraining Data）** 相关内容，包括数据收集、过滤、分词器（Tokenizer）以及知识库（Knowledge Base）的构建。

---

### **4.1 数据收集（Data Collection）**

预训练数据主要来源于 **英文和中文文本数据集**，包括网页、书籍、代码、有监督微调（SFT）数据和合成数据。具体如下：

- **英文数据**：主要来自 RedPajamaV2、SlimPajama 和 The Piles，总计约 200TB（过滤前）。
- **中文数据**：主要来自 Wanjuan、Wenshu 和 MNBVC，总计约 500TB（过滤前）。
- **代码数据**：来自 GitHub，选取高星数（高受欢迎度）的代码仓库。
- **SFT 数据**：在预训练中作为普通文本处理，所有 token 都参与损失计算，而非仅答案 token。
- **合成数据**：用于补充数据多样性。

---

### **4.2 数据过滤（Filtering）**

为了提高数据质量，采用了三步过滤流程：

1. **去重（Deduplication）**：  
   使用 **MinHash** 方法对大部分数据集进行去重，RedPajamaV2 数据集本身已提供去重标签。

2. **基于规则的过滤（Rule-based Filtering）**：  
   设计了一系列启发式规则，去除不适合训练的文本，例如：
   - 文档长度小于 50 词；
   - 单词平均长度超过 10 个字符；
   - 70% 以上的上下文为非字母字符；
   - 唯一词比例异常高；
   - 单字词熵值异常低等。

3. **基于模型的过滤（Model-based Filtering）**：  
   使用一个微调后的 **Tiny-BERT** 模型，依据由 XinYu-70B 模型生成的“信息量”评分对数据进行筛选。Tiny-BERT 模型通过优化超参数训练，用于对整个数据集进行评分并选择高质量文本。

> **备注**：为了与 Memory3 的设计理念（强调抽象知识、减少具体知识）一致，未来版本将转向更注重推理性和减少具体信息的模型过滤策略。

---

### **4.3 分词器（Tokenizer）**

- 分词器支持中英文混合文本。
- 英文词汇基于 LLaMA2 的 32000 个 token。
- 中文词汇通过 BPE（Byte Pair Encoding）在 20GB 的中文新闻和电子书语料上训练。
- 最终字典包含 60,299 个 token。

---

### **4.4 知识库（Knowledge Base）**

知识库在训练和推理中作为显式记忆的来源，其构建原则如下：

- **质量优先**：包含英文维基百科、WikiHow、中文百科、学术书籍、中文新闻、高质量代码和合成数据。
- **切片处理**：所有文本被切分为 128 个 token 的片段。
- **总量**：最终知识库包含约 1.1 × 10⁸ 个参考片段。

> **防止数据泄露**：在评估过程中，若检索到的参考片段与问题重叠度过高（≥80%），则被过滤，确保评估公平性。阈值设为 2/3，基于重叠度分析设定。

> **备注**：当前知识库的构建依赖于人工偏好，未来计划采用模型导向的方法，通过验证集损失的减少来衡量参考片段的实际效果。

---

### **总结**

本节全面介绍了 Memory3 模型预训练数据的构建流程，包括数据的来源、过滤策略、分词器设置和知识库的构建。重点在于通过多步过滤和模型评估确保数据质量，同时兼顾模型训练效率与推理能力。方法上结合了去重、规则过滤和模型评分，体现了对数据质量与模型可学习性的双重重视。


## 5 _ Pretrain



本节总结了论文中关于预训练（Pretrain）过程的详细内容，主要包括预训练的两个阶段：**预热阶段（Warmup Stage）** 和 **持续训练阶段（Continual Train Stage）**，以及在训练过程中使用的技术设置和遇到的问题。

### 1. 预训练总体设计
- **两阶段预训练** 和 **内存增强数据** 的设计参考了论文第3.6节。
- **预热阶段** 主要提升模型的**阅读理解能力**，为后续**内存形成**做准备。
- **持续训练阶段** 引入**显式记忆（Explicit Memory）**，增强模型的外部存储和调用能力。

---

### 2. 训练设置（Set-up）
- **训练框架**：使用 Megatron-DeepSpeed，采用混合精度训练（bfloat16 参数和激活，float32 AdamW 状态）。
- **批量大小**：约 400 万训练 token，序列长度为 2048（不包括参考 token）。
- **权重衰减**：使用常见的 0.1。
- **学习率调度**：采用 MiniCPM 中的“预热-稳定-衰减”（warmup-stable-decay）学习率调度，优于常规的 cosine 调度，但在实际训练中因**损失尖峰和发散**问题，不得不**手动降低学习率**以稳定训练。
- **训练数据量**：原计划两个阶段均使用 4T token 的预训练数据，但因训练稳定性问题，实际训练提前终止。

---

### 3. 预热阶段（Warmup Stage）
- **无显式记忆**，仅训练基础语言建模能力。
- **学习率调度**：先线性上升，保持稳定，最后 10% 步骤快速衰减。
- **训练问题**：频繁出现**损失发散**，每次发散后需从最近的检查点恢复并**降低学习率**。
- **训练终止**：在约 3.1T token 时，即使降低学习率也无法避免发散，训练被迫终止。

---

### 4. 持续训练阶段（Continual Train Stage）
- **引入显式记忆模块**，模型在训练时实时将训练数据中的参考内容编码为显式记忆。
- **训练速度下降**：每一步训练时间约为预热阶段的 **两倍**。
- **学习率调度**：仍采用类似 warmup 阶段的调度方式，初始学习率较低以稳定训练。
- **训练问题**：在约 120B token 时出现**不可挽回的损失发散**，远低于原计划的 4T token。
- **可能原因**：该阶段从 warmup 阶段**即将发散前的检查点初始化**，模型本身已处于不稳定状态，即使降低学习率也仅能**延缓发散**。

---

### 总结
预训练阶段分为两个主要阶段：预热阶段用于提升模型基础能力，持续训练阶段引入显式记忆以增强模型的外部存储功能。尽管采用了优化的学习率调度和训练设置，但在实际训练中仍面临严重的**损失发散问题**，导致训练提前终止。这表明在引入显式记忆后，训练稳定性面临较大挑战，可能需要更细致的策略或更强的硬件支持以完成整个 4T token 的训练目标。


## 6 _ Fine-tuning and Alignment



本章主要介绍了 **Memory3 模型的微调与对齐** 过程，具体包括 **监督微调（SFT）** 和 **直接偏好优化（DPO）** 两个阶段。以下是总结：

---

### 6.1 监督微调（Supervised Finetuning, SFT）

- **数据来源**：使用了多个公开的 SFT 数据集，包括 UltraChat、WizardLM、SlimOrca、ShareGPT、Capybara、Deita 和 MetaMathQA，这些数据集均来自 Hugging Face Hub。
- **合成数据**：此外还加入了人工生成的数据，重点涵盖多轮对话、数学、常识和知识领域。
- **数据处理**：每个样本包含一个或多个问答轮次，去除超过八轮的样本，最终组成总计约 70 万条样本的数据集（详见表 15）。
- **训练设置**：
  - 学习率调度：余弦退火（cosine schedule），最大学习率为 $5 \times 10^{-5}$，前 10% 是线性热身阶段；
  - 权重衰减：0.1；
  - 批大小：512；
  - 最大序列长度：2048 个 token；
  - 微调轮数：3 轮。

---

### 6.2 直接偏好优化（Direct Preference Optimization, DPO）

- **目的**：进一步对齐模型与人类偏好，提升其对话能力。
- **数据集**：包括通用对话（UltraFeedback Binarized）、数学问题（Distilabel Math）和代码相关问题（Synth Code）。
- **训练设置**：
  - 学习率调度：余弦退火，最大学习率为 $4 \times 10^{-6}$；
  - DPO 损失中的逆温参数 $\beta$ 设置为 0.01；
  - 微调后的效果在 **第 7.2 节（对话能力评估）** 中有展示。

---

**总结**：  
Memory3 模型通过监督微调和直接偏好优化两个阶段进行精细化调整，前者基于多样化的高质量数据集确保模型的通用能力，后者则专注于对齐人类偏好以提升对话质量。两阶段的设置在训练参数和数据选择上均做了详细优化，为模型的最终性能提供了坚实支持。


## 7 _ Evaluation



**章节总结：7. Evaluation（评估）**

本节对 Memory3 模型进行全面评估，涵盖其通用能力、对话技能、专业能力、事实性与幻觉控制，以及解码速度。评估对象包括 Memory3 与其他 SOTA 大型语言模型（LLMs）及基于 RAG（检索增强生成）的方法进行比较。

### 7.1 通用能力评估
使用 Huggingface leaderboard 上的任务以及两个中文任务，评估 Memory3 的通用能力。结果表明，尽管 Memory3 的模型参数仅为 2.4B，其平均性能优于许多更大参数规模的模型，如 Llama2-7B 与 Llama2-13B 的性能差异为 4.91%，而 Memory3 的性能提升达 2.51%，相当于增加了约 51% 的“有效模型规模”。此外，即使将显式记忆的向量压缩至原始大小的 8.75%，模型性能仍无明显下降。对比 Retro++ XXL（9.5B 参数）的零样本结果，Memory3 在多个任务上表现更优。

### 7.2 对话能力评估
在 MT-Bench 多轮对话基准测试中，Memory3 的表现优于多个主流模型，如 Vicuna-7B、Falcon-40B-Instruct 和 ChatGLM2-6B，尽管其模型参数较小（2.4B）。通过 DPO（直接偏好优化）微调，Memory3 的对话性能进一步提升。

### 7.3 幻觉与事实性评估
通过 TruthfulQA、HaluEval 和 HalluQA 等数据集测试幻觉问题。结果显示，Memory3 在多个任务上的得分高于其他主流模型，表明其显式记忆机制有助于减少幻觉，提高输出的事实准确性。尤其在中文任务中表现突出。

### 7.4 专业任务评估
Memory3 在法律和医学领域的专业任务中表现出色。通过引入领域相关文档作为显式记忆，模型无需微调即可快速适应新领域。在法律任务（JEC-QA）和医学任务（C-Eval、MMLU、CMMLU）中，Memory3 的性能优于多个基于 RAG 的方法，证明其在专业领域中的高效性和准确性。

### 总结
Memory3 凭借显式记忆机制，在通用能力、对话技能、幻觉控制以及专业任务表现上均优于或可比现有 SOTA 模型。其参数效率高，计算成本低，且在多领域适应性方面具有显著优势，展示了其在实际应用中的潜力。


## 7.5 _ Inference Speed



本章节主要评估了 **Memory3** 模型在推理速度（Inference Speed）方面的表现，并将其与传统的 RAG（Retrieval-Augmented Generation）模型进行比较，以衡量显式记忆机制在检索效率上的优势。

### 主要内容总结如下：

1. **评估目标**：
   - 评估 Memory3 的解码速度（以 tokens 每秒为单位）。
   - 与基于文本检索的 RAG 模型比较，衡量 Memory3 使用显式记忆的加速效果。

2. **实验设置**：
   - 使用 A800 GPU，所有模型均启用 Flash Attention。
   - 输入输出长度统一为 128 tokens，批量大小为 32。
   - Memory3 每 64 tokens 进行一次检索，平均每生成 64 tokens 进行 3 次检索。
   - 实验涵盖本地服务器和边缘设备（Jetson AGX Orin）两种场景。

3. **实验结果**：
   - 表 21 展示了不同模型在本地服务器和边缘设备上的吞吐量（with/without retrieval）。
   - Memory3-2B 模型吞吐量约为 733 tokens/s（带检索），而相同模型不带检索时约为 1131 tokens/s，说明检索带来约 35.2% 的性能下降。
   - 相较之下，其他模型如 Gemma、Llama、Qwen 等也在表中列出，供对比分析。

4. **性能下降原因分析**：
   - Memory3 的速度下降主要归因于两点：
     1. **从存储加载记忆键值对到 GPU 的开销**：由于 Memory3 高频检索，此操作带来显著延迟。
     2. **逐块注意力的 Python 实现**：由于每个块需要关注不同的记忆，当前使用 for 循环处理，效率较低。
   - 实际计算中，嵌入向量计算和向量索引检索的时间开销相对较小。

5. **优化方向**：
   - 作者表示计划优化代码，例如用 CUDA 实现逐块注意力，以将与记忆相关的开销降低至理论值 0.228% 附近。

### 总结：
本节通过对 Memory3 和其他 LLM 模型在推理速度上的对比，揭示了显式记忆机制在提升模型性能的同时，也带来了额外的计算开销。研究指出当前性能瓶颈，并提出了可能的优化策略，为后续改进提供了方向。


## 8 _ Conclusion



本章总结了研究的主要目标、方法和实验成果，同时提出了未来的改进方向。

**研究目标与方法**：  
本文旨在降低大型语言模型（LLM）的训练与推理成本，构建一个在性能上可媲美更大更慢模型但效率更高的LLM。研究通过“知识操作”的新视角分析模型，将模型成本视为“知识”在不同记忆格式之间传输的开销。研究识别出两个效率低下的原因：知识的放置不当和知识遍历问题，并通过引入一种新颖的记忆格式——**显式记忆（explicit memory）**，配合新的训练方案和架构予以解决。

**实验结果**：  
初步实验模型 **Memory3-2B** 在性能和速度上超越了许多更大规模的SOTA模型以及基于RAG的模型，展示了显式记忆设计的有效性。

**未来工作方向**：  
1. **高效的抽象知识训练**：通过筛选训练数据，最大化抽象知识并减少具体知识，使训练成本更接近人类的学习效率，理想情况下模型应能自行评估数据质量，忽略无用信息。
2. **类人能力的实现**：利用显式记忆，模型可实现无限上下文处理、记忆巩固（显式转隐式记忆）和有意识推理等认知功能，从而提升推理能力和效率。
3. **显式记忆的紧凑表示**：借鉴人类记忆的分类（情景记忆与语义记忆），当前模型更接近情景记忆，未来可尝试加入从情景记忆中归纳出的语义记忆，以增强推理能力。

**工程改进方向**：  
还包括若干工程优化方向，如稀疏注意力匹配、更稀疏的记忆头、完整保留上下文的记忆提取、基于机器偏好的知识库编译、减少显式记忆的时间消耗等。

总体而言，本文提出了一个通过显式记忆机制提高模型效率和能力的创新架构，并为未来的研究和优化提供了清晰的路线图。


## Acknowledgement



本章节为致谢部分，内容总结如下：

该研究得到了中国国家自然科学基金委（NSFC）“可解释与通用的下一代人工智能”重大项目（编号：92270001）的支持。作者还感谢多位教授和研究人员在研究过程中提供的有益讨论，包括徐志强教授、林周涵教授、刘芳睿、杭良凯、陶子阳、王霄星、王铭泽、金永奇、何昊天、黄冠华、胡祎荣等。


## Appendix A Cost Estimation



本章节 **Appendix A: Cost Estimation** 主要对 Memory3 模型中三种知识存储方式（**隐式记忆**、**显式记忆** 和 **外部信息**）的计算成本进行估算，并通过比较来展示显式记忆在成本上的优势。

### 模型参数设定
- 使用 2.4B 参数的 **Memory3 模型** 作为主干模型。
- 模型结构包括：
  - Transformer 层数 $ L = 44 $
  - 查询头数 $ H = 40 $，键值头数 $ H_{kv} = 8 $
  - 头维度 $ d_h = 80 $，隐藏维度 $ d = 3200 $
  - 词汇表大小 $ n_{\text{vocab}} = 60416 $
  - 显式记忆层数 $ L_{\text{mem}} = 22 $
  - 记忆长度 $ l_{\text{mem}} = 8 $，参考长度 $ l_{\text{ref}} = 128 $，块长度 $ l_{\text{chunk}} = 64 $

### 隐式记忆（Implicit Memory）成本
- **写入成本**：通过训练模型参数来存储知识，假设每个知识只需一次训练步。计算结果约为 **2.24 TFlops**。
- **读取成本**：由于隐式记忆的参数会在每次模型调用中使用，其读取成本难以准确估算，但这里下界设为 **0 TFlops**（为了显示其竞争力）。
- 总成本为 $ c_{\text{implicit}}(n) \geq 2.24 $，与使用次数无关。

### 显式记忆（Explicit Memory）成本
- **写入成本**：包括自注意力层、MLP 层和 token 稀疏化操作。计算结果约为 **0.308 TFlops**。
- **读取成本**：涉及块与记忆之间的注意力计算，计算结果约为 **0.000144 × n**，其中 $ n $ 为使用次数。
- 总成本为 $ c_{\text{explicit}}(n) = 0.308 + 0.000144n $，随着使用次数增加而增长。

### 外部信息（External Information，如 RAG）成本
- **写入成本**：为 **0 TFlops**，因为外部信息以纯文本形式存储。
- **读取成本**：考虑到引用文本对模型的影响，估算为 **0.624 × n**。
- 总成本为 $ c_{\text{external}}(n) \geq 0.624n $，随着使用次数线性增长。

### 综合比较
三种存储方式的成本函数分别为：
- 隐式记忆：$ c_{\text{implicit}}(n) \geq 2.24 $
- 显式记忆：$ c_{\text{explicit}}(n) = 0.308 + 0.000144n $
- 外部信息：$ c_{\text{external}}(n) \geq 0.624n $

通过比较可以发现：
- 当知识的使用次数 $ n \in (0.494, 13400) $ 时，**显式记忆是最优选择**，其成本远低于隐式记忆和外部信息。

### 拓展讨论
- **知识保留问题**：模型参数更新（如微调）会导致隐式和显式记忆的丢失。为此，通常会在微调中保留部分预训练数据。
- **研究方向**：设计更鲁棒的架构，使记忆对模型更新具有更强的稳定性，是一个值得探索的方向。

**总结**：本附录通过详细的计算和比较，展示了显式记忆在成本上的显著优势，特别是在知识使用次数中等的场景下，显式记忆是更高效的选择。


## Appendix B Vector Compression



本章节总结如下：

在附录B中，作者介绍了用于向量压缩的向量量化器（vector quantizer），其采用了FAISS库中的复合索引，索引类型为 **OPQ20x80-Residual2x14-PQ8x10**。该量化器能够将一个80维的bfloat16向量压缩为14维的uint8向量，压缩比约为11.4。

为了训练该量化器，作者从知识库中均匀独立地采样参考文本，并使用 **Memory3-2B-SFT** 模型将其编码为显式记忆（key-value向量），然后输入到量化器中进行训练。这种训练方式避免了对特定评估任务所检索参考内容的偏向性。


## Appendix C Supplementary Evaluation Results



本附录补充评估了 Memory3-2B 模型在不同训练阶段和参数设置下的性能表现：

1. **不同训练阶段的性能变化**（表22）：
   - Memory3-2B 在三个训练阶段（warmup、continual train、SFT）的测试分数逐步提升。
   - 例如，在英文任务中的平均得分从 warmup 阶段的 42.13 提升到 continual train 阶段的 45.12，最终在 SFT 阶段达到 63.31。
   - 作者指出，若在 warmup 阶段减少损失差异，可以进一步提升 continual train 阶段的性能。

2. **检索过滤阈值的影响**（表23）：
   - 在评估任务中引入过滤机制以防止复制行为，过滤阈值设置为 2/3。
   - 表23 显示不同阈值（如 no filter、80%、2/3）对测试分数的影响较小，说明大多数任务的问题未在知识库中重复出现。

3. **few-shot 与 0-shot 模式的影响**（表24）：
   - 在 few-shot 模式下，显式记忆带来的性能提升约为 2.51%。
   - 在 0-shot 模式下，显式记忆的提升增加至 3.70%，尤其在 GSM8k 任务中提升显著（从 10.46% 提升至 13.50%）。

总结来看，Memory3 的性能在不同训练阶段和任务设定下表现出稳定的提升，且显式记忆在 0-shot 模式下更能发挥优势。
