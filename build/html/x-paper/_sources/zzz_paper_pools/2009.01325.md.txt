# 2009.01325_Learning to summarize from human feedback

* 首页: <https://arxiv.org/abs/2009.01325>
* PDF: <https://arxiv.org/pdf/2009.01325>
* 引用: 2965


## 摘要（Abstract）

本论文提出了一种基于人类反馈来训练摘要生成模型的方法。作者通过收集大量人类对摘要质量的偏好数据，训练了一个奖励模型（reward model），并使用该模型指导强化学习（RL）过程，从而优化摘要生成。实验表明，该方法在ROUGE指标和人类评估上均优于监督学习基线。

**重点**：方法核心是利用人类偏好数据训练奖励模型，进而通过强化学习优化摘要生成。

---

## 1. 引言（Introduction）

引言部分指出，传统的摘要模型依赖于监督学习，使用参考摘要作为训练目标，但这种方式存在局限，例如无法很好捕捉人类对摘要质量的主观判断。作者提出使用人类反馈来训练模型，使生成的摘要更符合人类偏好。

**重点**：强调人类反馈在提升摘要质量中的作用，提出“人类反馈 + 强化学习”的新范式。

---

## 2. 相关工作（Related Work）

本节回顾了三类相关工作：

1. **自动摘要方法**：包括基于抽取和基于生成的模型。
2. **强化学习在摘要中的应用**：已有研究使用ROUGE等自动指标作为奖励函数。
3. **人类反馈在机器学习中的应用**：如在对话系统、图像生成中的使用。

**精简**：总结已有方法的局限性，为本文方法提供理论基础。

---

## 3. 方法（Methods）

本节是论文的核心部分，详细介绍了方法流程，分为三个阶段：

### 3.1 收集人类偏好数据（Collecting Human Feedback）

- 让标注者对同一文章的多个摘要进行两两比较，选出更优者。
- 构建了一个包含数十万条比较数据的偏好数据集。

**重点**：数据质量直接影响奖励模型的准确性。

### 3.2 训练奖励模型（Reward Modeling）

- 使用偏好数据训练一个神经网络模型，预测人类对摘要的偏好。
- 模型输入为文章和摘要对，输出为偏好得分。

**重点**：奖励模型是连接人类反馈与强化学习的关键桥梁。

### 3.3 强化学习训练摘要模型（Reinforcement Learning）

- 使用奖励模型作为奖励函数，通过PPO等强化学习算法优化摘要生成模型。
- 在训练过程中，模型学习生成更符合人类偏好的摘要。

**重点**：强化学习阶段是生成高质量摘要的核心。

---

## 4. 实验设置（Experiments）

### 4.1 数据集

- 使用CNN/DM数据集进行训练和评估。
- 构建了专门的人类偏好数据集。

### 4.2 模型结构与训练细节

- 使用基于Transformer的模型结构。
- 奖励模型和摘要模型分别训练。

**精简**：介绍实验所用数据、模型架构和训练流程。

---

## 5. 结果与分析（Results and Analysis）

### 5.1 ROUGE指标结果

- 与监督学习模型相比，RL模型在ROUGE指标上表现更优。

### 5.2 人类评估结果

- 通过人工评估（如摘要的连贯性、相关性、简洁性）验证模型效果。
- RL模型在多个维度上优于基线模型。

### 5.3 奖励模型分析

- 奖励模型能够有效预测人类偏好。
- 与人类判断的一致性较高。

**重点**：人类评估结果是验证方法有效性的关键依据。

---

## 6. 讨论（Discussion）

讨论了方法的局限性和未来方向：

- 数据收集成本较高。
- 奖励模型可能存在偏差。
- 可扩展到其他自然语言生成任务。

**重点**：指出方法的现实挑战和潜在应用前景。

---

## 7. 结论（Conclusion）

总结全文，强调通过人类反馈训练摘要模型的有效性，展示了将人类偏好引入训练过程的潜力。

**重点**：强调人类反馈在提升生成质量中的价值。

---

如需进一步细化某一部分内容（如实验细节或模型结构），可继续提问。


## Abstract



## 总结

### 摘要（Abstract）
本文探讨了如何通过优化人类偏好来显著提升摘要生成的质量。作者收集了大量高质量的人类对摘要的比较数据，训练了一个奖励模型（Reward Model）来预测人类偏好的摘要，并使用强化学习（Reinforcement Learning, RL）微调摘要生成策略。该方法在Reddit的TL;DR数据集上取得了显著成果，生成的摘要质量优于仅使用监督学习训练的更大模型，并且能够迁移到CNN/DM新闻文章数据集上。作者还进行了广泛的分析，验证了奖励模型在新数据集上的泛化能力，并指出优化奖励模型比优化ROUGE指标更能提升摘要质量。此外，作者公开了包含64,832个摘要比较的数据集。

### 引言（Introduction）
大规模语言模型预训练已成为提升自然语言处理（NLP）任务性能的重要手段。然而，传统的监督微调方法存在与实际目标（生成高质量输出）之间的不一致。这种不一致主要源于最大似然目标无法区分重要错误（如编造事实）和次要错误（如选择同义词），并且模型会为所有人类示范分配概率质量，包括低质量的示范。此外，采样过程中的分布偏移也可能导致性能下降。尽管非均匀采样策略（如束搜索）可以提升质量，但也可能引入重复等不良效果。

作者提出了一种基于人类反馈的训练方法，通过收集人类对摘要的偏好数据，训练奖励模型，并使用强化学习优化生成策略。该方法在Reddit的TL;DR数据集上取得了显著成果，生成的摘要质量优于监督学习模型，并且能够迁移到新闻文章数据集CNN/DM上。作者还进行了广泛的分析，验证了奖励模型的有效性，并公开了数据集和代码。

### 相关工作（Related Work）
相关工作主要包括使用人类反馈训练摘要模型的强化学习方法。Bohm等人（2019）通过人类评分训练奖励函数，并使用强化学习优化摘要生成。Ziegler等人（2019）也在多个任务上训练Transformer模型以优化人类反馈，但他们的方法是在线训练，且生成的摘要较为提取式。与之相比，本文使用更大的模型，采用离线设置收集人类反馈，确保标签者与研究人员之间的一致性，并对算法进行了改进，如分离策略网络和价值网络。

此外，人类反馈也被用于对话、翻译、语义解析、故事生成、评论生成和证据提取等领域。奖励建模方法最早用于学习排序，后来被应用于基于反馈或点击数据的搜索结果排序。还有研究使用人类反馈训练模拟环境中的智能体。同时，也有大量研究使用强化学习优化自动指标（如ROUGE、BLEU）以提升NLP任务性能。最后，还有研究通过改进架构和预训练过程来提升摘要性能。

### 方法与实验细节（Method and Experiment Details）

#### 高层方法（High-level Methodology）
本文的方法基于Ziegler等人（2019）的工作，但采用离线设置。初始策略通过监督学习在Reddit TL;DR数据集上进行微调。整个过程包括三个步骤：（1）从现有策略中收集样本并发送给人类评估者；（2）从人类比较中学习奖励模型；（3）使用强化学习优化策略。作者提供了更详细的流程描述，包括奖励模型和策略训练的细节以及质量控制过程。

#### 数据集与任务（Datasets and Task）
作者使用Reddit的TL;DR数据集，包含约300万个帖子及其摘要。为了确保质量，作者对数据集进行了过滤，保留了24-48个token的摘要。最终数据集包含123,169个帖子，其中约5%作为验证集。任务是生成不超过48个token的高质量摘要，根据人类判断评估摘要质量。

#### 收集人类反馈（Collecting Human Feedback）
与Ziegler等人（2019）相比，作者改进了人类数据收集过程。首先，采用离线设置，交替发送大量比较数据给标签者，并重新训练模型。其次，与标签者保持密切联系，提供详细指导，确保标签者与研究人员之间的一致性。结果表明，标签者与研究人员的一致率为77% ± 2%，而研究人员之间的一致率为73% ± 4%。

#### 模型（Models）
所有模型均为Transformer解码器，类似于GPT-3。作者使用1.3B和6.7B参数的模型进行实验。预训练模型通过自回归预测下一个token进行训练。监督基线模型通过监督学习预测摘要。奖励模型通过预测人类偏好的摘要进行训练。策略模型通过强化学习优化奖励模型输出。

### 结果（Results）

#### 从人类反馈中生成Reddit帖子摘要（Summarizing Reddit Posts from Human Feedback）
使用人类反馈训练的策略显著优于监督学习基线。6.7B的人类反馈模型在61%的情况下优于监督学习模型（后者为43%）。此外，人类反馈模型生成的摘要质量优于数据集中的人类示范。控制摘要长度后，人类反馈模型仍优于参考摘要约65%的时间。

#### 迁移到新闻文章摘要（Transfer to Summarizing News Articles）
人类反馈模型在未经过新闻特定微调的情况下，也能生成高质量的CNN/DM新闻文章摘要。6.7B的人类反馈模型生成的摘要几乎与CNN/DM参考摘要质量相当，尽管生成的摘要较短。

#### 理解奖励模型（Understanding the Reward Model）
随着对奖励模型的优化，策略与人类偏好的一致性先提升后下降。轻度优化时模型表现良好，但过度优化后奖励模型与人类偏好反相关。奖励模型的性能随着数据量和模型规模的增加而提升。6.7B的奖励模型接近单个人类的判断准确性。

#### 分析自动摘要指标（Analyzing Automatic Metrics for Summarization）
作者研究了各种自动指标作为人类偏好的预测器。结果表明，学习到的奖励模型在CNN/DM数据集上表现优于其他指标。ROUGE在监督基线模型样本中与人类偏好的一致性为57%，但在人类反馈模型样本中下降至50%。优化ROUGE的效果不如优化奖励模型。

### 讨论（Discussion）
本文的局限性包括训练最终模型所需的时间和成本较高，数据收集过程也较为昂贵。未来方向包括将该方法应用于其他任务（如对话、翻译、问答、语音合成和音乐生成），以及探索更高效的人类反馈方法。作者还讨论了该技术的潜在社会影响，包括对就业的影响和可能的滥用风险。

### 致谢（Acknowledgements）
作者感谢Beth Barnes、Geoffrey Irving、Ben Mann等人的支持，以及所有合同工提供的数据。

### 参考文献（References）
提供了与本文相关的大量文献，涵盖强化学习、奖励建模、摘要生成、自然语言处理等多个领域。


## Appendix



### 附录总结

#### 附录 A：TL;DR 数据集细节
- **数据集组成**：TL;DR 数据集主要来自 Reddit 的多个子论坛，其中约 54.3% 来自 "relationships" 子论坛，13.2% 来自 "AskReddit"，7.5% 来自 "relationship_advice"，其余来自其他较小的子论坛。
- **数据预处理**：去除了重复帖子，重新解析 TL;DR 摘要，过滤非顶级帖子和不在白名单中的子论坛内容，去除包含敏感内容的帖子，并限制帖子长度不超过 512 个 token。最终保留了 287,790 个帖子，其中 5% 用于验证集。
- **参考摘要过滤**：进一步过滤参考摘要，去除包含“Edit”、“Update”或“P.S.”开头的摘要，去除低质量和长度不符合要求的摘要（24-48 token）。最终保留 123,169 个帖子，其中 1913 个用于模型选择。

#### 附录 B：模型训练细节
- **模型超参数**：不同大小的模型（1.3B、3B、6.7B、13B）使用不同的层数、模型维度和头数，最大学习率分别为 2e-4、1.6e-4、1.2e-4 和 1e-4。
- **训练过程**：模型使用 fp16 激活和 Adam 优化器进行训练。预训练模型在 CommonCrawl、Webtext、书籍和维基百科上训练，学习率使用余弦调度。
- **监督基线**：使用预训练模型初始化，学习率分别为 6.35e-5、5.66e-5、2.83e-5 和 2.83e-5，批量大小为 128。
- **奖励模型**：使用监督基线初始化，奖励头权重初始化为 𝒩(0, 1/(d_model+1))，学习率分别为 1.5e-5 和 5e-6，批量大小为 64。
- **PPO 训练**：使用单独的策略网络和价值网络，初始学习率分别为 1.5e-5 和 7e-6，批量大小分别为 512 和 256，运行 100 万次 episode。
- **输入格式**：不同任务使用不同的输入格式，如 TL;DR 使用 "SUBREDDIT: r/{subreddit} TITLE: {title} POST: {post} TL;DR:"，CNN/DM 使用 "{article} TL;DR:"。

#### 附录 C：人类数据收集细节
- **确保高质量数据**：通过 labeler 上岗培训、收集比较数据、提供反馈和研究人员校准来确保数据质量。labeler 来自 Upwork、Scale 和 Lionbridge，使用自建网站进行数据收集。
- **labeler 人口统计**：labeler 来自不同种族、国籍、年龄和教育背景，但以白人和美国人为主。
- **labeler 网站**：自建网站用于数据收集，支持不同任务的渲染，如摘要比较和 Likert 评估。
- **labeler 指导**：提供详细的指导，包括 Reddit 术语翻译和摘要评估的维度（本质、清晰度、准确性、目的、简洁性、风格）。
- **数据组成**：训练数据来自不同策略的摘要，包括监督基线、PPO 和 BoN 策略。
- **示例比较任务**：提供随机和困难的比较任务示例，展示 labeler 评估的复杂性。

#### 附录 D：基线选择
- **基线选择**：由于监督学习基线需要大量高质量演示，成本高昂，因此使用监督和零样本模型作为基线，并与 T5 模型进行比较。

#### 附录 E：CNN/DM 首三句与参考摘要
- **首三句表现**：labeler 更偏好首三句摘要，因其长度更长，覆盖更多内容。但参考摘要在某些情况下引入新信息，导致评分较低。

#### 附录 F：控制摘要长度
- **长度控制**：使用逻辑回归模型控制摘要长度，发现摘要长度是影响质量的混杂因素。在 TL;DR 数据集上，人类反馈模型在不同长度上均优于监督基线和参考摘要。

#### 附录 G：附加结果
- **价值函数消融**：使用单独的价值函数网络优于共享网络。
- **模型评估**：在 TL;DR 和 CNN/DM 数据集上，覆盖与整体评分强相关，所有模型在连贯性上得分较高。
- **BoN 优化**：优化 ROUGE 指标的表现不如优化奖励模型。
- **ROUGE 分数**：人类反馈模型在 TL;DR 数据集上的 ROUGE 分数略低于监督模型，但在 CNN/DM 数据集上表现更好。
- **bigram 重叠**：模型在 CNN/DM 数据集上比在 TL;DR 数据集上复制更多内容。
- **奖励模型验证集**：奖励模型对小编辑、句子重排和角色交换敏感，但有时偏好较差的人工摘要。
- **评估指标一致性**：奖励模型与 labeler 的一致性较高，而 ROUGE 和监督基线的 logprob 与 labeler 的一致性较低。

#### 附录 H：样本
- **随机样本**：提供 TL;DR 和 CNN/DM 数据集上的随机样本和人类评估结果。
- **过度优化样本**：展示过度优化奖励模型的样本，虽然长度较长且质量较低，但仍反映帖子的大致内容。

### 总结
附录部分详细描述了 TL;DR 数据集的构建和预处理、模型训练细节、人类数据收集过程、基线选择、CNN/DM 数据集的首三句与参考摘要比较、摘要长度控制、附加实验结果以及样本展示。重点包括数据集的组成、预处理步骤、模型超参数、人类数据质量控制、奖励模型验证和样本分析。
