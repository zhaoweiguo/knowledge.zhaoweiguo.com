# 2505.02099_MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents

* 首页: <https://arxiv.org/abs/2505.02099>
* PDF: <https://arxiv.org/pdf/2505.02099>


MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents



文章标题：**MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents**

### 总结：

本文介绍了 **MemEngine**，这是一个专为**基于大型语言模型（LLM）的智能体**设计的**统一且模块化的记忆库**。该库的主要目标是帮助开发者构建和管理LLM智能体的**高级记忆系统**，以提升智能体的长期记忆能力、上下文理解能力和任务执行效率。

---

## 核心点总结：

### 1. **动机与背景**
- LLM智能体在任务执行中需要**长期记忆**，以提升决策一致性与效率。
- 当前LLM的记忆管理能力有限，缺乏统一、模块化、可扩展的解决方案。
- **MemEngine** 应运而生，旨在提供一个**可插拔、可定制的记忆系统**，以支持LLM智能体的复杂交互与长时记忆需求。

### 2. **MemEngine 的核心特性**
- **统一性（Unified）**：提供统一的接口，支持多种记忆类型与存储方式。
- **模块化（Modular）**：记忆系统可灵活组合，各组件（如记忆存储、检索、更新）可独立开发和替换。
- **可扩展性（Extensible）**：支持自定义记忆模块，便于开发者根据任务需求进行扩展。
- **兼容性（Compatibility）**：与主流LLM框架（如LangChain、Hugging Face等）兼容，便于集成使用。

### 3. **MemEngine 的核心组件**
- **记忆存储（Memory Storage）**：支持多种存储后端（如SQL、向量数据库、内存等），支持结构化与非结构化数据。
- **记忆检索（Memory Retrieval）**：提供高效、准确的检索算法（如TF-IDF、向量相似度等），以获取相关记忆片段。
- **记忆更新（Memory Update）**：支持记忆的动态更新与过期机制，确保记忆的时效性与准确性。
- **记忆接口（Memory Interface）**：标准化的API，使开发者能够方便地调用记忆功能，而不必关心底层实现。

### 4. **应用场景**
- **任务持续性（Task Persistence）**：帮助LLM智能体记住任务上下文，实现长期任务的连续处理。
- **个性化交互（Personalized Interaction）**：通过记忆用户偏好与历史对话，提升交互体验。
- **知识积累（Knowledge Accumulation）**：在多轮对话或任务中不断积累知识，提升智能体的推理能力。
- **多智能体协作（Multi-agent Collaboration）**：支持多个智能体之间的记忆共享与协同。

### 5. **实验与评估**
- 作者在多个任务场景中对MemEngine进行了评估，包括对话、任务规划与多智能体协作。
- 实验结果表明，MemEngine能够显著提升LLM智能体的记忆能力与任务执行效率。

### 6. **结论与展望**
- MemEngine为LLM智能体的记忆管理提供了一个**统一、模块化、高效**的解决方案。
- 未来的研究方向包括：**更高效的记忆压缩算法**、**多模态记忆管理**、**与强化学习的结合**等。

---

### 总体评价：
MemEngine是一篇具有实际应用价值的研究成果，尤其适用于需要长期记忆和复杂交互的LLM智能体开发场景。其**模块化设计与可扩展性**使其具备高度的灵活性与实用性，值得开发者与研究人员进一步探索与使用。


## Abstract



## 摘要总结

### 研究背景
近年来，基于大型语言模型（LLM-based）的智能体（Agent）已在多个领域中广泛应用于各种场景。其中，**记忆能力（Memory Mechanism）** 作为智能体的关键组成部分，吸引了工业界和学术界的广泛关注。

### 研究现状
尽管已有许多关于先进记忆模型的研究成果，但目前仍然**缺乏一个统一的实现框架**，来整合和开发这些模型，这对实际应用和进一步研究造成了一定阻碍。

### 本文贡献
为了解决上述问题，本文提出了一个**统一且模块化的库**，称为 **MemEngine**。该框架支持开发多种先进的记忆模型，并能够方便地实现和扩展。基于 MemEngine，作者实现了大量来自最新研究的内存模型。

### 项目开源
为了促进社区发展，本文项目已经**开源**，并提供在 GitHub 上：[MemEngine GitHub 项目链接](https://github.com/nuster1128/MemEngine)。

### 重点内容
- **提出 MemEngine 框架**：这是本文的核心贡献，提供了一个统一的平台用于开发和使用 LLM-based 智能体的记忆模型。
- **内存模型的可扩展性与插件化**：MemEngine 支持**用户友好、可插拔的记忆使用方式**，方便研究人员根据需求定制或扩展。
- **实现大量现有模型**：基于该框架，作者实现了大量来自现有研究的记忆模型，便于复现和对比实验。

### 总结
本文旨在通过构建统一的框架 MemEngine，推动大型语言模型智能体中记忆机制的研究与应用，具有较高的实用价值和可扩展性。


## 1. Introduction



## 1. 引言（Introduction）

随着**大规模语言模型（LLM）**的快速发展，基于LLM的智能体（agent）因其在执行复杂任务和承担多样化角色方面的能力，已被广泛应用于多个领域（Wang 等，2024）。其中，**记忆（memory）**模块是智能体最关键的组成部分之一，它决定了智能体如何存储历史数据、反思已有知识，并回忆有用信息以支持决策（Zhang 等，2024）。

### 记忆模块的重要性

在复杂任务中，记忆模块能够记录过去与环境交互中的关键信息，并为当前任务提供先前轨迹中的经验。在角色扮演和社交模拟中，记忆模块还能够突出每个角色的个性和特征，使不同角色具有独特性。

### 现有研究的不足

尽管已有研究提出多种记忆模型，但这些模型的实现方式差异较大，**缺乏统一框架**，给开发者在实验中尝试不同模型带来困难。此外，许多基础功能（如检索、摘要）在不同模型中**重复实现**，导致开发效率低下。同时，很多学术模型与智能体**紧密结合、不可插拔**，难以跨智能体复用。

---

### MemEngine：一个统一且模块化的记忆库

为解决上述问题，我们开发了**MemEngine**——一个统一且模块化的记忆库，旨在提升基于LLM的智能体中高级记忆模型的开发效率与使用便利性。我们的主要特点如下：

---

#### 统一且模块化记忆框架

我们提出了一个由**三个层次**组成的统一记忆框架，用于组织和实现现有的研究模型：

- **最低层**是记忆功能（memory functions），实现基本功能（如检索）作为所有记忆操作的基础支持。
- **中间层**是记忆操作（memory operations），构成基本的记忆操作（如记忆回放），用于构建不同记忆模型。
- **最高层**是记忆模型（memory models），实现多种现有的研究模型（例如，MemoryBank，Zhong 等，2024），可方便地应用于不同的智能体中。

这三层结构**模块化设计**，高层模块可复用低层模块，提高了实现的效率和一致性。我们还提供了：
- **配置模块**：方便修改不同层级的超参数和提示。
- **工具模块**：用于方便地保存和展示记忆内容。

---

#### 丰富的记忆实现

基于统一的模块化框架，我们实现了大量来自近期研究的记忆模型，其中许多被广泛应用于不同的应用场景。这些模型**可轻松切换和测试**，并支持通过调整超参数和提示以优化在不同智能体和任务中的应用效果。

---

#### 方便且可扩展的记忆开发

基于模块化的记忆操作和功能，研究人员可以**方便地开发自己的高级记忆模型**。他们也可以**扩展已有操作和功能**，构建新的模块。为了更好地支持开发，我们提供了**详细的文档说明与示例**，指导用户进行定制开发。

---

#### 可插拔且用户友好的记忆使用

MemEngine 提供了**多种部署方式**，为基于LLM的智能体赋予强大的记忆能力。我们还提供了以下**多种使用模式**：
- 默认模式
- 配置模式
- 自动模式

此外，记忆模块**可插拔**，能够轻松应用于不同的智能体框架。MemEngine 还兼容一些主流的基于LLM的智能体框架，如**AutoGPT**。这些特性共同提升了库的**用户友好性**。

---

### 总结

MemEngine 是首个在统一且模块化框架下实现大量记忆模型的研究库，**兼顾开发便利与使用便捷**。为了推动社区发展，我们已将项目开源在 GitHub（https://github.com/nuster1128/MemEngine），并提供了详细的**文档**（https://memengine.readthedocs.io/en/latest/），支持应用与开发。


## 2. Comparison with Relevant Libraries



## 2. 与相关库的比较

本章节主要比较了当前用于增强大语言模型（LLM）代理记忆能力的现有库，并突出了MemEngine与其他库相比的优势。

### 已有库分类

已有库可以分为两类：

1. **集成在代理库中的记忆模块**：这些库在构建LLM代理系统的同时内置了记忆功能，例如：
   - AutoGen
   - MetaGPT
   - CAMEL
   - AgentScope
   - LangChain
   - AgentLite
   - CrewAI
   - AutoGPT
   - AgentVerse

2. **独立的记忆库**：专注于为LLM代理提供记忆支持的独立库，例如：
   - Memary
   - Cognee
   - Mem0
   - Agentmemory

这些库大多提供了**即插即用的记忆组件**，支持**基本的读写操作**，但**高级的记忆操作**（如反思和优化）实现较少。

### MemEngine 的优势

**MemEngine** 作为本文提出的一个库，具有以下几个**重点优势**：

1. **统一且模块化的框架**：MemEngine 提供了一个统一的框架，支持多种研究模型的实现与集成，便于研究人员进行扩展和定制。
2. **支持高级记忆操作**：与大多数现有库不同，MemEngine 实现了**反思**和**优化**等高级记忆功能。
3. **提供完整的默认模型**：MemEngine 包含全面的默认模型，便于快速部署和使用。
4. **支持高级模型定制**：允许研究人员根据具体需求**定制和扩展记忆模型**，满足多样化应用场景。

### 对比表格详解（Table 1）

表格[1]从多个维度对MemEngine与现有库进行了详细对比，包括：

| 功能特性 | 集成在代理库中的记忆模块 |
| --- | --- |
| **即插即用集成** | ✅ AutoGen, CAMEL, AgentScope, LangChain, AgentLite, CrewAI |
| **基本读写支持** | ✅ 所有库均支持 |
| **反思和优化支持** | ❌ 仅 AutoGen 支持 |
| **完整的默认模型** | ❌ 无库支持 |

| 功能特性 | 独立记忆库 |
| --- | --- |
| **即插即用集成** | ✅ Memary, Cognee, Mem0, Agentmemory, Zep, MemEngine |
| **基本读写支持** | ✅ 所有库均支持 |
| **反思和优化支持** | ✅ 仅 Zep 和 MemEngine 支持 |
| **完整的默认模型** | ✅ 仅 MemEngine 支持 |
| **高级模型定制** | ✅ 仅 MemEngine 和 Zep 支持 |

**重点强调**：MemEngine 是唯一一个**在统一框架下实现完整默认模型和高级记忆功能**的库，同时提供**高度可定制的模块化设计**，适合进行先进记忆系统的研究和开发。


## 3. MemEngine Library



以下是对“3. MemEngine Library”章节的结构化总结，重点突出关键内容，精简次要信息：

---

## 3. MemEngine Library

### 3.1. Overview（概述）

MemEngine Library 的框架结构如图1所示，分为三个层级：

- **底层**：实现基本功能作为标准支持。
- **中层**：提供基础的记忆操作，用于实现核心流程。
- **高层**：整合了多种已有研究的记忆模型。

此外，还提供了**配置模块**（Configuration module）和**工具模块**（Utility module），以提升开发与使用的便捷性。

> ⭐ 重点：三层结构清晰，提供了良好的模块化支持，方便扩展与定制。

---

### 3.2. Memory Models（记忆模型）

MemEngine 实现了多种记忆模型，并统一通过标准接口（reset、store、recall、manage、optimize）调用，实现**无缝切换**。

**实现的具体模型包括：**

- **FUMemory（Full Memory）**：简单地将所有信息拼接为一个字符串，是“长上下文记忆”。
- **LTMemory（Long-term Memory）**：通过语义相似性检索相关信息。
- **STMemory（Short-term Memory）**：维护最近信息，拼接成字符串。
- **GAMemory**：基于生成式代理的记忆模型，具有**加权检索**与**自我反思**机制。
- **MBMemory**：多层记忆结构，支持动态摘要与遗忘机制。
- **SCMemory**：自我控制记忆模型，只回忆必要的信息。
- **MGMemory**：类操作系统结构的层级记忆模型。
- **RFMemory**：通过优化轨迹提升记忆能力。
- **MTMemory**：树状结构的记忆模型，组织信息更高效。

> ⭐ 重点：支持多种记忆模型，并且通过接口统一调用，便于替换与对比研究。

---

### 3.3. Memory Operations（记忆操作）

MemEngine 提供了多种通用记忆操作，用于构建不同类型的记忆模型，包括：

- **Store**（存储）：接收环境观测，处理后存储为记忆内容，可能建立索引或摘要以辅助回忆。
- **Recall**（回忆）：基于当前查询或观察，提取有用信息辅助决策。
- **Manage**（管理）：重新组织信息，比如记忆反思，也支持“遗忘机制”。
- **Optimize**（优化）：通过历史经验优化记忆能力，属于“学习记忆”的过程。

> ⭐ 重点：这些操作是构建记忆模型的基础，部分模型会使用相同操作（如 LTMemoryRecall），部分模型会实现自定义操作（如 MTMemoryStore）。

---

### 3.4. Memory Functions（记忆功能）

MemEngine 提供的底层功能模块，用于支持记忆操作的实现，包括：

- **Encoder**（编码器）：将文本转化为嵌入表示。
- **Retrieval**（检索）：根据语义相关性、重要性、时间等维度，检索最相关信息。
- **Reflector**（反思器）：从现有信息中提取更高层次的洞察。
- **Summarizer**（摘要器）：生成简要摘要，强调重点。
- **Trigger**（触发器）：动态调用工具或函数，如调用 LLM 决定调用哪个函数。
- **Utilization**（使用）：整合不同部分的记忆内容，输出统一信息。
- **Forget**（遗忘）：模拟认知心理学中的“遗忘”机制，适用于模拟类代理。
- **Truncation**（截断）：在 LLM token 限制下处理记忆上下文长度。
- **Judge**（判断）：评估信息的重要性等属性，例如 GAMemory 用它为观察打分。
- **LLM 接口**：方便调用不同大语言模型。

> ⭐ 重点：这些功能模块为记忆操作提供了灵活的组件支持，例如 GAMemoryStore 就调用了 LLMJudge 进行评分。

---

### 3.5. Memory Configurations（记忆配置）

为了便于开发者和研究者快速设置与调整，MemEngine 提供了统一的配置模块：

1. **层级配置**：对应三层记忆结构，支持参数与提示词（prompt）的定制。
2. **默认配置**：提供全面的默认参数与提示，用户仅需修改特定部分。
3. **配置方式**：支持静态（如文件）和动态（如字典）配置。

> ⭐ 重点：配置灵活，降低使用门槛，提升开发效率。

---

### 3.6. Memory Utilities（记忆工具）

MemEngine 提供了若干辅助工具模块，与核心模块松耦合：

- **Storage（存储）模块**：作为数据库，持久化保存记忆内容。
- **Display（显示）模块**：可视化记忆内容。
- **Client（客户端）模块**：通过 FastAPI 远程调用 MemEngine。
- **自动选择器（Auto Selector）**：帮助开发者根据任务自动选择合适模型与参数。

> ⭐ 重点：工具模块增强了 MemEngine 的实用性和部署灵活性。

---

## 总结

MemEngine 是一个模块化、统一化的记忆库，支持多种记忆模型与操作，提供灵活的配置与工具，适用于 LLM 基的智能代理系统开发。其核心优势在于：

- **模块化设计**：底层功能、中层操作、高层模型分离，易于扩展。
- **支持多种记忆模型**：便于研究与对比。
- **接口统一**：提供标准接口，便于模型替换。
- **工具丰富**：提供配置、存储、显示、客户端等辅助模块，提升开发效率。

> 本章节重点介绍了 MemEngine 的整体架构、记忆模型、操作、功能、配置与工具模块，为开发者提供了详尽的使用指导与实现基础。


## 4. Usage of MemEngine



## 4. MemEngine 的使用方式

在本节中，我们描述了如何利用 MemEngine 为基于大语言模型（LLM）的智能体赋予先进的记忆能力。我们将使用方式分为两个方面：（1）使用预实现的记忆模型；（2）定制新的记忆模型。

### 4.1 使用预实现的记忆模型

我们提供了两种主要的部署方式：

- **本地部署**  
  开发者可以通过 pip、conda 或源码在本地 Python 环境中安装本库。之后，他们可以为智能体创建记忆模块，并通过统一的接口在程序中执行记忆操作。

- **远程部署**  
  另一种方式是将库安装在计算服务器上，并通过端口启动服务。随后，开发者可以从轻量设备远程发送 HTTP 请求，通过客户端执行记忆操作。

部署完成后，有三种使用预实现记忆模型的模式：

1. **默认模式**：库中提供了一套完整的超参数和提示词，适用于默认使用场景。
2. **可配置模式**：开发者可以根据自身应用需求，手动调整部分超参数和提示词。
3. **自动模式**：库会根据任务的具体标准，从提供的参数和模型范围内自动选择最适合的记忆模型、超参数和提示词。

此外，我们的库还兼容一些知名工具和框架，例如 vllm 和 AutoGPT。

### 4.2 定制新的记忆模型

我们的库为开发者提供了定制高级记忆模型的支持，并提供完整的文档和示例。定制新的模型主要从三个方面入手：

1. **定制记忆功能**  
   研究人员可能需要在模型中实现新的功能，以扩展已有功能或增加新特性。例如，可以扩展 LLMJudge 功能，设计一个用于检测数据中毒的 BiasJudge。

2. **定制记忆操作**  
   在开发新模型时，定制记忆操作是至关重要的，因为它们构成了详细处理流程的主要管道。例如，可以实现一个新型的记忆回溯操作，结合一系列高级设计和组合的记忆功能。

3. **定制记忆模型**  
   通过将新定制的记忆操作与已有操作相结合，研究人员可以设计出适合其应用场景的多样化记忆模型。

**重点总结**  
- 本节重点介绍了 MemEngine 的两种部署方式（本地和远程）和三种使用模式（默认、可配置、自动），便于开发者灵活使用预实现的记忆模型。
- 在定制新模型方面，强调了三个核心步骤：功能、操作和模型的设计与实现，提供了高度的可扩展性与灵活性。


## 5. Conclusion



### 5. 结论

在本论文中，作者介绍了一个**统一且模块化的库**，用于开发基于大语言模型（LLM）的智能体的**高级记忆功能**。这一工具的设计目标是提高智能体的记忆能力和灵活性，从而更好地适用于复杂任务和应用场景。

**重点内容：**  
- 提出的库具有**统一性**和**模块化**的特点，便于开发者扩展和维护。
- 当前已实现的功能聚焦于文本记忆等基础形式。
- **未来计划**中，作者提到将支持**多模态记忆**（如视觉和音频记忆），旨在进一步丰富和增强智能体的记忆能力，拓展其应用范围。

### 致谢

本研究得到了多个**国家级和高校级科研项目**的支持，包括：
- **国家自然科学基金**（编号62422215和62472427）
- **北京市杰出青年科学家计划**（编号BJJWZYJH012019100020098）
- **中国人民大学的多个科研平台和建设基金**
  
此外，研究还得到了**华为创新研究计划**的支持，特别感谢**MindSpore**、**CANN计算架构**和**昇腾AI处理器**在本研究中的技术支持。

**精简讲解：**  
致谢部分列举了多个支持本研究的基金和平台，体现了本工作的学术价值和产业合作背景。
