# 2503.09149_Memory-enhanced Retrieval Augmentation for Long Video Understanding

* 首页: <https://arxiv.org/abs/2503.09149>
* PDF: <https://arxiv.org/pdf/2503.09149>


Memory-enhanced Retrieval Augmentation for Long Video Understanding



以下是论文章节《Memory-enhanced Retrieval Augmentation for Long Video Understanding》的内容总结：

---

## **Memory-enhanced Retrieval Augmentation for Long Video Understanding**

### **摘要 / 引言**
本章聚焦于**长视频理解**任务中的挑战，尤其是如何在处理长时间视频序列时，有效捕捉和利用视频中的**长期依赖关系**和**关键信息**。作者提出了一种**基于记忆增强的检索增强机制**（Memory-enhanced Retrieval Augmentation），旨在提升模型对长视频中关键帧和上下文信息的建模能力。

---

### **1. 背景与动机**
- **长视频理解的挑战**：传统模型在处理长视频时面临信息遗忘、计算资源限制等问题。
- **现有方法的局限性**：如滑动窗口、注意力机制等方法难以有效保留视频中的关键历史信息。
- **提出问题**：如何在有限资源下，高效地检索并利用视频中的关键片段来辅助当前理解？

---

### **2. 方法概述**
作者提出了一种**结合记忆机制与检索机制的框架**，主要包括以下两个核心模块：

#### **2.1 视频记忆库（Memory Bank）**
- 用于**存储视频中提取的关键帧特征**。
- 通过一个**更新策略**（如FIFO或基于重要性的选择）维护记忆内容，确保保留最具代表性的信息。

#### **2.2 检索增强模块（Retrieval-Augmented Module）**
- 在处理当前视频片段时，从记忆库中**检索相关的历史片段特征**。
- 将检索到的信息与当前输入融合，增强模型对上下文和长期依赖的理解。

---

### **3. 模型结构细节**
- **特征提取器**：使用预训练的视觉模型（如ResNet、TimeSformer）提取视频帧特征。
- **记忆更新机制**：
  - 动态更新记忆库，避免冗余信息。
  - 可采用注意力机制评估帧的重要性。
- **检索机制**：
  - 使用相似度匹配（如点积或余弦相似度）从记忆中检索最相关的帧。
- **融合策略**：
  - 将检索结果与当前帧特征进行拼接或加权融合，送入后续的推理模块。

---

### **4. 实验与结果**
- **数据集**：在多个长视频理解任务数据集上进行验证，如TVQA、Charades等。
- **评估指标**：准确率、召回率、mAP等。
- **主要结果**：
  - 提出的方法在多个任务上优于现有模型。
  - 检索增强机制显著提升了模型对长期依赖的建模能力。
  - 记忆机制有效缓解了信息遗忘问题，同时控制了计算开销。

---

### **5. 消融实验与分析**
- 对记忆库大小、更新策略、检索方式等进行了详细分析。
- 结果表明：
  - 合理的记忆容量和更新策略对性能有显著影响。
  - 检索增强模块在长视频中比在短视频中提升更明显。

---

### **6. 结论**
本章提出了一种**结合记忆机制与检索机制的新方法**，用于提升长视频理解任务的性能。该方法在保持计算效率的同时，有效增强了模型对长期上下文的建模能力，为后续研究提供了新的思路。

---

### **亮点总结**
- ✅ **创新点**：首次将记忆机制与检索机制结合用于长视频理解。
- ✅ **实用性**：适用于资源受限场景下的视频处理。
- ✅ **有效性**：在多个任务上验证了方法的优越性。

--- 

如需进一步了解具体实现细节或实验配置，可继续提问。


## Abstract



## 摘要（Abstract）总结：

本节介绍了MemVid这一新型的基于记忆增强的检索增强生成（RAG）方法，旨在解决长视频理解（LVU）中信息丢失的问题。当前的长上下文视觉-语言模型（LVLMs）由于采用压缩或直接下采样策略，导致信息损失；虽然检索增强生成（RAG）方法在一定程度上缓解了这一问题，但其依赖显式查询，适用性受限。

MemVid受人类认知记忆机制启发，提出四步处理流程：
1. **整体记忆视频信息**；
2. **基于记忆推理任务所需信息**；
3. **根据信息需求检索关键帧或片段**；
4. **聚焦于检索结果生成最终答案**。

为了提升模型基于记忆的推理能力并实现端到端最优性能，作者还提出了一种**课程学习策略**：先通过监督学习训练模型理解已标注的推理结果，再逐步通过强化学习探索和强化更合理的推理路径。

实验结果显示，MemVid在多个主流LVU基准数据集（如MLVU、VideoMME、LVBench）上均表现出优于现有LVLMs和RAG方法的效率与效果。

---

**重点内容强调**：
- MemVid的核心创新在于结合**记忆机制**与**检索增强生成**，避免了传统方法的信息丢失问题。
- 提出的**课程学习策略**（监督学习 + 强化学习）显著提升了模型的推理能力和整体性能。
- 实验结果验证了MemVid在多个LVU任务上的**优越性**。

**非重点内容简要说明**：
- 图1展示了MemVid与其他方法的框架对比，强调其通过整体记忆生成有用线索，从而更准确地定位细节进行问答。


## 1. Introduction



## 1. 引言（Introduction）总结

本节介绍了**长视频理解**（Long-video understanding）在现实应用中的重要性，如视频分析、自动驾驶和具身智能（embodied AI），并指出当前主流视觉-语言模型（VLMs）在处理长视频时的局限性。传统模型如 [InternVL]、[Qwen2VL]、[Bai2023QwenVL] 主要针对图像或短视频设计，难以有效处理长序列视频。虽然一些扩展模型（如 [LongVA]、[LongVILA]、[Shu2024VideoXL]）尝试扩大上下文窗口，但仍面临**信息丢失**和**计算成本高**的问题。

接着，文章讨论了**检索增强生成**（RAG）在长视频理解中的潜力与局限。标准RAG方法在处理明确问题（如“橘猫什么时候躺在主人怀里？”）时表现良好，但在处理**隐含复杂信息**的问题（如“第五分钟时，为什么橘猫会躺在主人怀里？”）时效果不佳。这类问题需要模型具备**推理能力**，识别事件之间的隐含关系，而不仅仅是直接检索关键词。

文章通过图示对比了**人类处理长视频理解的方式**：人类会先通看视频形成整体记忆，在回答问题时进行推理并有选择地检索关键片段。这种结构化流程启发作者提出了一种新的RAG框架——**MemVid**（Memory-enhanced retrieval augmentation for long Video understanding）。

MemVid 的核心流程包括四个步骤：
1. 生成视频整体信息的记忆；
2. 基于记忆进行问题推理；
3. 检索关键视频片段；
4. 基于检索结果生成答案。

该流程由三个模块驱动：**记忆模块**（memorizer）、**检索模块**（retriever）和**生成模块**（generator）。作者重点优化**记忆模块**，并引入**课程学习框架**（curriculum learning）以提升其推理能力。训练过程分为两个阶段：
- 第一阶段：使用监督学习，基于高质量标注数据训练记忆模块生成结构化推理输出；
- 第二阶段：探索多种推理路径，强化能生成高质量答案的推理轨迹。

最后，作者在多个长视频理解基准（如 VideoMME、MLVU、LVBench）上进行了大量实验，验证了 MemVid 的有效性。实验结果显示，MemVid 在性能上优于现有 RAG 方法，并在成本效益方面显著优于主流长视频 VLMs。

### 贡献总结：
1. 提出首个结合**记忆与推理**的 RAG 框架 MemVid，专为长视频理解设计；
2. 设计基于**课程学习**的训练框架，提升记忆模块的推理能力；
3. 在多个基准上验证了 MemVid 的**高性能与高成本效益**。


## 2. Related Work



以下是对你提供的论文“Related Work”章节的总结，按照原文结构进行组织，重点内容进行了详细讲解，次要内容进行了精简。

---

## 2. Related Work

### 2.1. 大型视觉-语言模型（Large Vision-language Models）

本节回顾了当前大型视觉-语言模型（LVLMs）的发展情况。随着大语言模型（LLMs）的突破，多模态人工智能系统的研究迅速发展，多个代表性工作（如 InternVL、QwenLM、LLaVA、MiniGPT-4）在架构设计和训练方法上进行了探索。

- **InternVL** 是一个具有代表性的模型，其视觉模型参数达到60亿，并通过大规模多模态预训练实现了与LLMs的良好对齐。
- **QwenVL** 在 QwenLM 的基础上引入了专用视觉编码器、统一的输入输出接口、三阶段训练策略以及多语言多模态数据集，提升了模型能力。
- 随着研究的深入，视觉-语言模型的应用从静态图像扩展到视频理解。当前主流方法是将视频帧编码为视觉表示，再转换为LLMs可处理的token形式。
  - **ST-LLM** 提出直接输入原始时空token的方法，有效建模视频序列。
  - **VideoChat** 和其升级版 **VideoChat2** 通过可训练的神经适配器提升时间推理能力，并通过多场景指令微调进一步优化。
  - 早期模型多采用 **BLIP-2 的 Q-Former** 进行特征融合，但近期更倾向于简化设计。
    - **VideoLLaVA** 和 **MiniGPT4-Video** 使用线性变换将视觉特征映射到语言空间。
    - **Video-ChatGPT** 和 **Valley** 则采用token池化机制提取关键视觉信息。
- 数据质量的重要性日益凸显：
  - **LLaVA-Video** 强调高质量合成数据对视频指令理解的显著提升。
  - **Qwen2VL** 通过大规模训练数据和自适应分辨率技术实现了跨任务的广泛适用性。

**挑战**：当前多模态系统在处理长视频时受限于上下文长度（通常最多128帧），这是研究与应用之间的重要瓶颈。

---

### 2.2. 长视频视觉-语言模型（Long Large Vision-language Models）

本节聚焦于处理长视频的视觉-语言模型。长视频通常包含大量冗余信息，只有部分片段对任务有意义。现有方法主要通过**记忆机制**或**压缩模块**来应对这一挑战。

- **MovieChat** 和 **MA-LMM** 使用记忆库存储历史视觉特征，并通过整合策略进行压缩。
- **LLAMA-VID** 采用上下文注意力模块，将每帧压缩为仅两个token，增强长程理解能力。
- **LongVLM** 引入token合并模块，融合局部与全局特征。
- **Video-CCAM** 使用带有因果交叉注意力掩码的跨注意力机制。
- 一些模型（如 **LongVA**、**LongVILA**、**LongLLaVA**）尝试直接增强模型对长视频的理解能力。

**问题**：尽管这些方法扩展了视频处理长度，但往往在冗余处理和计算复杂度（通常是二次增长）之间做出妥协，效果并不理想。

---

### 2.3. 基于检索增强的视频理解（Retrieval-augmented Video Understanding）

本节介绍了**检索增强生成**（RAG）在视频理解中的应用。RAG是一种在文本领域广泛应用的技术，通过检索器获取相关信息以辅助生成，特别适合处理长视频中的冗余数据。

- **DrVideo** 和 **Goldfish** 分别对关键帧和视频片段生成文本描述，并基于这些描述构建RAG框架。
  - **问题**：文本描述与原始视频内容之间存在语义差距，影响检索效果。
- 由于缺乏上下文和深层语义信息，直接使用查询检索视频内容效果不佳。
- 为解决这一问题，作者提出了一种**生成式上下文感知查询扩展机制**，直接作用于视频内容。
  - **优势**：
    - 提升检索性能；
    - 在有限输入长度内聚焦关键内容；
    - 降低计算成本。

---

整体来看，本章系统梳理了视觉-语言模型的发展脉络，从基础模型到视频理解，再到长视频处理与检索增强机制，层层递进，为后续提出的方法（MemVid）提供了充分的背景支持。


## 3. Methodology



### 3. Methodology

#### 3.1. Overview of MemVid
**重点内容：**
MemVid 是一种用于长视频理解的增强型框架，旨在解决传统方法（如稀疏采样和标准RAG）在处理长视频时的信息丢失和检索不足问题。其核心思想是通过构建一个全局视频记忆模块，模拟人类认知过程，引导上下文感知的检索。

**结构与流程：**
1. **Memorizing（记忆）**：通过记忆模型将视频压缩为全局记忆 ℳ，保留整体理解。
2. **Reasoning（推理）**：基于问题 Q 和全局记忆 ℳ 推理出任务导向的检索线索 𝒞。
3. **Retrieving（检索）**：根据线索 𝒞 从视频中检索相关片段，并聚合结果。
4. **Focusing（聚焦）**：基于检索到的片段生成最终答案。

**优势：**
相比传统方法，MemVid 通过记忆和推理机制提升了检索的准确性和上下文理解能力，从而提高长视频问答的准确性。

---

#### 3.2. Reasoning-oriented Memory Module
**重点内容：**
设计了一个基于键值缓存（KV Cache）的记忆模块，具备以下特性：
- 支持对视频的全局理解
- 支持推理过程
- 灵活生成检索线索

**实现方式：**
- 使用预训练视觉编码器将视频压缩为特征 F
- 利用因果Transformer语言模型将特征转换为推理导向的KV缓存
- 当问题 Q 到达时，结合记忆 ℳ 和问题嵌入推理出检索线索 𝒞

**关键公式：**
- 视觉特征提取：F = Ev(𝒱′)
- KV缓存更新：K ← Concat(K, Kt), V ← Concat(V, Vt)
- 线索生成：𝒞 = ℛ(Concat(ℳ; Eq(Q)))

**优势：**
该模块通过动态推理线索，提升了检索的针对性和上下文适应性。

---

#### 3.3. Clue-guided Retrieval
**主要内容：**
- 将长视频划分为非重叠的固定时长片段作为候选检索池
- 使用预训练视频检索器为每个线索和片段生成文本嵌入
- 通过余弦相似度排序，选取 top-k 片段
- 结合局部（检索片段）和全局（全视频）采样策略，提升上下文覆盖

**精简说明：**
该部分描述了如何基于推理线索进行高效检索，并通过采样策略平衡局部与全局信息。

---

#### 3.4. Curriculum Learning Framework
**重点内容：**
提出了一种课程学习框架，用于优化记忆模块（Memorizer），解决其训练中缺乏监督信号的问题。

**两个阶段：**
1. **监督微调预热（Supervised Fine-Tuning Warmup）**
   - 使用强大的教师模型生成高质量推理线索
   - 通过 next-token prediction 优化记忆模块
   - 损失函数：ℒ_NTP(θ) = -∑ log Pθ(w_t | w_<t)

2. **基于生成反馈的强化学习（Reinforcement Learning with Generation Feedback）**
   - 使用 DPO（Direct Preference Optimization）优化线索质量
   - 构建偏好对 (yi+, yi−)，基于生成器的正确性进行优化
   - 损失函数：ℒ_dpo = -∑ log σ(β·[log πθ(y+)/πS(y+) - log πθ(y−)/πS(y−)])

**优势：**
双阶段训练策略提升了模型的泛化能力和端到端性能。

---

### 总结
MemVid 通过引入记忆增强机制，结合推理、检索与生成，有效解决了长视频理解中的信息丢失和检索不准确问题。其核心创新包括：
- 基于KV缓存的记忆模块，支持全局理解与动态推理
- 线索引导的检索机制，提升检索相关性
- 课程学习框架，优化记忆模块的训练过程

该方法在多个视频理解基准上取得了优于现有模型的性能表现。


## 4. Experiments



## 4. 实验总结

### 4.1. 实验设置

#### 4.1.1. 基准与评估指标
本研究在三个具有不同特征的长视频基准上进行了全面实验，以提供多维度评估：

- **VideoMME**：包含2700个专家整理的问题，对应900个不同长度的视频（短、中、长），提供有字幕和无字幕两个版本。
- **MLVU**：涵盖3分钟到2小时的视频，包含9个任务（如动作识别、事件定位、计数等），评估全局和局部理解能力。
- **LVBench**：专为超长视频设计，平均时长4101秒，任务包括关键信息检索、事件理解、时间定位等。

这三个基准共同构建了全面的评估框架：VideoMME 提供不同视频长度的细粒度分析，MLVU 测试多样任务能力，LVBench 专注于超长视频处理。

#### 4.1.2. 基线模型
比较了三类基线模型：

1. **闭源模型**：如 GPT-4V、GPT-4o、Gemini-1.5-Pro，性能强但架构不可比。
2. **开源视觉语言模型（VLMs）**：包括通用模型和长上下文模型（如 LongVA、LongVILA、VideoCCAM 等），扩展了传统VLM的上下文长度。
3. **基于RAG的VLMs**：如 Goldfish、SALOVA-Qwen、Video-RAG，通过检索关键片段或文本增强理解。其中，RAGsimple 作为无记忆模块的对照模型。

#### 4.1.3. 实现细节
- 使用 **LanguageBind-Large** 进行10秒片段划分，生成4个查询相关线索和草稿答案。
- 输入截断为128帧，α=0.6，与 Qwen2VL-7B 对齐。
- 训练数据来自 TVQA-Long、NExT-QA、ActivityNet-QA，生成10,000条合成线索和答案。
- DPO训练使用 CinePile 数据集，筛选1000对高质量训练样本。
- 实验在单节点8×A800 GPU上运行。

---

### 4.2. 总体结果

MemVid 在三个长视频基准上均表现优异，尤其在7B模型中达到SOTA：

1. **优于传统下采样方法**：相比 Qwen2VL，在MLVU上提升+8.7%，在VideoMME（长）上提升+6.2%（无字幕）和+3.7%（有字幕）。
2. **优于长视频VLMs**：尽管输入帧数更少，MemVid 在VideoMME上超越 Video-XL +8.2%（无字幕）和+4.7%（有字幕）。
3. **优于闭源模型**：在LVBench上，MemVid 以30倍更少帧数超越 Gemini 1.5 Pro，提升11.3%。
4. **优于RAG方法**：相比 RAGsimple，在MLVU和VideoMME上分别提升+5.8%、+3.4%、+3.0%。

---

### 4.3. 消融实验

评估MemVid各模块的有效性：

- **无推理模块（MemVid w/o reasoning）**：性能下降2.4%（MLVU）、1.6%/0.4%（VideoMME）。
- **无记忆模块（MemVid w/o memory）**：性能下降1.9%（MLVU）、1.2%/0.3%（VideoMME）。
- **训练策略**：零样本 → SFT → DPO，逐步提升约3%。

结果验证了记忆增强机制和课程学习框架的有效性。

---

### 4.4. 泛化性分析

#### 4.4.1. 任务性能
MemVid 在NeedleQA、Count、Order、TutorialQA等多个任务上均优于 RAGsimple，尤其在Count（+8%）、TutorialQA（+6%）、Order（+3%）上表现突出，即使在训练数据未覆盖的任务上也具备良好泛化能力。

#### 4.4.2. 帧数分析
MemVid 在16~128帧范围内均优于均匀采样和标准检索策略，且帧数越多优势越明显（如128帧提升2.9%）。

#### 4.4.3. 不同下游架构
MemVid 可适配不同规模和架构的VLM（3B~72B），在VILA-1.5（+10.1%）、LongVA（+3.5%）、Qwen2VL（+2.4%）上均提升性能，尤其在小模型和低帧数下效果显著。

---

### 4.5. 效率分析

MemVid 在效率上显著优于长视频VLM（如VideoXL）：

- **帧数减少93.8%**（1024→64）
- **延迟降低35.1%**
- **内存减少34.9%**
- **性能提升17.3%**

表明MemVid能高效提取关键帧，兼顾性能与成本。

---

### 4.6. 案例分析

图示案例显示，MemVid 能通过上下文推理生成细粒度线索，准确检索关键片段（如家庭关系、教堂哀悼），从而生成正确答案，而 RAGsimple 无法捕捉关键信息。

---

## 总结

MemVid 通过**记忆增强的检索机制**，在多个长视频基准上实现SOTA，具备以下优势：

- **高效提取关键帧**，减少输入帧数和计算成本。
- **强泛化能力**，适配不同任务、模型架构和帧数限制。
- **优于闭源模型和RAG方法**，尤其在复杂任务和低资源条件下表现突出。
- **模块化设计**，可灵活集成到不同VLM中。

该方法为长视频理解提供了高效、通用的解决方案。


## 5. Conclusion



## 5. 结论

本节总结了论文的核心贡献与实验成果。作者提出了一种基于RAG的新型长视频理解方法——MemVid，其设计灵感来源于人类的认知记忆机制。MemVid的工作流程包含四个关键步骤：**整体视频信息的记忆、任务相关信息的推理、关键帧的检索以及聚焦生成最终答案**。为了进一步提升模型在记忆引导下的推理能力并优化整体表现，作者还引入了**课程学习策略**。

在实验部分，作者在多个LVU基准数据集（如MLVU、VideoMME、LVBench）上进行了大量测试。结果表明，MemVid在性能上**显著优于现有的RAG方法和主流的LVU模型**，验证了该方法的有效性和优越性。

> **重点内容强调**：  
- MemVid是首个将RAG与记忆机制结合用于长视频理解的工作。  
- 提出的四步流程有效提升了模型对长视频中关键信息的捕捉与推理能力。  
- 课程学习策略有助于模型逐步学习复杂任务，提升整体表现。  
- 实验结果显示MemVid在多个权威LVU任务中具有领先优势。
