# 2506.06326_MemoryOS: Memory OS of AI Agent

* 首页: <https://arxiv.org/abs/2506.06326>
* PDF: <https://arxiv.org/pdf/2506.06326>


## Abstract


本论文指出，**大语言模型（LLMs）面临两个关键挑战：固定的上下文窗口限制和内存管理能力不足**，这导致其**长期记忆能力薄弱**，并在与AI代理交互时**个性化体验受限**。

为了解决这些问题，作者**创新性地提出了一种“记忆操作系统”——MemoryOS**，旨在为AI代理提供**全面且高效的内存管理机制**。该系统**借鉴操作系统中的内存管理机制**，设计了一个**分层的存储架构**，并包含四个核心模块：**记忆存储、更新、检索和生成**。

具体来说，MemoryOS的架构分为三个层级：
- **短期记忆**
- **中期记忆**
- **长期个性化记忆**

关键操作包括：
- **短期到中期记忆的更新**采用基于对话链的FIFO（先进先出）机制
- **中期到长期记忆的更新**采用分段页式组织策略

实验结果显示，MemoryOS在**LoCoMo基准测试中表现出色**，相比基线模型，在GPT-4o-mini上的F1得分平均提升了**49.11%**，BLEU-1得分提升了**46.18%**，表明其在**长对话中具有更强的上下文连贯性和个性化记忆保持能力**。

此外，该系统已**开源实现**，代码可在GitHub上获取。


## 1 Introduction

### 核心问题
本节指出，尽管大语言模型（LLMs）在文本理解和生成方面表现出色，但由于其依赖**固定长度的上下文窗口**进行记忆管理，因此在**维持对话连贯性**方面存在固有局限。这种设计在对话存在时间间隔或长期交互时，容易导致记忆断裂，表现为**事实不一致**和**个性化程度下降**。

在需要**长期用户适应**、**跨会话知识保留**或**稳定角色表示**的场景中，这一问题尤为突出，成为当前研究领域的一个重大挑战。

---

### 当前解决方案分类
作者将当前LLMs中的记忆机制分为三类：

1. **知识组织方法（Knowledge-organization methods）**  
   通过将记忆结构化为语义网络或笔记形式，实现灵活的存储与检索。例如A-Mem结构。

2. **检索机制导向方法（Retrieval mechanism-oriented approaches）**  
   强调通过语义检索和遗忘机制实现长期记忆更新，如MemoryBank。

3. **架构驱动方法（Architecture-driven methods）**  
   采用分层结构与显式读写操作管理上下文，例如MemGPT。

这些方法通常**各自为政**，分别聚焦于存储结构、检索机制或更新策略等单一维度，**缺乏统一的记忆操作系统**来实现系统性、综合性的记忆管理。

---

### 本文贡献与方法
受操作系统内存管理机制的启发，作者提出了一种**统一的记忆操作系统**——**MemoryOS**，这是该领域的首次系统性尝试。

#### MemoryOS 的核心模块（如图1所示）
1. **记忆存储（Memory Storage）**  
   将信息分为短期、中期和长期三个层级，形成**三层次分层记忆架构**。
   
2. **记忆更新（Memory Updating）**  
   采用基于对话链和热度机制的**分页更新策略**，动态刷新记忆内容。

3. **记忆检索（Memory Retrieval）**  
   利用语义分割技术，从不同层级中高效检索相关信息。

4. **响应生成（Response Generation）**  
   整合检索到的记忆信息，生成**连贯且个性化**的响应。

这四个模块协同工作，构建了一个**统一的记忆管理框架**，实现了对长期对话记忆的全面管理。

---

### 主要贡献总结
1. 提出首个面向AI代理的记忆操作系统MemoryOS，解决长期对话中的连贯性与角色一致性问题。
2. 设计了**三层次记忆架构**，并整合四大核心模块，实现对用户偏好的动态捕捉与演化。
3. 通过大量实验验证了MemoryOS在多个基准数据集上的有效性与高效性，特别是在处理**长对话交互**方面表现优异。

--- 

### 总结
本节从LLMs在长期对话中的记忆局限出发，分析了现有方法的不足，并提出MemoryOS这一系统性解决方案。其创新性在于借鉴操作系统内存管理思想，构建了一个**统一、分层、动态、可检索**的记忆管理框架，具有重要的理论与应用价值。


## 2 Related Work


### 2.1 LLM 智能体的记忆机制

当前的大语言模型（LLMs）在处理需要长期连贯性的复杂任务时面临挑战，主要原因是其固定长度的设计难以维持长时间对话的连续性，导致记忆碎片化、事实不一致和个性化能力下降。

为解决这些问题，LLM 的记忆系统研究主要分为三类：

#### 1. 知识组织方法（Knowledge-organization）
这类方法侧重于捕捉和结构化模型的中间推理状态，以提升记忆一致性：
- **Think-in-Memory (TiM)**：通过持续更新思维链（chains-of-thought）来保持推理连贯。
- **A-Mem**：构建跨会话的知识网络，增强记忆的关联性。
- **Grounded Memory**：结合视觉语言模型与知识图谱，实现上下文感知的推理，适用于智能助手。

#### 2. 检索机制导向方法（Retrieval mechanism–oriented）
这类方法通过引入外部记忆库来增强模型的记忆能力：
- **MemoryBank**：使用向量数据库存储对话和用户特征，并基于遗忘曲线进行更新。
- **AI-town**：以自然语言形式保存记忆，并加入“反思”机制过滤相关信息。
- **EmotionalRAG**：结合语义相似度与智能体当前情绪状态进行记忆检索，提升情感适配性。

#### 3. 架构驱动方法（Architecture-driven）
这类方法通过修改模型架构来更有效地管理上下文：
- **MemGPT**：采用类似操作系统的内存管理机制，支持显式的读写操作。
- **Self-Controlled Memory (SCM)**：引入双缓冲机制和记忆控制器，实现有选择性的记忆召回。

> **重点总结**：三类方法各有侧重，知识组织和架构驱动更注重模型内部结构优化，而检索机制导向则依赖外部记忆库。这些方法共同目标是提升长期记忆一致性、个性化和上下文管理能力。

---

### 2.2 操作系统中的内存管理

操作系统中广泛采用**段页式内存管理**，以在逻辑结构和物理内存利用之间取得平衡。

#### 经典方法：
- **Multics系统**：将内存划分为段，段内再分页，支持高效管理、保护和共享。
- **段信息**（如大小、访问权限）可防止外部碎片。
- **分页机制**可减少内部碎片。

#### 高级策略：
- 使用基于优先级的淘汰策略（如 LRU、工作集模型）保留热点数据。
- **Zheng 等人** 的研究表明，粗粒度分段与细粒度分页结合，可降低多核处理器的内存管理开销。

#### 与 MemoryOS 的联系：
MemoryOS 受操作系统启发，将记忆划分为逻辑段（如对话主题），段内再细分“页”，并采用热度机制管理记忆内容，保留重要信息、淘汰低频信息，从而提升上下文管理与个性化能力。

> **重点总结**：操作系统内存管理的段页式结构和热度优先策略，为 MemoryOS 提供了设计灵感，使其在记忆组织和管理上更高效、灵活。


## 3 MemoryOS

### 3.1 概览架构
MemoryOS 是一个面向 AI 智能体的综合记忆管理系统，旨在通过动态更新记忆和检索语义相关上下文，确保在长时间对话中实现连贯且个性化的交互。

其架构由四个模块组成：
- **记忆存储**：采用三级结构：短时记忆（STM）、中时记忆（MTM）和长期个人记忆（LPM），分别用于存储实时对话、重复话题摘要和用户/智能体偏好。
- **记忆更新**：包括 STM 到 MTM 的 FIFO 更新机制，以及 MTM 到 LPM 的基于热度的分段更新策略。
- **记忆检索**：采用两阶段检索机制，先通过语义匹配找到相关 MTM 段落，再结合 LPM 的个性信息和 STM 的上下文信息。
- **响应生成**：整合 STM、MTM 和 LPM 中的检索结果，生成连贯且个性化的回复。

### 3.2 记忆存储模块
该模块采用三级结构实现记忆的组织与存储：

#### 短时记忆（STM）
- 以“对话页”为单位存储实时对话数据，包括用户问题、模型回复和时间戳。
- 每个对话页包含上下文链（dialogue chain），用于维护连续对话中的上下文一致性。
- 通过 LLM 生成元信息，评估当前页与历史页的语义连续性，并生成摘要。

#### 中时记忆（MTM）
- 采用**分段分页**结构，将相同主题的对话页归为一个段落。
- 段落的相似性通过 ℱscore 评估，结合语义向量相似度（cosine）和关键词 Jaccard 相似度。
- 页面与段落的相似度超过阈值 θ 时，合并到该段落中，确保语义一致性。

#### 长期个人记忆（LPM）
- 包含用户画像（User Persona）和智能体画像（Agent Persona）。
  - **用户画像**：包括静态属性（性别、姓名、出生年份）、知识库（User KB）和动态特征（User Traits）。
  - **智能体画像**：包括角色设定（Agent Profile）和交互中形成的动态属性（Agent Traits）。

### 3.3 记忆更新模块
负责动态更新记忆内容，包括：

#### STM 到 MTM 更新
- 使用 FIFO 策略，当 STM 队列满时，将最早对话页迁移到 MTM。

#### MTM 到 LPM 更新
- **热度评分 Heat**：由访问次数（Nvisit）、对话页数量（Linteraction）和时间衰减因子（Rrecency）组成。
- 当 MTM 段落数超过容量时，淘汰热度最低的段落。
- 热度超过阈值 τ 的段落迁移到 LPM，用于更新用户特征、知识库和智能体特征。
- 更新后，段落的 Linteraction 重置为 0，降低其热度，避免重复更新。

### 3.4 记忆检索模块
负责从 STM、MTM 和 LPM 中检索相关信息，支持响应生成：

#### STM 检索
- 检索所有对话页，因为 STM 存储当前对话的最新上下文。

#### MTM 检索
- **两阶段检索**：
  1. 使用 ℱscore 选择 top-m 个相关段落；
  2. 在这些段落中选择 top-k 个最相关的对话页。
- 检索后更新段落的访问次数和时间衰减因子。

#### LPM 检索
- 用户知识库（User KB）和智能体特征（Agent Traits）各检索 top-10 条最相关条目。
- 用户画像、智能体画像和用户特征信息全部使用，因其存储了偏好和个性信息。

### 3.5 响应生成模块
- 将用户查询与从 STM、MTM 和 LPM 检索到的信息整合为最终提示（prompt），输入 LLM 生成回复。
- 结合：
  - STM 的近期上下文；
  - MTM 的历史对话页和摘要；
  - LPM 的个性化信息；
- 确保回复：
  - 上下文连贯；
  - 内容有深度；
  - 与用户和智能体身份一致；
- 实现连贯、准确、个性化的交互体验。


## 4 Experiments


### 4.1 实验设置

#### 数据集
实验基于两个数据集：GVD 和 LoCoMo。

- **GVD 数据集**：包含15个虚拟用户与助手在10天内的多轮对话，每天至少涉及两个话题。
- **LoCoMo 数据集**：用于评估长期对话记忆能力，对话平均有300轮、约9000个token。问题分为四类：单跳、多跳、时间相关和开放域，用于系统评估大语言模型（LLM）的记忆能力。

#### 评估指标
- **GVD**：使用 Memory Retrieval Accuracy（准确率）、Response Correctness（正确性）、Contextual Coherence（连贯性），由 DeepSeek-R1 自动评分。
- **LoCoMo**：采用 F1 和 BLEU-1 指标评估模型表现。

#### 对比方法
与以下代表性记忆方法进行对比：

- **TiM**：通过存储推理结果而非原始对话，使用 LSH 检索上下文，通过反思更新记忆。
- **MemoryBank**：基于艾宾浩斯遗忘曲线动态调整记忆强度，构建用户画像。
- **MemGPT**：采用双层记忆结构，主上下文用于快速访问，外部上下文用于长期存储。
- **A-Mem**：动态生成结构化笔记，构建知识网络，实现记忆演化。
- **MemoryOS（本文方法）**：综合记忆管理框架，包含记忆存储、更新、检索和生成四个核心模块。

#### 实验结果表格
- **表1（GVD 数据集）**：MemoryOS 在 Acc、Corr、Cohe 三项指标上均优于其他方法，分别提升3.2%、5.4%、1.0%。
- **表2（LoCoMo 数据集）**：MemoryOS 在 F1 和 BLEU-1 指标上显著优于其他方法，平均提升达 32.35%~125.61%。
- **表3（效率分析）**：MemoryOS 在 LLM 调用次数和 token 消耗上优于 MemGPT 和 A-Mem，效率更高。

#### 实现细节
- 使用 8-H20 GPU 进行实验。
- STM（短期记忆）队列长度为7，MTM（中期记忆）段最大长度为200。
- 用户知识库和代理特征最大容量为100条。
- 热度阈值 τ=5，α=β=γ=1。
- 检索参数：top-m=5，top-k=5（GVD）、10（LoCoMo）。
- 相似度阈值 θ=0.6，时间常数 μ=1e+7。

---

### 4.2 主要结果

实验结果见表1和表2，主要观察如下：

1. **MemoryBank 表现最差**，说明仅靠遗忘机制不足以有效管理记忆；TiM 通过存储“思考”减少重复推理，但其单阶段哈希检索无法保留跨主题依赖。

2. **A-Mem 和 MemGPT 表现较强**，但缺乏系统性记忆管理机制：
   - MemGPT 的扁平 FIFO 队列导致长对话中主题混杂；
   - A-Mem 的图结构虽增强语义，但多步链接生成导致延迟和误差累积。
   - **MemoryOS** 通过分层 STM/MTM/LPM 架构、分段分页、热度淘汰机制和用户画像模块，有效保持主题一致性与用户偏好。

3. **MemoryOS 表现最优**：
   - 在 LoCoMo 上 F1 和 BLEU-1 平均提升 49.11% 和 46.18%；
   - 在 GVD 上仍比 A-Mem 提升 3.2%，显示其在复杂长上下文任务中的鲁棒性。

4. **效率优势显著**：
   - LLM 调用次数远低于 A-Mem（4.9 vs 13）；
   - token 消耗远低于 MemGPT（3874 vs 16977）。

#### 图3：超参数分析
- 设置不同 top-k 值（5,10,20,30,40）分析对 LoCoMo 性能影响。
- 性能随 k 增加提升，但超过阈值后效果下降，最终选择 k=10 以平衡性能与计算开销。

#### 图4：案例研究
- 展示 MemoryOS 在记忆用户长期偏好和对话细节上的优势。
- 例如：回忆“湿地公园看风景、跑步、看到松鼠”等信息，并在用户想吃汉堡时提醒“别忘了你想变瘦”。

---

### 4.3 消融实验

评估 MemoryOS 各模块贡献，分别移除 MTM、LPM、Chain 和整个 MemoryOS：

- 移除 MemoryOS 后性能大幅下降，说明记忆系统对长对话至关重要。
- **MTM 影响最大**，其次是 LPM，Chain 影响最小。

---

### 4.4 超参数分析

分析从 MTM 中检索的 top-k 对话页数对性能的影响：

- 随着 k 增加，性能提升，但超过一定值后效果下降。
- 设置 k=10 以在性能与计算开销之间取得平衡。

---

### 4.5 案例研究

通过对比默认 LLM 和引入 MemoryOS 的 LLM 回应，展示 MemoryOS 在记忆用户长期偏好和历史对话中的优势：

- 能准确回忆用户几周前提到的“去湿地公园”的细节；
- 结合 MTM 的分段分页与对话链机制，实现跨轮次信息检索；
- 用户画像模块可记住“想变瘦”的目标，并在后续对话中主动提醒，提升对话连贯性和用户体验。


## 5 Conclusion

本节总结了 MemoryOS 这一创新性 AI 代理记忆管理系统的设计理念与核心贡献。

- **核心创新**：受操作系统内存管理机制的启发，作者首次提出了名为 **MemoryOS** 的 AI 记忆管理系统。该系统采用**分层记忆存储架构**，有效解决了传统对话系统中固定上下文窗口带来的限制。

- **关键技术**：  
  - 引入类似操作系统的**段页式存储机制**，对对话历史进行高效存储与更新。  
  - 通过**热度驱动的淘汰策略（heat-driven eviction）**，动态识别并保留关键信息，实现跨层级记忆的语义化检索与管理。

- **个性化支持**：系统集成了**人格模块（persona module）**，能够通过提取用户的个性化特征，捕捉其偏好变化，从而确保对话内容在长时间交互中保持一致性与个性化。

- **应用价值**：MemoryOS 将操作系统的设计理念与 AI 记忆管理相结合，使大语言模型（LLMs）能够在长期交互中维持连贯、个性化的对话体验，显著提升了 AI 在真实应用场景中的人机对话能力。

总结来说，这一章节强调了 MemoryOS 在突破传统对话系统限制方面的开创性作用，并展望了其在提升 AI 对话智能方面的潜力。



