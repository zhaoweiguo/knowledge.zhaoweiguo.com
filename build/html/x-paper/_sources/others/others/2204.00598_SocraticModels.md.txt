# 2204.00598_SocraticModels: Composing Zero-Shot Multimodal Reasoning with Language

* 首页: <https://arxiv.org/abs/2204.00598>
* PDF: <https://arxiv.org/pdf/2204.00598>
* 引用: 637(2025-08-22)
* 组织: Google
* 官网: <https://socraticmodels.github.io/>

## 总结

**苏格拉底模型（Socratic Models, SMs）**
* 目标：
    * 旨在通过语言（如通过提示）组合多个大型预训练模型，以执行新的下游多模态任务，而无需进一步训练
* 本质:
    * 是一个计算图，
    * 其中每个节点是一个模块（如语言模型 LM、视觉语言模型 VLM、音频语言模型 ALM 或外部 API），
    * 边表示模块之间通过语言进行通信
* 核心
    * 将外部模块（如VLM、Web Search、机器人策略）整合到SMs体系中，通过语言协调多模态信息处理。
* 核心思想
    * 是在不进行任何微调（fine-tuning）的情况下，通过巧妙的“组合”与“对话”，让多个预先训练好的、功能各异的大型基础模型（Foundational Models）协同工作，实现强大的多模态推理能力。
* 核心问题与动机
    * 近年来，AI领域出现了多种在庞大数据上训练的“基础模型”，例如：
        * 大语言模型 (LLMs)：如GPT系列，精通文本、代码、逻辑推理，但“看不见”图像和声音。
        * 视觉-语言模型 (VLMs)：如CLIP，理解图像与文本的关联，但不擅长复杂的逻辑推理或获取特定领域的知识。
    * 这些模型各自在特定领域内存储了海量的“常识”，但它们的“知识域”几乎不重叠
        * 传统的做法是收集新的数据，将这些模型微调成一个更大的多模态模型，但这过程成本高昂
* 灵感来源
    * 来源于古希腊哲学家苏格拉底的“问答法”（Socratic method）——通过提问和对话来激发批判性思维并推导出结论。
* 苏格拉底模型框架
    * 将一个大型多模态任务分解成多个子任务，让不同的专家模型（如VLM、LLM、音频模型）像在举行一场“圆桌会议”一样，通过“语言”这一通用接口进行相互“提问”和“回答”，最终协作解决一个它们单独都无法完成的任务。 
    * 整个过程是零样本（zero-shot）的，即不需要针对新任务进行额外的训练
* 核心贡献：
    * 提出了一种低成本、模块化、可扩展的新范式，释放了现有基础模型的组合潜能。
* 关键优势：
    * 无需微调，避免了数据收集和训练的巨大成本；
    * 高度灵活，可以轻松接入新的模型或外部工具（如API、数据库）；
    * 可解释性强，模型间的“对话”流程易于理解和调试。




**应用**
* 三个具体领域的应用
    * 以用户为中心的感知(Egocentric Perception)
        * 生成“关键时刻”描述（Key Moments）
            * 从视频中提取关键帧，并通过VLM生成描述。
            * 这些描述被递归地总结成语言描述的世界状态历史（language-based world-state history）。
        * 应用任务：
            * 总结（Summarization）
            * 开放性问答（Q&A）
            * 预测（Forecasting）
        * 优势：
            * 将视频理解任务转化为阅读理解（reading comprehension）问题，无需训练视频模型。
            * 利用LM的强泛化能力，实现零样本推理
    * 多模态辅助对话(Multimodal Assistive Dialogue)
    * 机器人感知与规划(Robot Perception & Planning)
* 这些应用展示了如何通过将外部模块（如网络搜索、机器人策略）集成到SMs中，以实现新的多模态功能

**关键技术：多模态信息提示（Multimodal-Informed Prompting）**
* 这是实现模型间“对话”的关键技术。
* 例如，要回答一个关于视频的问题：
    * 视觉模型（VLM）作为“眼睛”
        * 先将视频的关键帧转换成文本描述（例如：“一个人正在厨房切西红柿”）。
    * 大语言模型（LLM）作为“大脑”
        * 将这些文本描述作为提示（Prompt）的一部分，
        * 连同用户的问题（例如：“他接下来需要做什么？”）一起输入给LLM。
    * LLM进行推理
        * LLM利用其强大的常识和推理能力，基于视频描述来回答问题
        * （例如：“他接下来可能需要热锅，准备炒菜”）。






## Abstract


本文探讨了**大规模预训练模型**（例如“基础模型”）在不同数据领域上训练所展现出的**不同能力**。这些数据领域虽然广泛，但彼此之间可能**几乎没有交集**。

例如，**视觉-语言模型（VLMs）**通常在**互联网规模的图像字幕**上进行训练，而**大型语言模型（LMs）**则进一步在**不包含图像的互联网文本**（例如电子表格、SAT题目、代码等）上训练。因此，这些模型在不同领域的**常识知识**表现形式也各不相同。

本文的核心贡献在于提出了一种名为**苏格拉底模型（Socratic Models, SMs）**的模块化框架。该框架强调模型之间的**协同能力**，允许在**无需微调**的前提下，通过**多模态提示**，**零样本地组合多个预训练模型**，从而让它们**互相交流信息**，并**获取新的多模态能力**。

重点强调的是，通过极少的工程量，SMs 不仅在**零样本图像描述生成**和**视频到文本检索**任务上**与当前最优方法相当**，还能够**启用新的应用场景**，例如：

1. **对第一视角视频进行自由问答**；
2. **与人类进行多模态辅助对话**（例如烹饪食谱），并通过**外部API和数据库**（如网络搜索）实现交互；
3. **机器人感知与规划**。

总结来说，本文提出了一种通过**模型间的协作与组合**，实现**多模态能力增强**的创新方法，具有广泛的应用前景。


## 1 Introduction


### 概述

本节介绍了**Socratic Models (SMs)** 这一新型系统框架，其核心思想是通过语言作为中介，将多个**预训练模型**（如 BERT、GPT-3、CLIP）以及**外部模块**（如 API）组合起来，以实现**零样本多模态推理**。这一框架无需额外训练或微调，即可通过语言引导各模块之间的协作，完成复杂任务。

---

### 预训练模型的能力差异与互补性

- **当前大模型的多样性**：如 BERT、GPT-3 和 CLIP 等模型在多种任务中展现了强大能力，例如零样本图像分类、高层规划等。
- **数据来源不同，能力互补**：
  - **视觉-语言模型 (VLMs)**：如 CLIP，主要依赖图像和视频标题进行训练，擅长将文本与视觉内容关联。
  - **语言模型 (LMs)**：如 GPT、BERT，训练数据更广泛，涵盖小说、表格、测试题等，具备更强的文本理解和生成能力。
- **模型间的知识差异**：VLMs 更偏向视觉语义理解，而 LMs 在阅读理解、逻辑推理等方面表现更优。

---

### 提出：Socratic Models (SMs) 框架

- **核心思想**：SMs 提出一个模块化的框架，模型间通过语言进行交互，而不依赖于联合训练。
- **组成模块**：
  1. **预训练模型**（如 LMs 和 VLMs）
  2. **外部接口模块**（如网络搜索 API、机器人执行模块）
- **任务处理方式**：通过构建**语言引导的多模态对话**，联合多个模型对任务进行推理。
- **目标**：打破传统多模态模型需要大量领域特定数据或统一模型结构的限制，转而利用已有模型的**零样本能力**，通过语言进行跨模态推理。

---

### SMs 的应用与表现

- **定量成果**：
  - 在标准多模态任务上达到了**零样本 SOTA 表现**，包括：
    1. 图像描述（MS COCO）
    2. 上下文图像描述（Concadia 数据集，CIDEr 提升 11.3 → 38.9）
    3. 视频理解（MSR-VTT，R@1 提升 40.7 → 44.7）
- **新应用场景**：
  1. **第一人称感知的开放推理**
  2. **多模态辅助对话**（如指导用户完成烹饪步骤）
  3. **机器人感知与规划**（如顺序抓取与放置）

---

### SMs 的创新意义

- **问题重构**：将一个领域的难题转化为另一个领域的任务。例如，将第一人称视频理解问题转化为**阅读理解**问题，利用语言模型的能力进行解答。
- **核心贡献**：
  1. 提出 SMs 框架，利用语言进行多模型协作。
  2. 提出关键组件：
     - 多模态提示方法
     - 基于语言的世界状态历史（用于视频理解）
  3. 展示出 SMs 在多个任务上的**强量化性能**
  4. 提供了多个实际应用场景（如开放第一人称感知、多模态助手、机器人规划）

---

### 局限性与讨论

- **模型不可靠性**：SMs 的表现受限于其依赖的预训练模型，可能存在幻觉或推理错误。
- **更广泛的影响**：包括模型间的协调困难、任务复杂度提升以及对语言中介依赖的潜在问题。
- **代码与资源**：已公开访问 [socraticmodels.github.io](https://socraticmodels.github.io/)。

---

### 总结

本节主要介绍了 SMs 框架提出背景、模型能力的互补性、框架核心思想与关键组件，以及其在多个任务上的优异表现和应用场景。同时指出了当前 SMs 的优势与局限性，为全文奠定了基础。


## 2 Problem Setting, Background, and Related Work

### 问题设定

本文致力于构建基于大规模预训练模型的**多模态应用**。这些应用可以看作是一种**迁移学习**的形式（Caruana, 1997；Thrun, 1998），即从一系列**辅助任务**（如文本补全、图文相似度）中学习到的知识被迁移到新的**下游目标任务**（如图像描述生成、机器人路径规划）中。

我们关注的是一组目标任务，每个任务 i 都需要一个映射函数 f^i: X^i → Y^i。我们特别关注以下几种情况：

1. **多模态输入或输出**：输入 X^i 和/或输出 Y^i 可能包含多种模态（如语言、视觉、音频、机器人动作）；
2. **任务数量众多**；
3. **每个任务的训练数据可能很少或没有**；
4. **已有在辅助任务上预训练好的模型**。

### 预训练权重的迁移学习

当前，**利用预训练权重进行迁移学习**是深度学习中的主流方法。具体来说，就是使用在辅助任务上预训练的模型权重作为目标任务模型中某些参数的初始化。这些参数可以**保持冻结**，也可以进行**微调**。

- 在**无监督设置**中，预训练深模型的研究已广泛展开（Hinton et al., 2006；Bengio et al., 2006；Vincent et al., 2008；Raina et al., 2007；Mesnil et al., 2012）。
- 在**有监督设置**中，ImageNet 预训练模型（Deng et al., 2009）在图像识别（Girshick et al., 2014；Donahue et al., 2014；Zeiler & Fergus, 2014；Sermanet et al., 2013）中广泛使用。
- 在**自然语言处理（NLP）**中，各种形式的预训练模型也十分常见（Mikolov et al., 2013；Pennington et al., 2014；Dai & Le, 2015；Ramachandran et al., 2016；Peters et al., 2018；Devlin et al., 2018；Brown et al., 2020）。

在多模态学习中，常会保留某些子模块（如只关联一种模态的模块）**不进行微调**，以提高下游任务的性能（Zhai et al., 2021；Kulkarni et al., 2019；Florence et al., 2019；Tsimpoukelli et al., 2021；Zakka et al., 2022）。

### 多模态联合训练

对于每个目标任务 i，通常会收集一个**大规模的多模态数据集**，并训练一个特定于该任务的映射函数 f^i_{θ_i}，其中部分参数可能来自预训练模型（可冻结或微调）。

这种方法的优势在于其遵循了“**构建大规模数据集 -> 训练大规模模型**”的方法论（Sutskever et al., 2014），在数据和计算资源充足的情况下表现优异。

### 多模态联合训练与预训练模型结合的应用

已有研究表明，将**大规模预训练模型**与**多模态联合训练**结合，可以在多个下游任务中取得良好效果，例如：

- **图像生成**：CLIP + GPT-2（Mokady et al., 2021）；
- **视频理解**：CLIP + BERT（Gao et al., 2021）；
- **视觉问答**：Song et al., 2022；
- **语音与文本建模**：Song et al., 2022；Bapna et al., 2022。

这些模型通常会在任务特定数据上进行微调。虽然此方法在数据丰富的场景中表现良好，但本文研究表明，**少样本模型（SMs）**在数据稀缺或获取成本高的任务中也可作为强替代方案。

### 多模态概率推理

另一种方法是**多模态概率推理**，例如**贝叶斯方法**，其中一个模型作为先验，另一个作为证据，实现不同模态的联合推理（Karpagavalli & Chandra, 2016；Ahn et al., 2022）。

一个典型应用是在**自动语音识别（ASR）**中，先训练不同的语言模型，再通过先验知识将它们迁移到语音到文本系统中（Karpagavalli & Chandra, 2016）。

### 多专家混合（Mixture-of-Experts）

**多专家混合（MoE）**是组合多个模型输出的常见方法（Jordan & Jacobs, 1994；Masoudnia & Ebrahimpour, 2014）。当前研究已探索了跨模态（如视觉和音频）的 MoE 模型（Liu et al., 2019）。将这些方法应用于近年来的**预训练基础模型**中，可能是一个有前景的方向。

### 零样本与少样本提示

最近，**零样本或少样本提示**方法（Brown et al., 2020）在迁移学习中表现突出。在该方法中，只需提供几个示例（或不提供），即可让一个大型预训练语言模型完成新任务。进一步改进如**链式思维提示（Chain-of-Thought prompting）**（Wei et al., 2022）显著提升了目标任务的性能（Chowdhery et al., 2022），并拓展了模型能力。本文工作正是基于这些方法，**将提示方法拓展到多模态领域**。


## 3 Socratic Models



本节介绍了一种称为**苏格拉底模型（Socratic Models, SMs）**的框架，该框架旨在通过语言（如通过提示）组合多个大型预训练模型，以执行新的下游多模态任务，而**无需进一步训练**。以下是本节内容的总结：

---

### 3.1 Socratic Models（苏格拉底模型）

**苏格拉底模型（SMs）** 是一种通过语言作为中间表示，将多个大型预训练模型组合起来的框架，以实现新的多模态任务。它是一种不同于其他多模态方法（如联合多模态训练）的组合方式，并可能与之互补。

- SMs 本质上是一个**计算图**，其中每个节点是一个模块（如语言模型 LM、视觉语言模型 VLM、音频语言模型 ALM 或外部 API），边表示模块之间通过**语言进行通信**。
- 各模块之间通过将其他模型的输出**转化为语言**来传递信息，从而支持进一步推理。例如：VLM 检测视觉实体，LM 根据视觉信息生成可能的音频描述，ALM 选择最可能的音频，LM 再次推理可能的活动等。
- 模块输出使用不同颜色进行可视化：LM 输出为蓝色，VLM 输出为绿色，ALM 输出为紫色，提示文本为灰色，用户输入为品红色等。

---

### 3.2 多模态提示（Multimodal Prompting）

SMs 的一个**核心组件**是**多模态提示（multimodal prompting）**，即将非语言信息（如视觉、音频）嵌入到语言提示中，供语言模型进行推理。

- 一种多模态提示的方式是将其他模态中描述的实体（如视觉检测到的对象）**变量替换**到语言提示中。
- 示例：从视频中识别出视觉实体后，LM 生成可能的音频描述，ALM 选择最可能的音频，LM 再推理可能的活动，VLM 排名最可能的活动，最后 LM 输出总结。
- 该过程涉及多次模型之间的**来回交互**，形成一个“闭环反馈”。

图 2 中展示了 SM 的一个示例：通过多模态提示，SM 系统可以对第一视角图像（egocentric image）进行零样本标注，总结活动内容。例如，系统可以判断“我正在爬楼梯，并可能听到脚步声”。

---

### 3.3 实现方式与优势

虽然 SMs 可以看作是“让模型互相对话”，但实际上，某些模型需要**简单的预处理和后处理**来生成语言。

- 例如，视觉-语言相似度模型（如 CLIP）本身不生成文本，但可以基于预定义的类别名称库进行零样本检测，并返回最可能的类别。
- SMs 的设计**无需训练**，模型之间的交互由**提示模板**进行脚本化控制。
- 这种无需训练的特性具有**实际优势**：可以快速开发新应用，只需少量创意性编程。

---

### 3.4 SMs 的动机与背景

SMs 是对当前主流“预训练权重”范式的**一种反应**，该范式在将基础模型迁移到新任务时存在以下限制：

1. **微调成本高**：大型模型（如 100B 参数）的微调在计算和数据收集方面都非常昂贵，尤其在新多模态应用（如 AR 或机器人）中难度更大。
2. **泛化性下降**：微调可能会降低模型对分布偏移的鲁棒性和泛化能力。
3. **知识过时**：基础模型的知识可能因训练时滞而“过时”，且难以访问实时数据或专有信息。

尽管存在这些限制，大型预训练模型仍将是未来许多智能系统的核心。SMs 提供了一种**系统层面的解决方案**，通过利用这些模型的零样本和少样本能力，组合多个模型，以应对多模态任务中的挑战。

---

### 总结

本节介绍了苏格拉底模型（SMs）的核心思想：通过语言作为媒介，组合多个预训练模型来完成新任务，无需训练。重点内容包括：

- **SMs 的结构**：计算图，模块通过语言通信。
- **关键组件**：多模态提示，允许模型间交互与反馈。
- **优势**：无需训练，快速开发新应用。
- **动机**：应对当前迁移学习范式（微调预训练模型）的局限性。

SMs 为多模态任务提供了一种灵活、高效、创新的系统框架。


## 4 Evaluation: Methods and Results


### 4.1 Socratic 图像描述（MS COCO 数据集）：VLM + LM

**方法**  
本节评估了 Socratic Models（SMs）在图像描述任务上的效果，任务基于 MS COCO 数据集（Chen 等，2015；Lin 等，2014）。  
SMs 的方法是通过 VLM（视觉语言模型）与 LM（语言模型）之间的引导式交互进行图像描述生成，形式为：  
$$
\text{caption} = f^{3}_{\text{VLM}}(f^{2}_{\text{LM}}(f^{1}_{\text{VLM}}(\text{image})))
$$  
第一步，使用 VLM 零样本检测图像中的地点、物体类别、图像类型（如照片、卡通等）以及人数，并将这些信息作为提示输入给 LM。  
第二步，LM 生成多个候选描述，并通过非零温度采样（例如 0.9）确保多样性。  
第三步，VLM 对候选描述进行重新排序，选择评分最高的描述作为最终输出。

**结果**  
在 MS COCO 的 100 张测试图像上，SMs 的 CIDEr 分数为 44.5，显著优于零样本方法（ZeroCap，CIDEr 18.0），但略低于微调模型（如 ClipCap，CIDEr 152.4）。  
此外，SMs 生成的描述更丰富，但可能因与数据集标注不一致而影响评分。若加入少量样本（3-shot）进行微调，SMs 的 CIDEr 可提升至 76.3，甚至超过 MAGIC 等微调模型。  
未来工作可以考虑结合开放词汇的物体检测器以提取更具体的视觉细节。

### 4.2 Socratic 上下文图像描述（Concadia 数据集）：VLM + LM

**方法**  
Concadia 数据集（Kreiss 等，2021）包含图像和相关文章文本，分别评估图像描述和图像标题生成能力。  
SMs 方法与 MS COCO 类似，但加入了文章文本作为上下文，并通过 LM 提示生成描述，无需 VLM 重新排序。

**结果**  
在 Concadia 上，SMs 在零样本条件下显著优于之前基于微调的方法（Kreiss 等，2021），CIDEr 分数分别为 38.9（标题）和 22.6（描述），远超 11.3 和 17.4。  
若加入真实图像描述作为上下文，SMs 的 CIDEr 可高达 93.8，表明其潜力。  
此外，研究发现语言模型在某些情况下仅凭文章文本也能生成合理标题，可能与数据分布或训练集重叠有关。

### 4.3 Socratic 视频到文本检索：VLM + LM + ALM

**方法**  
SMs 被扩展用于视频到文本检索任务，以 MSR-VTT 数据集（Xu 等，2016）为基准。  
方法结合了 VLM（如 CLIP）、LM（如 GPT-3）和 ALM（音频语言模型），通过音频转录与文本生成增强视觉理解。  
具体步骤包括：  
1. 使用 ALM（如 Google Cloud Speech-to-Text）转录音频；  
2. 使用 LM 生成视频摘要；  
3. 通过 RoBERTa 计算生成文本与候选文本的相似性，并与 CLIP 的视频特征结合，重新排序。

**结果**  
SMs 在 MSR-VTT 上实现了零样本新纪录，在包含长音频转录的子集（n=1,007）上 R@1 达 54.9，接近微调模型 CLIP2Video（R@1 54.6）。  
这表明，在具备足够音频信息的网络视频中，SMs 可以几乎媲美微调模型，而无需训练。  
该方法的提升主要来自于音频与语言的多模态推理，未来可进一步优化 VLM 的性能以提升整体效果。

---

**总结**  
本节展示了 Socratic Models 在图像描述、上下文图像生成和视频到文本检索任务中的有效性。尽管在某些任务上仍落后于微调模型，但 SMs 在零样本条件下表现优异，特别是在结合语言推理与视觉信息时。未来可通过引入更强大的视觉检测和文本生成模型进一步提升性能。


## 5 Applications: Methods and Demonstrations



这一章节（**第五部分：应用 - 方法与演示**）主要介绍了Socratic Models（SMs）在三个具体领域的应用：**以用户为中心的感知（Egocentric Perception）**、**多模态辅助对话（Multimodal Assistive Dialogue）** 和 **机器人感知与规划（Robot Perception & Planning）**。这些应用展示了如何通过将外部模块（如网络搜索、机器人策略）集成到SMs中，以实现新的多模态功能。下面是对每个子章节的详细总结：

---

### 5.1 以用户为中心的感知（Egocentric Perception: User + VLM + LM + ALM）

#### **重点内容：**
- **应用场景**：SMs可用于处理第一视角（egocentric）视频的感知任务，如**内容摘要**、**开放性推理问题回答**和**未来事件预测**。这些任务在增强现实（AR）和机器人领域有重要应用。
- **挑战**：第一视角视频由于视角特殊、缺乏时间整理等原因，与传统通用互联网视频（第三视角）差异较大，导致基于数据驱动的模型难以迁移，因而需要零样本（zero-shot）解决方案。
- **解决方案**：SMs无需训练专用领域的大型数据集，而是通过**视觉语言模型（VLM）** 和 **语言模型（LM）** 的组合，完成感知任务。

#### **详细方法：**
1. **生成“关键时刻”描述（Key Moments）**：
   - 从视频中提取关键帧，并通过VLM生成描述。
   - 这些描述被递归地总结成**语言描述的世界状态历史（language-based world-state history）**。

2. **应用任务：**
   - **(i) 总结（Summarization）**：将一天的活动进行摘要，如“我睡了觉、喝了咖啡、看了电视、洗了衣服……”。
   - **(ii) 开放性问答（Q&A）**：基于世界状态历史回答各种问题，例如时间、因果关系、上下文问题，如“我什么时候最后一次喝咖啡？”或“为什么我今天去了前廊？”。
   - **(iii) 预测（Forecasting）**：通过语言模型生成未来事件的预测，如“我将在下午3点开车去公园”。

3. **优势**：
   - 将视频理解任务转化为阅读理解（reading comprehension）问题，无需训练视频模型。
   - 利用LM的强泛化能力，实现零样本推理。

---

### 5.2 多模态辅助对话（Multimodal Assistive Dialogue: User + VLM + LM + Web Search）

#### **重点内容：**
- **应用场景**：SMs可用于引导用户完成日常任务（如烹饪），通过**对话**提供步骤指导、替换建议和**视觉参考**。
- **实现方式**：
  - 利用**语言模型（LM）**进行对话；
  - 通过**VLM**搜索单词对应图像；
  - 通过**网络爬虫（Web Search）**检索图像或视频内容。

#### **详细方法：**
1. **对话模板设计**：
   - 为用户提供结构化对话流程，例如：“Step: 第一步……”、“(image) 描述……”。
   - 特定关键词（如“image”）触发VLM和Web Search的联合工作。

2. **示例流程**：
   - 用户询问“什么是中碗”，系统回复并触发图像搜索；
   - 用户进一步询问“如何判断硬性泡沫”，系统再次调用图像搜索辅助说明；
   - 系统还提供替代材料建议（如“可以不用水”）。

3. **优势**：
   - 实现自然语言与视觉信息的结合；
   - 能够实时提供图像或视频帮助，提升用户体验。

---

### 5.3 机器人感知与规划（Robot Perception & Planning: User + VLM + LM + Policies）

#### **重点内容：**
- **应用场景**：SMs可用于使机器人理解并执行自然语言指令，如“把红色积木放在蓝色积木上”。
- **实现方式**：
  - 利用**VLM**识别环境中的物体；
  - 使用**LM**进行多步骤规划；
  - 调用**预训练的语言条件机器人策略（language-conditioned robot policies）**执行动作。

#### **详细方法：**
1. **系统流程**：
   - **VLM识别物体** → 生成环境描述；
   - **LM生成动作计划**（自然语言或伪代码）；
   - **策略模块执行**动作（如CLIPort）。

2. **示例操作**：
   - 移动积木到指定角落；
   - 叠放积木；
   - 取消上一步操作；
   - 将积木放进碗中；
   - 按颜色分类。

3. **优势**：
   - 扩展了机器人原本训练的指令范围；
   - 支持通过自然语言与用户进行交互；
   - 利用LM的能力生成代码，提升灵活性。

---

### 总结

本节通过三个应用案例，展示了**Socratic Models（SMs）**在多模态任务中的强大能力：  
- **Egocentric Perception**：通过语言理解视频内容，实现零样本推理与记忆辅助；
- **Multimodal Dialogue**：结合视觉与语言，提供辅助性任务指导；
- **Robotics**：通过语言与视觉信息，使机器人理解并执行复杂指令。

这些应用的核心在于：**将外部模块（如VLM、Web Search、机器人策略）整合到SMs体系中，通过语言协调多模态信息处理。**


## 6 Discussion



### Socratic Models（苏格拉底模型，SMs）的框架介绍  
Socratic Models 是一种模块化框架，通过结构化的对话（即通过提示）在多个大型预训练模型之间进行协作，以共同完成新的多模态任务。它利用了基础模型中已经存储的常识性知识，这些模型是基于不同数据域（如文本到文本、文本到图像、文本到音频）预训练的，其中可能包含互联网规模的数据。论文中展示的图像字幕、视频到文本的检索、第一视角感知、多模态对话、机器人感知与规划等系统，仅是 SMs 框架的几个例子。这些例子展示了如何通过 SMs 框架，构建简单系统，将已有的基础模型适配到新的多模态功能中，而无需额外的领域数据收集或模型微调。

#### 重点内容：  
- **零样本能力**：SMs 能够在不依赖额外数据或微调的情况下，实现对新任务的直接处理。  
- **鲁棒性保持**：相比微调后的模型，SMs 在面对数据分布变化时保持更好的鲁棒性（这是微调后模型性能下降的常见问题）。  

#### 潜在研究方向：  
- **元学习苏格拉底对话**：未来可以研究如何通过元学习自动优化模块间的交互规则。  
- **扩展多模态接口**：不仅限于语言，还可以在模块之间传递图像等其他模态数据。

---

### 更广泛的影响（Broader Impacts）

SMs 提供了一种新思路，鼓励利用现成的大型预训练模型构建 AI 系统，而无需额外的数据收集或模型训练。这种方式带来了许多实际优势，同时也伴随着潜在的风险。

#### 优点：  
- **可解释性强**：通过语言接口（如对话输出），即使是非专业人士也能理解系统行为。  
- **进入门槛低**：SMs 可以用最小的计算资源开发出新功能，特别适用于传统上数据稀缺的应用场景。  
- **无需训练**：论文中展示的所有结果均未使用模型训练，这降低了部署成本。  

#### 风险与注意事项：  
- **滥用风险**：由于系统易于构造，可能被用于未预期或恶意的用途，需要长期监控与管理。  
- **数据偏见问题**：由于模型基于互联网数据训练，系统可能表现出其中的偏见，因此需谨慎使用并验证结果的准确性。  

#### 重点内容：  
- **平衡利弊**：作者呼吁广泛讨论如何最大化 SMs 的积极影响（如推动多模态应用的发展），同时限制恶意行为者的滥用能力。  

---

### 总结要点  
Socratic Models 是一种利用多个预训练模型通过结构化对话进行协作的框架，具有**零样本适应能力**和**高鲁棒性**。它为多模态任务提供了一种新的实现路径，同时也带来了**可解释性提升**和**低门槛部署**的优势。然而，其**潜在的滥用风险**和**模型偏见问题**也需要引起重视和谨慎管理。


## Acknowledgments and Disclosure of Funding


作者感谢 Debidatta Dwibedi、Matthew O’Kelly 和 Kevin Zakka 对提升本文的出色反馈，感谢 Anelia Angelova、Jean-Jacques Slotine、Jonathan Tompson 和 Shuran Song 提供富有成效的技术讨论。此外，作者感谢 Kan Huang 提供应用支持，Ahmed Omran、Aren Jensen、Malcolm Slaney 和 Karolis Misiunas 在音频模型方面的建议，以及 Cody Wanner 提供的 YouTube 视频支持。该部分重点列举了在论文撰写过程中提供帮助的多位研究人员与技术人员，体现了团队合作与学术交流的重要性。


## Appendix A Overview


本附录部分涵盖了多个补充内容，旨在进一步说明和扩展正文中的实验、应用和未来工作的讨论。

1. **模型选择的无监督评估**（i）：  
   重点介绍了在没有标注数据的情况下，如何通过无监督方法对模型进行评估，这是模型选择过程中的一个重要补充。该部分对于理解模型性能评估的多样性具有较高价值。

2. **主实验的附加说明**（ii）：  
   提供了主实验中未详细展开的细节，帮助读者更全面地理解实验设计和结果。

3. **对以自我为中心感知的应用的更多细节**（iii）：  
   详细描述了模型在以自我为中心的感知任务中的应用情况。该部分是应用研究的重要补充，展示了模型在具体实际任务中的表现。

4. **基于语言描述的世界状态历史的视频搜索扩展**（iv）：  
   介绍了模型在视频搜索任务中的扩展应用，特别是如何利用语言输入来检索过去的世界状态历史。这一部分展示了模型在多模态理解方面的能力。

5. **机器人感知与规划实验的更多细节**（v）：  
   对机器人感知和规划实验进行了更详细的说明，是理解模型在机器人领域应用的重要部分。

6. **未来工作的附加讨论**（vi）：  
   探讨了模型在未来可能的发展方向，包括在演绎推理（如 SMs）中的应用潜力。这一部分具有较强的前瞻性，但内容较为概括。

7. **更广泛的影响（如能源与资源消耗）**（vii）：  
   简要提到了模型在实际应用中可能带来的能源和资源消耗问题，属于较为次要的补充内容。

最后，附录还提供了代码访问链接：[socraticmodels.github.io](https://socraticmodels.github.io/)，方便读者进一步查阅和复现实验。


## Appendix B Unsupervised Socratic Model Selection

### 模型组合与互补性
在模型组合中，通过一个模型弥补另一个模型的弱点，为**非监督式模型性能评估**提供了新的思路。本文关注的不是**“某个模型表现有多好”**（绝对性能），而是提出一个替代性问题：**“这个模型能否弥补另一个模型的弱点？”**  
这种方法强调模型之间的**互补性**，而非独立性能评估。

重点在于，这种评估方式不需要任何标注数据，因此适合**非监督学习环境**。例如，当我们评估一个视觉-语言模型（VLM）的性能时，不是直接评估其绝对表现，而是评估它在与一个**弱语言模型（wLM）**组合时，能否接近其与**强语言模型（sLM）**组合时的表现。如果一个VLM即使在与弱LM组合时仍能表现出色，则说明该VLM具有更强的鲁棒性与互补性。

---

### Strope 等人的方法（Strope et al., 2011）
Strope 等人提出了一种**不需要真实标注**的模型评估方法。其核心思想是：  
- 假设有两个LM：一个是**强LM**（sLM），一个是**弱LM**（wLM）；
- 对于多个VLM，分别与这两个LM进行组合，比较它们的输出；
- 通过比较VLM + wLM与VLM + sLM之间的**语义距离**，评估VLM在不同组合下的补偿能力。

这种方法的优势在于：
- **无需人工标注**；
- 可以通过**语义相似度评估**（如使用RoBERTa等模型计算句子嵌入）来衡量输出之间的差距；
- 能够**有效指导模型选择**，因为其结果与绝对性能评估（例如ImageNet准确率）呈**正相关**。

---

### 应用于苏格拉底模型（Socratic Models）的非监督评估
本文将上述方法扩展到**苏格拉底模型（Socratic Models, SMs）**，用于评估在**第一人称视觉感知（egocentric perception）**任务中，不同模型之间的协同性能。

具体步骤如下：
1. **生成伪真实数据（pseudo ground truth）**：使用一个基线VLM与强LM（sLM）组合，生成预测结果作为参考；
2. **构建对比**：将基线VLM和新VLM分别与弱LM（wLM）组合，生成预测；
3. **评估相似度**：将两个组合的预测与伪真实数据进行比较，使用另一个独立的LM（如RoBERTa）进行语义嵌入计算相似度；
4. **避免偏差**：为了避免模型自身生成的伪真实数据带来的偏差，必须使用**独立的基线VLM**来生成伪真实数据，而非使用同一VLM。

---

### 实验结果与分析（表5）
表5展示了使用GPT-3（Davinci作为sLM，Curie作为wLM）与不同结构的CLIP模型（如ViT和ResNet）组合后的非监督评估结果。主要发现包括：
- **性能与ImageNet准确率存在正相关**：例如，ViT-L/14（76.2%）在非监督评估中得分更高；
- **相关系数**：通过ViT-B/16和RN50x16模型评估，相似性得分与ImageNet准确率的相关系数分别为0.41和0.46；
- **灰色数据说明**：在相同VLM上生成的伪真实数据与预测之间的比较存在**不公平优势**，因此这些数据在表格中被标记为灰色，表明其不可靠。

---

### 总结
本附录提出了一个**非监督的模型评估方法**，通过衡量VLM与弱LM组合后的输出与强LM组合的输出之间的差距，评估VLM的补偿能力。这种方法适用于**苏格拉底模型**，特别是在没有标注数据的背景下，能够有效衡量不同模型之间的**协同与互补性**。实验结果表明，该方法与传统性能指标（如ImageNet准确率）具有良好的相关性，能够支持模型选择决策。


## Appendix C Additional Notes on Experiments


### 模型选择
本实验中使用的模型来自大型预训练“基础”模型（foundation models），如Bommasani等人（2021）所述。实验中选择了公开可获取的模型，以便研究社区可以复现系统。具体模型包括：

- **ViT-L/14**（428M参数）：用于图像-文本相似度任务（CLIP模型）；
- **ViT-B/32**：用于MSR-VTT数据集；
- **ViLD**（Gu等人，2021）：用于开集目标检测任务；
- **Wav2CLIP**（Wu等人，2021）：用于音频-图像匹配；
- **Google Cloud Speech-to-text API**：用于语音转文本；
- **GPT-3**（175B参数）与**RoBERTa**（355M参数）：作为语言模型（LM）。

所有模型均为“开箱即用”，未进行额外微调。实验可在单台配备NVIDIA V100 GPU和网络访问的机器上运行，用于调用外部API（如GPT-3和Google语音转文本）。

---

### C.1 MS COCO 上的图像描述（Image Captioning）

本部分评估了MS COCO数据集上的图像描述任务。为了降低GPT-3 API的运行成本，实验在测试集中随机选取了100张图像作为子集进行评估。结果显示，使用该子集的评估指标（如BLEU-4、CIDEr等）与原始完整测试集的结果相近。

表格（表6）对比了几种基线模型（如ClipCap、MAGIC、ZeroCap）在完整数据集与子集上的表现。部分模型在子集上表现更优，可能是由于子集样本更具有代表性。

此外，部分模型（如ClipCap）通过使用图像-文本配对数据进行微调，而其他模型（如MAGIC）则采用“零样本”策略，仅使用图像和文本未配对的数据。

---

### C.2 Concadia 上的上下文图像描述（Contextual Image Captioning）

在Concadia数据集上，评估了模型在生成图像描述时结合图像和相关文章文本的能力。实验发现，Socratic Models（SMs）在结合视觉语言模型（VLMs）和语言模型（LMs）时表现优异。即使仅使用LM（如GPT-3）也能取得不错的成绩。

表7中显示，当去除了VLM的信息时，虽然图像描述性能下降（CIDEr下降2.0），但图像标题生成性能却有所提升（CIDEr上升1.2）。这说明：

1. VLM对图像描述的生成更为重要；
2. 图像标题与文章文本之间可能存在分布相关性，LM可单独利用这种相关性；
3. Concadia数据集可能与LM的训练数据存在重叠，这一方面需要进一步研究。

---

### C.3 MSR-VTT 1k-A 上的视频-文本检索（Video-to-text Retrieval）

本部分评估了视频-文本检索任务，使用了MSR-VTT数据集的1k-A子集，该子集由Yu等人（2018）提出。实验结果显示，Socratic Models（SMs）在零样本设置下表现优异（R@1为60.7），优于基于CLIP的基线模型（R@1为58.0）。

表8对比了多种方法（包括微调和零样本方法）在视频-文本检索上的表现：

- CLIP相关方法（如CLIP2TV、DRL）在R@1等指标上表现良好；
- SMs 在不进行微调的情况下，仅使用GPT-3和Wav2CLIP等模型，就能达到较好的性能；
- 实验还指出，此前某些方法（如Portillo-Quintero等）在评估时使用的标注数据不完整，导致性能偏低，修正后其性能显著提升。

最后指出，虽然当前趋势是仅使用视觉特征进行视频-文本检索，但SMs通过结合语言模型中的常识推理能力，重新引入了音频模态，为视频理解任务提供了新的思路。

---

### 总结

本附录提供了论文中实验部分的详细补充说明，包括模型选择、图像和视频任务的评估结果。重点在于：

- 使用公开模型降低成本，便于复现实验；
- 在多个数据集上的实验结果表明，Socratic Models在零样本设置下仍能取得良好表现；
- 结合LM与VLM的优势，特别是语言模型中存储的常识推理能力，是当前方法取得成功的关键；
- 部分模型（如GPT-3）即使不依赖视觉模型，也能在某些任务上取得较高性能，值得进一步研究其内部机制与数据重叠问题。

--- 

整体来看，附录详细补充了实验设计和结果，强调了Socratic Models在多模态任务中的灵活性与高效性。


## Appendix D Egocentric Perception Appendix


### **Background（背景）**

第一视角感知（Egocentric Perception）是计算机视觉中的一个重要但具有挑战性的研究领域，其目标是理解佩戴设备（如头戴式摄像机）获取的第一人称视角图像或视频内容。早期研究主要依赖于手工设计的特征，如光流（optical flows）来捕捉运动轨迹，或通过人类凝视、手部动作和物体特征来识别动作（如“接收包裹”等）。由于数据量有限，手工特征在早期研究中占据主导地位。

随着数据量的增加和深度学习的发展，研究转向基于学习的特征表示。例如，使用预训练的卷积神经网络（如ResNet或CLIP）提取特征，或者对这些特征进行微调或从头训练。近年来，研究重点包括学习如何捕捉与手部、物体、凝视等相关的视觉表示，甚至结合多模态信息（如视频+音频），例如Kazakos等人（2019）提出多模态嵌入，Furnari等人（2019）研究第一视角视频的动作预测。

尽管研究不断进步，**数据不足仍然是主要瓶颈**。为解决这一问题，研究者构建了大规模第一视角视频数据集，如EPIC-Kitchens、Charades-Ego和Ego4D。

---

### **D.1 Why Egocentric Perception?（为什么研究第一视角感知？）**

第一视角感知之所以重要，是因为其在增强现实（AR）、机器人等实际应用中具有广泛前景。其数据特点与传统第三人称视角数据（如ImageNet、Kinetics）不同，第一视角视频通常包含**非典型视角、缺乏时间整理、场景复杂**等特点。因此，传统基于第三人称数据预训练的模型难以直接迁移到第一视角任务中。

研究者指出，虽然面临挑战，例如**数据获取困难、标注成本高**等，但Socratic Models（SM）通过结合视觉语言模型（VLM）、大语言模型（LM）和音频语言模型（ALM）等预训练模型，可以在无需微调的情况下完成多种复杂任务，如视频搜索、图像描述、开放问答、因果推理和未来事件预测等。

---

### **D.2 Additional Details on Language-Based World-State History from Video（基于语言的视频世界状态历史）**

为支持开放问答，系统构建了**语言描述的“世界状态历史”**，记录视频中每个关键帧的信息，包括**地点、物体、活动**等。系统采用“苏格拉底模式”（Socratic Dialogue）进行多模型协作，例如：

- **“Where am I?”**：使用VLM识别场景（如Places365）。
- **“What do I see?”**：使用VLM识别物体（如OpenImages）。
- **“What am I doing?”**：使用LM生成活动建议，并由VLM重新排序，筛选最相关的活动。

该方法的优势在于：**生成的描述更贴近真实场景**，且不局限于预定义的活动类别。例如，“接收到包裹”这一常见活动在标准数据集中并未出现，但LM可以基于场景和物体推断出合理的活动描述。

此外，系统还可以**融合音频信息**（通过ALM，如Wav2CLIP），以提升视频摘要的准确性。例如，在识别“脚步声”时，音频信息可以补充视觉模型无法识别的背景细节。

#### **3.3-A: Socratic Egocentric Image Summaries（苏格拉底式第一视角图像摘要）**

系统为每个关键帧生成图像摘要，格式如下：

> *“I am in a {place}. I see a {object}. I am {activity}.”*

生成过程包括以下步骤：
1. **VLM识别地点和物体**。
2. **LM生成活动建议**。
3. **VLM重新排序活动**。
4. **LM生成完整总结**。

#### **3.3-B: Adding Audio into Single-Moment Summaries（融合音频信息）**

系统使用ALM（如Wav2CLIP）识别声音，并结合视觉上下文生成多模态摘要。例如，当视觉模型检测到“楼梯”，LM可建议可能的声音（如“脚步声”），ALM对这些声音进行排名，最终整合到摘要中，如：“I am climbing a staircase and I hear footsteps.”

尽管音频信息有助于提升摘要质量，但当前ALM的鲁棒性仍不如VLM，因此多数实验仍基于视觉-语言模型。

#### **3.3-C: Compiling a Language-Based World-State History（构建语言世界状态历史）**

系统将多个关键帧的摘要整合为“语言世界状态历史”，供后续推理使用。由于直接使用全部帧会导致文本过长，系统采用两种方式压缩历史：
1. **均匀采样**：固定时间间隔采样关键帧，结合递归摘要。
2. **基于搜索的采样**：根据问题内容，通过VLM/ALM检索最相关的关键帧。

两种方法各有优劣，均匀采样更为全面，但可能错过关键事件；搜索采样更精准，但可能忽略间接相关信息。

---

### **D.3 Open-Ended Reasoning on Egocentric Video（基于第一视角视频的开放推理）**

SM框架支持多种任务，包括：

#### **(i) Summarization（摘要）**

系统可生成一天的活动摘要，如：“I slept in a bed, made coffee, worked on a computer, and drank wine.”。该功能可用于辅助记忆或护理记录。

#### **(ii) Open-ended Q&A（开放问答）**

LM可根据世界状态历史回答二分类、上下文推理或时间推理问题。例如：
- **回忆类问题**：Did I eat dinner today?
- **因果类问题**：Why did I go to the front porch?
- **时间类问题**：When did I last drink coffee?

系统还能解释答案，如：“I was seen eating a sandwich in a kitchen at 5:27 PM.”

#### **(iii) Forecasting（未来预测）**

系统可通过语言描述预测未来活动，例如：
- *4:35 PM: I am in a home and see a television.*

该方法超越了传统的分类预测，可生成更丰富的未来场景描述。

#### **(iv) Corrections（纠正）**

系统支持用户对生成摘要进行人工纠正，提升交互性。例如，用户可提供更准确的描述，如“my family, not friends”。

#### **(v) Video Search（视频检索）**

系统支持根据问题类型检索图像或音频片段，例如“where did I leave my remote control”会触发图像搜索，“what did my daughter’s laugh sound like today?”会触发音频搜索。

---


### **总结与展望**

尽管SM在多种第一视角感知任务中表现出色，但仍存在以下挑战：
- **数据质量**：依赖VLM和ALM的检测精度，错误检测可能影响最终结果。
- **采样策略**：如何高效选取关键帧仍是研究难点。
- **模型交互方式**：当前依赖人工设计的提示，未来可探索更自动化的模型协作方式。

**未来方向**包括：
- 提升检测和描述模型的精度（如使用更鲁棒的ALM）。
- 构建更丰富的第一视角数据集。
- 探索更自然的模型协作机制，如语言循环推理。

---

以上是对《Appendix D: Egocentric Perception Appendix》章节内容的全面总结，按原文结构组织，重点突出系统方法、多模态融合与推理能力，弱化了实验细节与具体数据。


## Appendix E Scaling Up Socratic Video Search

本章节讨论了如何扩展**Socratic Video Search**的搜索算法，以应对大规模视频数据的查询需求。原算法（SMs）在构建**世界状态历史**（Sec. D.2）和进行**视频检索**（Sec. D.3）时，依赖于在**潜在空间中**（如视频帧和文本片段的VLM特征）进行匹配，可以抽象为**最大点积搜索**（dot-product-maximization key search）问题。然而，当数据量较大（如长视频）时，传统的线性搜索方法计算成本过高，因此提出了三种改进方案。

---

### **MIP-Search**

**重点内容**：  
该方法借鉴了**最大内积搜索**（MIP-Search）的预处理技术，用于优化视频帧的**潜在表示**，以实现**次线性查询机制**。具体技术包括：  

- **哈希表索引**（如LSH-Hashing）：通过**二进制化表示**的哈希值对视频帧进行分组，存储在多个哈希表中。  
- **低代价哈希方法**（如带符号的随机投影）：将MIP问题转化为**最小角距离搜索**问题。  

**查询流程**：  
先在哈希表中找出最相似的哈希条目，再在这些条目对应的子集中进行**线性搜索**，从而获得最终排序结果。  

**小结**：该方法在查询效率上有所提升，但未解决**空间复杂度**问题。

---

### **Associative Memories**

**重点内容**：  
为解决内存限制问题，提出使用**线性注意力机制**（linear attention）与**连续联想记忆模型**（MCAM）结合的方法。  

- **MCAM模型**：本质上是**可微分字典**，具有**理论上可证的少样本检索能力**。  
- 它使用**能量函数**（基于潜在表示的点积）进行检索，但直接存储所有模式会导致空间爆炸。  
- 为避免显式存储，采用**FAVOR+机制**（来自线性注意力Transformer模型Performer）对能量函数进行**线性化处理**。  

**优点**：  
1. **字典大小与存储模式数无关**，仅与用于线性化的随机特征数量成线性关系。  
2. 实现**常数时间查询机制**，代价是**压缩所有模式**，可能丢掉部分信息。  

**小结**：该方法在空间效率上有显著优势，适合大规模视频数据检索，但可能存在信息损失问题。

---

### **Random Feature Trees**

**重点内容**：  
该方法融合了MIP-Search和线性注意力的思想，引入了**随机特征树**（RFT）结构。  

- RFT是一个**平衡树结构**，叶子节点对应视频帧的潜在表示，非叶子节点表示子集的特征和。  
- 通过**线性化的softmax分布**（使用FAVOR+机制）对视频帧进行采样，查询时间复杂度为**对数级**。  

**小结**：该方法在**查询速度和空间效率**之间达到了良好的平衡，适用于大规模视频检索任务。

---

### **总结**

本章节主要围绕如何高效地在大规模视频数据中检索出与文本最相关的内容。提出了三种方案：  
1. **MIP-Search**：通过哈希预处理实现次线性查询，但未解决空间问题。  
2. **Associative Memories**：结合线性注意力与MCAM模型，显著降低空间复杂度，适合内存受限场景。  
3. **Random Feature Trees**：融合MIP与线性注意力的优势，实现对数时间的高效检索。  

这三种方法在不同性能指标（如时间、空间、精度）之间进行权衡，为Socratic Models在实际应用中的扩展提供了技术支撑。


## Appendix F Additional Notes on Robot Experiments

### 系统结构概述

本系统使用了以下核心组件：

1. **视觉语言模型 (VLM)**：采用 **ViLD (Gu et al., 2021)** 进行开放词汇目标检测，对场景中的物体进行描述。
2. **语言模型 (LM)**：将VLM输出的物体描述作为上下文输入，LM作为**多步骤规划器**，接收自然语言指令并生成一系列子任务。
3. **预训练的基于语言的机器人策略**：使用 **CLIP-conditioned No-Transport 基线 (Zeng et al., 2020)**，具体实现受 **CLIPort (Shridhar et al., 2022)** 启发，用于执行“拾取-放置”等任务。
4. **伪代码生成机制**：LM生成的指令采用伪代码格式，例如 `robot.pick_and_place("A", "B")`，再转换为固定模板句子（如“Pick the A and place it on the B.”）输入机器人策略。

> ⚠️ **重点说明**：由于机器人策略的训练数据有限，LM生成的某些语言表达可能超出策略的能力范围。因此，系统通过模板化指令（与训练数据中的语句更一致）来提高执行成功率。

---

### 示例任务与完整流程

以下为系统在不同场景中执行的多个多步骤任务示例，展示了其对语言指令的理解能力、顺序推理（如顺时针/逆时针）和对多种物体的处理能力。

#### 示例任务 1：
- **物体**：["cyan block", "yellow block", "brown block", "green bowl"]
- **指令**：将所有方块移动到左上角。
- **执行步骤**：
  - Step 1. 拾起 brown block 放到左上角
  - Step 2. 拾起 cyan block 放到左上角
  - Step 3. 拾起 yellow block 放到左上角

#### 示例任务 2：
- **指令**：将黄色方块放在绿色碗里 → 然后撤销该动作
- **执行**：
  - Step 1. 拾起 yellow block 放到 green bowl
  - Step 1. 拾起 yellow block 放回左上角

> ⚠️ **重点说明**：系统支持对指令的撤销操作，说明其具备一定的**状态回滚**能力。

#### 示例任务 3：
- **指令**：堆叠方块 → 然后拆开
- **执行**：
  - Step 1. 拾起 pink block 放到 orange block 上
  - Step 2. 拾起 cyan block 放到 pink block 上
  - 后续步骤：将方块拆回原位

#### 示例任务 4：
- **指令**：将所有方块按颜色放入对应的碗中
- **执行**：
  - Step 1. 拾起 orange block 放入 orange bowl
  - Step 2. 拾起 red block 放入 red bowl
  - Step 3. 拾起 purple block 放入 purple bowl

> ⚠️ **重点说明**：系统能够理解颜色匹配等复杂指令，并依此执行多对象操作。

---

### 系统局限性

尽管系统性能良好，但也存在以下几点限制：

1. **VLM 和 CLIP 的鲁棒性问题**：
   - 由于场景是模拟的，物体并非真实自然的物品，VLD 和 CLIP 的识别能力受限。
   - 需要特定相机视角（如俯视图）、颜色限制（如红、绿、黄、蓝）以及对物体名称的优化（如将“block”称为“box”，“bowl”称为“circle”）来提升识别效果。

2. **语言生成与执行策略的匹配度**：
   - LM可能生成超出机器人策略训练集的语言表达，导致执行失败。
   - 使用固定模板指令能更好地适配策略的数据分布，从而提升成功率。

---

### 图表说明

- **图13**：展示了主论文第5.3节中完整的人机对话与机器人执行过程。
- **图14**：补充了多个系统可以**零样本执行**的多步骤任务实例，涵盖了堆叠、排序、方向控制（顺时针/逆时针）等复杂指令。

---

### 总结

本节附录详细说明了机器人系统的组成、执行流程、示例任务和系统当前的限制。该系统通过结合视觉语言模型、语言规划器和语言条件机器人策略，实现了对自然语言指令的多步骤推理与执行。尽管存在识别和语言适配方面的挑战，但其展示了**零样本控制机器人执行复杂任务的潜力**，也为未来鲁棒的视觉语言模型提供了方向。


## Appendix G Socratic Deductive Reasoning


### 概述
在**自我中心感知**（egocentric perception）的背景下，将视频问答（Q&A）建模为**情境记忆**（SMs，Situation Models）中的阅读理解问题，可以直接利用大语言模型（LLMs）进行逻辑推理的能力，这种方式将常识推理与从互联网规模数据中学到的知识相结合。

### 示例场景
文中举出一个实际例子说明系统如何进行推理：

- 8:00 AM：去杂货店买了橙汁、巧克力和面包。  
- 8:15 AM：去了加油站加油。  
- 8:30 AM：开车回家并把杂货放在厨房。  
- 8:45 AM：开始在平底锅里煎蛋。  
- 9:00 AM：狗进了厨房。  
- 9:15 AM：带狗散步。  
- 9:30 AM：狗生病了。  

**问题**：为什么狗会生病？  
**答案**：狗可能吃了不该吃的东西，比如巧克力。

要得出这个答案，系统需要**连接多个观察点**，例如狗进入厨房、杂货在厨房、杂货中有巧克力。这种推理展示了SMs在多领域信息间进行**演绎推理**的潜力。

### 研究方向与挑战
这一结果提出了两个重要的研究问题：

1. **如何更好地构建语言驱动的世界状态历史**，以捕捉更相关的信息，提高推理结果的准确性；
2. **如何通过“推理链提示”**（chain of thought prompting，Wei et al., 2022）将多步骤问题分解为更小的中间问题，从而更好地引导大模型进行推理。

**例如**：  
问题：“可能因为什么我正在劈柴？”  
答案：“可能的原因包括：需要柴火取暖、想表达某种态度、或者需要锻炼。”

每个假设可以由后续的子程序在更高分辨率上逐步探索，直到得出最终结论。

### 结论
这些研究方向为**多模态数字助理**提供了实现更高级别的实用性和分析能力的路径，意味着未来AI助手将能更智能地理解、推理和协助人类处理复杂任务。


## Appendix H Broader Impact: Energy and Resource Consumption


本节重点讨论该研究对能源及其他资源消耗的潜在影响。

### 1. **减少训练资源消耗**
本研究提出的方法可能为一种新的、高效的机器学习模型提供路径，这些模型可以在**最小的训练资源消耗**下完成构建，前提是已有**大型预训练模型**可供使用。这意味着，如果已有模型足够强大，我们可以通过其进行扩展应用，而无需消耗额外的大量计算资源。这为如何将大型预训练模型**重新定向**到各种**多模态任务**中提供了可能的解决方案。

**重点内容强调**：
- 该方法有助于**降低训练资源的消耗**。
- 如果已有大型预训练模型，可以**降低新任务所需的计算资源**。
- 为多模态任务提供**低资源消耗的模型复用方式**。

### 2. **促进低功耗推理硬件的采用**
研究中提出的“共享模块”（SMs）展示了如何利用**固定（预训练）模型**以“零样本”方式（zero-shot）解决各种应用问题。这一特性可能有助于推动新型机器学习加速器的普及，例如：
- **固定模拟电路**（如 Reuther 等 [2020]）
- **光学衍射网络**（如 Lin 等 [2018]）

这些硬件具有**更低的功耗**和**更紧凑的体积**，适合用于边缘计算和移动设备等对功耗敏感的场景。

**重点内容强调**：
- SMs 展示了**零样本推理能力**，支持多种任务。
- 这种能力有助于推动**低功耗、小体积的硬件**（如模拟/光学加速器）的发展与应用。
- 对**边缘设备和移动终端**有重要意义。

---

### **总结要点**：
- 本研究的潜在影响在于：**减少训练资源消耗**，并**促进低功耗推理硬件的使用**。
- 通过复用大型预训练模型，可以**降低计算资源需求**，并**拓展模型的多模态应用场景**。
- SMs 的提出为**固定硬件加速器**（如模拟或光学）的应用提供了理论支持和技术基础。
