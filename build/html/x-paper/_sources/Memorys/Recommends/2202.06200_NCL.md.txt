# 2202.06200_NCL: Improving Graph Collaborative Filtering with Neighborhood-enriched Contrastive Learning

* 首页: <https://arxiv.org/abs/2202.06200>
* PDF: <https://arxiv.org/pdf/2202.06200>
* 引用: 595(2025-09-13)
* 组织:
    * Renmin University of China
    * Beijing Key Laboratory of Big Data Management and Analysis Methods
* GitHub: <https://github.com/RUCAIBox/NCL>


## 总结

**背景**
* 图协同过滤方法(Graph Collaborative Filtering, GCF)面临的两个主要问题
    * 数据稀疏或噪声问题
        * 用户-物品交互数据通常稀疏或噪声较大，而基于图的方法对数据稀疏性较为敏感，难以学习可靠的表示
    * 高阶关系利用不足
        * 现有基于 GNN 的协同过滤方法依赖显式交互链接来学习节点表示，而高阶关系（如用户或物品相似性）未能被有效利用


**NCL(Neighborhood-enriched Contrastive Learning)**
* 通过显式地将潜在邻近节点纳入对比对，从而提升推荐性能。
* 从两个方面引入邻居信息
    * 结构邻居（Structural Neighbors）
        * 通过高阶路径连接的节点
        * 用 GNN 第 k 层的输出表示 k-hop 邻居，并设计结构感知的对比学习目标，将节点与其结构邻居的代表性嵌入拉近。
    * 语义邻居（Semantic Neighbors）
        * 在图中可能不直连但语义上相似的节点
        * 设计一种基于原型的对比学习目标，捕捉节点与其语义原型之间的相关性
        * 原型可视为语义邻居在表示空间中的中心点
* 核心思想
    * 不仅利用用户-物品之间的交互信息，还利用用户与用户之间、物品与物品之间的“邻居”信息来增强模型的表示学习能力。 
    * 它通过两种对比学习策略来实现：
        * 结构对比学习 (Structural Contrastive Learning)：利用图结构本身定义的“结构邻居”。
        * 原型对比学习 (Prototypical Contrastive Learning)：利用在嵌入空间中语义相似的“语义邻居”。






## Abstract


**背景与问题：**  
近年来，图协同过滤（Graph Collaborative Filtering）方法被提出作为一种有效的推荐方法，通过建模用户-项目交互图来捕捉用户的偏好。然而，这些方法在实际场景中常常受到数据稀疏性的影响，从而限制了其性能。

为了解决这一问题，已有研究尝试将对比学习（Contrastive Learning）引入图协同过滤，以增强推荐效果。然而，现有方法通常通过随机采样构建对比对，忽略了用户或项目之间的邻接关系，未能充分挖掘对比学习的潜力。

**方法提出：**  
为了解决上述问题，本文提出了一种新的对比学习方法：**Neighborhood-enriched Contrastive Learning（NCL）**。该方法通过显式地将潜在邻近节点纳入对比对，从而提升推荐性能。

具体而言，NCL 从两个方面引入邻居信息：

1. **结构邻居（Structural Neighbors）：**  
   从交互图的结构中引入用户或项目的结构邻居，并开发了一个新的结构对比目标函数，将用户（或项目）与其结构邻居作为正对比对。在实现中，用户（或项目）及其邻居的表示来源于不同图神经网络（GNN）层的输出。

2. **语义邻居（Semantic Neighbors）：**  
   假设表示相似的用户属于语义邻域，将其纳入原型对比目标函数中，以进一步挖掘语义空间中的潜在邻接关系。

**实现与实验：**  
NCL 方法可以通过 EM 算法进行优化，并且可以推广到各种图协同过滤模型中。在五个公开数据集上的大量实验表明，NCL 显著优于基线模型，特别是在 Yelp 和 Amazon-book 数据集上分别提升了 **26%** 和 **17%** 的性能。


**关键词：** 推荐系统、协同过滤、对比学习、图神经网络


## 1. Introduction

![](https://img.zhaoweiguo.com/uPic/2025/09/W34vwa.jpg)

Figure 1.Comparison of existing self-supervised learning approaches

* 现有的自监督学习方法忽略了用户（或项目）与所提出的邻里丰富的对比学习方法（我们的方法）之间的相关性。

### 背景介绍
在信息爆炸时代，推荐系统在发现用户偏好和高效提供在线服务中扮演着重要角色（Ricci 等，2011）。协同过滤（CF）是一种经典的推荐方法（Sarwar 等，2001；He 等，2017），它可以从隐式反馈（如点击、表达、交易等）中生成有效的推荐。近年来，图神经网络（GNN）被用于增强协同过滤（He 等，2020；Wang 等，2019），将交互数据建模为图（例如用户-物品交互图），并应用 GNN 学习有效的节点表示，称为图协同过滤（Graph Collaborative Filtering）。

### 现有问题
尽管近年来图协同过滤方法取得了显著成功，但仍面临两个主要问题：

1. **数据稀疏或噪声问题**：用户-物品交互数据通常稀疏或噪声较大，而基于图的方法对数据稀疏性较为敏感，难以学习可靠的表示（Wu 等，2021b）。
2. **高阶关系利用不足**：现有 GNN 基的协同过滤方法依赖显式交互链接来学习节点表示，而高阶关系（如用户或物品相似性）未能被有效利用。这类关系已被证明在推荐任务中具有重要意义（Sarwar 等，2001；Wu 等，2019；Sun 等，2019b）。

尽管已有研究尝试通过对比学习缓解数据稀疏问题（Wu 等，2021b；Yao 等，2020），但它们通常采用随机采样节点或扰动子图的方式构造对比样本，缺乏针对推荐任务设计的有意义对比学习任务。

### 提出思路
除了用户-物品的直接交互，还存在多种潜在关系（如用户相似性）对推荐任务有益。本文旨在设计更有效的对比学习方法，以在神经图协同过滤中利用这些有用关系。具体而言，我们关注**节点级别的关系**，相比图级别的关系更高效。我们将这些额外关系定义为节点的**丰富邻居（Enriched Neighborhood）**，包括：

1. **结构邻居**：通过高阶路径连接的节点。
2. **语义邻居**：在图中可能不直连但语义上相似的节点。

我们希望通过利用这些丰富邻居来提升节点表示的学习质量，即更好地刻画用户偏好或物品特征。

### 提出方法
为整合并建模丰富邻居，我们提出了一种**模型无关的对比学习框架**——**Neighborhood-enriched Contrastive Learning (NCL)**，用于推荐任务。该方法基于两种扩展邻居构建节点级别的对比学习目标。

由于节点级对比学习通常需要逐对学习，对于大规模邻域来说效率较低，因此我们为每种邻居学习一个**代表性嵌入**（representative embedding），使得每个节点的对比学习只需与两种代表性嵌入进行对比（结构或语义）。

- **结构邻居**：我们利用 GNN 第 k 层的输出表示 k-hop 邻居，并设计**结构感知的对比学习目标**，将节点与其结构邻居的代表性嵌入拉近。
- **语义邻居**：我们设计一种**基于原型的对比学习目标**，捕捉节点与其语义原型之间的相关性。原型可视为语义邻居在表示空间中的中心点。由于原型是隐含的，我们进一步采用**期望最大化（EM）算法**（Moon，1996）来推断这些原型。

### 实验验证
通过引入这些额外关系，实验结果表明，我们的方法在隐式反馈推荐任务中显著优于原始 GNN 方法，也优于现有的对比学习方法。本文的贡献可以总结为三个方面：

1. 提出了一种模型无关的对比学习框架 **NCL**，结合结构邻居和语义邻居，提升神经图协同过滤的效果。
2. 提出了为两种邻居学习代表性嵌入的方法，大幅提高算法效率。
3. 在五个公开数据集上进行了广泛实验，证明了 NCL 方法在多个竞争性基线（包括 GNN 和对比学习推荐方法）中的优越性。


## 2. Preliminary



本节主要介绍协同过滤（Collaborative Filtering, CF）的基本概念，以及基于图神经网络（Graph Neural Network, GNN）的协同过滤方法的基本框架。

### 1. 协同过滤（Collaborative Filtering, CF）简介

协同过滤是一种基础的推荐系统方法，其目标是**基于用户的历史交互行为（如点击、浏览、购买等）来推荐用户可能感兴趣的物品**。通常将用户集合记为 $\mathcal{U} = \{u\}$，物品集合记为 $\mathcal{I} = \{i\}$，用户与物品之间的交互行为可以用一个**二值矩阵 $R \in \{0,1\}^{|\mathcal{U}|\times|\mathcal{I}|}$** 表示，其中 $R_{u,i} = 1$ 表示用户 $u$ 与物品 $i$ 有过交互，否则为 0。

基于这个交互矩阵，推荐系统可以预测用户与物品之间潜在的交互关系，从而进行推荐。

### 2. 基于图神经网络的协同过滤方法

与传统的协同过滤方法不同，**基于图神经网络的方法** 将交互数据 $R$ 转换为一个**交互图 $\mathcal{G} = \{\mathcal{V}, \mathcal{E}\}$**：

- **节点集合 $\mathcal{V}$**：包含用户和物品，即 $\mathcal{V} = \mathcal{U} \cup \mathcal{I}$。
- **边集合 $\mathcal{E}$**：表示用户与物品之间的实际交互，即 $\mathcal{E} = \{(u,i) \mid u \in \mathcal{U}, i \in \mathcal{I}, R_{u,i} = 1\}$。

通过图结构，GNN 可以建模用户与物品之间的多跳关系（multi-hop relationships），从而提取更丰富的交互信息。

### 3. GNN 的表示学习过程

基于 GNN 的协同过滤方法通常分为两个阶段：

#### (1) 信息传播（Propagation）

在每一层 $l$ 中，节点 $u$ 的表示 $\mathbb{z}_u^{(l)}$ 通过其邻居节点信息进行更新：
$$
\mathbb{z}_u^{(l)} = f_{\text{propagate}}\left(\left\{\mathbb{z}_v^{(l-1)} \mid v \in \mathcal{N}_u \cup \{u\}\right\}\right)
$$

- $\mathcal{N}_u$：表示用户 $u$ 的邻居集合（在图中与其相连的物品和用户）。
- $f_{\text{propagate}}$：信息聚合函数，用于聚合邻居节点在第 $l-1$ 层的表示，生成节点 $u$ 在第 $l$ 层的表示。
- 每次传播后，节点的表示会融合其 $l$-hop 邻居的信息。

#### (2) 信息读取（Readout）

经过 $L$ 层的迭代传播后，最终的用户表示 $\mathbb{z}_u$ 由所有层的表示进行聚合：
$$
\mathbb{z}_u = f_{\text{readout}}\left([\mathbb{z}_u^{(0)}, \mathbb{z}_u^{(1)}, \ldots, \mathbb{z}_u^{(L)}]\right)
$$

- $f_{\text{readout}}$：用于将多层表示压缩成最终的用户表示。
- 初始表示 $\mathbb{z}_u^{(0)}$ 通常由用户嵌入向量 $\mathbb{e}_u$ 初始化。

**物品的表示也可通过相同方式获得。**

---

### 总结

本节重点介绍了：

1. **协同过滤的基本任务和交互矩阵的定义**，即基于用户与物品的交互数据进行推荐。
2. **基于图神经网络的协同过滤方法**：将交互数据建模为图结构，通过 GNN 捕捉用户与物品之间的复杂关系。
3. **GNN 的表示学习过程**，包括**信息传播**和**信息读取**两个阶段，用于逐步生成用户和物品的最终表示。

这些表示是后续推荐任务中的关键输入，也为更高级的模型设计奠定了基础。


## 3. Methodology

这篇论文提出了一种名为 **NCL (Neighborhood-enriched Contrastive Learning)** 的方法，旨在通过对比学习来改进基于图神经网络的协同过滤推荐系统。

![](https://img.zhaoweiguo.com/uPic/2025/09/dGQGVq.jpg)

Figure 2: Overall framework of our proposed neighborhood-enriched contrastive collaborative filtering method.

### 整体框架概述

该方法的核心思想是：**不仅利用用户-物品之间的交互信息，还利用用户与用户之间、物品与物品之间的“邻居”信息来增强模型的表示学习能力。** 它通过两种对比学习策略来实现：

1.  **结构对比学习 (Structural Contrastive Learning)**：利用图结构本身定义的“结构邻居”。
2.  **原型对比学习 (Prototypical Contrastive Learning)**：利用在嵌入空间中语义相似的“语义邻居”。

最终，模型通过一个多任务学习框架，将传统的推荐排序损失（BPR Loss）与这两个对比学习损失结合起来进行训练。

---

### 各部分详解

#### 3.1. 图协同过滤主干 (Graph Collaborative Filtering Backbone)

*   **目的**：这是模型的基础，一个标准的图神经网络（GNN），用于生成用户和物品的最终表示（embedding）。
*   **所用模型**：采用了 **LightGCN**。它的特点是移除了GNN中复杂的非线性激活和特征变换，只保留最核心的**邻域聚合（Neighborhood Aggregation）** 操作。
*   **如何工作**：
    1.  **传播（公式2）**：每一层，一个用户（或物品）的表示通过聚合其直接相连的物品（或用户）的表示来更新。公式中的 `1/sqrt(|N_u| * |N_i|)` 是一种归一化处理，防止嵌入值过大。
    2.  **读出（公式3）**：经过 L 层传播后，将每一层得到的表示（包含不同阶数的邻居信息）进行加权平均，得到最终的用户和物品表示 `z_u` 和 `z_i`。
    3.  **预测（公式4）**：通过用户和物品最终表示的内积来计算用户喜欢某个物品的预测分数 `y_hat_{u,i}`。
*   **基础损失函数（公式5）**：使用 **BPR (Bayesian Personalized Ranking) 损失**。它的思想是：对于一个用户，模型对其已交互物品（正样本）的预测分数应高于其未交互物品（负样本）的预测分数。`ℒ_BPR` 就是用来优化这个目标的。

**小结**：这一部分是推荐系统的主干，负责学习用户和物品的基本表示，并通过BPR损失学习用户-物品间的交互关系。但它忽略了用户-用户、物品-物品间的关系。

---

#### 3.2. 与结构邻居的对比学习 (Contrastive Learning with Structural Neighbors)

*   **动机**：用户很可能喜欢其“相似用户”（即邻居）喜欢的物品。但主干的BPR损失没有显式地捕捉这种“用户与用户”或“物品与物品”的关系。
*   **核心思想**：将**一个用户（物品）与其在图上的结构邻居**视为正样本对，拉近它们的表示；将其与图上的非邻居视为负样本对，推远它们的表示。
*   **如何找结构邻居**：
    *   用户-物品交互图是二分图（Bipartite Graph）。在二分图上，经过**偶数层（如第2, 4, 6...层）** GNN传播后，一个用户的表示 `z_u^(k)` 聚合了k-hop邻居的信息。由于二分图的特性，k为偶数时，这些邻居都是**用户**（例如：用户A -> 物品1 -> 用户B，用户B就是用户A的2-hop邻居）。这些用户就是当前用户的“结构邻居”。
*   **损失函数（公式6, 7, 8）**：
    *   **公式6 (`ℒ_SU`)**: 对于一个用户u，将其**最终表示 `z_u`（或视为第0层表示 `z_u^(0)`）** 与其**第k层（k为偶数）的输出表示 `z_u^(k)`** 作为正样本对。分母中，`z_u^(k)` 要与批次中所有其他用户的第0层表示计算相似度作为负样本。
    *   **公式7 (`ℒ_SI`)**: 对物品侧进行同样的操作。
    *   **公式8 (`ℒ_S`)**: 总的结构对比损失是用户和物品损失的加权和。

**小结**：这部分利用图结构，显式地定义了邻居关系，并通过对比学习让模型学习到“相似的用户/物品应该有相似的表示”这一特性。

---

#### 3.3. 与语义邻居的对比学习 (Contrastive Learning with Semantic Neighbors)

*   **动机**：结构邻居可能存在噪声（比如两个用户偶然点击了同一个热门物品，但他们兴趣并不相似）。此外，有些语义上很相似的用户/物品可能在图上距离很远（没有直接或间接的连接），无法被结构对比学习捕捉到。
*   **核心思想**：引入“**原型**”来代表一组语义相似的节点（用户或物品）。将一个用户（物品）与其所属的**原型（聚类中心）** 视为正样本对。
*   **如何找语义邻居（原型）**：
    *   使用 **K-means 聚类算法** 对所有用户的最终表示 `e_u` 进行聚类。每个聚类中心 `c_i` 就是一个“用户原型”，代表了一类具有相似兴趣的用户。同理对物品进行聚类得到物品原型。
*   **损失函数（公式10, 11, 12）**：
    *   **公式10 (`ℒ_PU`)**: 对于一个用户u，将其表示 `e_u` 与其所属的原型 `c_i` 作为正样本对。分母中，`e_u` 要与所有其他用户原型计算相似度作为负样本。
    *   **公式11 (`ℒ_PI`)**: 对物品侧进行同样的操作。
    *   **公式12 (`ℒ_P`)**: 总的原型对比损失是用户和物品损失的加权和。
*   **优化挑战**：聚类过程（E步）不可微，无法直接端到端训练。
*   **解决方案**：使用 **EM算法** 进行优化。
    *   **E步（期望步）**：固定模型参数，用K-means对当前所有用户/物品的表示进行聚类，分配原型（`Q(c_i|e_u)`）。
    *   **M步（最大化步）**：固定原型分配，优化模型参数（通过最小化 `ℒ_P`）来更新用户/物品的表示，使得同一簇内的表示更接近原型。

**小结**：这部分超越了图结构的限制，在语义空间（嵌入空间）中挖掘更深层次的邻居关系，能够缓解数据稀疏和噪声问题，捕捉到更多潜在的相似性。

---

#### 3.4. 优化 (Optimization)

*   **总损失函数（公式13）**：
    `ℒ = ℒ_BPR + λ1 * ℒ_S + λ2 * ℒ_P + λ3 * ||Θ||_2`
    *   这是一个**多任务学习**框架。
    *   `ℒ_BPR` 是主任务损失，负责学习用户-物品交互。
    *   `ℒ_S` 和 `ℒ_P` 是辅助的对比学习损失，负责学习更好的用户/物品表示。
    *   `λ1`, `λ2`, `λ3` 是超参数，用于平衡不同损失项的重要性。

---

#### 3.5. 讨论 (Discussion)

*   **创新点**：首次在图协同过滤中**同时**利用了结构邻居和语义邻居。
*   **与之前工作的区别**：
    1.  **结构对比**：不同于其他图对比学习工作通常对图进行增广（如随机丢弃边、节点）来创造正样本，NCL直接利用GNN中间层产生的表示作为正样本，**更高效**（无需额外图增广计算和存储）。
    2.  **原型对比**：不同于其他领域（CV）将原型视为独立实例的聚类，NCL的原型代表的是**具有相似交互行为**的用户/物品组，更符合推荐系统的任务特性。
*   **复杂度**：时间和空间复杂度都与节点数量N成线性关系，是高效的。

### 总结

NCL方法通过一个精巧的多任务学习框架，将传统的BPR损失与两种新颖的对比学习损失相结合：

1.  **结构对比损失**：让模型从**图连接结构**中学习邻居关系。
2.  **原型对比损失**：让模型从**数据分布（语义）** 中学习邻居关系。

两者相辅相成，共同作用，学习到更丰富、更鲁棒的用户和物品表示，从而最终提升推荐的准确性。它有效地解决了协同过滤中常见的数据稀疏性和噪声问题。

## 4. Experiments


本节对所提出的 **NCL（Neighborhood-enriched Contrastive Learning）** 方法进行广泛实验验证，评估其在图协同过滤任务中的有效性。实验部分分为几个主要部分，包括实验设置、整体性能分析以及对NCL方法的进一步分析。

---

### 4.1 实验设置

#### 4.1.1 数据集

为了全面评估NCL的性能，作者在五个公开数据集上进行了实验，分别是：**MovieLens-1M（ML-1M）**、**Yelp**、**Amazon Books**、**Gowalla** 和 **Alibaba-iFashion**。这些数据集在领域、规模和稀疏性上各有不同，具有代表性。对于Yelp和Amazon Books，为了保证数据质量，作者过滤掉了交互次数少于15的用户和物品。

作者将每个数据集的交互分为训练集（80%）、验证集（10%）和测试集（10%），并为每个正样本随机采样一个负样本构成训练集。

| 数据集       | 用户数    | 物品数    | 交互数      | 稠密度     |
|--------------|-----------|-----------|-------------|------------|
| ML-1M        | 6,040     | 3,629     | 836,478     | 0.03816    |
| Yelp         | 45,478    | 30,709    | 1,777,765   | 0.00127    |
| Books        | 58,145    | 58,052    | 2,517,437   | 0.00075    |
| Gowalla      | 29,859    | 40,989    | 1,027,464   | 0.00084    |
| Alibaba      | 300,000   | 81,614    | 1,607,813   | 0.00007    |

#### 4.1.2 对比模型

作者将NCL与以下基线方法进行比较：

- **BPRMF**：基于矩阵分解的协同过滤方法。
- **NeuMF**：使用多层感知机（MLP）学习用户和物品的匹配函数。
- **FISM**：基于物品的协同过滤方法。
- **NGCF**：使用图神经网络（GNN）学习高阶关系。
- **Multi-GCCF**：扩展NGCF，在用户和物品之间的高阶关系中传播信息。
- **DGCF**：通过解耦表示学习进行推荐。
- **LightGCN**：简化GNN结构，提升推荐性能。
- **SGL**：自监督学习方法，通过对比学习增强推荐效果。

#### 4.1.3 评估指标

使用两个广泛采用的Top-N推荐指标：**Recall@N** 和 **NDCG@N**，其中N分别设置为10、20和50。作者采用**全排序策略**（Full-ranking strategy）评估所有未交互的候选物品。

#### 4.1.4 实现细节

所有模型均在**RecBole**框架下实现。模型优化使用Adam优化器，嵌入维度设为64，批量大小设为4096。超参数（如对比学习中的温度τ、邻域系数α）通过网格搜索确定。为防止过拟合，使用早停法（patience设为10），并以**NDCG@10**为验证指标。

---

### 4.2 整体性能分析

从实验结果（表2）可以看出，NCL在所有五个数据集上均表现优于现有方法，尤其在**稀疏数据集**（如ML-1M、Yelp）中效果更佳。主要观察点如下：

1. **GNN基方法优于传统方法**：NGCF、LightGCN等图神经网络方法在大多数数据集上表现出色，尤其是**LightGCN**在多个指标中表现最佳，表明其简化结构在推荐任务中具有鲁棒性。

2. **自监督方法SGL表现突出**：对比学习方法SGL在所有数据集上均优于其他监督模型，但其仅考虑原始图和增强图之间的对比，忽略了其他潜在关系（如用户相似性）。

3. **NCL优于所有基线方法**：NCL在所有指标上均优于现有方法，尤其在**Top-10推荐**中提升最显著，说明其能更有效地将相关物品排在前列，具有现实推荐意义。此外，NCL在稀疏数据集上的提升更明显，这归功于其引入的**邻域增强对比学习**机制。

---

### 4.3 NCL的进一步分析

为了更深入分析NCL的有效性，作者从多个角度进行了实验，包括消融实验、数据稀疏性影响、邻域类型影响、温度和原型数量等。

#### 4.3.1 消融实验

NCL利用**结构邻域**和**语义邻域**两种类型的信息。作者通过消融实验验证两种邻域的作用。结果显示，去掉任一种邻域都会导致性能下降，但两种邻域**互为补充**，共同提升了推荐效果。

#### 4.3.2 数据稀疏性影响

作者按用户交互数量将用户分为5组，分别评估NCL和LightGCN的性能。结果显示，随着交互数的减少（即数据越稀疏），NCL的提升越明显，说明其**有效缓解了稀疏性问题**。

#### 4.3.3 结构邻域的影响

结构邻域对应于GNN的不同跳数（1-hop、2-hop、3-hop）。结果表明，**1-hop邻居**表现最佳，符合直觉（用户或物品与其直接邻居更相似）。此外，NCL在效率和效果之间达到了良好平衡。

#### 4.3.4 系数α的影响

α用于平衡结构邻域建模中的两个损失函数。实验发现，当α约为1时，性能最佳，说明**用户和物品的高阶相似性对推荐有帮助**。

#### 4.3.5 温度τ的影响

温度τ在对比学习中起关键作用。实验发现，τ值太大会导致性能下降，且在稀疏数据集（如Yelp）上应使用较小的τ值。

#### 4.3.6 原型数量k的影响

通过调整原型数量k，验证了**语义邻域**对推荐性能的提升。结果显示，k约为1000时效果最佳，且k=0时性能显著下降，说明语义邻域对推荐至关重要。

#### 4.3.7 在不同GNN架构上的应用

NCL是模型无关的，作者将其应用于NGCF、DGCF和LightGCN上。结果显示，NCL在所有模型上均带来性能提升，其中在LightGCN上提升最明显，可能与其**同空间输出结构**有关。

#### 4.3.8 表征可视化

通过可视化项嵌入的分布，作者发现NCL学习到的表示更**均匀**，而LightGCN的表示聚集在几个簇中。这表明NCL的对比学习机制有助于学习更具判别力的表示，从而更好地建模用户和物品的多样性。

---

### 总结

本节通过多个实验验证了NCL方法在图协同过滤中的有效性。NCL通过引入**结构和语义邻域的对比学习机制**，在多个数据集上均优于现有方法，尤其是在**稀疏场景**中表现更佳。消融实验和参数分析进一步验证了其设计的合理性，且NCL具有良好的模型通用性，适用于多种GNN架构。


## 5. Related work


### 核心内容概括

作者首先介绍了这两个领域的背景和发展，然后分别指出了它们目前存在的**局限性**，最后自然地引出了自己论文的**创新点**：**将对比学习与用户/物品间的潜在邻居关系相结合，以解决现有方法的不足**。


### 分部分详解

#### 1. 基于图的协同过滤 (Graph-based collaborative filtering)

*   **是什么？** 这是一种推荐系统技术。与传统方法（如矩阵分解、自编码器）不同，它把用户和物品的交互数据（如点击、购买）组织成一张图（Graph），然后从图结构信息中学习用户和物品的表示。
*   **发展历程：**
    1.  **早期**：使用随机游走（Random Walks）来提取图结构信息。
    2.  **现在主流**：使用图神经网络（GNN），例如NGCF和LightGCN。它们利用交互图中的高阶关系（例如，朋友的朋友喜欢的物品）来提升推荐效果。
    3.  **进一步**：有些研究通过构建更多的交互图来捕捉更丰富的关联关系。
*   **存在的局限性（缺点）：**
    *   **没有显式解决数据稀疏问题**：用户-物品的交互数据通常非常稀疏（一个用户只和极少物品有交互），这会影响模型的学习效果。
    *   **只关注交互记录，忽略了用户与用户之间、物品与物品之间潜在的“邻居关系”**：例如，两个用户可能因为喜好非常相似而成为“邻居”，即使他们没有直接交互过同一个物品。现有方法没有充分利用这种信息。

#### 2. 对比学习 (Contrastive learning)

*   **是什么？** 一种自监督学习技术，核心思想是通过比较正样本对（相似的）和负样本对（不相似的）来学习数据表征。它在计算机视觉（CV）、自然语言处理（NLP）和图数据挖掘等领域取得了巨大成功。
*   **在图领域的应用分类：**
    *   **节点级对比学习**：比较图中不同节点之间的关系。例如GRACE方法，通过随机删除边或掩盖节点特征来生成对比样本。
    *   **图级对比学习**：比较整张图之间的关系。例如MVGRL方法，使用图扩散来变换图形。
    *   **原型对比学习**：受计算机视觉启发，通过捕捉“原型”（一类数据的聚类中心）来进行对比，以学习图中的语义信息。
*   **在推荐系统中的应用：** 例如SGL方法，就将对比学习用在了基于图的推荐中。
*   **存在的局限性（缺点）：**
    *   **现有方法通常通过随机采样来构建对比样本对**。这种方式没有针对性，可能不是最优的。
    *   **没有充分考虑推荐场景中用户之间或物品之间特有的关系**。在推荐系统中，用户和物品的相似性是有具体业务含义的，随机采样可能无法有效利用这种领域知识。

### 作者如何引出自己的研究

在分别指出两个领域的不足之后，作者巧妙地建立了自己工作的出发点：

1.  **问题一**：图推荐方法**忽略了用户/物品的潜在邻居关系**。
2.  **问题二**：对比学习方法在推荐中**随机构建样本对，没有利用好这些关系**。

**因此，作者的创新点就非常明确了：**
> “在本文中，我们提出通过对比学习来**显式地建模这些（用户/物品的）潜在邻居关系**。”

这意味着他们的方法不是随机生成对比样本，而是有目的地利用用户之间、物品之间的相似性（邻居关系）来构建更有效的对比样本对，从而更好地解决数据稀疏性问题，提升推荐性能。


## 6. Conclusion And Future Work



### 总结
本研究提出了一种新颖的对比学习范式，称为**Neighborhood-enriched Contrastive Learning (NCL)**，旨在**显式地将节点的相关性纳入图协同过滤的对比学习中**。NCL 从两个方面考虑用户（或物品）的邻居：**图结构层面**和**语义空间层面**。

1. **图结构层面的邻居**：通过开发一种新的结构对比目标函数，将结构邻居纳入基于图神经网络（GNN）的协同过滤方法中。这一部分是**本章的重点内容**，强调了如何利用交互图的结构信息提升模型性能。

2. **语义空间层面的邻居**：通过聚类用户/物品的嵌入向量，得到原型（prototype），并将语义邻居纳入原型对比目标函数中。这部分也体现了 NCL 对语义信息的**有效利用**。

通过在五组公开数据集上的广泛实验，验证了所提出 NCL 方法的有效性和优越性。

### 未来工作
未来的研究方向包括以下两个方面：

- **扩展推荐任务**：将 NCL 框架推广到其他类型的推荐任务中，例如**序列推荐**。
- **统一邻居利用机制**：探索一种**更统一的公式化方法**，以更好地融合和利用不同类型的邻居信息。

### 致谢
本研究得到了以下基金项目的支持：

- 国家自然科学基金项目（项目号：61872369 和 61832017）
- 北京卓越青年科学家计划（项目号：BJJWZYJH012019100020098）
- 北京人工智能研究院（BAAI）的支持

**通讯作者**为**Xin Zhao**。


## Appendix A Pseudo-code for NCL

![](https://img.zhaoweiguo.com/uPic/2025/09/JaaBeO.jpg)

Algorithm 1: Neighborhood-enriched Constrastive Learning (NCL)

### **输入（Input）**
- **二部图**：$\mathcal{G} = \{\mathcal{U} \cup \mathcal{I}, \mathcal{E}\}$，表示用户（$\mathcal{U}$）和物品（$\mathcal{I}$）之间的交互关系。
- **训练数据集**：$\mathcal{X}$，用于模型训练的用户-物品交互数据。
- **聚类数量**：$K = \{k_m\}_{m=1}^{2M}$，表示对用户和物品分别进行聚类的数量，其中前 $M$ 个用于用户聚类，后 $M$ 个用于物品聚类。
- **学习率**：$\alpha$，用于梯度下降优化。

### **输出（Output）**
- **用户与物品的表示**：$\{ \mathbb{z}_u, \mathbb{z}_i \}$，经过图卷积和学习后的嵌入表示。

---

### **算法流程**

#### **1. 初始化（Initialize）**
- 随机初始化用户嵌入 $\mathbb{e}_u$ 和物品嵌入 $\mathbb{e}_i$。

#### **2. 迭代过程（While Not Convergence）**
迭代直至模型收敛，包括**E-step**和**M-step**两部分。

---

### **E-step（Expectation Step）**
- **作用**：基于当前的嵌入表示 $\mathbb{e}_u$ 和 $\mathbb{e}_i$，进行聚类，得到用户和物品的聚类中心（原型）。

- **具体操作（for $m=1$ to $M$）**：
  - 对用户嵌入 $\mathbb{e}_u$ 进行 $k_m$-means 聚类，得到用户原型 $\mathbb{c}^u_k$。
  - 对物品嵌入 $\mathbb{e}_i$ 进行 $k_{m+M}$-means 聚类，得到物品原型 $\mathbb{c}^i_k$。

> **重点**：这里通过聚类的方式学习用户和物品的类别中心，作为后续对比学习的参考锚点。

---

### **M-step（Maximization Step）**
- **作用**：基于当前聚类中心进行图卷积和模型更新。

- **具体操作**（通过数据加载器加载小批量数据）：
  - 使用图卷积操作 $\text{GraphConv}(\mathcal{G}, \mathbb{e}_u, \mathbb{e}_i)$，得到用户和物品的表示 $\mathbb{z}_u$ 和 $\mathbb{z}_i$。
  - 计算损失函数 $\mathcal{L}(\mathbb{z}_u, \mathbb{z}_i, \mathbb{c}^u, \mathbb{c}^i)$，该损失函数用于衡量嵌入与聚类中心之间的关系（如对比学习）。
  - 使用梯度下降更新嵌入表示：
    - $\mathbb{z}_u = \mathbb{z}_u - \alpha \frac{\partial\mathcal{L}}{\partial \mathbb{z}_u}$
    - $\mathbb{z}_i = \mathbb{z}_i - \alpha \frac{\partial\mathcal{L}}{\partial \mathbb{z}_i}$

> **重点**：通过反向传播优化嵌入表示，使得嵌入更符合聚类中心的分布，是模型训练的核心部分。

---

### **最终输出**
- 在模型收敛后，再次通过图卷积 $\text{GraphConv}(\mathcal{G}, \mathbb{e}_u, \mathbb{e}_i)$ 得到最终的嵌入表示 $\mathbb{z}_u$ 和 $\mathbb{z}_i$。
- 返回最终的嵌入表示。

---

### **总结**
- **算法结构清晰**：整个算法采用经典的 E-M 分步迭代结构。
- **核心思想**：
  - 通过 **k-means 聚类** 获取锚点（原型）；
  - 通过 **图卷积** 学习用户与物品的表示；
  - 通过 **对比学习损失** 优化表示，使得嵌入更符合聚类结构。
- **关键步骤**：
  - 图卷积（GraphConv）；
  - 对比损失计算；
  - 梯度下降更新嵌入。

---

此伪代码为 **Neighborhood-enriched Contrastive Learning (NCL)** 提供了完整的实现流程，重点在于利用聚类和图结构信息进行用户与物品表示学习。


## Appendix B Case Study on Selected Neighbors

![](https://img.zhaoweiguo.com/uPic/2025/09/wMO7Ll.png)

Figure 7.Case study of the contrastive items sampled from random and proposed structural and semantic neighbors.


为了进一步分析结构邻居（structural neighbors）和语义邻居（semantic neighbors）之间的差异，作者在 **Alibaba-iFashion 数据集** 中随机选择了一个中心物品（central item），并分别提取了其结构邻居和语义邻居。

对于这两种类型的邻居，作者统计了每个类别中的物品数量，并对结果进行了归一化处理，最终在 **图 7** 中进行了可视化。同时，为了进行对比，作者还报告了随机采样的物品集合。

从图中可以观察到，**随机采样的邻居是不可控的**，这种无序性限制了对比学习（contrastive learning）的潜力。相比之下，**所提出的结构邻居和语义邻居更加相关**，更适合作为对比学习中的正样本对（contrastive pairs）。

### 重点总结：
- **研究目的**：比较结构邻居与语义邻居在对比学习中的适用性。
- **方法**：在真实数据集中随机选取一个中心物品，分别提取结构邻居和语义邻居，并进行可视化。
- **结果**：
  - 随机采样的邻居缺乏相关性，不利于对比学习；
  - 所提出的邻居类型（结构和语义）相关性更强，更适合作为对比样本。

### 图 7 简介：
图 7 展示了从随机邻居、结构邻居和语义邻居中采样的对比样本，清晰地说明了三种方法在邻居相关性方面的差异。

（图示略，见原文）
