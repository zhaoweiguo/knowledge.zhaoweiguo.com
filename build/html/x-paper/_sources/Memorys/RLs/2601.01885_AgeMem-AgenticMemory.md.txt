# 2601.01885_AgeMem: Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents

* 首页: <https://arxiv.org/abs/2601.01885>
* PDF: <https://arxiv.org/pdf/2601.01885>
* 引用: 
    * 0(2026-01-09)
* 组织: 
    * 1Alibaba Group,
    * 2School of Cyber Science and Engineering, Wuhan University


## 总结

**总结**
* 里面有几个 prompt
* 里面有几个公式

**关键内容**
* 架构层面的创新：
    * 把长期记忆和短期记忆操作封装成 Tool接口，接合 RL 实现统一管理，使基于LLM的智能体能够通过工具化的操作，自主决策何时及如何管理其长短期记忆
* 训练方法上的创新：
    * 使用三阶段渐进式强化学习策略，该训练策略与步进式GRPO算法相结合，有效解决了长周期任务中因记忆操作导致的稀疏奖励和信用分配难题。
* 范式的转变：
    * 记忆管理从系统设计者的责任，转变为智能体自身可以通过经验掌握的核心认知能力
* 未来可能的研究方向：
    - 扩展工具集：探索更丰富、更细粒度的记忆操作工具（如更复杂的记忆关联、优先级设置或基于置信度的存储策略等），以支持更复杂的智能体行为
    - 扩大评估范围：在更多样化的任务和场景中验证框架的通用性，进一步巩固其经验基础。
    - 探索更高效的学习范式：可以研究如何减少对大量交互数据的需求，例如通过模仿学习或课程学习来初始化策略，以降低训练成本。


![](https://img.zhaoweiguo.com/uPic/2026/01/gJW9ZN.jpg)

Figure 1: Comparison between independent and unified memory management frameworks

* 图解
    *  (Left) Traditional framework with static STM and trigger-based LTM. 
    *  (Middle) Independent framework with an additional Memory Manager controlling LTM in an agent-based manner, while STM remains static. 
    *  (Right) The proposed AgeMem framework, where LTM and STM are jointly and intelligently managed via explicit tool-based operations.

    * 左图：传统框架（静态STM + 触发式LTM）
        - 工作方式：
          - 短时记忆（STM）：是静态的。它仅包含当前的对话上下文，模型被动地使用这些信息，没有主动管理（如总结、过滤）的能力。
          - 长时记忆（LTM）：管理是触发式的。记忆的存储、更新或检索操作由预定义的规则或启发式算法在特定时刻触发（例如，在对话结束时自动保存）。
        - 核心缺陷：LTM和STM是松散耦合的模块，各自独立优化。STM缺乏智能，而LTM的操作是僵化的，无法根据任务需求灵活调整。
    * 中图：独立框架（静态STM + 基于智能体的LTM）
        - 工作方式：
          - 短时记忆（STM）：仍然是静态的，与左图相同，模型无法主动管理上下文。
          - 长时记忆（LTM）：管理是基于智能体的。系统引入了一个额外的、专门的记忆管理器（通常是一个小型的辅助LLM），由它来决策何时以及如何操作LTM（如存储什么、检索什么）。
        - 进步与局限：相比左图，LTM的管理更加灵活和智能。然而，STM的管理依然原始，且两个记忆系统仍然是分离的。依赖外部记忆管理器也增加了系统的复杂性和推理成本。
    * 右图：本文提出的AgeMem框架（统一的记忆管理）
        - 工作方式：
          - 核心创新：将LTM和STM的管理直接集成到主智能体的决策策略中，实现了统一和联合的管理。
          - 实现机制：通过一套基于工具的接口，将记忆操作（如ADD, UPDATE, RETRIEVE, SUMMARY等）暴露给主LLM智能体。智能体可以像调用其他工具一样，在推理过程中自主决定何时调用哪个记忆操作，来管理何种信息。
          - 智能体角色：智能体不再只是一个被动的推理者，而是一个主动的记忆管理者。它可以为了最终任务目标，协同地管理短期上下文和长期知识。
        - 优势：打破了LTM和STM之间的壁垒，使得记忆管理能够进行端到端的优化。智能体可以学习到更自适应、更高效的内存使用策略。
    * 总结
        - 从左到右的对比，清晰地展示了记忆管理范式的演进
        - 从“规则驱动”到“学习驱动”：从依赖预定义规则（左图）到由智能体学习决策（右图）。
        - 从“分离”到“统一”：将两个独立的记忆系统整合为一个协同工作的整体。
        - 从“被动”到“主动”：智能体从被动使用记忆变为主动、自主地管理记忆。


**背景：长时记忆管理**
* 现有方法分类
    1. 基于触发器的范式：如Mem0及其图变体Mem0g，它们依赖于预定义的、可扩展的“提取-更新”管道。这类方法通常有固定的规则来决定何时存储或更新记忆。
    2. 基于智能体的范式：如A-Mem，它引入了一个专门的“记忆管理器”智能体来做出记忆决策，提供了更多的灵活性。
* 局限性
    - 依赖启发式规则或外部控制器：记忆的更新策略往往是硬编码的或由外部模块决定，而非由主智能体自主、自适应地学习。
    - 与STM管理脱节：这些LTM系统通常被设计为独立模块，与短时记忆的管理是分离的，缺乏统一的优化。

**背景：短时记忆管理**
* 现有方法分类
    - 主导范式——RAG：检索增强生成 是当前最主要的方法。它通过从外部知识源检索信息并将其注入提示中来扩展上下文。论文提到了MainRAG等工作。
    - 进阶方法——上下文压缩：为了应对长上下文带来的噪声和成本问题，出现了如ReSum等方法，它通过定期将对话历史总结压缩成紧凑的推理状态来控制上下文长度。
* 局限性：
  - 被动性与刚性：RAG的检索往往是自动触发的，而非由智能体主动控制；ReSum的总结节奏也通常是预定义的。
  - 可能引入噪声：检索到的信息可能不相关，成为干扰项。
  - 与LTM管理分离：STM管理策略同样没有与LTM的构建和更新进行联合优化。

**背景：强化学习**
* GRPO通过比较一个批次内不同轨迹的相对质量来优化策略，无需显式的价值函数，提高了稳定性。


**本文创新**
* 创建一个统一的框架，使智能体能够通过强化学习，自主地、联合地学习如何管理其全部记忆（包括LTM和STM）
* 记忆管理工具接口
    - LTM工具：包括ADD（添加）、UPDATE（更新）、DELETE（删除），用于对持久化知识库进行增、改、删操作。
    - STM工具：包括RETRIEVE（从LTM中取回信息到上下文）、SUMMARY（总结压缩上下文）、FILTER（过滤上下文中的无关信息）。
    - 设计意义：此设计将记忆管理从系统预设的流水线，转变为智能体内部可自主决策的、工具化的原子操作，为后续的强化学习优化奠定了基础。

**三阶段渐进式强化学习策略**
1. 阶段一：LTM构建
  - 目标：学习识别并存储有价值的信息到长时记忆中。
  - 设置：智能体在休闲对话中接触任务背景信息 Iq。此时，任务目标 q尚未揭示。
  - 训练重点：智能体学习在不知道最终任务的情况下，判断哪些信息具有长期价值，并调用ADD等工具将其存入LTM。此阶段结束后，STM被重置，但LTM得以保留，这迫使智能体在后续阶段必须依赖LTM。
2. 阶段二：STM控制
  - 目标：学习在干扰环境下管理短时上下文。
  - 设置：STM被清空，智能体开始接收大量语义相关但实际无关的干扰信息。
  - 训练重点：智能体学习调用FILTER和SUMMARY工具，主动地过滤噪声、压缩信息，以保持上下文的清洁和高效，为复杂推理做准备。
3. 阶段三：集成推理与记忆协调
  - 目标：学习综合运用前两个阶段技能，解决最终任务。
  - 设置：智能体收到正式的任务查询 q。
  - 训练重点：智能体必须协调所有能力：从LTM中RETRIEVE相关知识，在推理中动态管理STM，最终生成答案。最终的任务奖励在此阶段产生，并用于评估整个轨迹的优劣。

* 优势
    * 它将复杂的记忆管理任务分解，让智能体先专注练习基本功（存、取），
    * 再在完整任务中练习综合运用，极大地降低了学习难度，并保证了LTM操作能获得必要的训练信号。

**步进式GRPO算法**
* 为了将最终任务成功的奖励有效地传递到早期（尤其是阶段一）的记忆决策上，论文改进了GRPO算法，提出了步进式GRPO。
- 挑战：在阶段一存储一条信息，其价值要到阶段三才能体现。这种奖励是稀疏和延迟的。
- 解决方案：
  1. 分组与归一化：对同一任务进行K次独立试验，形成一个“组”。计算组内最终奖励的均值 μGq和标准差 σGq。
  2. 计算终端优势：对每条轨迹，其最终奖励被归一化为优势值：AT(k,q)=σGq+ϵrT(k,q)−μGq。这个值衡量了该轨迹在组内的相对表现。
  3. 优势广播：这是“步进式”的关键。计算出的终端优势值 AT(k,q)会被赋值给该轨迹中的每一个时间步，即 At(k,q)=AT(k,q),∀t。




## From Moonlight


### 三句摘要

1. 🧠 本文提出了Agentic Memory (AgeMem) 框架，通过将长期记忆 (LTM) 和短期记忆 (STM) 管理直接整合到大型语言模型 (LLM) 智能体的策略中，并将其暴露为工具操作，使其能够自主管理信息。
2. 🚀 为训练这种统一的记忆管理行为，研究者设计了一种三阶段渐进式强化学习策略，并引入了step-wise GRPO来解决记忆操作导致的稀疏和不连续奖励问题。
3. 🏆 实验结果表明，AgeMem在五个长期基准测试中持续超越了现有基线，显著提升了任务性能、长期记忆质量并优化了上下文使用效率。

### 关键词

- Agentic Memory: AgeMem 是一个统一的框架，旨在将大型语言模型 (LLM) 代理的长期记忆 (LTM) 和短期记忆 (STM) 管理直接整合到代理的决策策略中。它通过将记忆操作（如存储、检索、更新、总结或丢弃信息）公开为代理可调用的工具（tools）来实现这一点，从而使 LLM 代理能够自主地学习何时、何地以及如何执行这些记忆管理动作。这种统一的方法旨在克服现有系统中 LTM 和 STM 分离带来的局限性，从而实现端到端的优化和更灵活的记忆控制。
- Long-Term Memory (LTM): 长期记忆（LTM）指的是代理能够持久存储用户或任务特定的知识和信息，以便在较长时间跨度内进行访问和利用。它区别于当前对话的即时上下文，提供了持续积累的知识库。LTM 对于需要记忆大量历史信息、用户偏好或领域知识的任务至关重要，能够支持代理在多次交互甚至跨会话进行连贯且有上下文感知的推理。
- Short-Term Memory (STM): 短期记忆（STM）是指代理当前输入上下文中包含的信息，通常指当前对话的最近消息或当前提示（prompt）中的内容。STM 的容量受到 LLM 有限上下文窗口的严格限制。有效的 STM 管理对于防止信息丢失、减少冗余并保持当前任务相关的关键细节至关重要，尤其是在处理长对话或复杂任务时，需要通过检索、总结或过滤等机制来优化其内容。
- Unified Memory Management: 统一记忆管理是指将代理的长期记忆 (LTM) 和短期记忆 (STM) 管理作为一个整体进行协调和优化的方法。与传统方法将 LTM 和 STM 分别处理（例如，LTM 基于触发器，STM 依赖 RAG）不同，统一管理旨在使代理能够以集成的、端到端的方式自主决定如何存储、检索、更新、总结或丢弃信息，从而实现 LTM 和 STM 之间的协同工作，以最大化代理在长远推理任务中的性能。
- Reinforcement Learning (RL): 强化学习（RL）是一种机器学习方法，代理通过与环境互动来学习做出决策，目标是最大化累积奖励。在该论文中，RL 被用于训练 LLM 代理自主管理其 LTM 和 STM。代理通过执行动作（包括语言生成和记忆操作工具）来与环境交互，并根据任务性能和记忆管理质量获得奖励信号，从而逐步学习最优的记忆策略。
- Progressive Reinforcement Learning Strategy: 三阶段渐进式强化学习策略是本文提出的一种用于训练 AgeMem 框架的策略。该策略将训练过程分解为三个连续的阶段：第一阶段侧重于 LTM 的构建；第二阶段通过引入干扰信息来训练 STM 的控制能力（如过滤和总结）；第三阶段则要求代理在 LTM 和 STM 协同工作的情况下完成最终任务。这种分阶段的方法有助于逐步学习和协调复杂的记忆管理行为，从而克服单一阶段训练的困难。
- Step-wise GRPO: Step-wise GRPO（Group Relative Policy Optimization）是本文为解决强化学习中稀疏和不连续奖励问题而设计的一种算法变体。它通过将整个轨迹（包括所有三个阶段）的最终奖励传播到每个时间步，为每个中间的记忆决策提供一致的学习信号。这种方法能够有效地将远期任务成果与早期记忆操作联系起来，实现长距离信用分配，从而改进策略优化，尤其适用于 AgeMem 这种涉及多阶段、操作离散的场景。
- Tool-based Actions: 基于工具的动作是指将特定的功能（如记忆操作）封装成一个一个的“工具”，LLM 代理在决策时可以直接调用这些工具来执行相应的任务。在 AgeMem 中，ADD, UPDATE, DELETE, RETRIEVE, SUMMARY, FILTER 等记忆管理操作被实现为工具，代理可以通过输出结构化的工具调用来执行这些操作。这种机制将记忆管理从外部依赖转化为代理内在能力的一部分，使其能够自主地学习和执行这些动作。
- Context Window: 上下文窗口是指大型语言模型（LLM）在处理输入时一次能够考虑的最大文本量。LLM 的上下文窗口大小是有限的，这限制了它们在长对话或处理大量信息时的能力。AgeMem 框架通过有效的 STM 管理（如过滤和总结）旨在缓解这一限制，确保代理能在有限的上下文窗口内处理和利用关键信息，从而进行长远推理。
- Retrieval-Augmented Generation (RAG): 检索增强生成（RAG）是一种常用的技术，通过从外部知识库中检索相关信息并将其注入到 LLM 的提示（prompt）中，来增强 LLM 的知识和推理能力。在本文中，RAG 被用作一种与 AgeMem 的基于工具的 STM 管理方法进行比较的基线。虽然 RAG 有助于扩展上下文，但它通常不如 AgeMem 中通过学习到的工具进行精细控制的 STM 管理那样灵活和高效。
- Memory Operations: 记忆操作是指代理为管理其长期记忆 (LTM) 和短期记忆 (STM) 而执行的具体动作。在 AgeMem 框架中，这些操作被抽象为一组工具，包括：ADD (添加 LTM), UPDATE (更新 LTM), DELETE (删除 LTM), RETRIEVE (从 LTM 检索至 STM), SUMMARY (总结 STM 内容), 和 FILTER (过滤 STM 内容)。这些操作使代理能够动态地、有策略地处理和维护其记忆系统。
- Long-Horizon Reasoning: 长远推理是指需要代理在多个时间步、处理大量信息并进行复杂多步推理才能完成的任务。这类任务的挑战在于 LLM 有限的上下文窗口和信息遗忘问题。AgeMem 框架旨在通过其统一的、可学习的记忆管理机制，提升 LLM 代理在长远推理任务中的表现，使其能够有效积累、维护和利用信息，从而进行更具连贯性和深度的推理。

### 摘要

本文提出了一种名为 Agentic Memory (AgeMem) 的统一框架，旨在解决大型语言模型 (LLM) 代理在长程推理中因有限上下文窗口而面临的记忆管理挑战。现有方法通常将长时记忆 (LTM) 和短时记忆 (STM) 作为独立组件处理，依赖启发式规则或辅助控制器，这限制了适应性和端到端优化。AgeMem 将 LTM 和 STM 管理直接整合到代理的策略中，通过工具化操作（Tool-based Actions）的方式，使 LLM 代理能够自主决定何时、何地、以及如何存储、检索、更新、总结或丢弃信息。

**核心方法学**

AgeMem 的核心在于将记忆操作视为代理决策过程中的一等公民，并通过强化学习 (RL) 进行端到端优化。

**1. 问题形式化 (Unified RL Formulation)**

在每个时间步 $t$，代理观察到的状态为 $s_t \in \mathcal{S}$，它由对话上下文（短时记忆）$C_t$、长时记忆存储 $M_t$ 和任务规范 $T$ 组成：$s_t = (C_t, M_t, T)$。任务规范 $T$ 包括输入查询 $q$、上下文信息 $I_q$ 以及（仅用于训练）期望答案 $A_q$。

给定状态 $s_t$，代理通过参数化策略 $\pi_\theta$ 选择一个混合动作 $a_t \in \mathcal{A}$，其中包含语言生成和记忆操作：$\pi_\theta(a_t|s_t) = P(a_t|s_t; \theta)$，$\theta$ 表示 LLM 的参数。对于一个轨迹 $\tau = (s_1, a_1, \ldots, s_T, a_T)$，累积奖励定义为：
$R(\tau) = \sum w_i \cdot R_i(\tau) + P_{\text{penalty}}(\tau)$
其中 $R_i$ 捕获任务性能和记忆质量，而 $P_{\text{penalty}}$ 惩罚冗余存储、过度工具使用和不受控的上下文扩展。优化目标是：
$\theta^* = \arg \max_\theta \mathbb{E}_{\tau \sim \pi_\theta} [R(\tau)]$
这种形式化将记忆管理视为代理策略的组成部分，用可学习机制取代了手工设计的启发式规则。

**2. 记忆管理工具接口 (Memory Management via Tool Interface)**

AgeMem 通过一套明确的工具接口向 LLM 代理暴露记忆相关操作。这些工具包括：
- **LTM 工具：**
    - `ADD`：将新知识添加到 $M_t$。新的记忆条目表示为 $m_{\text{new}} = \langle c, \text{enc}(c), \text{metadata} \rangle$，其中 $c$ 是要存储的内容，$\text{enc}(c)$ 是其嵌入向量。然后 $M_{t+1} = M_t \cup \{m_{\text{new}}\}$。
    - `UPDATE`：修改 $M_t$ 中的现有条目。对于现有记忆 $m_i$，操作为 $m_i \leftarrow \langle c', \text{enc}(c'), \text{metadata}' \rangle$。
    - `DELETE`：从 $M_t$ 中移除条目。操作为 $M_{t+1} = M_t \setminus \{m_i\}$。
- **STM 工具：**
    - `RETRIEVE`：根据语义相似度从 $M_t$ 检索相关条目到 $C_t$。操作为 $\text{RETRIEVE}(q, k) = \text{TopK}(M_t, \text{sim}(q, m_i), k)$，其中相似度 $\text{sim}(q, m_i) = \frac{\text{enc}(q)^\top \text{enc}(m_i)}{\|\text{enc}(q)\| \|\text{enc}(m_i)\|}$。
    - `SUMMARY`：压缩 $C_t$ 中的对话片段以减少 Token 占用。给定上下文索引子集 $s$，操作为 $C'_t = C_t \setminus \{u_i | i \in s\} \cup \{\text{Summarize}(\{u_i\}_{i \in s})\}$。
    - `FILTER`：从 $C_t$ 中过滤掉不相关或冗余的片段。根据与特定标准 $c$ 的相似度，移除相似度超过阈值 $\theta$ 的消息：$C'_t = \{u_i \in C_t | \text{sim}(c, u_i) < \theta\}$。
通过将这些工具纳入动作空间，记忆控制从外部启发式管道转变为决策的内在组成部分。代理通过结构化的系统提示来指导工具调用，确保遵循推理、工具调用和最终答案的一致格式。

**3. 三阶段渐进式强化学习策略 (Three-Stage Progressive RL Strategy)**

为学习统一且稳定的记忆行为，AgeMem 提出一个三阶段训练策略：
- **阶段 1 (LTM Construction)：** 代理暴露于上下文信息 $I_q$ 中，目标是识别显著信息并将其存储到 $M_t$。代理可能调用 LTM 相关工具（ADD, UPDATE, DELETE）。长时记忆 $M_t$ 在所有阶段持续存在。
- **阶段 2 (STM Control under Distractors)：** 短时上下文 $C_t$ 被重置，但 $M_t$ 保留。代理被呈现语义相关但无关或误导性的干扰信息。目标是通过工具化操作（如 FILTER, SUMMARY）学习主动的 STM 控制，以抑制噪声并保留有用信息。
- **阶段 3 (Integrated Reasoning and Memory Coordination)：** 代理收到一个需要精确推理和有效记忆检索的正式查询 $q$。代理必须从 $M_t$ 检索相关知识，适当管理上下文 $C_t$，并生成最终答案。
所有三个阶段形成一个完整的轨迹 $\tau^{(q)}_k = (\tau^{(1)}_k, \tau^{(2)}_k, \tau^{(3)}_k)$，其中 $T = T_1 + T_2 + T_3$。

**4. 逐步 GRPO 实现统一管理 (Step-wise GRPO for Unified Management)**

为将长程任务奖励与跨所有阶段的记忆决策联系起来，本文采用了一种逐步变体的 Group Relative Policy Optimization (GRPO)。
对于任务 $q$，令 $G_q = \{\tau^{(q)}_1, \ldots, \tau^{(q)}_K\}$ 为并行 Rollout 组。每个轨迹产生一个终末奖励 $r^{(k,q)}_T = R(\tau^{(q)}_k)$。终末步的组归一化优势计算为：
$A^{(k,q)}_T = \frac{r^{(k,q)}_T - \mu_{G_q}}{\sigma_{G_q} + \epsilon}$
其中 $\mu_{G_q}$ 和 $\sigma_{G_q}$ 是 $G_q$ 内奖励的均值和标准差，$\epsilon$ 防止除零。这个优势被广播到同一轨迹的所有先前步骤：$A^{(k,q)}_t = A^{(k,q)}_T$。这使得最终任务结果能够监督每个中间记忆决策，实现跨异构操作的长程信用分配。
随后，通过最大化目标函数来更新策略：
$J(\theta) = \mathbb{E}_{(e_t,A_t)\sim\mathcal{E}} \left[ \rho_t A_t - \beta D_{KL}[\pi_\theta\|\pi_{\text{ref}}] \right]$
其中重要性比率 $\rho^{(k,q)}_t = \frac{\pi_\theta (a_t|s_t)}{\pi_{\theta_{\text{old}}} (a_t|s_t)}$ 控制更新幅度，$D_{KL}$ 是 KL 散度惩罚，$\beta$ 平衡探索和训练稳定性。

**5. 奖励函数设计 (Reward Function Design)**

总轨迹级别奖励定义为：
$R(\tau) = w^\top R + P_{\text{penalty}}$
其中 $w = [w_{\text{task}}, w_{\text{context}}, w_{\text{memory}}]^\top$ 是可调系数，$R = [R_{\text{task}}, R_{\text{context}}, R_{\text{memory}}]^\top$ 对应任务完成、上下文管理和长时记忆管理的奖励。
- **任务完成奖励 ($R_{\text{task}}$)：** 通过 LLM 裁判评估代理答案 $A_{\text{pred}}$ 与期望答案 $A_q$ 的匹配度 $S_{\text{judge}}(A_{\text{pred}}, A_q) \in [0, 1]$。
$R_{\text{task}} = \begin{cases} S_{\text{judge}}(A_{\text{pred}}, A_q), & \text{if has answer} \\ P_{\text{no-answer}}, & \text{otherwise} \end{cases}$
- **上下文管理奖励 ($R_{\text{context}}$)：** 评估 STM 行为，包括：
    - 压缩效率：$R_{\text{compression}} = \max\left(0, 1 - \frac{T_{\text{used}}}{T_{\text{max}}}\right)$。
    - 预防性操作：$R_{\text{preventive}} = \mathbb{1}[\text{tool invoked before overflow}]$。
    - 信息保留：$R_{\text{preservation}} = \mathbb{1}[\text{key tokens preserved}]$。
- **记忆管理奖励 ($R_{\text{memory}}$)：** 评估 LTM 操作，包括：
    - 存储质量：$R_{\text{storage}} = \frac{N_{\text{high\_quality}}}{\max(1, N_{\text{total}})}$。
    - 维护：$R_{\text{maintenance}} = \mathbb{1}[\text{update or delete performed}]$。
    - 语义相关性：$R_{\text{relevance}} = S_{\text{LLM}}(R, q)$，LLM 评估检索记忆集 $R$ 与查询 $q$ 的语义相关性。
- **惩罚项 ($P_{\text{penalty}}$)：** 惩罚超出最大对话轮次或上下文溢出等不良行为，如 $P_{\text{penalty}} = \sum P_k \cdot \mathbb{1}[\text{violation}_k]$。

**实验结果**

实验在 ALFWorld, SciWorld, PDDL, BabyAI, 和 HotpotQA 五个长上下文基准测试上进行。AgeMem 在两个 LLM 主干 (Qwen2.5-7B-Instruct 和 Qwen3-4B-Instruct) 上均显著优于现有基线。
- **总体性能：** AgeMem 在所有数据集上的平均性能最高，相较于无记忆基线分别有 49.59% 和 23.52% 的相对增益。相较于最佳基线 (Mem0 和 A-Mem)，AgeMem 平均提升了 4.82 和 8.57 个百分点。RL 训练贡献了 8.53 和 8.72 个百分点的改进。
- **长时记忆质量 (Memory Quality, MQ)：** AgeMem 在 HotpotQA 上实现了最高的 MQ 分数（0.533 和 0.605），表明其统一记忆管理框架不仅提高了任务性能，还促进了高质量、可重用知识的存储。
- **STM 管理效率：** AgeMem 相较于没有 STM 工具的变体 (-RAG) 成功减少了提示 Token 使用量，Qwen2.5-7B-Instruct 上减少 3.1%，Qwen3-4B-Instruct 上减少 5.1%，验证了学习到的 STM 管理工具能有效控制上下文扩展。
- **工具使用分析：** RL 训练显著增加了长时记忆工具（特别是 ADD 和 UPDATE）的使用。对于短时记忆工具，RL 导致更平衡的工具使用，FILTER 的频率显著增加，表明主动的上下文控制。
- **消融研究：** 逐步添加 LTM、RL 训练和 STM 工具显示了累积的性能提升，证实了统一记忆管理与端到端 RL 对于最优代理性能的重要性。特别是，STM 工具的添加带来了显著的提升。
- **奖励函数：** 全面奖励函数 (All-Returns) 相较于仅关注答案的奖励 (Answer-Only) 策略，带来了更快且更高的收敛性能，并显著提高了记忆质量，表明奖励记忆相关行为能够提高记忆组织和任务性能。

**结论**

AgeMem 作为一个统一的记忆管理框架，使 LLM 代理能够通过可学习的工具化动作共同控制长时和短时记忆。通过将记忆操作直接集成到代理策略中并通过渐进式强化学习进行训练，AgeMem 用端到端优化的解决方案取代了启发式记忆管道。广泛的实验证明了 AgeMem 在复杂代理任务中的鲁棒性和有效性，突出了统一、以代理为中心的记忆策略对于构建可扩展和自适应 LLM 代理的重要性。





## Abstract


本论文提出了一种名为 **Agentic Memory（AgeMem）** 的统一记忆管理框架，旨在解决大语言模型（LLM）代理在**长时序推理任务**中面临的上下文窗口有限的问题。现有方法通常将**长期记忆（LTM）** 和 **短期记忆（STM）** 作为独立模块处理，依赖启发式规则或辅助控制器，限制了模型的自适应性和端到端优化能力。

AgeMem 的核心创新在于：

- **统一记忆管理机制**：将 LTM 和 STM 的操作（如存储、检索、更新、总结、删除）直接集成到代理的策略中；
- **工具化记忆操作**：将记忆操作暴露为工具调用，使 LLM 能够自主决定何时、何地存储或删除信息；
- **三阶段渐进式强化学习策略**：用于训练统一的记忆行为；
- **Step-wise GRPO 算法**：解决记忆操作带来的稀疏和不连续奖励问题。

实验结果表明，AgeMem 在五个长时序任务基准上，**在多个 LLM 主干模型上均优于强记忆增强基线方法**，具体体现在：

- 更高的任务完成性能；
- 更高质量的长期记忆维护；
- 更高效的上下文使用。

### 重点内容强调：

- **数学与算法部分**：
  - 提出了 **Step-wise GRPO**（一种改进的策略梯度算法），用于处理记忆操作中常见的稀疏奖励问题；
  - 三阶段训练策略包括：预训练、渐进式微调、端到端强化学习优化。

- **实验部分**：
  - 在多个长时序任务（如复杂问答、任务规划等）上验证了 AgeMem 的有效性；
  - 使用不同 LLM（如 Llama、ChatGLM 等）作为主干模型，验证了方法的泛化能力；
  - 表格数据显示 AgeMem 在任务成功率、记忆质量指标（如相关性、冗余度）和上下文利用率方面均优于现有方法。

### 总结：

AgeMem 提出了一种全新的、统一的记忆管理机制，通过将记忆操作工具化并集成到 LLM 代理的决策流程中，显著提升了其在长时序任务中的表现。该方法不仅提升了任务性能，还增强了模型对上下文资源的利用效率，为未来 LLM 代理的长期记忆研究提供了新思路。


## 1 Introduction

### 1.1 背景与问题
在涉及多步推理和复杂流程的长视野（long-horizon）任务中，大语言模型（LLM）代理的性能受到其当前可关注信息的限制，这些信息统称为**代理记忆**（agent’s memory）。记忆通常分为两类：
- **长期记忆**（LTM）：持久存储用户或任务特定知识；
- **短期记忆**（STM）：包含当前输入上下文的信息。

高质量的LTM支持高效的知识检索，而有效的STM管理则减少冗余、保留关键上下文。两者联合管理对于缓解有限上下文窗口的限制至关重要，是提升复杂推理任务中代理性能的关键。

### 1.2 现有方法的局限性
当前研究大多将LTM和STM视为**独立模块**，分别处理：
- STM常通过**检索增强生成**（RAG）方法增强，如MainRAG和ReSum，依赖外部检索或周期性摘要扩展可用上下文。但这些方法依赖预定义策略或启发式规则，可能忽略关键细节或引入噪声。
- LTM管理分为两类：
  - **触发式**（trigger-based）：在预设时刻执行固定操作；
  - **代理式**（agent-based）：引入专门的记忆管理器决定存储内容和方式。但多数方法仍依赖人工规则或辅助专家模型，限制了适应性并增加系统复杂度。

因此，现有架构通常为：
- (a) 静态STM + 触发式LTM；
- (b) 静态STM + 代理式LTM。

两种记忆系统被**独立优化**，再以临时方式组合，导致记忆构建碎片化，在长视野任务中表现不佳。

### 1.3 统一记忆管理的挑战
实现LTM与STM统一管理面临三大挑战：
- **C1：功能异质性协调**：LTM负责存储、更新、删除，STM负责检索、摘要、删除，需设计统一机制协调其协同工作。
- **C2：训练范式不匹配**：LTM训练依赖会话级信息，STM训练模拟长视野上下文，标准强化学习（RL）难以处理记忆操作带来的**碎片化、不连续经验**。
- **C3：部署限制**：许多系统依赖辅助专家模型进行记忆控制，增加推理成本和训练复杂度，如何在不依赖外部模型的情况下实现统一记忆管理仍是一个开放问题。

### 1.4 提出方法：Agentic Memory（AgeMem）
为解决上述问题，本文提出**Agentic Memory（AgeMem）**，统一管理LTM与STM的框架：
- 不同于将记忆作为外部组件的设计，AgeMem将LTM与STM**集成进代理决策过程**；
- 通过统一的**基于工具的接口**，LLM自主调用并执行LTM和STM的记忆操作；
- 设计**三阶段渐进式强化学习策略**：
  1. 首先学习LTM存储能力；
  2. 然后学习STM上下文管理；
  3. 最后在完整任务设置下协调两者；
- 引入**逐阶段GRPO**（Group Relative Policy Optimization）机制，解决训练阶段间经验碎片化问题，缓解RL中稀疏、不连续奖励带来的训练困难。

### 1.5 实验与结果
在五个长上下文、高推理需求的基准任务上评估AgeMem，结果显示其**持续优于强基线方法**，验证了统一记忆管理的有效性。

---

### 主要贡献总结：
1. 提出**Agentic Memory（AgeMem）**框架，实现LLM代理对LTM与STM的自主、统一管理；
2. 设计**三阶段渐进式RL策略**与**逐阶段GRPO机制**，实现统一记忆管理行为的有效端到端学习；
3. 在多个模型与长视野任务上进行全面评估，验证AgeMem在复杂代理任务中的**鲁棒性与有效性**。


## 2 Background and Related Work


### 长期记忆（LTM）

**重点内容：**  
长期记忆（LTM）对于基于大语言模型（LLM）的智能体在长时间跨度下的运行至关重要。近年来，研究者提出了多种LTM架构设计，如 LangMem、A-Mem、Mem0 和 Zep。这些方法通过模块化设计、知识图谱或结构化记忆单元来组织和检索信息。

- **LangMem** 提供了一个支持多种记忆类型的模块化框架。
- **A-Mem** 借鉴 Zettelkasten 方法，通过链接结构化知识单元来促进记忆整合。
- **Mem0** 提出了一种可扩展的提取-更新流程，并扩展为图结构用于结构化推理。
- **Zep** 使用时间知识图谱表示记忆，支持跨会话和时间感知推理。

**问题与挑战：**  
尽管这些方法在信息组织和检索方面有效，但它们大多依赖**预定义的记忆结构或启发式更新规则**。随着记忆增长，系统复杂度上升，缺乏基于学习的自适应策略来决定记忆的优先级与遗忘机制。

**本文贡献：**  
与之不同，本文提出一种**自适应记忆策略的学习机制**，使智能体能根据任务需求和长期效用动态决定记忆的存储、更新或遗忘。

---

### 短期记忆（STM）

**重点内容：**  
短期记忆（STM）主要关注上下文的选择与检索。当前主流方法是**检索增强生成（RAG）**，通过将检索内容注入提示中来扩展可用上下文。

- **RAG** 及其变体（如 MemInsight、RAP）在信息补充方面表现良好，但**无法根本解决长周期任务中的上下文爆炸问题**，且可能引入无关或干扰信息。

**改进方法：**
- **ReSum** 通过周期性压缩交互历史为紧凑的推理状态，使智能体能突破上下文窗口限制。
- 但其压缩策略仍依赖**预定义调度机制**，过度压缩可能丢失关键细节。

**本文贡献：**  
本文提出的方法使智能体能够**学习何时以及如何检索、总结或过滤上下文**，从而在效率与信息保留之间实现更灵活的平衡。

---

### 面向大语言模型的强化学习

**重点内容：**  
强化学习（RL）已成为提升LLM智能体决策与推理能力的重要手段。代表性方法包括：

- **GRPO**：通过基于采样轨迹相对质量的策略优化，提升训练稳定性，无需显式价值函数。
- GRPO及其变体（如 DeepSeekMath、Terminology-aware GRPO）在复杂推理任务中表现优异。

**问题与挑战：**  
现有基于RL的系统通常将记忆视为**静态或外部组件**，难以适应记忆操作中常见的**非连续、碎片化轨迹**。

**本文贡献：**  
本文将强化学习**直接集成到记忆管理过程中**，实现语言生成与记忆操作的**统一训练**，从而更有效地支持智能体的长期行为优化。


## 3 Method


本文提出了一种统一的记忆框架 **Agentic Memory（AgeMem）**，使大语言模型代理能够以端到端的方式自主管理长期记忆（LTM）和短期记忆（STM）。AgeMem 通过一组专用工具将记忆管理能力直接集成到代理中，并通过**三阶段渐进策略**学习最优的记忆管理策略。

---

### 3.1 问题建模（Problem Formulation）

**统一的强化学习框架**：  
在每个时间步 $ t $，代理观察到状态 $ s_t = (C_t, \mathcal{M}_t, \mathcal{T}) $，其中：
- $ C_t $：短期记忆（当前对话上下文）
- $ \mathcal{M}_t $：长期记忆库
- $ \mathcal{T} $：任务规范（包括输入查询 $ q $、上下文信息 $ I_q $ 和训练用的期望答案 $ A_q $）

代理从一个**混合动作空间**中选择动作 $ a_t \in \mathcal{A} $，包括语言生成和记忆操作。其策略由参数化策略 $ \pi_\theta $ 定义：

$$
\pi_\theta(a_t | s_t) = P(a_t | s_t; \theta)
$$

轨迹 $ \tau = (s_1, a_1, \dots, s_T, a_T) $ 的累积奖励定义为：

$$
R(\tau) = \sum w_i \cdot R_i(\tau) + P_{\text{penalty}}(\tau)
$$

其中：
- $ R_i $ 衡量任务表现和记忆质量
- $ P_{\text{penalty}} $ 惩罚冗余存储、工具滥用和上下文膨胀

优化目标为：

$$
\theta^* = \arg\max_\theta \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]
$$

**三阶段轨迹结构**：  
每个轨迹分为三个阶段：
1. **阶段1（LTM构建）**：代理在对话中识别重要信息并存入LTM。
2. **阶段2（STM控制）**：引入干扰内容，要求代理通过选择性保留和压缩来管理STM。
3. **阶段3（综合推理）**：代理需结合STM和LTM完成任务。

关键设计：
- LTM在所有阶段中持续存在
- STM在阶段1和2之间重置，防止信息泄露

每一步收集经验元组 $ e_t = (s_t, a_t, r_t, \log \pi_{\theta_{\text{old}}}(a_t | s_t)) $，用于**逐步信用分配**，使代理能将长期奖励归因于特定记忆决策。

---

### 3.2 基于工具接口的记忆管理（Memory Management via Tool Interface）

AgeMem 通过**显式工具接口**向代理暴露记忆操作功能，使记忆控制成为决策的一部分。工具包括：

| 工具     | 目标   | 功能                         |
|----------|--------|------------------------------|
| Add      | LTM    | 向记忆库中添加新知识         |
| Update   | LTM    | 修改记忆库中的条目           |
| Delete   | LTM    | 删除记忆库中的条目           |
| Retrieve | STM    | 从LTM中检索信息到当前上下文  |
| Summary  | STM    | 对当前上下文进行摘要         |
| Filter   | STM    | 过滤掉无关内容               |

这些工具使代理能够根据任务结构、历史和上下文**自适应地管理记忆**。

---

### 3.3 三阶段渐进式强化学习策略（Three-Stage Progressive RL Strategy）

每个任务实例 $ q \in \mathcal{T} $ 生成一个完整轨迹：

$$
\tau_k^{(q)} = (\tau_k^{(1)}, \tau_k^{(2)}, \tau_k^{(3)}), \quad k=1,\dots,K
$$

其中：
- $ K $：独立rollout次数
- 每个子轨迹 $ \tau_k^{(i)} $ 对应特定训练阶段

**阶段1：LTM构建**  
代理在自然对话中识别重要信息并存储到LTM。轨迹为：

$$
\tau_k^{(1)} = \{e_t\}_{t=1}^{T_1}
$$

**阶段2：STM控制**  
重置STM，保留LTM，引入干扰内容。目标是学习使用工具过滤或压缩上下文。轨迹为：

$$
\tau_k^{(2)} = \{e_t\}_{t=T_1+1}^{T_1+T_2}
$$

**阶段3：综合推理与记忆协调**  
代理需结合LTM和STM生成最终答案。轨迹为：

$$
\tau_k^{(3)} = \{e_t\}_{t=T_1+T_2+1}^{T}
$$

最终轨迹为：

$$
\tau_k^{(q)} = (e_1, e_2, \dots, e_T), \quad T = T_1 + T_2 + T_3
$$

所有任务和rollout的经验汇总为：

$$
\mathcal{E} = \bigcup_{q=1}^{B} \bigcup_{k=1}^{K} \{e_t \mid e_t \in \tau_k^{(q)}\}
$$

---

### 3.4 逐步GRPO用于统一管理（Step-wise GRPO for Unified Management）

采用**逐步GRPO**（Generalized Reward Policy Optimization）将任务奖励与各阶段的记忆决策连接。

对于任务 $ q $，设 $ G_q = \{\tau_1^{(q)}, \dots, \tau_K^{(q)}\} $ 为一组rollout，终端奖励为：

$$
r_T^{(k,q)} = R(\tau_k^{(q)})
$$

计算组归一化优势：

$$
A_T^{(k,q)} = \frac{r_T^{(k,q)} - \mu_{G_q}}{\sigma_{G_q} + \epsilon}
$$

该优势被**广播到整个轨迹**的所有步骤：

$$
A_t^{(k,q)} = A_T^{(k,q)}
$$

经验集扩展为：

$$
\mathcal{E} = \bigcup_{q,k}^{B,K} \{(e_t, A_t) \mid e_t \in \tau_k^{(q)}, A_t = A_t^{(k,q)}\}
$$

目标函数为：

$$
J(\theta) = \mathbb{E}_{(e_t, A_t) \sim \mathcal{E}} \left[ \rho_t A_t - \beta D_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}] \right]
$$

其中：
- $ \rho_t = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} $：重要性比率
- $ D_{\text{KL}} $：KL散度惩罚
- $ \beta $：平衡探索与训练稳定性的系数

---

### 3.5 奖励函数设计（Reward Function Design）

总奖励为：

$$
R(\tau) = \mathbf{w}^\top \mathbf{R} + P_{\text{penalty}}
$$

其中：
- $ \mathbf{w} = [w_{\text{task}}, w_{\text{context}}, w_{\text{memory}}]^\top $
- $ \mathbf{R} = [R_{\text{task}}, R_{\text{context}}, R_{\text{memory}}]^\top $

**任务完成奖励 $ R_{\text{task}} $**：  
使用LLM评分器 $ S_{\text{judge}}(A_{\text{pred}}, A_q) \in [0,1] $ 评估任务完成质量。

**上下文管理奖励 $ R_{\text{context}} $**：  
评估STM行为，包括：
- 压缩效率（节省token使用）
- 预防性操作（提前摘要/过滤）
- 信息保留（防止关键内容丢失）

**记忆管理奖励 $ R_{\text{memory}} $**：  
评估LTM操作，包括：
- 存储质量（高价值条目比例）
- 维护操作（更新/删除）
- 语义相关性（与查询的相关性）

**惩罚项 $ P_{\text{penalty}} $**：  
惩罚行为如对话轮次超限或上下文溢出，确保代理保持高效安全的记忆实践。

---

总结：  
本章系统介绍了 AgeMem 的设计与实现，包括其强化学习框架、记忆工具接口、三阶段训练策略、GRPO优化方法和奖励函数设计。通过将记忆管理内化为代理策略的一部分，AgeMem 实现了对长期和短期记忆的统一、自适应控制，适用于复杂任务的长期推理。


## 4 Experiments


### 4.1 实验设置

#### 数据集
论文选取了五个广泛使用的LLM代理研究数据集：ALFWorld、SciWorld、PDDL、BabyAI和HotpotQA。这些数据集涵盖了具身动作、基于游戏的推理和知识密集型问答任务，提供了多样化的评估场景。HotpotQA数据集包含问题和支持事实，因此AgeMem在HotpotQA训练集上仅使用强化学习（RL）进行微调，然后在所有数据集上直接评估。

#### 评估指标
主要任务完成指标包括：
- ALFWorld、SciWorld、BabyAI使用成功率（SR）
- PDDL使用进度率（PR）
- HotpotQA使用LLM作为评判（J）
此外，使用LLM评估器衡量长期记忆质量（MQ）

#### 基线模型与LLM主干
与四个代表性代理LTM系统进行比较：LangMem、A-Mem、Mem0、Mem0g（Mem0的图变体），并引入AgeMem-noRL（未使用RL微调）作为对比。在短时记忆（STM）消融研究中，将STM工具与RAG方法进行比较。主干模型使用Qwen2.5-7B-Instruct和Qwen3-4B-Instruct。

#### 实现细节
使用Agentscope框架构建代理，使用Trinity框架进行微调。所有奖励函数的权重系数统一设置为1.0，无需手动调整。

#### 表2：五个基准的性能比较
AgeMem在Qwen2.5-7B-Instruct和Qwen3-4B-Instruct上分别取得41.96%和54.31%的平均性能，优于所有基线模型。与无记忆基线相比，相对提升分别为49.59%和23.52%。与最佳基线（Mem0和A-Mem）相比，平均提升4.82和8.52个百分点。RL训练使AgeMem-noRL的性能分别提升8.53和8.72个百分点。

---

### 4.2 主要结果

#### 与基线模型的比较
AgeMem在两个主干模型上均取得最佳性能，验证了三阶段渐进式RL策略的有效性。

#### 长期记忆质量
使用LLM评估器评估HotpotQA上的记忆质量（MQ），AgeMem在两个主干模型上的MQ得分分别为0.533和0.605，表明其统一的记忆管理框架不仅提升任务性能，还促进高质量知识的存储。

#### 短时记忆管理有效性
通过测量HotpotQA上不同配置下的提示词数量，AgeMem相比RAG方法显著减少提示词使用量。在Qwen2.5-7B-Instruct上减少3.1%，在Qwen3-4B-Instruct上减少5.1%，表明其有效的上下文管理能力。

#### 工具使用分析
表3展示了HotpotQA上RL微调前后的工具使用统计。RL训练显著增加长期记忆工具的使用频率，尤其是Add和Update操作。在Qwen2.5-7B-Instruct上，Add操作从0.92增加到1.64，Update操作从接近零增加到0.13。在Qwen3-4B-Instruct上也观察到类似趋势。对于短时记忆工具，RL训练使工具使用更加平衡，Filter操作频率显著增加，表明主动上下文控制能力增强。

---

### 4.3 消融研究

#### LTM-STM组件
图4展示了使用Qwen2.5-7B-Instruct作为主干模型的三个代表性数据集上的消融研究结果。添加LTM（+LT）使性能提升10.6%、14.2%和7.4%。加入RL训练（+LT/RL）进一步提升性能，特别是在HotpotQA上提升6.3%。完整AgeMem系统（+LT/ST/RL）取得最佳结果，整体提升13.9%、21.7%和16.1%。添加STM工具在SciWorld和HotpotQA上分别提升3.1%和2.4%，验证了学习的上下文管理优于静态RAG方法。

#### 奖励函数
图5和表4展示了Qwen2.5-7B-Instruct在HotpotQA上的GRPO训练过程中完整奖励函数（All-Returns）与仅任务奖励（Answer-Only）的比较。完整奖励函数导致更快的收敛速度和更高的最终性能。All-Returns策略在LLM作为评判得分（0.544 vs 0.509）和记忆质量（0.533 vs 0.479）上均优于Answer-Only策略，尽管使用更多提示词（2117 vs 2078），但整体性能更好，表明额外的上下文和记忆操作对推理质量有显著贡献。


## 5 Conclusion


本章总结了论文的核心贡献与实验结果。作者提出了一种统一的记忆管理框架——**Agentic Memory（AgeMem）**，该框架使基于大语言模型（LLM）的智能体能够通过**可学习的、基于工具的操作**，统一控制**长期记忆与短期记忆**。

AgeMem 的关键创新在于将记忆操作集成到智能体的策略中，并通过**渐进式的强化学习策略**进行训练，从而替代传统基于启发式规则的记忆流程，实现**端到端优化**。

实验部分在多个**长时程任务（long-horizon）基准测试**中进行，结果表明：

- **任务性能提升**：AgeMem 在多个任务中表现出更强的推理与决策能力；
- **记忆质量提高**：所维护的记忆内容更相关、更有效；
- **上下文使用高效**：在保持较低上下文开销的前提下完成任务。

这些结果强调了构建**统一的、以智能体为中心的记忆策略**的重要性，并指出了一条构建**可扩展、自适应的 LLM 智能体**的有效路径，尤其适用于需要长期推理的场景。

> **总结**：本章从方法论、训练策略、实验验证三方面验证了 AgeMem 的有效性，为未来 LLM 智能体的记忆系统设计提供了新思路。


## Limitations


本节指出了AgeMem方法目前存在的两个主要局限：

1. **内存管理工具的固定性**：  
   当前AgeMem使用的是**固定的一组内存管理机制**，虽然这种设计提供了清晰且有效的抽象，但在未来的工作中可以进一步扩展，以支持**更细粒度的控制**。这意味着当前的内存管理策略缺乏灵活性，可能在某些复杂或特定场景下不够高效。

2. **实验任务的覆盖范围有限**：  
   虽然作者在多个具有代表性的**长时程任务（long-horizon）基准测试**中评估了AgeMem的性能，但为了更全面地理解该框架的适用性和泛化能力，**未来可以引入更多样化的任务和环境**来进一步验证其效果。

### 总结：
本节强调了AgeMem在内存管理灵活性和任务覆盖广度方面的改进空间，属于对当前方法的客观反思，为后续研究提供了方向。


## Appendix A Detailed Design and Implementation of AgeMem


### A.1 内存管理工具

AgeMem提供了一组结构化的工具，供代理在执行动作时调用。这些工具包括：

#### 检索（Retrieve）
根据语义相似性从长期记忆中检索相关信息。公式如下：
$$ \text{Retrieve}(q,k)=\text{TopK}(\mathcal{M}_t, \text{sim}(q,m_i), k) $$
其中相似度函数定义为：
$$ \text{sim}(q,m_i)=\frac{\text{enc}(q)^\top \text{enc}(m_i)}{\|\text{enc}(q)\| \|\text{enc}(m_i)\|} $$

#### 添加（Add）
将新信息存储到长期记忆中。新记忆条目创建方式如下：
$$ m_{\text{new}}=(c, \text{enc}(c), \text{metadata}) $$
更新记忆存储：
$$ \mathcal{M}_{t+1}=\mathcal{M}_t \cup \{m_{\text{new}}\} $$

#### 更新和删除（Update and Delete）
更新现有记忆：
$$ m_i \leftarrow (c', \text{enc}(c'), \text{metadata}') $$
删除过时或错误的记忆：
$$ \mathcal{M}_{t+1}=\mathcal{M}_t \setminus \{m_i\} $$

#### 摘要（Summary）
压缩对话历史以防止上下文溢出。定义如下：
$$ C_t' = C_t \setminus \{u_i \mid i \in s\} \cup \{\text{Summarize}(\{u_i\}_{i \in s})\} $$

#### 过滤（Filter）
根据语义相似性过滤掉不相关或冗余的消息。定义如下：
$$ C_t' = \{u_i \in C_t \mid \text{sim}(c, u_i) < \theta\} $$
默认阈值 $\theta=0.6$。

#### 工具调用作为结构化动作
每个工具通过指定其函数名和所需参数的模式暴露。代理的策略输出语言标记（用于文本生成）或结构化工具调用（用于内存操作）。

### A.2 奖励函数设计

整体轨迹级奖励定义如下：
$$ R(\tau)=\mathbf{w}^\top \mathbf{R} + P_{\text{penalty}} $$

#### 任务完成奖励 $R_{\text{task}}$
$$ R_{\text{task}} = \begin{cases} S_{\text{judge}}(A_{\text{pred}}, A_q), & \text{if has answer}, \\ P_{\text{no-answer}}, & \text{otherwise}, \end{cases} $$
其中 $P_{\text{no-answer}}=-1.0$。

#### 上下文管理奖励 $R_{\text{context}}$
分解为三个归一化组件：
$$ R_{\text{context}} = \sum_{i=1}^{3} \alpha_i R_i $$
其中 $R_i \in \{R_{\text{compression}}, R_{\text{preventive}}, R_{\text{preservation}}\}$。

#### 内存管理奖励 $R_{\text{memory}}$
分解为三个关键组件：
$$ R_{\text{memory}} = \sum_{j=1}^{3} \beta_j R_j $$
其中 $R_j \in \{R_{\text{storage}}, R_{\text{maintenance}}, R_{\text{relevance}}\}$。

#### 惩罚项 $P_{\text{penalty}}$
$$ P_{\text{penalty}} = \sum_{k=1}^{2} P_k \cdot \mathbb{1}[\text{violation}_k] $$

### A.3 AgeMem算法

#### 训练概述（算法1-2）
核心训练循环遵循生成-然后-优化的范式。对于每个任务 $q$，生成 $K$ 个独立的 rollout 轨迹，形成一个完整的 episode。

#### 阶段特定的 rollout 过程（算法3-5）
##### 阶段1：LTM构建
代理在随意对话中逐渐接触上下文信息 $I_q$，并构建初始记忆存储 $\mathcal{M}$。

##### 阶段2：STM控制下的干扰
重置短期上下文 $C$，代理接收语义相关但最终无关的干扰消息，学习主动调用 Filter 或 Summary。

##### 阶段3：集成推理和内存协调
代理接收目标查询 $q$，协调从长期记忆 $\mathcal{M}$ 的检索，上下文管理操作，并进行多步推理以产生最终答案 $A_{\text{pred}}$。

#### 算法细节
- **算法1**：AgeMem训练（第1部分）
- **算法2**：AgeMem训练（第2部分）
- **算法3**：阶段1：LTM构建
- **算法4**：阶段2：STM控制下的干扰
- **算法5**：阶段3：集成推理和内存协调

该算法通过三阶段的 rollout 设计，反映了增强记忆的任务解决的自然进展，并通过终端奖励 $R(\tau)$ 对三阶段轨迹进行多维评估。


## Appendix B Case Study: AgeMem in Action


本节通过三个代表性案例展示了 AgeMem 如何通过强化学习实现统一内存管理。每个案例比较了智能体在 RL 训练前后的行为，突出了其学习到的内存策略。案例基于一个个性化学习助手场景，智能体帮助用户根据其偏好和约束制定定制化的学习计划。

---

### B.1 案例 1：长期记忆的构建与维护

**重点内容：**  
展示了 AgeMem 如何通过 Add_memory、Update_memory 和 Delete_memory 工具，有选择性地构建、更新和维护长期记忆，确保信息的准确性和时效性。

#### 训练前（Baseline）：
- 智能体无法识别哪些信息需要存储，导致信息冗余或遗漏。
- 用户更新偏好（如从 60 分钟改为 120 分钟）时，智能体无法识别旧信息已过时。

#### 训练后（AgeMem）：
- **Add_memory**：在对话初期识别关键信息（如用户是视觉学习者、偏好 60 分钟会话、Python 基础等），并存储为结构化记忆。
- **Update_memory**：当用户将学习时长从 60 分钟改为 120 分钟时，智能体更新原有记忆条目，而非简单覆盖。
- **Delete_memory + Add_memory**：当用户确认 120 分钟为最终偏好后，智能体删除包含历史信息的记忆条目，并重新创建干净的新条目，确保记忆状态一致。

**分析：**  
训练后的智能体能够有效管理长期记忆，保持信息的准确性和一致性，避免冗余和冲突。

---

### B.2 案例 2：干扰下的短期上下文管理

**重点内容：**  
展示了 AgeMem 如何通过 Filter_context 和 Summary_context 工具，识别并过滤干扰信息，保持任务焦点，防止上下文溢出。

#### 训练前（Baseline）：
- 智能体将所有信息（包括无关内容如量子计算、烘焙等）都保留在上下文中，导致信息稀释和资源浪费。

#### 训练后（AgeMem）：
- **Filter_context**：识别与任务无关的信息（如量子计算、区块链、烘焙等），并主动过滤，保持上下文聚焦于机器学习课程规划。
- **Summary_context**：当上下文积累过多细节（如每日安排、工具列表、资源链接）时，智能体进行摘要处理，保留关键信息（如 3 天结构、120 分钟会话、工具列表等），防止上下文溢出。

**分析：**  
训练后的智能体能够有效管理短期上下文，保持任务焦点，提升响应效率和质量。

---

### B.3 案例 3：集成任务执行与记忆协调

**重点内容：**  
展示了 AgeMem 的完整工作流程，智能体需从长期记忆中检索信息、管理短期上下文，并协调多个记忆操作完成任务。

#### 训练前（Baseline）：
- 智能体无法有效存储或检索用户偏好，导致生成的响应通用化，无法满足个性化需求。

#### 训练后（AgeMem）：
- **Retrieve_memory**：根据用户请求，智能体从长期记忆中检索关键信息（如用户是视觉学习者、偏好 120 分钟会话、Python 基础、对计算机视觉感兴趣等）。
- **任务执行**：基于检索到的信息，智能体生成高度个性化的学习计划，包括时间块、主题、资源等，完全匹配用户的学习风格和偏好。

**示例响应：**
- **上午会话（9:00 AM - 11:00 AM）：** 使用图表和流程图介绍 ML 概念，强调监督学习与无监督学习的对比。
- **下午会话（1:00 PM - 3:00 PM）：** 可视化 Python 复习，包括 NumPy 数组操作、Pandas 数据框处理等。
- **晚间练习（5:00 PM - 7:00 PM）：** 自主练习，强调可视化反馈。

**分析：**  
训练后的智能体能够有效整合长期记忆与当前任务，生成高度个性化、上下文感知的响应，显著优于基线模型的通用化输出。

---

### 总结

这三个案例展示了 AgeMem 的三阶段渐进式训练如何使智能体掌握复杂的内存管理策略：

- **案例 1**：通过 Add_memory、Update_memory 和 Delete_memory 实现长期知识的有选择性存储与维护。
- **案例 2**：通过 Filter_context 和 Summary_context 实现短期上下文的主动控制，防止干扰和溢出。
- **案例 3**：整合上述能力，通过 Retrieve_memory 访问长期记忆，协调内存系统完成任务。

在每个案例中，经过 RL 训练的智能体均显著优于基线模型，能够智能地识别何时、如何应用内存工具，从而实现更聚焦、一致和个性化的交互体验。


## Appendix C Experimental Implementation

### C.1 数据集细节

本节详细介绍了实验中使用的五个数据集的统计信息和特点：

1. **ALFWorld**（shridhar2020alfworld）  
   - 一个基于模拟环境的具身AI基准，要求代理根据自然语言指令完成家务任务。  
   - 包含数千个训练环境和多个验证/测试集，涵盖6种任务类型（如拾取与放置、清洁与放置等）。  
   - **重点**：任务需要长期与对象交互，适合评估规划和记忆管理能力。

2. **SciWorld**（wang2022scienceworld）  
   - 模拟科学实验的交互式环境，代理需执行多步骤实验回答科学问题。  
   - 覆盖物理、化学、生物等多个领域，强调程序推理和假设驱动探索。  
   - **重点**：适合测试代理在长时间交互中保留和检索知识的能力。

3. **PDDL**（chang2024agentboard）  
   - 使用规划领域定义语言（Planning Domain Definition Language）构建的规划基准。  
   - 评估代理在多个领域中生成有效动作序列解决符号规划问题的能力。  
   - **重点**：测试结构化推理和中间状态管理能力。

4. **BabyAI**（chevalier2018babyai）  
   - 基于网格世界的自然语言导航任务，代理需根据组合语言指令导航和操作对象。  
   - **重点**：适合评估短期上下文跟踪和指令理解能力。

5. **HotpotQA**（yang2018hotpotqa）  
   - 多跳问答数据集，包含约90k训练问题，需在多个维基百科段落中推理。  
   - 每个问题标注了支持事实，适合评估长期记忆存储与检索。  
   - **重点**：在实验中用于强化学习训练，支持事实提供结构化上下文信息。

---

### C.2 基于大语言模型（LLM）的评估细节

1. **记忆质量（MQ）评估**  
   - 使用LLM作为评估器，比较模型预测的支持事实与真实支持事实的匹配程度。  
   - **评估指标**：
     - 是否覆盖所有预期事实
     - 预测事实是否与问题相关
     - 是否包含无关事实  
   - **评分标准**（0.0~1.0）：
     - 1.0：完全匹配，无无关事实
     - 0.8-0.9：基本正确，有轻微遗漏或一个无关事实
     - …（依此类推）  
   - 使用 **Qwen-Max** 作为评估模型，独立评估确保一致性。

2. **LLM-as-a-Judge 评估（HotpotQA）**  
   - 评估代理回答的正确性，比较代理答案与真实答案的匹配程度。  
   - **评分标准**同上，用于衡量生成答案的质量。  
   - **重点**：通过LLM评估，避免人工标注成本，提高评估效率。

---

### C.3 基线配置

所有基线实现均基于其官方开源代码库，确保公平比较：

1. **LangMem**（langmem2025）  
   - 官方实现：https://langchain-ai.github.io/langmem/  
   - 使用默认超参数和内存机制。

2. **A-Mem**（xu2025mem）  
   - 基于Zettelkasten设计，官方实现：https://github.com/WujiangXu/A-mem-sys/  
   - 使用推荐的内存整合参数。

3. **Mem0**（chhikara2025mem0）  
   - 官方实现：https://github.com/mem0ai/mem0  
   - 使用默认提取-更新流程，图结构变体（Mem0g）启用图构建参数。

4. **AgeMem-noRL**  
   - 与AgeMem使用相同工具接口，但不使用强化学习，用于评估RL的贡献。

5. **RAG变体**（AgeMem-noRL-RAG 和 AgeMem-RAG）  
   - 使用标准RAG流程，基于余弦相似度检索记忆并附加到上下文。  
   - **重点**：用于比较静态检索与学习的STM管理之间的差异。

---

### C.4 实现细节

1. **训练配置**  
   - 使用 **Trinity RL框架**（pan2025trinity）进行策略优化，采用 **step-wise GRPO算法**。  
   - 每个任务使用 **K=8** 独立rollout进行组归一化。  
   - KL散度系数 **β=0.1**。

2. **奖励权重**  
   - 所有奖励权重设为 **1/3**：  
     $$
     w_{\text{task}} = w_{\text{context}} = w_{\text{memory}} = 1/3
     $$  
   - **重点**：保证任务性能与记忆管理均衡学习。

3. **模型设置**  
   - 最大上下文长度：8,192 tokens  
   - 最大响应长度：2,048 tokens  
   - 上下文超出限制时给予惩罚，鼓励主动使用STM管理工具。  
   - 实验平台：8块NVIDIA RTX 4090 GPU，每块48GB内存。

--- 

### 总结

本附录详细说明了实验所用数据集的特点、基于LLM的评估方法、基线模型配置以及训练和模型参数设置。其中，**HotpotQA的记忆质量评估**和**强化学习训练配置**是关键内容，体现了模型在长期记忆管理和任务完成之间的平衡能力。


## Appendix D Additional Results


### D.1 消融研究

本节提供了 Qwen3-4B-Instruct 模型的补充消融实验结果。图9展示了在三个代表性数据集上，LTM（长期记忆）、STM（短期记忆）和 RL（强化学习）组件对模型性能的逐步贡献。图中显示：

- **Base**：无记忆基线模型；
- **+LT**：仅使用 LTM 工具（AgeMem-noRL-RAG）；
- **+LT/RL**：使用 LTM 和 RL（AgeMem-RAG）；
- **+LT/ST/RL**：完整 AgeMem 系统（包含 LTM、STM 和 RL）。

绿色箭头表示相比基线模型的性能提升。实验结果与 Qwen2.5-7B-Instruct 的趋势一致，验证了该方法在不同模型规模下的泛化能力。

---

### D.2 Qwen3-4B 上的奖励函数消融实验

为验证多组件奖励函数设计在不同模型架构和规模下的泛化能力，我们在 Qwen3-4B-Instruct 上进行了与主文相同的奖励函数消融实验。

#### D.2.1 收敛性分析

图10展示了 Qwen3-4B-Instruct 上 All-Returns（完整奖励）与 Answer-Only（仅答案奖励）策略的训练收敛曲线。主要观察如下：

- **更稳定的训练动态**：All-Returns 策略的收敛曲线更加平滑，尤其在训练后期（第70至100步）波动更小，表明 Qwen3 架构可能更适合奖励学习任务。
- **持续优势**：尽管提升幅度小于 Qwen2.5-7B-Instruct，All-Returns 策略在整个训练过程中始终优于 Answer-Only，验证了奖励设计的鲁棒性。

#### D.2.2 定量结果

表5展示了在 HotpotQA 数据集上使用 Qwen3-4B-Instruct 的奖励函数消融结果，比较了 All-Returns 与 Answer-Only 策略。指标包括：

- **J（↑）**：LLM-as-a-Judge 评分；
- **TN（↓）**：生成的 token 数量；
- **MQ（↑）**：记忆质量；
- **TC（-）**：工具调用次数。

| 策略         | J（↑） | TN（↓） | MQ（↑） | TC（-） |
|--------------|--------|---------|---------|---------|
| Answer-Only  | 0.546  | 2164    | 0.415   | 7.21    |
| All-Returns  | 0.555  | 2191    | 0.605   | 8.67    |

结果显示：

- **All-Returns 显著提升记忆质量（MQ）**：从 0.415 提升至 0.605，说明奖励中间记忆行为有助于构建更可靠的记忆结构。
- **更高的工具调用频率**：从 7.21 增至 8.67，表明模型更积极地利用记忆操作。
- **仅小幅增加 token 消耗**：从 2164 增至 2191，说明性能提升主要来自更高效的记忆使用，而非上下文扩展。

这些结果与 Qwen2.5-7B-Instruct 上的观察一致，进一步验证了奖励函数设计的跨模型鲁棒性。
