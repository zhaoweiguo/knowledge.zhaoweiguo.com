# 2404.11672_MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory

* 首页: <https://arxiv.org/abs/2404.11672>
* PDF: <https://arxiv.org/pdf/2404.11672>
* 引用: 19(2025-08-28)
* 组织
    * 1Center for Information and Language Processing, LMU Munich, Germany
    * 2Munich Center for Machine Learning, Germany
    * 3Microsoft, Berlin, Germany
* GitHub: <https://github.com/amodaresi/MemLLM>


## 总结

**背景**
* 当前LLMs的优势与局限
    - **优势**：
      - 当前最先进的LLMs在知识密集型任务中表现出色，主要依赖于其参数中存储的信息。
    - **局限**：
      - 对于**低频实体与概念**，LLMs的记忆能力有限。
      - **时间退化**：模型的知识库可能过时。
      - **局部修改问题**：模型编辑策略可能在无关领域造成性能下降。
      - **批量编辑困难**：模型编辑通常按单条修改处理，难以应对多条事实的并发更新。
      - **泛化问题**：同时更新多个事实时，模型难以保持已有编辑的稳定性。
* 参数化记忆方式的局限
    * 知识失真与生成幻觉。
    * 容量有限，且缺乏可解释性。
* 非参数化记忆方式的尝试与问题
    * 非参数记忆组件（如 API 形式）：允许模型与外部记忆交互。
        * 优点：
            * 在对话、长文本生成和问答任务中表现增强。
        * 缺点：
            * 高度依赖提示，需针对每项任务进行定制。
            * 缺乏结构，导致解释性差与互操作性差。
    * 检索增强生成（RAG）：
        * 虽然可以提供实时知识，但其存储结构松散，更新事实时容易产生矛盾。


**MemLLM**
* 简介
    * 引入一个显式记忆组件（explicit memory component）
    * 引入结构化且显式的读写记忆模块来增强LLMs的新方法
    * MemLLM通过实现与记忆的动态交互，提升了LLMs在使用存储知识方面的能力
* 核心特点
    1. **长期存储能力**：可以存储包括低频信息在内的数据，突破上下文窗口限制。
    2. **读写访问权限**：模型在处理文本或与用户交互时，可将信息写入记忆，并在需要时读取。
    3. **API训练方式**：
       - 通过**微调**训练模型使用 API 进行读写。
       - 提供一个可公开使用的训练数据集，用于训练模型的读写能力。
       - 不需要对模型结构进行修改即可实现记忆功能。
    4. **结构化记忆设计**：
       - 采用类似数据库的**结构化模式**。
       - **可解释性强**：人类可查看、编辑记忆内容。
       - **可扩展性好**：数据库具备良好的扩展性。
       - **互操作性强**：记忆内容可导入导出，与其他支持结构化记忆的LLM共享。




## Abstract


本段摘要主要概述了MemLLM方法的研究背景、问题、解决方案及其效果。

**问题定义**：当前的大型语言模型（LLMs）虽然在许多知识相关任务上表现良好，但它们依赖于参数作为隐式的存储机制，存在多方面的限制。例如，它们难以记忆稀有事件，并且当事实随时间变化时，也难以更新知识。此外，参数化存储的不可解释性使得防止模型产生幻觉（hallucination）变得困难。

**现有方法的局限性**：通过模型编辑或为LLMs添加专门用于记忆的参数，只能部分解决上述问题。

**解决方案**：本文提出MemLLM，这是一种通过引入结构化且显式的读写记忆模块来增强LLMs的新方法。MemLLM通过实现与记忆的动态交互，提升了LLMs在使用存储知识方面的能力。

**实验结果**：实验表明，MemLLM在语言建模任务（尤其是知识密集型任务）中显著提高了LLMs的性能和可解释性。

**研究意义**：作者认为，MemLLM是通过记忆增强使LLMs更贴近事实、更可靠的重要一步。


## 1 Introduction


本节介绍了当前大语言模型（LLMs）在知识密集型任务中的表现及其局限性，并提出了一种新的解决方案 **MemLLM**，其核心是引入一个**显式记忆组件**（explicit memory component）。

---

### 1.1 当前LLMs的优势与局限

- **优势**：当前最先进的LLMs在知识密集型任务中表现出色，主要依赖于其参数中存储的信息。
- **局限**：
  - 对于**低频实体与概念**，LLMs的记忆能力有限。
  - **时间退化**：模型的知识库可能过时。
  - **局部修改问题**：模型编辑策略可能在无关领域造成性能下降。
  - **批量编辑困难**：模型编辑通常按单条修改处理，难以应对多条事实的并发更新。
  - **泛化问题**：同时更新多个事实时，模型难以保持已有编辑的稳定性。

---

### 1.2 参数化记忆方式的局限

- **附加参数记忆池**（如 Wang et al. 的研究）可以用于知识的长期存储，但存在：
  - **知识失真**与**生成幻觉**。
  - **容量有限**，且**缺乏可解释性**。

---

### 1.3 非参数化记忆方式的尝试与问题

- **非参数记忆组件**（如 API 形式）：允许模型与外部记忆交互。
  - **优点**：在对话、长文本生成和问答任务中表现增强。
  - **缺点**：
    - **高度依赖提示**，需针对每项任务进行定制。
    - **缺乏结构**，导致**解释性差**与**互操作性差**。
- **检索增强生成（RAG）**：虽然可以提供实时知识，但其**存储结构松散**，更新事实时容易产生矛盾。

---

### 1.4 MemLLM：引入显式结构化记忆组件

**MemLLM** 是一个包含显式记忆组件的语言模型，其核心特点包括：

1. **长期存储能力**：可以存储包括低频信息在内的数据，突破上下文窗口限制。
2. **读写访问权限**：模型在处理文本或与用户交互时，可将信息写入记忆，并在需要时读取。
3. **API训练方式**：
   - 通过**微调**训练模型使用 API 进行读写。
   - 提供一个可公开使用的训练数据集，用于训练模型的读写能力。
   - 不需要对模型结构进行修改即可实现记忆功能。
4. **结构化记忆设计**：
   - 采用类似数据库的**结构化模式**。
   - **可解释性强**：人类可查看、编辑记忆内容。
   - **可扩展性好**：数据库具备良好的扩展性。
   - **互操作性强**：记忆内容可导入导出，与其他支持结构化记忆的LLM共享。

---

### 1.5 实验验证

- 在 **Re-DocRED 数据集** 上的实验表明：
  - MemLLM 相比无记忆组件的基线模型，**困惑度更低**。
  - 在**命名实体识别**任务中表现尤为突出。
- 在**知识编辑任务**中，MemLLM 也优于无记忆方法。

---

**总结**：本节系统分析了当前LLMs在知识存储与编辑任务中的不足，并提出了 MemLLM 作为解决方案，其核心优势在于引入**结构化、可解释、可扩展的显式记忆组件**。


## 2 Related work


### 外部记忆增强

将记忆作为外部组件来增强大型语言模型（LLM）的能力，可以提升其处理长上下文和存储事实与知识的可靠性。常见的外部记忆结构包括数据库、知识库和知识图谱等，LLM 通过自然语言或形式语言与这些结构进行交互。例如，Retrieval Augmented Generation (RAG) 通过从大规模文档中检索相关的文本片段，来提高事实的准确性。还有一些方法通过存储先前上下文的摘要信息，来提高在长文本生成、摘要、问答和对话连贯性等方面的表现。

整体而言，本文提出的框架与这些外部记忆方法一致，但其**结构化的存储方式**是其突出点。这种方式便于大规模知识的编辑，并使模型的输出生成过程更加**可解释**。虽然也存在类似的结构化存储方法，但它们通常是**任务特定的**。例如，ChatDB 需要预先定义数据库模式，而本文方法则适用于**通用语言建模**，无需复杂的提示工程。

知识图谱也是一种结构化存储格式，可以看作由三元组组成的记忆，与本文的记忆结构相似。虽然已有研究利用 LLM 从文本中提取三元组并生成知识图谱，或让 LLM 交互式地查询图谱以回答问题，但本文的创新之处在于，通过微调训练使 LLM 具备**记忆读写功能**。这使得记忆成为模型处理文本的**核心组件**，且能够灵活处理新知识和更新知识。尽管已有类似集成系统的研究，但本文是首个实证成功应用于语言建模和知识编辑的系统，并提供了可用于训练的语言模型数据集。

### 记忆作为状态

“记忆”一词也可以指**基于循环结构的模型**，如 LSTM，它们通过向量表示过去的上下文。Transformer 模型则通过**记忆 token** 和 **记忆池** 来实现类似的功能。虽然最近的研究提出了一些基于向量或参数的记忆系统，用于处理长距离依赖关系，但它们通常受限于记忆向量的容量。而本文提出的 MemLLM 不受此类架构限制，具备**显式、可解释和可编辑的记忆结构**，是其显著优势。

### 知识编辑

知识编辑的目标是**高效地修改模型行为**，同时不影响其他知识。方法主要包括**元学习**和**定位-编辑**两类参数方法，以及**基于记忆的方法**。元学习通过超网络在测试时调整模型权重；定位-编辑通过对知识表达触发的权重进行定位和修改。基于记忆的方法则不修改模型权重，而是使用外部记忆，如 SERAC、GRACE 和 DEFER 等方法，通过检索先前的编辑来处理新输入。WISE 方法则通过引入**辅助记忆**和**路由网络**，决定使用主模型还是辅助记忆。

评价表明，**批量编辑**和**连续编辑**是知识编辑中最具挑战性的任务。现有方法通常只能处理少量编辑，编辑数量增加时性能显著下降。而由于本文 MemLLM 的**显式记忆结构**，可以处理大量编辑任务，同时保持较高的性能，是其显著优势。


## 3 Methodology


本节介绍了 MemLLM 如何通过微调，使大语言模型（LLM）具备显式的读写内存能力。重点在于如何训练模型从文本中提取知识写入内存，并在适当的时候读取内存信息以提高语言建模效果。下面按照原文结构进行总结，保留原标题，重点部分着重讲解，次要内容适当精简。

---

### 3.1 内存结构

MemLLM 使用**三元组（relation triples）**来存储信息，每个三元组的形式为：  
**r = ⟨e_s, t, e_o⟩**，其中：  
- e_s 为**主体（subject）**，  
- t 为**关系（relation）**，  
- e_o 为**客体（object）**。  

例如：**⟨Washington D.C., capital of, United States⟩**。  
三元组中的实体和关系以**原始文本**和**向量形式**分别存储在独立的表中，并通过**唯一ID**进行索引。主表“Triple Memory”通过三个 ID 连接两个实体表和一个关系表，确保三元组的**唯一性**，防止重复存储相同事实。

**重点内容**：  
- 三元组结构和存储方式是实现显式记忆的基础。  
- **向量相似性**用于匹配实体和关系，而非完全匹配，以应对不同表达方式（如“US”与“USA”）。  
- 检索时使用**相似度阈值**（τe 和 τt）确定候选实体和关系，并进一步结合三元组的整体相似度（τr）筛选最终结果。

---

### 3.2 内存API与推理

MemLLM 通过定义**API**，使 LLM 能够主动执行**内存读**和**内存写**操作。

#### 内存写（Memory writes）

- **输入格式**：每个句子 sisubscript𝑠𝑖s\_{i} 作为输入，并通过标记 ({USER\_ST}) 和 ({USER\_END}) 标识为“焦点句子”。
- **任务**：LLM 从焦点句子中提取关系三元组，并以 API 命令写入内存。
- **API格式**：`{MEM_WRITE--> es1>>t1>>eo1; es2>>t2>>eo2; … }`

**重点内容**：  
- 上下文 S<isubscript𝑆absent𝑖S\_{<i} 用于支持句子中代词等引用的理解。  
- 训练模型仅从焦点句子中提取关系，而非整个上下文。

#### 内存读（Memory reads）

- **API格式**：`{MEM_READ( esq1>>tq1>>; tq2>>eoq2; … )--> e1,e2,e3,… }`  
- LLM 可在解码过程中生成多个查询，检索内存中的实体并作为上下文的一部分继续生成文本。
- **过滤机制**：若检索结果为空、数量过多（超过30个）或出现新的 API 调用（“({”），则清除之前的结果，保证输出的自然流畅。

**重点内容**：  
- 内存读调用在适当位置提升生成质量，例如在生成实体名前检索相关三元组，减少幻觉。
- 提出的过滤机制提高了检索结果的实用性，避免模型生成大量无关信息。

---

### 3.3 微调 LLM

为生成内存读写 API 命令，MemLLM 使用**标注好的实体与关系数据集**进行微调，例如 Re-DocRED 和 Wikidata 格式标注的 Wikipedia 数据。

#### 内存写训练数据（Memory-write data）

- **方法**：从每个句子中提取带有实体提及的关系三元组，并构造训练样本。
- **训练目标**：模型学习在焦点句子中识别并提取三元组，生成正确的 `MEM_WRITE` API 命令。
- **空三元组处理**：若句子没有关系信息，训练模型生成“不包含任何三元组”的命令，防止生成虚假信息。

#### 内存读训练数据（Memory-read data）

- **方法**：对每个目标实体 etarget，收集其参与的三元组，生成对应查询。
- **训练目标**：模型学习何时发起内存读、生成有效查询以及如何使用返回结果进行生成。
- **数据构造**：将查询、查询结果和上下文拼接为完整训练样本。损失函数作用于 API 调用部分和后续生成文本，但不作用于查询结果本身。

**重点内容**：  
- 训练数据的生成考虑了模型在推理时的真实条件，例如使用训练好的内存写模型生成的“不完美”内存来训练内存读模型。
- 数据集中去除模糊或无用的查询，提升模型的实用性。

---

### 总结

本节系统性地介绍了 MemLLM 的核心方法，包括：
1. **内存结构**：基于三元组存储实体与关系，结合向量相似性实现灵活检索。
2. **API机制**：定义了内存读写接口，使 LLM 在生成过程中主动与外部内存交互。
3. **微调方法**：使用高质量标注数据训练模型生成 API 命令，并结合真实内存交互数据优化模型表现。

**重点强调**：通过结构化的显式记忆，MemLLM 提高了 LLM 的事实一致性与推理能力，尤其是在生成实体和减少幻觉方面具有显著优势。


## 4 Experiments


### 4.1 实验设置

本节介绍了 MemLLM 的训练与评估数据集、模型设置及过滤远监督关系的方法。

#### 数据集
为了训练和评估 MemLLM，作者构建了三个标注了实体和关系的数据集：
- **Re-DocRED**：维基百科文本，标注了命名实体、共指信息和96种关系（句内和句间），含有许多 DocRED 中缺失的关系。
- **DocRED 的远监督训练集**：包含 >100K 文档，但每文档关系较少。该数据集规模大，有助于提高训练效果和模型鲁棒性。
- **Re-DocRED 的反事实变体**：通过实体替换策略生成，用于增强模型对预训练知识偏见的鲁棒性。

#### 模型与训练
作者基于 Mistral-7B 模型，使用 **LoRA** 进行微调，分别训练了 **memory-write 模型** 和 **memory-read 模型**。基线模型包括原始 Mistral-7B 和禁用记忆功能的 memory-read 模型，用于评估记忆对性能的提升。

#### 4.1.1 过滤远监督关系
由于远监督数据易引入噪声（如未在文本中明确提及的关系），作者采用 **few-shot 过滤方法**，利用大模型（Mixtral）检测关系在焦点句中的证据。作者设计了三种提示策略：
1. **Baseline**：直接回答“是/否”。
2. **Justification**：回答后附带理由。
3. **Reasoning**：生成关系句子 + 推理 + 回答。

结果显示，**Reasoning 策略**效果最好（F1=0.80），表明推理提示对提高远监督数据质量非常有效。过滤后的数据与 Re-DocRED 的反事实变体一起用于初始化微调，然后进一步使用 Re-DocRED 的监督数据进行训练。

---

### 4.2 混淆度评估（Perplexity Evaluation）

本节评估了 MemLLM 在语言建模任务上的性能，重点关注记忆组件对模型生成能力的提升。

#### 方法
- **数据源**：英语维基百科（完整版、摘要）、Re-DocRED 文本。
- **评估指标**：
  - **OVERALL PPL**：整体文本的困惑度。
  - **TARGET PPL**：目标实体部分的困惑度。
  - **ENTITY PPL**：所有命名实体的困惑度。

模型的生成概率公式为：
$$ p(w_i|w_{<i}) = p(w_i|w_{<i}, MR) \cdot p(MR|w_{<i}) + p(w_i|w_{<i}) \cdot (1 - p(MR|w_{<i})) $$

其中，$ p(MR|w_{<i}) $ 是模型在当前位置发起记忆读取（Memory Read, MR）的概率。

#### 结果分析
- MemLLM 在 **所有三项指标** 上均优于基线模型，说明记忆组件显著提升了模型的语言建模能力。
- **TARGET PPL** 从 3.510 降至 2.986，表明模型在关键实体上的预测能力得到显著增强。
- 作者进一步分析了 **记忆读取过程中的误差来源**，包括查询生成、实体匹配、记忆位置选择等，并通过消融实验验证了这些因素对性能的影响。
- **记忆存储规模的影响较小**，表明模型具有良好的可扩展性。
- **结构化三元组存储（structured triple）** 相比于命题句子存储（proposition-based），节省了 80% 的内存使用量，同时减少了噪声，提高了检索精度。

---

### 4.3 知识编辑评估（Knowledge Editing Evaluation）

本节评估了 MemLLM 在知识编辑任务上的表现，包括可靠性（REL）、泛化性（GEN）和局部性（LOC）。

#### 方法
- **数据集**：ZsRE（闭卷问答数据集），结合 NQ 提取局部性测试样本。
- **任务**：给定编辑指令（如 “It is or they are Naples”），模型应将其存储到记忆中，并在后续的问答中应用该编辑。

#### 结果
- **MemLLM 的平均得分（AVG）为 0.84**，在三个基线模型中表现最好。
- **可靠性 0.78，泛化性 0.76，局部性 0.97**，说明模型能够有效提取并应用编辑内容，同时对无关知识的干扰较小。
- **定性分析**：作者分析了 22% 的可靠性损失，发现主要问题包括：
  - 内存写入失败（45/216）。
  - 读取查询或关系提取错误（95/216）。
  - 回答时未正确使用编辑内容（63/216）。
- **限制**：模型支持的关系只有 96 种，导致部分问题无法处理。例如，涉及 “IUCN 保护等级” 或 “家族谱系” 的问题无法被正确处理。

---

### 总结

- **训练设置**：MemLLM 使用 Re-DocRED、DocRED 等数据集进行训练，并通过提示策略有效过滤远监督噪声。
- **语言建模评估**：MemLLM 显著降低目标实体和整体文本的困惑度，表明记忆组件增强了模型对实体信息的处理能力。
- **知识编辑评估**：MemLLM 在知识编辑任务中表现出色，支持编辑的可靠性、泛化性和局部性均优于基线模型。
- **内存优化**：结构化三元组存储大幅减少内存使用，显著提升检索精度。
- **局限性**：目前支持的关系种类有限，未来可扩展更多关系种类以进一步提升性能。


## 5 Conclusion

本节总结了本文的核心贡献与实验结果。

- **提出MemLLM方法**：作者提出了一种新的方法MemLLM，该方法通过为大型语言模型（LLM）提供一种显式且结构化的记忆机制，从而增强其能力。这是本文的重点贡献。

- **发布训练数据集**：为了推广该方法的应用，作者发布了一个训练数据集。该数据集可以用于扩展任何标准的LLM，使其具备结构化记忆的能力。这也是本文的重要部分，因为它为其他研究者提供了复现和扩展的基础。

- **实验结果展示**：作者通过实验验证了MemLLM的有效性。结果显示，MemLLM在语言建模任务中表现更好（通过熵值衡量），并且在ZsRE（Zero-shot Relation Extraction）任务中优于当前最先进的知识编辑方法。这部分是本文的实证核心，说明了MemLLM在实际应用中的优势。

总的来说，本节强调了MemLLM的创新性、实用性以及在多个任务上的优越性能。


## Limitations


**1. 当前 MemLLM 存在一些局限性。** 虽然结构化的基于关系的记忆机制在事实性和可解释性方面有所提升，但它仍然存在一些不足。当前版本的 MemLLM 仅支持 Wikidata 中常见的 96 种关系类型。然而，为了实现全面的知识提取与存储，模型应具备提取更多关系类型的能力。

**2. 无法处理复合关系。** 当前版本的 MemLLM 无法识别和利用由多个已提取关系推导出的复合关系。例如，如果提取了 (California, country, United States) 和 (Apple Inc., located in, California)，则应能推导出 (Apple Inc., located in, United States) 的关系。但 MemLLM 并未实现这一推理能力。

**3. 不具备内存感知能力。** 如果某个事实未被存储在记忆中，但在解码过程中需要该事实，那么 MemLLM 只能依赖其参数化的知识继续生成内容，或者产生幻觉（hallucination）。

**4. 与 RAG 方法的比较。** 虽然检索增强生成（Retrieval-Augmented Generation, RAG）方法广泛用于更基于事实的文本生成，但由于其非结构化的格式，我们未将其纳入知识编辑的对比实验中。在 RAG 中修改某个事实需要定位并修改所有相关文本片段和嵌入，这在实际操作中是极不现实的。

**5. 与非参数和半参数方法的比较。** MemLLM 的记忆不是预先填充的知识库（KB），而是通过“记忆写入”过程存储编辑后的事实，这与非参数和半参数编辑方法类似。尽管如 ChatDB 这样的方法也使用非参数结构化记忆，但它们依赖数据库表，更适合分析任务，且要求用户提前定义任务特定的数据库模式，并在提示中明确指定。相比之下，MemLLM 使用更通用的记忆结构，能够同时支持语言建模和问答任务，且无需额外调整方法或记忆格式。

**6. 提示方法的局限性。** 与微调方法相比，基于提示的方法本身更依赖于存储在记忆中的具体内容，因此在事实性方面较弱。

**7. 未来工作展望。** 上述所有局限性均被留待未来研究。在本文中，我们打下了构建更复杂、全面方法的初步基础。


## Appendix A Memory-write Decoding Method


本章节介绍了一种用于MemLLM中内存写入操作的解码方法。虽然可以使用贪心解码（greedy decoding）来处理内存写入，但作者指出，经过微调的模型可能会在尚未提取所有关系之前就提前结束内存写入过程。为了确保模型能够捕获所有相关的关系，作者采用了**延迟停止策略**（late stopping strategy）。

### 核心方法介绍：

- **贪心解码类似操作**：与贪心解码类似，模型在每一步都选择得分最高的token作为下一个输出，**除非该token是闭合标记“)}”**。
- **闭合标记的处理**：当“)}”成为最高得分token时，系统不会立即结束生成，而是：
  - 记录该位置；
  - 计算到该位置为止序列的**平均对数概率得分**；
  - 选择**第二高得分的token**（通常是“;”分隔符），并继续以贪心方式解码。

### 延迟停止机制的运行逻辑：

- 每次预测到“)}”时，记录其位置和得分；
- 持续生成，直到连续K=5次没有得分提升；
- 此时停止生成，并选择得分**最高位置**作为最终的截断点。

### 重点总结：

- **延迟停止策略**是本文的**核心重点**，用于避免模型过早结束内存写入；
- 通过**跟踪闭合标记的出现位置与得分**，确保模型生成完整的关系表示；
- 与标准贪心解码相比，该方法在生成质量与完整性之间取得了更好的平衡。

### 精简内容：

- 模型使用微调后的贪心解码可能导致内存写入不完整；
- 闭合标记的处理方式与传统方法不同，是实现完整写入的关键；
- 参数K=5是控制停止条件的超参数，对生成效果有影响。


## Appendix B Filtering Ambiguous Queries



### 目标
本节旨在通过排除可能导致模糊或广泛输出的查询，帮助模型更有效地利用存储的内存内容。为了从内存中获取精确的输出，需要构造能够匹配目标实体或相关实体的查询。

### 核心方法
为了减少内存返回结果的范围和不确定性，作者决定**过滤掉那些可能产生广泛输出的查询**。这种过滤基于对查询实体和关系类型的直观判断。

### 重点内容：模糊查询模式
在**表格4**中，作者列出了可能导致模糊结果的查询模式。这些模式是基于查询的实体和关系类型所总结出的。**任何匹配这些模式的查询都会被排除掉**。这是本节的重点内容，因为这是实现过滤机制的核心依据。

### 其他内容
其余内容主要是对筛选逻辑的原理说明，强调了为何需要减少模糊输出，以提升模型对内存内容的利用效果。这些内容相对次要，可以简要理解为对方法合理性的补充说明。


## Appendix C Memory-read Data Generation


本部分介绍了 **MemLLM** 中用于生成“记忆读取”（memory-read）训练数据的算法流程。该算法的目的是从文档中提取信息，并结合外部记忆（如 Wikipedia 事实）生成训练数据，以帮助模型在训练过程中学习如何使用内存。

> **参考部分**：对此过程的详细描述请见 [Section 3.3](https://arxiv.org/html/2404.11672v3#S3.SS3 "3.3 Finetuning the LLM")，本文仅对其伪代码进行说明。

---

### Algorithm 1 Memory-read Data Generation

#### **输入：**
- **D**：文档集合（Re-DocRED 数据集，其中每个文档为 d ∈ D）

#### **输出：**
- **𝒟_MR**：生成的“记忆读取”训练数据（每个样本为 d' ∈ 𝒟_MR）

#### **辅助函数：**
1. **Triples(e_target, d)**：  
   返回文档 d 中包含实体 `e_target` 作为主语或宾语的所有三元组（triples）。用于从文档中提取与目标实体相关的知识。

2. **QueryResult(q, M)**：  
   给定一个查询 `q` 和一个外部记忆 `M`，返回查询结果（实体集合）。例如，通过 Wikipedia 提取的事实。

#### **变量定义：**
- **positionIdx**：实体在文档中的位置索引。
- **SeenTriples、SeenEntities**：用于记录已处理的三元组和实体，避免重复。
- **𝒟_MR**：最终生成的训练数据列表。

---

### **算法流程（伪代码）总结**

#### **步骤 1：初始化变量**
- `SeenTriples` 和 `SeenEntities` 初始化为空集合。
- `𝒟_MR` 初始化为空列表。

#### **步骤 2：遍历文档 d ∈ D**
对每个文档 d，执行以下操作：

- 初始化 `prevReadPos` 为 0，记录前一个读取位置。
- 初始化 `𝒟_d` 为当前文档的训练数据列表。

#### **步骤 3：遍历文档中的实体 e_target**
按照实体在文档中的出现顺序处理每个目标实体 e_target：

- 初始化查询列表 `q` 和查询结果字典 `R`。
- 遍历所有与 `e_target` 相关的三元组（通过 `Triples(e_target, d)` 函数）：
  - 如果该三元组未被处理过，则提取另一个实体 `e_q`。
  - 如果 `e_q` 已经被处理过（在 `SeenEntities` 中），则构造查询 `q = triple - e_target`。
  - 如果 `q` 没有被添加过，且查询结果数量小于阈值 Q_thr，则将该查询 `q` 和结果 `R_q` 加入 `q` 和 `R` 中。
  - 将该三元组加入 `SeenTriples`，防止重复处理。

#### **步骤 4：构建训练样本 d'**
- 如果构建的查询 `q` 非空：
  - 设置 `currentReadPos` 为 `e_target` 在文档中的位置。
  - 构建 `d'.pretext` 为文档中从开头到 `currentReadPos` 的文本。
  - 构建 `d'.queries` 和 `d'.results`：
    - 按查询结果数量从小到大排序 `q`。
    - 取前 3 个查询加入 `d'.queries`。
    - 合并这些查询的结果到 `d'.results`。
  - 如果结果为空，则将 `e_target` 本身作为结果。
  - 如果 `𝒟_d` 中已有样本，将当前样本的 `posttext` 设置为前一个读取位置到当前位置的文档内容。
  - 更新 `prevReadPos` 为 `currentReadPos`。
  - 将 `e_target` 添加到 `SeenEntities`。

#### **步骤 5：处理文档末尾**
- 将文档中最后的 `posttext` 设置为从 `currentReadPos` 到文档结尾的内容。
- 将当前文档的所有训练样本 `𝒟_d` 合并到全局的 `𝒟_MR` 中。

#### **步骤 6：返回最终训练数据**
- 返回生成的 `𝒟_MR` 作为 MemLLM 的训练数据。

---

### **重点总结**

- 该算法的核心是**从文档中提取与实体相关的三元组**，并基于这些三元组构建查询，从外部记忆中获取结果，最终生成记忆读取训练样本。
- 通过**限制查询结果数量（Q_thr）**和**选择最多 3 个查询**，确保训练数据的质量和多样性。
- 每个训练样本（d'）包含：**前文（pretext）**、**查询（queries）**、**查询结果（results）**、**后文（posttext）**，用于训练模型在上下文中使用记忆。

---

### **不重要内容精简说明**

- 一些变量初始化和边界检查（如 `|d'.results| = 0` 时默认设为 `e_target`）是为了保证数据一致性，属于细节实现部分。
- `prevReadPos` 和 `posttext` 的处理主要是为了构建上下文连续性，对训练数据结构有帮助，但不是核心创新点。

---

**总结**：本附录提供了 MemLLM 模型中“记忆读取”训练数据的生成算法，通过提取文档三元组、构建查询、检索外部记忆并构造上下文，为模型提供结构化的训练样本。这是 MemLLM 微调流程中的关键步骤之一。


## Appendix D Hyperparameters Details



### 1. 模型微调配置
- 使用 **Mistral-7B-v0.1** 模型进行微调。
- 优化器采用 **Adam**（参考 Kingma & Ba, 2015）。
- 学习率设置为 **2×10⁻⁵**。
- 训练 **2 个 epoch**，批量大小为 **96**。
- 对于 **LoRA 参数**（低秩适配）：
  - **Dropout 率**：0.1
  - **秩（Rank）**：16
  - **Alpha 权重**：8

**重点**：此部分提供了模型微调的基本配置，尤其关注 LoRA 参数的设置，这些参数对模型性能和可扩展性有重要影响。

---

### 2. 写入内存的阈值设定（Qt⁢h⁢r）

- 基于 **Re-DocRED 数据集** 中三元组（实体-关系-实体）的分布情况，设定 **Qt⁢h⁢r = 30**。
- 在构造内存写入数据时发现，**95% 的句子最多包含约 30 个三元组**。
- 该阈值用于判断是否将三元组写入内存：**若实体数量超过 30，则可能包含过多与句子事实无关的实体**，从而降低信息价值。

**重点**：Qt⁢h⁢r 的设定是为了控制写入内存的三元组数量，避免信息过载并提高相关性。

---

### 3. 内存检索参数选择（τe、τt、τr）

- **τe（实体匹配阈值）**：控制实体匹配的严格程度。
  - τe 越大，匹配越严格，内存更“明确”，但可能遗漏相似实体。
  - 实验中设置为 **0.7**，模型编辑实验中设为 **0.85**。
- **τt（关系类型阈值）**：控制关系类型的匹配灵活性。
  - 适用于需要处理关系相似性的场景（如模型编辑）。
  - 实验中设置为 **0.2**。
- **τr（检索输出阈值）**：控制最终检索结果的数量。
  - τr 越大，检索结果越“明确”，但可能限制多样性。
  - 实验中设置为 **0.85**，模型编辑实验中设为 **0.6**。

**重点**：这组参数（τe、τt、τr）是模型在使用显式读写内存时的关键控制参数，其设置直接影响信息检索的精度与效率。不同任务场景下需权衡显式性与检索的灵活性。

---

### 总结

本附录详细列出了模型微调的超参数设置、内存写入阈值（Qt⁢h⁢r）的选择依据，以及内存检索中三个关键参数（τe、τt、τr）的设定原则和实验值。这些配置在模型性能和内存机制的有效性中起重要作用，尤其在处理实体-关系对的检索与存储方面。


## Appendix E Filtering Prompt



在图 [5](https://arxiv.org/html/2404.11672v3#A5.F5 "Figure 5 ‣ Appendix E Filtering Prompt ‣ MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory) 中，我们展示了在 DocRED 的远程监督子集上表现最佳的过滤提示（filtering prompt）。

**图5** 展示了用于远程监督数据集过滤的提示（prompt）。该提示包含了关系的自然表示、推理过程和最终答案。

该图的重点在于展示一种优化的提示结构，用于提升模型在远程监督数据上的过滤性能。远程监督数据通常存在噪声，因此通过设计合理的提示来提升模型对关系三元组的识别和过滤能力，是提升模型效果的重要手段。

**总结重点：**

- 图5 展示了在 DocRED 数据集远程监督子集上效果最优的提示。
- 该提示包含三个核心部分：关系的自然表示、推理过程和最终答案。
- 此提示设计有助于提升模型对关系的识别和过滤能力，从而提升整体性能。

**次要内容（精简讲解）：**

- 该图是论文中用于说明提示设计效果的一个示例。
