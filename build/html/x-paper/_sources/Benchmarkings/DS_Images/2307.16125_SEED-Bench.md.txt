# 2307.16125_SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension

* 首页: [https://arxiv.org/abs/2307.16125](https://arxiv.org/abs/2307.16125)
* PDF: [https://arxiv.org/pdf/2307.16125](https://arxiv.org/pdf/2307.16125)
* 引用: 673(2025-07-13)
* GitHub: 
    * <https://github.com/AILab-CVC/SEED>
    * <https://github.com/AILab-CVC/SEED-Bench>
* 组织:
    * 1Tencent AI Lab 
    * 2ARC Lab, Tencent PCG

## 总结

* 简介
    * 评测多模态大语言模型（Multimodal LLMs）在生成式理解（Generative Comprehension）方面的基准测试
    * 核心目标
        * 评估模型在处理视觉和语言相结合的任务时，是否能够生成连贯、准确且富有洞察力的理解与回应
    * 评价指标
        * 使用 BLEU、ROUGE、CIDEr 等自然语言生成指标，
        * 并结合人工评估（如事实准确性、逻辑性、流畅性等）
* 数据集
    * 19,242 个高质量多选题
        * 答案通过人工标注
        * 远高于现有基准（MME 的 2194 题和 MMBench 的 2974 题）
        * 涵盖图像和视频等多种模态的理解
    * 评估维度
        * 9个空间理解
            * 场景理解、实例身份识别、属性识别、位置定位、实例计数、空间关系、实例交互、视觉推理和文本识别
        * 3个时间理解
            * 动作识别、动作预测和过程理解
    * 数据来源
        * 空间理解的问题基于CC3M数据集
        * 时间理解的问题则来自SSV2、Epic-kitchen 100和Breakfast等数据集
    * 多选题生成
        * 预训练模型（如BLIP2、Tag2Text、SAM、OCR）提取图像中的信息
        * 再利用ChatGPT/GPT-4生成问题和选项
        * 人工标注者验证答案和分类





## LLM 总结

这篇文章提出了 **SEED-Bench**，这是一个用于评测多模态大语言模型（Multimodal LLMs）在生成式理解（Generative Comprehension）方面的基准测试。研究的核心目标是评估模型在处理视觉和语言相结合的任务时，是否能够生成连贯、准确且富有洞察力的理解与回应。

**主要内容总结如下：**

1. **背景与动机**：  
   虽然现有的多模态模型在图像识别和基本问答任务上表现出色，但缺乏针对“生成式理解”能力的系统性评估。生成式理解要求模型不仅能识别内容，还需进行推理、解释、总结甚至创造新的内容。因此，文章提出 SEED-Bench 作为新的评估基准。

2. **SEED-Bench 的设计特点**：  
   - **任务多样性**：涵盖图像描述生成、视觉推理、跨模态检索与生成等任务。  
   - **高质量数据集**：数据由多轮人工标注和筛选，确保质量与难度。  
   - **生成式评价指标**：使用 BLEU、ROUGE、CIDEr 等自然语言生成指标，并结合人工评估（如事实准确性、逻辑性、流畅性等）。

3. **实验与分析**：  
   - 对多个当前主流的多模态 LLM（如 LLaVA、MiniGPT-4、Flamingo 等）在 SEED-Bench 上进行评估。  
   - 实验结果显示，模型在某些任务上表现良好，但在需要深层理解或复杂推理的任务中仍存在明显不足。

4. **研究贡献**：  
   - 提出一个系统性且多样化的多模态生成式理解评测基准。  
   - 揭示当前多模态 LLM 在生成式任务中的挑战与局限性。  
   - 为未来模型改进和研究方向提供参考依据。

**总结**：  
SEED-Bench 是一个旨在推动多模态 LLM 在生成式理解方面发展的新基准。它通过多样化任务和严格评估方式，帮助研究人员更好地理解和提升模型在视觉-语言任务上的综合能力。


## Abstract

该论文介绍了名为 SEED-Bench 的基准测试，旨在评估多模态大语言模型（MLLMs）在生成式理解能力方面的表现。SEED-Bench 包含 19,000 道多选题，涵盖图像和视频等多种模态的理解，覆盖了 12 个评估维度，并且数据规模是现有基准的 6 倍。该基准通过自动过滤与人工验证相结合的方式构建问题，确保评估的客观性和效率。论文还评估了 18 个模型在所有维度上的表现，揭示了当前 MLLMs 的局限性，旨在为未来研究提供参考。最后，作者将发布并维护一个排行榜，供社区评估和研究模型能力。


## 1 Introduction



该节主要介绍了 **SEED-Bench** 这一全新的多模态大语言模型（MLLMs）评估基准。以下是该章节内容的总结：

---

### **1. 背景与动机**
- 近年来，**大语言模型（LLMs）** 在文本理解、推理和生成方面表现突出。
- 基于 LLMs，**生成式多模态大语言模型（MLLMs）** 在多模态理解和生成方面的能力也得到了提升。
- 然而，当前对 MLLMs 的评估方式存在以下问题：
  - **评估内容有限**：多数依赖少量定性案例或早期为封闭式任务设计的基准。
  - **评估方式主观**：如 VQA-v2 要求模型输出与标准答案完全一致（通常只有几个词），难以反映多模态生成模型的全面能力。
  - **缺乏标准化基准**：现有基准如 MME、MMBench 等虽然尝试构建客观评估体系，但数据量较小（均少于 3000 个样本），评估结果不够稳定，且依赖人工或 GPT 评估，效率低、主观性强。

---

### **2. 相关研究与现有基准比较**
- 已有工作尝试构建 MLLMs 评估基准，如：
  - **LVLM-eHub** 和 **LAMM**：利用现有视觉数据集和人工/GPT 评估模型输出质量。
  - **MME** 和 **MMBench**：采用 Yes/No 或多选题形式，提升评估的客观性。
- **不足之处**：
  - 依赖人工或 GPT 评估，效率和客观性差。
  - 数据量小，统计结果不稳定。
  - 多为图文理解任务，缺乏对视频等时序信息的评估。

---

### **3. SEED-Bench 的提出**
- 为了更全面地评估 MLLMs 的**生成式理解能力**（Generative Comprehension），作者提出了 **SEED-Bench**。
- **SEED-Bench 的特点**：
  - 覆盖 **图像与视频** 两种模态。
  - 包含 **12 个评估维度**（如空间理解、时间理解等），每个维度对应模型的不同能力方面。
  - 提供 **19,242 个高质量多选题**，答案通过人工标注，远高于现有基准（MME 的 2194 题和 MMBench 的 2974 题）。
  - 选项为 A/B/C/D 形式，便于自动化评估。
  - 通过自动化生成、过滤及人工验证相结合的方式构建数据，确保数据质量。

---

### **4. 数据生成与过滤流程**
- **图像信息提取**：使用多种基础模型提取图像信息，如图像描述、实例描述和文本元素。
- **视频信息提取**：依赖原始人工标注。
- **问题生成**：将提取的信息输入 ChatGPT 或 GPT-4，根据特定评估维度生成问题及四个选项。
- **过滤机制**：利用多个 LLMs 过滤掉无需视觉输入即可回答的问题。
- **人工验证**：标注正确答案并分类至相应评估维度，最终构建高质量数据集。

---

### **5. 评估与实验**
- 在 SEED-Bench 上评估了 **18 个模型**，包括 LLMs、ImageLLMs 和 VideoLLMs。
- **评估方法**：
  - 与 MMBench 中使用 ChatGPT 匹配答案的方式不同，作者采用 GPT-3 的 **log-likelihood 方法**，直接计算每个选项的可能性并选择最大值，避免依赖模型的指令跟随能力。
- **发现与结论**：
  - 当前大多数 MLLMs 在 12 个维度上的表现仍较有限。
  - 意外发现：**VideoLLMs 在时序理解能力上并未显著优于 ImageLLMs**，显示出当前模型设计的局限性。
- **目标**：
  - 通过 SEED-Bench 提供客观评估手段，为未来 MLLMs 的研究提供方向。
  - 后续将建立评估平台并维护排行榜，持续更新模型评估结果。

---

### **总结**
SEED-Bench 是一个大规模、多模态、面向生成式理解能力的 MLLMs 评估基准。通过覆盖图像和视频、12 个评估维度及高质量多选题，解决了现有评估方式在规模、客观性和覆盖范围上的不足。该工作不仅为当前 MLLMs 提供了全面评估工具，也为未来模型能力提升提供了研究方向。


## 2 Related Work



本章“相关工作”主要总结了多模态大语言模型（MLLMs）及其评估基准的研究现状，内容总结如下：

1. **多模态大语言模型（MLLMs）**：近年来，随着大语言模型（LLMs）的显著进展，研究者们开始探索生成式多模态大语言模型，以提升模型在多模态理解和生成方面的能力。一些研究还拓展到视频输入，利用LLMs的强大能力进行视频理解任务。本文提出的 SEED-Bench 对这些模型进行了全面的定量评估，以系统比较其在生成式理解任务中的表现。

2. **多模态大语言模型的评估基准**：随着MLLMs的快速发展，已有多个研究提出了相应的评估基准。例如：
   - GVT 聚合了视觉问答（VQA）、图像描述生成、对象计数和多类识别任务，但评估范围有限。
   - LVLM-eHub 整合了多个视觉基准，并开发了一个在线平台，通过人工标注进行模型表现比较，但存在偏见和高成本问题。
   - LAMM 利用实体提取和 GPT 来评估模型的回答质量，但依赖性强，影响评估的准确性和可靠性。
   - MME 和 MMBench 通过构建大量选择题（True/False 和 Multiple Choice）来提升评估的客观性，但数据规模较小，评估结果可能不稳定。

3. **SEED-Bench 的提出**：为了解决现有基准的不足，本文提出 SEED-Bench，它包含 19,000 多项选择题，覆盖 12 个评估维度，涵盖空间与时间理解，旨在为 MLLMs 提供一个更全面、客观的评估体系。


## 3 SEED-Bench



SEED-Bench是一个包含19,000个准确标注的多选题的基准测试，用于评估多模态大语言模型（MLLMs）在空间和时间理解方面的能力。本节介绍了SEED-Bench的评估维度（3.1节）、数据来源（3.2节）、多选题构建流程（3.3节）以及针对MLLMs的评估策略（3.4节）。

1. **评估维度**  
   SEED-Bench涵盖了9个空间理解和3个时间理解的评估维度，共计12个方面。空间理解包括场景理解、实例身份识别、属性识别、位置定位、实例计数、空间关系、实例交互、视觉推理和文本识别。时间理解则涵盖了动作识别、动作预测和过程理解。每个维度都有具体的示例问题，以评估模型对视觉信息和时间动态的理解能力。

2. **数据来源**  
   空间理解的问题基于CC3M数据集，通过过滤和标注生成丰富的图像描述。时间理解的问题则来自SSV2、Epic-kitchen 100和Breakfast等数据集，利用其精确的动作标注来构建问题。

3. **多选题生成**  
   多选题生成流程包括视觉信息提取和问题验证。通过预训练模型（如BLIP2、Tag2Text、SAM、OCR）提取图像中的信息，再利用ChatGPT/GPT-4生成问题和选项。为确保问题质量，使用LLM自动筛选，剔除不依赖视觉信息即可解答的问题，并由人工标注者验证答案和分类，最终构建出19,000个高质量多选题。

4. **评估策略**  
   SEED-Bench采用答案排序策略评估模型表现，计算模型生成每个选项的可能性，选择可能性最高的作为预测结果。该方法避免了模型对输出格式的依赖，提高了评估的客观性和准确性。

总结：SEED-Bench是一个全面、高质量的多模态大语言模型基准测试工具，专注于评估模型在空间和时间理解方面的能力，涵盖12个维度的19,000个问题，并提供严格的评估策略以确保评估的公平性和有效性。


## 4 Evaluation Results



本章节主要评估并分析了多种多模态大语言模型（MLLMs）在 **SEED-Bench** 测试基准上的表现。SEED-Bench 包含 12 个评估维度，分为空间理解（9 个维度）和时间理解（3 个维度），通过准确率（Acc）和排名（Rank）两项指标对模型进行比较。

---

### **4.1 模型评估范围**
评估共涉及 18 个模型，包括：
- **3 个 LLM 模型**：Flan-T5、Vicuna、LLaMA；
- **12 个 ImageLLM 模型**：如 BLIP-2、InstructBLIP、LLaVA、MiniGPT-4、VPGTrans 等；
- **3 个 VideoLLM 模型**：VideoChat、Video-ChatGPT、Valley。

每个模型都在 12 个维度上接受了评估，旨在全面衡量其在空间和时间理解方面的能力。

---

### **4.2 评估结果**
总体来看：
- **InstructBLIP（尤其是 InstructBLIP-Vicuna）** 在空间和时间理解方面均表现最佳，总准确率为 53.37%，排名第一。
- **ImageLLM 模型**（如 BLIP-2、InstructBLIP）在空间理解上的表现优于 VideoLLM，但在时间理解（如动作识别、过程理解）方面表现一般。
- **LLaVA** 在文本识别方面表现突出，但整体表现中等。
- **VideoLLM 模型**（如 Valley）在时间理解方面普遍表现不佳，甚至不如部分 ImageLLM 模型。
- **VPGTrans** 在过程理解和动作识别方面表现最佳。
- **LLaMA-Adapter V2** 在动作识别方面有一定优势。
- **部分模型**（如 OpenFlamingo、GVT）在多个维度上表现均衡。

---

### **4.3 分析与发现**
1. **模型整体表现有限**：
   - 多数 MLLMs 在 12 个维度上的平均准确率低于 50%。
   - 除了 BLIP 系列模型，其他 MLLMs 在多个维度上表现远不及 LLM。

2. **图像全局理解较强**：
   - MLLMs 在**场景理解**和**视觉推理**方面表现优于 LLM，显示出更强的全局图像理解能力。

3. **InstructBLIP 表现突出**：
   - 在 12 个维度中，InstructBLIP 在 8 个维度上排名第一。
   - 可能原因包括：使用了大量指令调优数据（1600 万样本），以及冻结 LLM 权重以减少灾难性遗忘。
   - 但其在**动作识别**和**过程理解**方面仍较弱，可能因训练数据与测试任务不一致。

4. **空间关系理解较弱**：
   - 即使是最优模型 InstructBLIP，其在空间关系理解上的准确率仅为 40%，显示出识别物体间精细空间关系的难度。

5. **文本识别能力不足**：
   - 多数模型在文本识别方面表现不佳（准确率低于 40%），主要由于多模态预训练数据中缺少丰富的文本元素。

6. **VideoLLM 的潜力与局限**：
   - **VideoChat** 在图像空间理解方面表现良好，表明其在图像和视频联合训练中未损失空间能力。
   - **大多数 VideoLLM** 在时间理解方面（如动作识别、过程理解）表现不佳，尤其是 Video-ChatGPT 和 Valley，甚至不如部分 ImageLLM。

7. **未来研究方向**：
   - 提高模型在**细粒度时间推理**和**空间关系理解**方面的能力。
   - 增加多模态预训练数据中**文本元素**的比例，以提升文本识别性能。
   - 针对视频数据的模型（VideoLLM）应进一步优化其**动作识别**和**时间关系建模**能力。

---

### **总结**
SEED-Bench 提供了对 MLLMs 在空间和时间理解能力上的系统评估。结果显示，尽管部分模型（如 InstructBLIP）在多个维度上表现优异，但整体来看 MLLMs 在时间理解方面仍存在明显短板，尤其是在细粒度动作识别和过程理解上。未来的研究应关注提升模型在这些关键任务上的表现，并优化多模态数据的预训练策略。


## 5 Conclusion



本文总结如下：

本文提出了一项大规模基准测试——SEED-Bench，用于对多模态大语言模型（MLLMs）在生成式理解能力方面进行全面、客观的评估。该基准测试包含19,000个经过人工精确标注的多项选择题，覆盖空间和时间理解的12个评估维度。作者设计了一种先进的流程，用于生成针对特定评估维度的多项选择题，以支持在多个领域中评估数据的扩展性。同时，通过自动筛选和人工验证相结合的方式，提升了生成问题与答案的质量。研究团队对18个模型进行了全面评估与比较，为未来研究提供了有价值的见解。作者计划发布并持续维护排行榜，为社区提供一个评估模型表现的平台，并将持续扩展SEED-Bench的评估维度和数据量。

此外，作者感谢了多位研究人员在建议、数据处理和讨论方面的支持与贡献。
