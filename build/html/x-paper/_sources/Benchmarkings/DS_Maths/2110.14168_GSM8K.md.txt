# 2110.14168_GSM8K: Training Verifiers to Solve Math Word Problems

* 首页: [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)
* PDF: [https://arxiv.org/pdf/2110.14168](https://arxiv.org/pdf/2110.14168)
* 组织: OpenAI
* 引用: 4789(2025-07-25)
* GitHub: <https://github.com/openai/grade-school-math>


## 总结

* 简介
    * 全称是Grade School Math 8K
    * 题目需要多步推理才能解决，涉及基础算术、分数、比例、简单代数等
    * 关键特点：所有题目均由人工精心编写，并附有详细的、逐步的自然语言解题步骤和最终答案
* 数据集
    * 8.5k个高质量、语言多样性的小学数学应用题，由人类编写
        * 其中7,500题用于训练，1,000题用于测试
    * 数据集构建的四个设计原则
        * 高质量
        * 高多样性
        * 适中难度
        * 自然语言解答
    * 来源
        * 最初通过在 Upwork ( upwork.com ) 上雇佣自由职业者来收集一千个问题和自然语言解决方案
        * 与 NLP 数据标记平台 Surge AI ( surgehq.ai ) 合作，扩大我们的数据收集规模
        * 所有问题被重新分配给不同工作者进行解答，以检验答案的一致性，不一致的问题被修复或丢弃
* 创新点
    * 说明
        * 传统思维链直接生成答案容易出错
        * 本文先用生成器出多个候选解，再用验证器筛选最优解
            * 思路类似人类检查作业
    * 核心创新：训练“验证器”
        * 核心思想： 将“求解数学题”的过程分解为两个阶段：
            * a. 生成候选解
                * 使用一个语言模型（生成器）为给定的问题生成多个（例如100个）不同的候选答案及其对应的完整推理步骤
            * b. 验证和选择最优解
                * 使用另一个专门训练的模型（验证器）来评估这组候选解
                * 然后，选择被验证器评估为最可能正确的那个候选解作为最终答案
* 关键洞见：“过程比结果更重要”
    * 通用性
        * 虽然论文聚焦于数学应用题（GSM8K）
        * 但提出的生成候选解 + 训练验证器进行选择的方法被认为是一种通用框架
        * 有潜力应用于其他需要复杂、精确、多步推理的任务领域
    * 核心洞见：在需要精确推理的任务中，建模和验证“过程”比直接预测“结果”更有效。

* 其他点
    * 测试时计算（Test Time Compute）
        * 在测试阶段，可以通过生成多个候选解并由验证器（verifier）进行评分，最终选择最高排名的解
        * 多数投票机制：为提升性能，可对排名靠前的多个解进行多数投票
    * 正则化（Regularization）




## LLM 总结

其主要研究内容是：通过训练验证器（verifiers）来解决数学应用题。论文的重点在于，与其直接训练模型生成完整的解题过程，不如训练模型验证某个解题过程是否正确。这种方法可以提高模型在数学问题上的推理能力和准确性。

总结如下：

1. **研究背景**：数学应用题的解决需要模型具备较强的逻辑推理和数学运算能力。传统方法直接训练模型生成解题步骤，存在步骤复杂、错误率高、难以验证等问题。

2. **研究方法**：论文提出了一种新的训练方法，即训练模型作为“验证器”，用于判断给定的解题过程是否正确。验证器的任务不是生成解题过程，而是判断已有的解题过程的正确性。

3. **训练方式**：验证器通过监督学习的方式进行训练，使用已经标注正确的解题过程作为正样本，以及错误的解题过程作为负样本。这样模型可以学习到正确解题的逻辑和格式。

4. **实验结果**：实验表明，通过训练验证器，模型可以显著提高在数学应用题上的表现。验证器不仅能准确判断其他模型生成的解题过程是否正确，还能在生成解题过程时表现更好，说明验证能力有助于提升推理能力。

5. **研究意义**：该方法为解决数学应用题提供了新的思路。通过将生成任务转化为验证任务，可以更有效地利用数据和模型的能力，提升整体求解效果，也为其他需要推理能力的任务提供了借鉴。

总体而言，这篇论文提出了一种通过训练验证器来增强数学问题求解能力的新方法，具有较强的创新性和实用性。


## Abstract



本文介绍了GSM8K数据集，这是一个包含8,500个高质量、语言多样性的小学数学应用题的数据集，旨在诊断当前语言模型在多步骤数学推理任务中的失败原因并支持相关研究。尽管当前最先进的Transformer模型在许多任务上能接近人类水平，但在简单的数学推理问题上表现不佳。为此，作者提出了一种验证方法：训练验证器来判断模型生成的答案是否正确，并在测试时生成多个候选答案，选择验证器评分最高的作为最终结果。实验表明，验证方法显著提升了GSM8K上的性能，并且相比微调方法，验证方法在数据增加时具有更好的扩展性。


## 1 Introduction

![](https://img.zhaoweiguo.com/uPic/2025/07/GsExKZ.png)

Figure 1: Three example problems from GSM8K. Calculation annotations are highlighted in red.


这段引言总结如下：

近年来，大型语言模型在多种任务中表现出色，但它们在需要多步骤数学推理的任务中仍存在明显不足。即使经过微调，模型在生成解决方案时也容易出现严重错误，且无法自我纠正，造成了推理过程的不可恢复性。当前依赖生成方法在复杂的数学数据集（如MATH）上实现良好表现需要极高的参数量，这促使研究者寻找更有效的解决方案。

为此，作者提出使用“验证器”（verifier）来评估模型生成的解决方案的正确性。与传统的生成方法相比，验证是一个更简单的任务，并且具有更高的灵活性。在测试时，通过验证器对多个候选解决方案进行排序，选择最优解，从而提升整体性能。

此外，作者发布了GSM8K数据集，包含8500道小学数学题，语言多样性高，但基于基础数学概念。尽管语言模型在该数据集上表现不佳，但其问题结构简单，为研究提供了一个可行的评估目标。

主要贡献包括：

1. 提出并发布GSM8K数据集，用于评估语言模型的非正式推理能力；
2. 实验证明，使用验证器带来的性能提升与模型规模扩大30倍相当，且验证器在数据量增加时更具扩展性；
3. 证明了Dropout在微调和验证过程中的显著正则化效果，有助于提升性能。


## 2 Dataset



本章节介绍了GSM8K数据集的相关信息：

GSM8K是一个包含8,500个高质量小学数学题的数据集，由人类编写。其中7,500题用于训练，1,000题用于测试。这些题目通常需要2到8步才能解决，解题过程主要依赖基本的算术运算（加、减、乘、除）。一个优秀的小学或初中生应当能够解决其中的每一个问题。

GSM8K的构建基于以下四个设计原则：

1. **高质量**：数据集避免通过容易出错的自动抓取方式生成，而是由人类编写并经过严格的质量控制，确保问题错误率低于2%。
2. **高多样性**：问题在语言表达和结构上高度多样，避免使用相同的模板或仅在细节上不同，从而提升测试集评估的准确性。
3. **适中难度**：问题难度对于当前最先进的语言模型具有挑战性，但又不至于完全无法解决，旨在帮助研究者了解模型在中等难度任务中的表现。题目知识范围不超过初级代数，大多数无需定义变量即可解决。
4. **自然语言解答**：所有问题的解答以自然语言形式呈现，而非纯数学表达式，以便更好地研究语言模型的内部推理过程，并鼓励问题编写者以多样化的语言风格详细解释解题步骤。

完整的GSM8K数据集可在GitHub上获取，文中还展示了部分示例问题，并在附录中提供了更多数据集的细节信息。


## 3 Related Work



该章节综述了与数学文字题（math word problems）相关的数据集和方法。

**3.1 相关数据集**  
早期的数学文字题数据集规模较小，不适合测试现代语言模型的极限。Dolphin18K 和 AQuA-RAT 虽然规模较大，但存在模板化严重、答案质量不高或缺少自然语言解题过程的问题。MathQA 是对 AQuA-RAT 的改进，但仍存在约 30% 的数据不一致问题。Ape210K 是目前最大的中文数学题数据集，但语言障碍和缺乏自然语言解题过程限制了其使用。ASDiv 是一个高质量且多样化的 2.3K 数学题数据集，GSM8K 虽然与其设计目标相似，但规模更大、提供自然语言解题过程，且解题步骤更多。MATH 数据集虽然更大更复杂，但对当前语言模型来说挑战性过高。此外，其他相关数据集还包括专注于符号数学、阅读理解和常识问答的，GSM8K 在这些问题类型上也具有一定的联系。

**3.2 相关方法**  
早期研究使用序列到序列模型解决数学文字题，后续工作通过设计专用的编码器-解码器架构取得了改进。近年来，基于 BERT 等大规模预训练语言模型的方法表现优异。此外，一些研究通过额外的预训练任务提升模型的数学推理能力，例如使用 Khan Academy 和 Mathematica 构建的 AMPS 语料库，或基于互联网课程构建的预训练数据。还有一些工作通过生成并选择多个模型输出的方法提升性能，例如样本生成与排序方法。与 Shen et al. 的工作类似，本文也使用生成与验证的结合方法，但重点在于自然语言解题过程，强调模型的可解释性和与人类认知的对齐，同时验证器在数据量增加时表现更优，并通过生成器与验证器的分离结构避免过拟合。


## 4 Methods



本文第四部分“方法”主要介绍了在解决 GSM8K（数学应用题数据集）任务中使用 GPT-3 系列模型的两种方法：**微调（Finetuning）** 和 **验证（Verification）**，并探讨了它们在不同模型规模和训练数据量下的表现。

---

### **方法概述**

- **微调（Finetuning）**：
  - 使用 GPT-3 的语言建模目标对模型进行微调。
  - 测试时采用低温采样（low temperature sampling）生成一个答案，并判断其是否正确。
  - 基线方法，但存在算术错误、过拟合等问题。

- **验证（Verification）**：
  - 采样多个高温解（high-temperature samples），并使用验证模型（verifier）对每个解进行评分。
  - 验证模型根据解是否得到正确答案进行训练，最终选择得分最高的解作为输出。
  - 通过验证模型识别模型生成解中的错误，提高整体准确性。

---

### 2. **模型与训练设置**

- 使用 GPT-3 系列的 6B 和 175B 两种模型规模进行实验。
- **175B 模型**在性能上显著优于 6B 模型。
- **微调策略**：在训练集上进行 20 个 epoch 的微调，测试性能取决于单次低温度（T=0）采样的结果。
- **验证策略**：训练验证器时，首先用微调后的模型生成 100 个解，再用验证器判断其正确性。

---

### 4.1 Finetuning

- 测试表现随训练集规模和模型规模增加而提升。
- **175B 模型**在训练集较大时表现最佳，但也存在算力和数据量需求过高的问题。
- **6B 模型**进行 100 次猜测时，初期表现较好，但随着训练周期增加，性能下降明显。
- **关键发现**：模型需要生成完整的自然语言解（包含中间步骤），否则性能会显著下降（如从 20.6% 降至 5.2%）。

---

### 4.2 Verification

![](https://img.zhaoweiguo.com/uPic/2025/07/e97z2d.png)

Figure 4: A diagram of the verification training pipeline.


- **验证流程**：
  1. 用 2 个 epoch 微调生成器模型；
  2. 为每个训练问题生成 100 个解并标记正确性；
  3. 使用这些生成的解训练 verifier 模型 1 个 epoch。
- **验证器的作用**：在测试时对多个生成解进行评分并选择最优解。
- **验证效果**：
  - 在小数据集上，验证效果不明显，因为容易过拟合；
  - 在大数据集上，验证显著提升性能；
  - 175B 验证器比 6B 验证器更早“起飞”，即更早超越微调基线。
- **验证器的训练细节**：可以结合语言模型目标作为辅助任务，提升验证能力。

---

### 4.3 Verification Ablations


- **解粒度验证**：
  - **Solution-level**：对整个解判断正确性；
  - **Token-level**：在每个 token 阶段判断正确性（效果更好）；
  - Token-level 虽训练更慢但最终表现更好，因它鼓励模型在生成过程中持续判断推理是否合理。

- **训练目标实验**：
  - 将验证目标与语言模型目标结合训练，效果优于仅用验证目标。

- **模型规模实验**：
  - 用大模型作为生成器、小模型作为验证器效果更好；
  - 验证器可能依赖粗略启发式判断，而非细致验证。

- **测试时计算资源实验**：
  - 增加每个问题的候选解数量（如 100 个）可提升性能；
  - 允许多个高分解共同决定最终答案也有助于提高准确率。


### 总结

本章通过系统实验比较了微调和验证两种方法在数学应用题求解中的效果，得出以下关键结论：

1. **175B 模型在性能上远优于小模型，但需要大量训练和测试资源**；
2. **验证方法在大数据集下明显优于微调方法**，尤其是在使用大模型时；
3. **在验证过程中，使用 token-level 验证器、结合语言模型目标、使用大生成器与小验证器组合** 等设计策略能显著提升效果；
4. **模型解题性能与生成完整解的步骤密切相关**，跳跃步骤会显著影响性能。

这些方法和实验为训练可靠的数学解题模型提供了有效的研究路径。


## 5 Additional Experiments



本章“5 Additional Experiments”主要围绕模型在测试时的计算策略以及正则化对模型性能的影响展开实验和分析，内容总结如下：

---

### 5.1 测试时计算（Test Time Compute）

在测试阶段，可以通过生成多个候选解并由验证器（verifier）进行评分，最终选择最高排名的解。实验显示：

- **生成解的数量**：随着每个测试问题生成的解数量增加（最多400个），验证器的性能先提升后下降，说明过多解可能引入对抗性解，干扰验证器判断。
- **多数投票机制**：为提升性能，可对排名靠前的多个解进行多数投票，仅根据最终答案选择。实验表明：
  - 当样本数量较少（如100个）时，仅允许前3-5个解参与投票最为有效。
  - 当样本数量较多（如3200个）时，允许前30个解参与投票效果最佳。

---

### 5.2 正则化（Regularization）

本节探讨了**Dropout**作为一种正则化手段对模型微调和验证器性能的影响：

- **Dropout设置**：采用残差Dropout（Residual Dropout），在每层的残差路径上设置20%的Dropout比例。由于GPT-3模型在预训练时未使用Dropout，因此在涉及Dropout的实验中，先对模型进行含Dropout的预训练，再进行微调，以缓解微调过程中的分布偏移。
- **微调（Finetuning）**：实验显示，在不同训练集规模下，使用Dropout显著提升了模型的微调性能。
- **验证器（Verifiers）**：
  - **Solution-level verifiers**：Dropout能有效缓解过拟合问题，其性能接近token-level verifiers。
  - **Token-level verifiers**：由于其本身对过拟合的敏感度较低，Dropout的增益较小，但仍有一定提升。为应对Dropout引入的噪声和更复杂的训练目标，实验中将batch size增加了4倍。

---

### 总结

这一章通过实验揭示了测试时增加解数量和多数投票机制对验证器性能的影响，并验证了Dropout在模型微调和验证器训练中的重要作用。实验结果表明，适当使用这些策略可以有效提升模型在数学问题解决任务中的表现。


## 6 Conclusion



本章总结了验证（verification）方法在模型性能上的优势。相比微调基线，验证方法在性能上有了显著提升。在完整数据集上，6B参数的验证模型甚至略微优于175B参数的微调模型，相当于模型规模扩大30倍所带来的性能提升。此外，研究发现，基于token的验证器比基于解决方案的验证器更不容易过拟合，所有方法在使用残差dropout正则化后都获得了更好的效果。作者认为验证方法在需要更复杂数学推理的问题分布中具有良好的扩展性，并希望GSM8K数据集能够促进性能更优的新方法的发展。


## Appendix A Dataset Details



本附录详细介绍了数据集的构建过程。研究人员最初通过Upwork平台雇佣自由职业者，收集了一千个问题及其自然语言解答。随后，他们与NLP数据标注平台Surge AI合作，扩大数据收集规模。在数据集构建完成后，所有问题被重新分配给不同工作者进行解答，确保没有人解答自己编写的问题，以检验答案的一致性。不一致的问题被修复或丢弃，进一步检查后仍有1.7%的问题存在争议，估计是由于错误或歧义所致，实际可能更多问题存在细微错误。

为帮助工作者编写问题，研究人员提供了由GPT-3 175B模型生成的示例问题，工作者可直接使用、修改或自创问题。研究人员还要求解答尽可能详细，并避免问题模板重复。为确保这一点，计算了问题之间的相似度，并据此向工作者提供反馈。


## Appendix B Hyperparameters



该章节总结了论文实验中使用的超参数配置及其相关实验结果。主要内容包括：

1. **超参数概览**：
   - 批量大小为32,000 tokens，最大样本长度为400 tokens，使用名为`reversible_50000`的分词器。
   - 优化器为Adam，其参数 β₁=0.9，β₂=0.95。
   - dropout 为 0.0（即未使用 dropout）。
   - 学习率调度使用线性衰减到 0 的方式。

2. **微调超参数**：
   - 训练轮数为 20 轮。
   - 采样温度为 0（即使用 argmax 采样）。
   - 基础学习率根据模型大小不同而变化（3B: 1.6×10⁻⁵，6B: 1.2×10⁻⁵，12B: 1.0×10⁻⁵，175B: 6.0×10⁻⁶），实际使用的学习率为基础学习率的 0.1 倍。

3. **验证器超参数**：
   - 验证器的训练轮数分别为生成器 2 轮，验证器 1 轮。
   - 验证器采样温度为 0.7，学习率为 1.0×10⁻⁵。
   - 损失函数为均方误差（MSE），损失权重为 1.0。
   - 每个训练问题生成 100 个完成（completion），测试问题同样使用 100 个完成。

4. **实验结论**：
   - 作者尝试了学习率和批量大小在表中值的上下一个数量级的范围，但未能找到显著性能提升。
   - 验证器的采样温度（如 0.7 vs 1.0）和损失函数（如 MSE vs 交叉熵）在消融实验中影响也较小。

5. **例外说明**：
   - 部分实验使用了不同的配置，如图 8(c) 中使用了 4 倍的批量大小和 300 个完成，图 8 中部分实验使用了 20% 的 dropout。
   - 图 7(a) 中验证器在训练时使用 100 个完成，但在测试时使用更多完成进行搜索。

总结来说，该章节提供了论文中所有实验的超参数设置，并指出对关键参数进行调整后性能变化不大，强调了当前设置的合理性与稳定性。


## Appendix C Calculator Annotations

![](https://img.zhaoweiguo.com/uPic/2025/07/FGbrz6.png)

Figure 9: A diagram of the calculator sampling procedure.


这段文字主要总结了训练数学问题求解模型中使用的“计算器注释”（Calculator Annotations）的生成和使用方法。以下是总结内容：

1. **注释生成方式**：  
   - 计算器注释并非由人工提供，而是通过硬编码逻辑和微调后的语言模型自动生成。
   - 生成逻辑并不完美，虽然很少生成错误注释，但常常会忽略一些可以添加注释的行。

2. **训练与测试阶段的区别**：  
   - 在训练阶段，注释的标记与解决方案中的其他标记一样，没有特殊处理。
   - 在测试阶段，当存在格式良好的注释时，模型会覆盖采样结果，特别是覆盖“=”之后以及<<…>>内的标记。

3. **计算器的实现**：  
   - 使用 Python 的 `eval` 函数对表达式中的标记进行求值（如图9所示）。
   - 若求值超时或发生错误，则跳过该注释，并按正常方式采样模型。

4. **计算器的缺陷与影响**：  
   - 原始版本的计算器在本文所有结果中存在一些轻微实现错误。
   - 因此，报告的测试性能实际上是一个略微的低估，且在大多数实验中误差幅度小于1%。
   - 修正计算器后，使用完整的 GSM8K 训练集，验证测试性能提高了约1%。

5. **图示说明**：  
   - 图9展示了计算器采样过程的示意图（图中未展示，但可参考原文）。

总结：本节详细介绍了用于模型训练和验证的“计算器注释”的自动生成机制、实现方式、存在的问题及其对模型性能的实际影响。


## Appendix D Example Model Solutions



本节为附录 D“示例模型解决方案”，主要展示了在 6B 和 175B 两种模型规模下，微调（finetuning）与验证（verification）方法的对比样本。文中提到这些样本在一定程度上经过挑选，以展示多样性。文中包含了八张未加标题的图像（编号为 example_solutions_1 至 example_solutions_8），用以直观展示不同模型规模和方法下的结果差异，但具体图像内容未给出描述。总结而言，该节通过示例呈现了不同规模模型在不同训练方法下的表现效果。


## Appendix E Verifier Details



本附录主要介绍了**验证器（Verifier）的实现细节**，主要包括以下几个要点：

1. **验证器的结构**：  
   验证器是在语言模型的基础上添加了一个**标量头（scalar head）**，该头由一个偏置参数和一个增益参数组成，用于对语言模型最终的logits进行缩放和偏移，从而输出每个token的预测结果（正确或错误）。这种设计允许验证器在保留语言建模能力的同时，执行验证任务。

2. **特殊token的作用**：  
   标量头仅作用于词汇表中的一个特殊token，该token用于表示验证结果。其他token仍用于语言建模任务，确保模型不会干扰原有的语言生成能力。

3. **验证器的初始化**：  
   验证器可以基于原始预训练模型或生成器（generator）进行初始化。实验表明，从生成器初始化的验证器表现更优，推测是因为验证器能更好地理解生成器所学习的语言分布。

4. **联合训练目标（Joint Objective）**：  
   在训练过程中，验证器同时学习语言建模任务和验证任务，两者的目标函数直接相加，没有权重区分。由于每个训练样本生成100个完成（completion），验证数据的比例被放大了100倍。一个训练周期定义为每个验证样本仅出现一次。

5. **训练过程中的掩码处理**：  
   在训练过程中，模型会**忽略问题部分的token**，仅在解（solution）部分的token上进行计算损失，如图12所示。这种方式确保验证器专注于评估答案部分的正确性。

总结来说，本节详细描述了验证器的结构设计、训练方式及其与生成器的交互机制，强调了如何通过联合训练目标使验证器在保持语言建模能力的同时，准确评估模型生成答案的正确性。


## Appendix F Verifier Visualization



该附录“Verifier Visualization”主要介绍了如何通过可视化token级验证器（verifier）的预测结果，来提高对模型输出质量的判断能力。具体总结如下：

1. **验证器的可解释性**：token级验证器通过为每个token生成一个评分，使得模型的判断过程变得可视化和可解释。验证器预测的值可以用背景颜色显示，绿色表示高分（预测正确），红色表示低分（预测错误）。

2. **可视化案例**：
   - 附录中展示了五个精选的例子，由一个经过微调的175B模型生成，并由一个训练在完整训练集上的175B token级验证器进行评分。
   - 表格第二列总结了验证器的预测结果，第三列是实际模型输出是否正确。若两者不一致，说明验证器判断错误。

3. **案例分析**：
   - **第一行**：真阳性（True Positive），验证器正确判断输出为正确。模型在推理过程中逐渐增强信心，这可能是训练过程中大量错误样例的结果。
   - **第二行**：假阴性（False Negative），问题中存在“4 times”和“4 potatoes”的歧义，导致验证器误判。
   - **第三行**：另一个假阴性，模型的最终答案虽然正确，但推理过程有误，验证器正确给出了低分。
   - **第四行**：模型初始正确，但推理中出现错误（如计算错误），验证器逐渐失去信心，最终判断为错误。
   - **第五行**：假阳性（False Positive），模型在第二步中错误地处理了变量绑定（如将钻石珠宝的价格错误地减去了400），验证器偶尔会犯这种错误。

4. **总结**：token级验证器提供了一种直观的方式，帮助理解模型的生成质量。它虽然并非完美，但通过可视化评分和分析错误案例，有助于改进模型训练和验证策略。
