# 1809.09600_HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering

* 首页: <https://arxiv.org/abs/1809.09600>
* PDF: <https://arxiv.org/pdf/1809.09600>
* 引用: 3188(2025-07-21)
* 组织:
    * ♠ Carnegie Mellon University
    * ♥ Stanford University 
    * ♣ Mila, Universite de Montreal(加拿大)
    * ♦ CIFAR Senior Fellow
    * † Google AI

## 总结

* 数据集
    * 113,000对基于维基百科的问答对
        * train-easy: 18,089 个单跳问题
        * train-medium: 56,814 个多跳问题
        * train-hard: 15,661 个较难的多跳问题
    * 旨在解决多跳推理（multi-hop reasoning）能力测试不足的问题
* 其他
    * 没别的可看的，大概有个了解就行


## Abstract

作者指出，现有的问答（QA）数据集无法有效训练QA系统进行复杂的推理并提供答案的解释。为此，他们提出了一个新的问答数据集 **HotpotQA**，包含113,000对基于维基百科的问答对。该数据集具有四个关键特点：

1. 问题需要从多个支持文档中查找并进行推理才能回答；
2. 问题多样化，不局限于任何预设的知识库或知识结构；
3. 提供用于推理的句子级别支持事实，使QA系统能够在强监督下进行推理并解释预测；
4. 引入了一种新的“事实比较”问题，用于测试系统提取相关事实并进行比较的能力。

实验结果显示，HotpotQA 对当前最先进的问答系统具有挑战性，而支持事实的使用有助于模型提升性能并实现可解释的预测。


## 1 Introduction



以下是对该论文章节内容的总结：

---

**1. 引言部分主要内容总结：**

本节介绍了HotpotQA数据集的背景、目的及其创新性，旨在解决当前问答数据集中存在的多跳推理（multi-hop reasoning）能力测试不足的问题。

1. **背景与问题分析：**
   - **当前数据集的局限性**：现有问答数据集（如SQuAD、TriviaQA、SearchQA）主要测试单跳推理（single-hop reasoning），即问题的答案可在单一段落或句子中找到，难以评估系统在多文档间进行推理的能力。
   - **基于知识库的数据集问题**：一些专门测试多跳推理的数据集（如QAngaroo、ComplexWebQuestions）依赖于已有知识库（KB），导致问题多样性和自然语言表达受限。
   - **监督方式不足**：几乎所有现有数据集只提供最终答案，缺乏支持答案的依据（supporting facts），不利于模型学习推理过程和生成可解释的回答。

2. **HotpotQA数据集的提出：**
   - **目标**：HotpotQA旨在通过自然语言处理多文档信息，进行多跳推理，并提供高质量的监督信息（即答案依据的支撑事实）。
   - **数据来源**：基于维基百科文章，通过众包方式构建，确保问题的自然性和多样性，且不受任何知识库结构的限制。
   - **数据收集流程**：设计了专门的数据收集流程，确保高质量的多跳问题生成。同时，要求工人提供支撑事实，这些也被纳入数据集，增强可解释性。
   - **新增题型**：引入了比较类问题（comparison questions），要求系统对两个实体的共同属性进行比较，以测试对语言和常识的理解。

3. **示例说明**：
   - 提供了一个典型问题示例（如关于Andrew Wood和其乐队的背景问题），展示了HotpotQA中多跳推理的必要性，并标出支撑事实。

4. **总结与贡献**：
   - HotpotQA为问答系统提供了一个新的挑战：在自然语言环境下进行多文档推理，并通过提供支撑事实来提升模型的可解释性。
   - 数据集已公开，希望为未来研究提供支持，并推动问答系统在多跳推理方向的发展。

---

**关键词总结：**  
多跳推理（multi-hop reasoning），单跳推理（single-hop reasoning），HotpotQA，问答数据集，自然语言推理，可解释性，比较问题，多文档推理，维基百科，众包。


## 2 Data Collection



本章内容主要介绍了 **HotpotQA 数据集的收集方法和流程**，旨在构建一个**多样化、可解释的、需要多跳推理**的问答数据集。以下是本章的总结：

---

### 1. **研究目标**
- 构建一个**基于文本**（而非知识库）的多跳问答数据集。
- 与基于知识图谱的方法不同，该方法更注重**文本内容的多样性**和**问题的可解释性**。
- 每个问题要求模型从多个段落中进行**多步推理**，并给出答案和支撑事实。

---

### 2. **数据收集的挑战**
- 直接让标注者从任意段落中提出多跳问题效率低，问题质量差。
- 因此，需要设计一个**结构化流程**来高效收集有质量的多跳问题。

---

### 3. **数据收集的主要步骤**

#### （1）**构建维基百科超链接图**
- 使用**英文维基百科的完整转储**作为语料库。
- 从每篇文章的**首段**中提取超链接，构建一个**有向图 G**，其中边 (a,b) 表示 a 的首段链接到 b 的文章。
- 观察到：超链接常表示两个实体之间的关系，适合用于多跳推理。

#### （2）**生成候选段落对**
- 从超链接图中，抽样边 (a,b)，其中 b 是预先筛选的“桥梁实体”。
- 通过这些桥梁实体，构建段落对，作为多跳问题的输入。

#### （3）**比较类问题**
- 除了基于桥梁实体的问题外，还设计了**比较类问题**，例如比较两位 NBA 球员的出场次数。
- 从维基百科中人工筛选了 42 个实体列表（如“世界最高峰”），从中随机抽取两个实体组成段落对。
- 引入**是/否问题**，增强推理复杂性（例如：“Iron Maiden 和 AC/DC 是否来自同一国家？”）。

---

### 4. **支撑事实的收集**
- 每个问题配以多个**支撑事实**（即决定答案的关键句子），帮助模型学习解释其推理过程。
- 这些支撑事实由标注者提供，作为模型训练和评估的监督信号。

---

### 5. **数据收集流程（算法 1）**
- 整体流程通过设定两种问题类型的比例（默认 75% 为桥梁实体问题，25% 为比较问题）进行控制。
- 每次迭代中，根据比例选择问题类型，再抽样段落对或实体对，让标注者生成问题和支撑事实。

---

### 6. **创新点**
- 提出**基于文本的比较类多跳问题**，这是前人工作中未涉及的。
- 强调**可解释性**，要求模型不仅给出答案，还需指出支撑答案的关键事实。
- 通过**人工筛选和结构化流程**，有效提升了问题质量和多样性。

---

### 总结
本章系统地介绍了 HotpotQA 数据集的构建方法，强调了多跳推理、问题多样性与可解释性。通过构建维基百科超链接图、设计桥梁实体和比较类问题、收集支撑事实，该方法实现了高质量、多样化、可解释的多跳问答数据集的构建。


## 3 Processing and Benchmark Settings



本章节主要介绍 **HotpotQA 数据集的处理和基准测试设置**，内容可以总结如下：

### 1. **数据收集与分组**
- 通过 Amazon Mechanical Turk 平台，使用 ParlAI 工具收集了 **112,779 个有效的问答样本**。
- 为了区分单跳（single-hop）和多跳（multi-hop）问题，首先从高产用户中随机抽样，创建了一个名为 **train-easy** 的子集，包含 **18,089 个主要为单跳问题的例子**。
- 接下来，使用当前最先进的问答模型对剩余的样本进行三折交叉验证，选出 **60% 被模型高置信度正确回答的多跳问题**，作为 **train-medium** 子集，共 **56,814 个例子**。
- 剩下的问题被标记为 **train-hard**，共 **15,661 个例子**，表示较难的多跳问题。

### 2. **训练与测试集划分**
- 将 train-easy、train-medium 和 train-hard 合并作为**默认的训练集**。
- 将 hard examples 随机划分为 **dev、test-distractor 和 test-fullwiki** 三个测试集，每个包含 **7,405 个例子**。
- 数据划分如表 1 所示：

| 数据集名称       | 用途     | 样本数量  |
|------------------|----------|-----------|
| train-easy       | 训练     | 18,089    |
| train-medium     | 训练     | 56,814    |
| train-hard       | 训练     | 15,661    |
| dev              | 验证     | 7,405     |
| test-distractor  | 测试     | 7,405     |
| test-fullwiki    | 测试     | 7,405     |
| **总计**         |          | **112,779** |

### 3. **两种基准测试设置**
- **distractor 设置**：
  - 每个问题中混入 **8 个通过 bigram tf-idf 检索的 Wikipedia 扰动段落**，与 **2 个真实支持段落** 混合后输入模型。
  - 目的是测试模型在噪声存在的情况下识别真实支持段落的能力。
- **fullwiki 设置**：
  - 模型需在**没有指定支持段落的前提下**，从**所有 Wikipedia 文章的首段**中检索并回答问题。
  - 更真实地模拟实际场景中多跳推理的挑战。
- 两种设置使用**不同的测试集**，以避免信息泄露。

### 4. **train-medium 的分析**
- train-medium 的多跳问题比例与 hard examples 接近（93.3% vs. 92.0%）。
- 但是，**Type II 类型的问题（如涉及多个实体需推理的问题）在 train-medium 中更常见**（32.0% vs. 15.0%）。
- 这表明现有模型在训练数据充足的情况下，可以较好地处理某些类型的多跳问题，但在真实检索场景下仍具有挑战性。

### 总结：
本章节详细说明了 HotpotQA 数据集的构建过程、数据划分逻辑、训练与测试设置，以及两种不同难度的基准测试模式（distractor 和 fullwiki），为后续的模型实验和评估提供了清晰的基础。


## 4 Dataset Analysis



本章节对HotpotQA数据集的**问题类型**、**答案类型**和**多跳推理类型**进行了详细分析，主要总结如下：

---

### 一、问题类型分析
- **定义与识别**：
  - 问题类型通过识别“核心问题词”（Central Question Word, CQW）和其周围词汇进行提取。
  - CQW定义为问题中前三个词内的疑问词，否则为最后一个疑问词。例如WH-词、系动词（如is、are）、助动词（如does、did）。
  - 问题类型由CQW及其右侧最多两个词以及左侧的常见介词（如in、by）确定。
- **类型分布**：
  - HotpotQA包含实体、地点、事件、日期、数字等多样化问题，以及比较性质的yes/no问题（如“Are both A and B …?”）。
  - 问题类型分布如图2所示，涵盖了超过250个问题的常见类型。

---

### 二、答案类型分析
- **答案类型分布**：
  - HotpotQA的答案类型多样化，如表2所示。
  - 最常见的是实体类（68%），其中“人物（Person）”占30%，其他如“组织（Group/Org）”（13%）和“地点（Location）”（10%）也较为常见。
  - 日期（9%）、数字（8%）和形容词（4%）等描述性属性也占有一定比例。
  - 少量问题的答案类型为事件（Event）或普通名词（Common noun）等。

| 答案类型       | 百分比 | 示例                |
|----------------|--------|---------------------|
| 人物（Person）   | 30%    | King Edward II, Rihanna |
| 组织/团体（Group/Org） | 13%    | Cartoonito, Apalachee |
| 地点（Location） | 10%    | Fort Richardson, California |
| 日期（Date）     | 9%     | 10th or even 13th century |
| 数字（Number）   | 8%     | 79.92 million, 17     |
| 艺术作品（Artwork） | 8%     | Die schweigsame Frau   |

---

### 三、多跳推理类型分析
- **分类与比例**：
  - 作者从开发集和测试集中抽取100个样本，手动分类了所需的推理类型，结果如表3所示。
  - **主要类型**包括：
    - **类型I（42%）**：通过“桥梁实体”完成第二跳推理（如通过球员姓名推导其所属球队）。
    - **类型II（15%）**：通过多个属性定位答案实体（如通过昵称和所属球队确定某人）。
    - **类型III（6%）**：通过桥梁实体推断目标实体的属性（如通过地点推断单位所在城市）。
    - **比较类型（27%）**：比较两个实体的属性（如成员人数是否相同）。
    - **其他类型（2%）**：需要超过两个支撑事实的问题。
  - 其余8%为单跳问题（6%）或不可回答问题（2%）。
- **推理策略**：
  - 多跳问题通常需要从两个段落中获取支撑事实，并通过桥梁实体连接问题和答案。
  - 解题策略包括将问题拆分为多个单跳问题，或通过属性交集确定答案。
- **数据质量**：
  - 多数支撑事实与标注一致，说明数据标注质量较高。

| 推理类型                         | 百分比 | 示例说明                                                                 |
|----------------------------------|--------|--------------------------------------------------------------------------|
| 类型I（链式推理）                 | 42%    | 通过桥梁实体完成第二跳（如球员→MVP→球队）                             |
| 类型II（多属性定位）              | 15%    | 满足多个属性后取交集（如昵称+球队→人物）                              |
| 类型III（通过桥梁属性推断）       | 6%     | 通过桥梁实体的共性属性推断答案（如地点→单位所在地）                    |
| 比较类型（实体比较）              | 27%    | 比较两个实体的属性（如国籍、人数、年龄等）                            |
| 其他类型（需多事实）              | 2%     | 需要超过两个事实支持的问题                                              |
| 单跳问题                         | 6%     | 只需一个段落即可回答                                                  |
| 无法回答的问题                   | 2%     | 当前上下文无法回答的问题                                              |

---

### 四、其他发现
- **训练集数据**：
  - 在训练集中（*train-medium* 和 *train-hard*），推理类型的比例略有不同，类型I占38%，类型II占29%，比较类型占20%。
- **数据多样性**：
  - 数据集涵盖了广泛的实体、属性和推理方式，适合作为多跳问答系统的研究基础。

---

### 总结
HotpotQA数据集通过全面的问题类型、多样的答案形式以及多跳推理的结构设计，展现了其在**可解释性多跳问答**任务中的丰富性和挑战性。分析表明，该数据集不仅覆盖了多种常见问题和答案类型，还引入了多种复杂推理形式，为研究多跳问答系统提供了高质量的数据基础。


## 5 Experiments



第五章“实验”主要围绕作者提出的HotpotQA数据集，对多跳问答模型的性能、可解释性以及人类表现进行了评估和分析。以下是该章节内容的总结：

---

### 5.1 模型架构与训练

作者基于Clark和Gardner（2017）的模型架构构建了一个基线模型，并在SQuAD数据集上实现了接近原论文的性能。模型整合了近年来问答任务中的关键技术，如字符级模型、自注意力机制（self-attention）和双向注意力机制（bi-attention）。此外，为了处理“是/否”类型的问题，作者在模型的最后添加了一个三分类器，分别预测“是”、“否”和“文本片段”类型的回答。

为提升模型的可解释性，作者引入了一个**支持事实预测组件**，通过二分类器预测每句话是否为支撑答案的关键句子，该任务与问答任务共享底层表示，并通过多任务学习联合优化。该架构以端到端方式实现，便于训练和调优。

---

### 5.2 实验结果

作者在两种基准设置下评估了模型性能：

1. **Distractor Setting（干扰段落设置）**：包含少量候选段落（2个正确段落+8个干扰段落）。
2. **Full Wiki Setting（全维基百科设置）**：从数百万维基段落中检索相关段落。

**主要发现包括：**

- 在Distractor Setting下，EM（精确匹配）为44.44%，F1为58.28%；在Full Wiki Setting下性能显著下降，EM为24.68%，F1为34.36%。这表明检索难度的增加对模型提出了更高挑战。
- 引入**支持事实的强监督**对问答性能有明显提升，去除该组件后模型性能下降。
- 模型在比较类问题（Comparison Questions）上的表现通常低于桥梁实体类问题（Bridge Entity Questions），这可能与数据构造和模型结构有关。
- 实验发现，自注意力机制和字符级模型对性能提升有显著贡献，说明单跳QA中的技术在多跳场景中仍有一定效果。
- 通过消融实验发现，结合所有训练数据（简单、中等和困难数据）训练模型可获得最佳性能。

作者还引入了**联合评估指标**，如**联合F1**和**联合EM**，用于同时评估答案和支撑事实的准确性，以衡量模型的可解释性。

---

### 5.3 人类表现评估

为评估人类在HotpotQA任务上的表现，作者随机选取了1000个样本，由至少三名工作人员提供答案和支撑事实，并与原始数据进行对比。

**主要发现包括：**

- 原始标注者在答案和支撑事实任务上表现优秀。
- 如果基线模型能直接获得正确段落，其在支撑事实任务上可接近人类水平，但在答案预测上仍有差距。
- 在存在干扰段落的Distractor Setting中，模型与人类之间的差距增加到30%左右。
- 人类在答案任务上的一致性较高，但在支撑事实任务上一致性较低，说明支撑事实的标注更具主观性。

作者还计算了人类表现的**上限（Upper Bound）**，发现大多数指标接近100%，说明大多数样本存在多个标注者之间的一致性。

---

### 总结

第五章系统地评估了作者提出的HotpotQA数据集上的模型性能和可解释性，验证了多跳问答任务的挑战性，并展示了支撑事实监督在提升模型性能和可解释性方面的有效性。此外，通过与人类表现的对比，揭示了当前模型与理想水平之间的差距，为未来研究指明了方向。


## 6 Related Work



本章节总结了当前主流的问答（QA）数据集的相关研究工作，并将其分为四类：

1. **单文档问答数据集**：如SQuAD，其问题相对简单，通常只需要文档中的一句话即可回答。

2. **多文档问答数据集**：如TriviaQA和SearchQA，包含多个文档作为上下文，增加了问答系统的挑战性。但这些文档是通过信息检索在问题生成后收集的，因此问题未必涉及多个文档之间的复杂推理。

3. **基于知识库的多跳问答数据集**：如QAngaroo和ComplexWebQuestions，利用预定义的知识库和逻辑规则生成QA对，以测试模型的多跳推理能力。但此类数据集的问题和答案多样性受限于知识库的结构，且由于知识库本身的不完整性，部分问题可能仅需单句回答。

4. **自由形式答案生成数据集**：如MS MARCO，包含来自用户的真实查询和人工生成的答案，系统需生成自由形式的答案，并通过ROUGE-L和BLEU-1等自动评估指标进行衡量。然而，这些指标与人类判断的相关性较低，评估结果可能存在偏差。


## 7 Conclusions



本章节总结了论文的主要结论：作者提出了 HotpotQA 数据集，这是一个大规模的问答数据集，旨在推动能够进行可解释、多跳推理的问答系统的发展。此外，作者还引入了一种新的事实比较类问题，用于测试系统在文本中提取和比较不同实体属性的能力。


## Appendix A Data Collection Details



本章节总结如下：

---

## 附录A 数据收集细节

### A.1 数据预处理

作者使用了2017年10月1日的英文维基百科数据转储，并通过 **WikiExtractor** 工具提取文本和超链接。为了进行分词和句子边界识别，使用了 **Stanford CoreNLP 3.8.0**。句子边界用于收集支持事实，而分词边界用于验证众包工作者（Turkers）提供的答案是否覆盖完整的词元，避免出现非完整的、无意义的答案。

---

### A.2 数据收集进一步细节

#### 维基百科页面的筛选

为了确保众包工作者能够提出高质量的多跳问题，作者手工从维基项目（WikiProject）中选取了591个受欢迎的页面类别。对于每个类别，从图结构 $ G $ 中采样 (a, b) 页对，其中 b 属于当前类别，并人工检查是否可以在这个页对上提出多跳问题。最终选择那些可以生成多跳问题概率较高的类别。

#### 激励机制

为了提高数据质量和收集效率，作者采用了两种激励机制：

1. 每收集200个样本后，奖励贡献样本最多的工作者；
2. 根据每小时产出的样本数量（生产力）给予奖励。

---

### A.3 众包工作者界面

作者基于 **ParlAI**（一个开源对话系统和数据收集工具）设计了众包工作者界面。通过将问答对的收集流程转化为系统导向对话，作者能够更好地控制输入内容，并为工作者提供实时反馈和提示，从而加快数据收集过程。

界面示例如图4所示（截图来自 Amazon Mechanical Turk 平台）。

---

**总结**：本章节详细介绍了 HotpotQA 数据集构建过程中的数据预处理方法、维基页面筛选策略、激励机制设计以及用于收集问题答案对的用户界面。这些设计有助于提高数据质量并加快数据收集进程。


## Appendix B Further Data Analysis



本附录对 HotpotQA 数据集进行了进一步的数据分析，重点考察了问题长度的分布情况。通过可视化分析（见图5），作者发现数据集中的问题不仅在类型上具有多样性，其长度也存在显著差异，反映出问题复杂度和细节覆盖程度的不同。图中显示了问题长度（以 token 为单位）与问题数量之间的分布关系，揭示了数据的多样性特征。


## Appendix C Full Wiki Setting Details



### 内容总结：

本部分介绍了在 **全维基百科设置（Full Wiki Setting）** 中使用的 **倒排索引筛选策略** 和 **训练集与困难样本的数据比较**，具体内容如下：

---

#### **C.1 倒排索引筛选策略（Inverted Index Filtering Strategy）**

1. **输入**：问题文本 $ q $、控制阈值 $ N $、n-gram 到维基文档的倒排索引 $ \mathcal{D} $。
2. **目标**：通过倒排索引策略高效筛选出可能包含答案的候选段落。
3. **算法流程**：
   - 从问题中提取 uni-gram 和 bi-gram 组成集合 $ r_q $。
   - 遍历 $ r_q $ 中的每个词，统计其在倒排索引中出现的维基文档。
   - 按照重合度（即文档中包含的关键词数量）递增筛选候选文档。
   - 当候选文档数量超过设定阈值 $ N $ 时，停止筛选。
4. **实验设置**：控制阈值 $ N=5000 $。
5. **评估说明**：
   - 如果金标准段落（gold paragraph）未出现在最终候选池中，其排名设为 $ |S_{cand}| + 1 $。
   - 因此，论文中报告的 MAP 和 Mean Rank 是真实值的上界。

---

#### **C.2 训练集（train-medium）与困难样本（dev/test）的比较**

1. **比较目的**：分析训练集中 `train-medium` 与困难样本（`dev` 和 `test`）在检索性能上的差异。
2. **结果展示**（见表 9）：
   - 各样本的检索性能（MAP、Mean Rank、CorAns Rank）接近。
   - 表明 `train-medium` 的难度接近 `dev` 和 `test`，说明在全维基设置下，这些样本都需要依赖检索模型作为前处理步骤。
3. **数据统计**：
   - `train-medium`：MAP 41.89%，Mean Rank 288.19，CorAns Rank 82.76
   - `dev`：MAP 42.79%，Mean Rank 304.30，CorAns Rank 97.93
   - `test`：MAP 45.92%，Mean Rank 286.20，CorAns Rank 74.85

---

### 总体总结：

本章详细介绍了在全维基百科设置下，如何通过倒排索引策略进行候选段落筛选，并通过实验验证了该方法的有效性。同时，通过比较 `train-medium` 与困难样本的检索性能，说明了当前数据集在难度上的均衡性，对模型的设计和评估具有指导意义。
