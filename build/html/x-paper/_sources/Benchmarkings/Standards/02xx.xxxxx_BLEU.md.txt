# 02xx.xxxxx_BLEU: a Method for Automatic Evaluation of Machine Translation

* [https://aclanthology.org/P02-1040/](https://aclanthology.org/P02-1040/)
* PDF: [https://dl.acm.org/doi/pdf/10.3115/1073083.1073135](https://dl.acm.org/doi/pdf/10.3115/1073083.1073135)
* 组织: IBM T. J. Watson Research Center
* 引用: 34680(2025-07-15)


## 总结

### 核心思想

BLEU主要基于两个核心概念：
1. **n-gram精确度（Precision）**：计算候选翻译中n-gram（连续的n个词）在参考翻译中出现的比例。通常会计算从1-gram到4-gram的精确度。
2. ** brevity penalty（简短惩罚）**：如果候选翻译的长度比参考翻译短，则会受到惩罚，因为短的翻译可能会遗漏信息。

### 概念和定义：

* n-gram：指的是连续的n个词的序列，用于衡量翻译的精确度。
* 精确度（Precision）：计算机器翻译中与参考翻译匹配的n-gram的比例。
* 惩罚因子（Brevity Penalty）：用于惩罚过短的翻译结果，以确保翻译的完整性。

### 公式

$$
\mathrm{BLEU}=\mathrm{BP} \cdot \exp \left(\sum_{n=1}^{N} w_{n} \log p_{n}\right)
$$


### 论文贡献点
* BLEU评分方法: 本文提出了一种新的自动评估机器翻译质量的方法——BLEU（Bilingual Evaluation Understudy），该方法通过比较机器翻译结果与一个或多个参考翻译之间的n-gram重叠度来量化翻译的质量。
* 实用性: BLEU评分能够快速、自动地评估翻译质量，克服了人工评估的时间和成本问题，适用于大规模机器翻译系统的评估。
* 实验验证: 论文通过大量实验验证了BLEU评分的有效性，展示了其在不同翻译任务中的一致性和可靠性。

### 论文局限性
* 对语义的忽视: BLEU评分主要关注n-gram的重叠，可能无法充分捕捉翻译的语义和上下文信息，导致在某些情况下无法准确反映翻译质量。
* 参考翻译的依赖性: BLEU评分的结果高度依赖于参考翻译的质量和数量，若参考翻译不够多样或质量不高，可能会影响评分的准确性。
* 对短句的偏好: BLEU评分在处理短句时表现较好，但对于长句或复杂句的翻译评估可能不够敏感。


### 计算步骤

* 参数
    * gram Num: N
        * 默认为4
        * 即4-gram
    * n-gram 权重: w_n
        * 默认平均权重: w_n = 1/N

* 生成n-gram: 修正精度 p_n 的计算公式(以 p_1 为例)
    * 词频统计:
        * key: 单词, value: 词频
        * 分别计算: Candidate 和 Reference 的词频
        * 修正精度 p_n 对应的 Candidate 的词频
            * Candidate 的词频大于 Reference 的词频的，修改为 Reference 的词频
            * $Count_{clip} = min(Count, Max\_Ref\_Count)$
    * 计算总 clipped count
        * Candidate count: Candidate 的词频之和
        * Candidate clipped count: Candidate 修正后的词频之和
    * $p_n = \frac{\text{Candidate 修正后的词频之和}}{\text{Candidate 的词频之和}}$

* 分别计算N-gram 的修改精度 p_n
    * 1-gram 修改精度 p_1(Unigram 精度)
    * 2-gram 修改精度 p_2(Bigram 精度)
    * ...
    * 
    * N-gram 修改精度 p_n

* 取加权几何平均
* 简洁惩罚(Brevity Penalty) BP
    $$
    BP = \begin{cases}
    1 & \text{if } c > r \\
    e^{(1 - \frac{r}{c})} & \text{if } c \leq r
    \end{cases}
    $$
* BLEU 得分
   * $BLEU = BP × exp(∑ w_n × log(p_n))$
* logBLEU 得分
   * $logBLEU = BP + ∑ w_n × log(p_n)$



### 多个Reference处理

* 多个 Reference 的好处
    * 语言的多样性（Synonymy / Paraphrase）
        * 用单一参考会造成 不公平惩罚（惩罚掉其实合理但不同的表达）。
        * 多个参考能更好地 捕捉表达的多样性。
        * 示例说明
            * 他每天早上跑步半小时。
            * 翻译1: He runs for half an hour every morning.
            * 翻译2: He goes running for thirty minutes each morning.

    * 最大限度奖励合理表达
        * 希望候选句中如果包含任何一个合理的表达，都能获得匹配分数
        * 所以取多个参考中出现次数的最大值（而不是平均）

* 避免重复奖励（重复匹配）
    * 核心原则
        * 对于候选句中每一个 n-gram，其匹配计数最多只能被参考句中 出现次数的最大值（max） 所“裁剪”。
    * 示例
        * 一个单词在参考1中最多出现2次，在参考2中最多出现1次
        * 则：认为最多出现2次



## Abstract

* 人工评估机器翻译虽然全面但代价高昂，不仅耗时长，还需要大量无法重复使用的人力。
* 作者提出了一种自动化的机器翻译评估方法，它快速、便宜、不依赖语言，评估结果与人工评估高度相关，且每次运行的额外成本很低。
* 这个方法可以在需要快速或频繁评估时，代替人工评估使用。


## 示例讲解

* 说明: 
    * 为了简单，只计算了 Unigram 精度
    * 精度也没有计算 log
    * 本示例只为讲解，具体要参见上面 **计算步骤**

### 📝 输入

* **Candidate 1**:
  `the cat is on the mat`

* **Candidate 2**:
  `the cat the cat on the mat`

* **References**:

  * Reference 1: `the cat is on the mat`
  * Reference 2: `there is a cat on the mat`

* ✂️Token 化结果

| 句子          | Tokens                                             |
| ----------- | -------------------------------------------------- |
| Candidate 1 | `["the", "cat", "is", "on", "the", "mat"]`         |
| Candidate 2 | `["the", "cat", "the", "cat", "on", "the", "mat"]` |
| Reference 1 | `["the", "cat", "is", "on", "the", "mat"]`         |
| Reference 2 | `["there", "is", "a", "cat", "on", "the", "mat"]`  |


### 🎯 Candidate 1

候选长度 $c = 6$，匹配词如下：

* “the”: ✅（最多出现 2 次）
* “cat”: ✅
* “is”: ✅
* “on”: ✅
* “mat”: ✅

✅ 匹配数量 Candidate clipped count = 6
✅ 总数量 Candidate count = 6

$$
P_1 = \frac{6}{6} = 1.0
$$

参考句长度最接近的是 6（Ref1） → $r = 6$

$$
BP = \min(1, \exp(1 - r/c)) = \min(1, \exp(1 - 1)) = 1
$$

$$
\text{BLEU-1} = BP \cdot P_1 = 1 \cdot 1 = 1.0
$$

---

### 🎯 Candidate 2

候选长度 $c = 7$

* “the”: 出现 3 次（参考最多允许 2 次）✅✅❌
* “cat”: 出现 2 次（参考最多允许 1 次）✅❌
* “on”: ✅
* “mat”: ✅

✅ 匹配数量 Candidate clipped count = 5（the ×2, cat ×1, on, mat）
✅ 总数量 Candidate count = 7（the ×3, cat ×2, on, mat）

$$
\text{Precision} = \frac{5}{7} \approx 0.7143
$$


* 候选句长度 $c = 7$
* 参考句长度中最接近 7 的是 6 → $r = 6$

$$
BP = \min(1, \exp(1 - \frac{r}{c})) = \min(1, \exp(1 - \frac{6}{7})) = 1
$$

✅ 第四步：计算 BLEU-1

$$
\text{BLEU-1} = BP \cdot \text{Precision} = 0.7143
$$



## 1. Introduction


### 为什么提出 BLEU 方法？

* 人类评估机器翻译的质量时，会考虑译文的完整性、忠实度、流畅度等多个方面。
* 但这种人工评估方法既昂贵又耗时，常常需要几周甚至几个月完成，不适合快速迭代。
* 为了帮助开发者更高效地改进翻译系统，作者提出了一种自动化评估方法——**BLEU（Bilingual Evaluation Understudy）**。
* 它便宜、快速、适用于多种语言，并且结果与人工评估高度一致。


### 核心观点是什么？

机器翻译的质量可以通过“和人工参考译文的相似度”来衡量。
这个方法需要两样东西：

1. 一个能量化“翻译相似度”的指标；
2. 一份高质量的人工参考译文。

BLEU 的基本做法是：比较机器翻译与多个参考译文在词组上的匹配情况（考虑词序、同义词等差异），计算一个加权平均分，作为最终的评分。


## 2.The Baseline BLEU Metric

* **翻译有多种可能**，不同的词汇或语序也可能是好的翻译。**人类能判断翻译好坏**，即使多个翻译都不同。
* 程序可以通过比较**n-gram（n个词组成的短语）匹配**，来判断哪个翻译更好。
* BLEU 的核心任务就是**统计候选翻译和参考翻译之间的 n-gram 匹配数量**。
* 匹配的位置不重要，**匹配得越多，翻译质量越好**。
* 接下来会从最简单的 **unigram（一词匹配）** 开始讲起。

```note
要点总结：BLEU 通过数词组匹配来判断翻译质量，匹配多的翻译更好。
```

### 2.1 Modiﬁed n-gram precision

* 这段话的核心讲的是 **改进的 n-gram 精确度（Modified n-gram precision）**，用于评估机器翻译质量。
* 💡 什么是“改进的 n-gram 精确度”？
    * **原始精确度**：计算机器翻译中有多少词（unigram）在参考译文中出现，然后除以总词数。
    * **问题**：这种方式可能会奖励重复词语，比如“the the the...”，因为这些词在参考中出现过，导致评分虚高。
* ✅ 改进方式：
    1. **参考词数上限**：每个词在参考译文中最多出现几次，就设一个“最大参考次数”。
    2. **截断重复**：如果机器翻译中一个词出现多次，只按它在参考译文中出现的最大次数计算，多余的就不计了。
    3. **评分公式**：所有词的“截断后次数”相加 / 候选译文总词数。
* 🔍 举例说明：
    * 候选译文：`the the the the the the the`
    * 参考译文：`The cat is on the mat.`
    * 参考中 “the” 最多出现 2 次 → 所以候选中的 7 个 “the” 被截断为 2 个
    * → **改进后精确度 = 2 / 7**
* 🌟 推广到 n-gram：
    * 不只是单词（1-gram），还可以是词组（如 2-gram、3-gram）
    * 同样用“最大参考次数”来限制重复匹配，防止虚高
    * **1-gram 捕捉翻译的“涵盖性”**（adequacy）
    * **长 n-gram 捕捉“流畅性”**（fluency）


#### 2.1.1 Modified n-gram precision on blocks of text

* 这段话讲的是 **BLEU 评分中的修改 n-gram 精度（modified n-gram precision）** 是怎么计算的
* **评估单位是句子**：
    * 虽然翻译系统通常在整个文档上评估
    * 但 BLEU 是**逐句评估**，然后汇总所有句子的结果。
* **n-gram 精度计算方法**：
    * 对每个候选翻译的句子，统计它的 n-gram（如2-gram、3-gram）和参考翻译中匹配的数量
        * 匹配数量是“剪裁”的，即不超过参考中出现的次数）
    * 然后将所有候选句子的匹配总数相加，除以候选翻译中所有 n-gram 的总数
        * 得到整体的 modified n-gram precision，记为 $p_n$。
    * 公式是： $p_n = \frac{\sum_{\text{所有候选}} \sum_{\text{n-gram}} \text{剪裁后的匹配数}}{\sum_{\text{所有候选}} \sum_{\text{n-gram}} \text{总数量}}$
* **BLEU强调整体一致性**：
    * BLEU 的目标是确保整个测试集上的平均得分与人类评分一致
    * 而不是单个句子的评分必须一致。
* **参考翻译的多样性很重要**：
    * 比如，“East Asian economy” 与 “economy of East Asia” 表达相同意思，但 n-gram 不同。
    * BLEU 会因为 n-gram 不一样而扣分。
    * 所以最好用多个风格不同的人工翻译做参考，这样误差可以相互抵消。


#### 2.1.2 Ranking systems using only modiﬁed

* 研究人员使用改进的 n-gram 精度（modified n-gram precision）来评估翻译质量。
* 他们对比了：
    * 高质量的人类翻译
    * 低质量的机器翻译

* 结果发现：
    * 改进的 n-gram 精度能明显区分好坏翻译，n 从 1 到 4 越大，区分效果越强。
    * 这个指标不仅能区分好坏，还能细致地区分人类翻译之间的优劣。
    * 研究还比较了两个人类翻译者（一个母语者、一个非母语者）和三个机器翻译系统的结果。
    * 得分排名与人工评价的排序一致。

* 结论是：虽然单一 n-gram 精度已有明显信号，但将多个 n-gram 精度综合成一个指标会更稳定可靠。


#### 2.1.3 Combining themodiﬁed n-gram

* 这段话讲的是 **BLEU 算法中如何组合不同长度的 n-gram 精度（modified n-gram precisions）**：
* **问题：** 
    * 不同长度的 n-gram 精度（1-gram、2-gram、3-gram 等）差别很大
    * 越长的 n-gram 精度越低，呈指数下降
* **解决方法：** 
    * 如果直接用加权平均（线性平均）会不合理，因为忽略了这种下降趋势。
* **BLEU 的做法：**
   * 用的是所有 n-gram 精度的**对数的平均**（即几何平均），这样更能反映实际质量。
   * 使用 **统一权重**（即 1-gram、2-gram 等都一样重要）。
* **结果：**
   * 用几何平均比算术平均更符合人类判断，效果更好。
   * 用最多 4-gram 效果最佳，但 3-gram 或 5-gram 也差不多。


### 2.2 Sentence length(句子长度对翻译评估的影响)

1. **长度不能太长也不能太短**：
    * 好的翻译不应该太长或太短，评估指标应能体现这一点。

2. **改进的 n-gram 精确度部分解决了这个问题**：
   * 多余的词会被惩罚。
   * 如果某个词在候选翻译中比参考翻译中出现得更多，也会被惩罚。
   * 但这种方法无法完全避免翻译太短的情况。


#### 2.2.1 The trouble with recall

* 通常会使用“精确率+召回率”来衡量质量，但 BLEU 不这么做。
* 原因是：参考翻译里可能用了不同的同义词，一个好的翻译只会选一个表达方式，而不是全部“召回”。


#### 2.2.2 Sentence brevity penalty

* 如果候选翻译比参考翻译**长**，已经会被 n-gram 精确度惩罚；
* 但如果**太短**，需要额外的惩罚机制 → 引入 **长度惩罚因子（brevity penalty）**。
* 目标是让句子在长度、词汇、词序上都与参考翻译相似。
* 这个惩罚不是按句子平均，而是对整个语料库计算，以避免短句被过度惩罚。


### 2.3 BLEU details

1. **BLEU 得分 = 修改后的 n-gram 精度的几何平均 × 简洁惩罚项（brevity penalty，BP）**
   * 计算时会对候选翻译和参考翻译进行**大小写统一**（case folding），但不做其他文本归一化处理。

2. **精度部分**：
   * 先计算 1-gram、2-gram、... 到 N-gram（通常 N=4）的修改精度 `p_n`，
   * 再对这些 `p_n` 取加权**几何平均**（权重和为1）。

3. **简洁惩罚 BP**：
   * 如果候选翻译长度 c > 参考翻译长度 r，BP = 1（没有惩罚）；
   * 如果 c ≤ r，BP = exp(1 - r/c)（长度太短就有惩罚）。

4. **最终 BLEU 得分**：

   * $BLEU = BP × exp(∑ w_n × log(p_n))$
   * 在对数域下表示为： 
       * $logBLEU = min(1 - r/c, 0) + ∑ w_n × log(p_n)$

5. **默认设置**：
   * N = 4（用1到4-gram）
   * 每种 n-gram 权重相同，即 `w_n = 1/4`


## 3.The BLEU Evaluation

### BLEU评估简要说明：

1. **BLEU分数范围**是0到1，1表示翻译和参考完全一致，但几乎不可能达到。就算是人类翻译，也很难得分为1。
2. **参考翻译越多，BLEU分数越高**。所以，不同参考数量下的分数不能直接比较。

例：同一位人类翻译在使用4个参考时得分为0.3468，使用2个参考时只有0.2571。


### 关于参考翻译数量：

* 通过**从4个参考中随机选一个**，模拟“只有一个参考”的情况，结果显示：系统的排名顺序基本没变。
* 结论：只要测试语料足够大，即使每句话只有一个参考翻译，也能进行可靠评估，前提是参考译文不要都来自同一个人，以避免风格单一。


## 4.The Human Evaluation

* 分为两个评审小组：
  * **单语组**：10位以英语为母语的人。
  * **双语组**：10位以中文为母语、长期在美国生活的人。

* 所有人都不是专业翻译。
* 从500句测试语料中随机挑出部分句子，对5个翻译系统的输出进行评分。
* 每个中文句子配5个英文翻译，共250对翻译对。
* 所有评分都在网页上进行，句子和系统顺序随机。
* **评分标准**：1分（很差）到5分（很好）。
  * 单语组只看**流畅性和可读性**。
  * 双语组更关注**翻译是否准确（Adequacy）**。


### 4.1 Monolingual group pairwise judgments

* 系统S2比S1好不少（平均差0.326分），S3比S2略好（差0.114分），都在统计上显著（95%置信水平）。
* 人类翻译H1比所有系统都好，但略逊于H2。
    * H1不是母语者（中英文都不是）。
    * H2是以英语为母语的人，表现最好。


### 4.2 Bilingual group pairwise judgments

* 他们也认为S3略优于S2（有统计意义）。
* 对于两个人工翻译H1和H2，他们认为两者差不多（没有显著差异）。
* 说明双语组更看重**翻译内容是否正确**，不太在意语言流畅度。


## 5.BLEU vs The Human Evaluation

* 研究者对五个系统的翻译结果进行了 **BLEU 分数** 和 **人工评价** 的对比分析：
    1. **单语评估组（monolingual group）** 和 BLEU 分数之间的相关性非常高，达到 **0.99**，说明 BLEU 与人工评价高度一致。
    2. 尤其值得注意的是：BLEU 能很好地区分两个得分接近的系统（S2 和 S3）。
    3. **双语评估组（bilingual group）** 和 BLEU 的相关性也很高，为 **0.96**。
    4. 为了进一步比较，研究者以表现最差的系统为参考点，对其他系统的分数（BLEU、单语、双语）进行归一化处理（即按最大最小值线性缩放），结果显示 BLEU 与单语评价非常接近。
    5. 图表还显示了机器翻译系统和人工翻译之间仍有明显差距。
    6. 最后一点：相比之下，**双语评估组对人工翻译H1比H2更宽容**，但单语组认为两者之间存在较大流畅度差异。

* 简而言之：BLEU 与人工评分高度一致，特别是在区分细微差异方面表现出色，但也有一些细节上双语评估更宽容。

## 6.Conclusion

* 作者认为 BLEU 评分可以加快机器翻译（MT）的研究与开发，因为它能帮助研究人员快速判断哪些建模方法有效。他们引用了一项研究，说明 BLEU 在从多种语言（如阿拉伯语、中文、法语、西班牙语）翻译成英语时，与人工评价高度相关。
* BLEU 的优势在于：它不是评估每个句子的人工判断是否完全一致，而是通过整套测试语料的平均效果，达到整体评估的准确性，体现“量变引起质变”。
* 作者还认为，BLEU 不仅可以用于机器翻译，也可能适用于文本生成类任务，比如文本摘要。
* 最后，他们感谢了支持该研究的机构和参与人工评估的评审者们。















