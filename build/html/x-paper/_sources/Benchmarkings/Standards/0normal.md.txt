# 通用


## 评测标准

* 准确率，代表模型判定属于这个分类的标题里面判断正确的有多少，有多少真的是属于这个分类的。
* 召回率，代表模型判定属于这个分类的标题占实际这个分类下所有标题的比例，也就是没有漏掉的比例。
* F1 Score，是准确率和召回率的调和平均数，也就是 F1 Score = 2/ (1/Precision + 1/Recall)


* 总结来说，准确率是一个全局的指标，它考虑了所有类别的分类结果；而精确率和召回率则更侧重于正样本的分类效果。精确率关注预测为正样本的实例中有多少是真正的正样本，而召回率则关注所有正样本中有多少被模型预测出来。在实际应用中，我们通常会根据具体的任务需求来选择合适的评估指标。

* 基础定义:
    * 真正例（TP）：模型预测为正，实际也为正
    * 假正例（FP）：模型预测为正，但实际为负
    * 真反例（TN）：模型预测为负，实际也为负
    * 假反例（FN）：模型预测为负，但实际为正


## 准确率(Accuracy)

* 准确率是分类模型正确分类的样本数占总样本数的比例。
* 计算公式为::
```
准确率 = (TP + TN) / (TP + FP + TN + FN)
    = 真正例 + 真反例 / 总样本数
```

* 准确率是一个简单直观的指标，但它可能在一些特定场景下并不那么有用，比如当正负样本数量极度不平衡时。
* 例如
    * 在一个信用卡欺诈检测系统中，正常交易（负样本）的数量可能远大于欺诈交易（正样本）的数量。在这种情况下，即使模型将所有的交易都预测为正常，准确率也会非常高，但这显然不是一个好的欺诈检测模型。
    * 美国全年平均有 8 亿人次的乘客，并且在 2000-2017 年间共发现了 19 名恐怖分子，这个模型达到了接近完美的准确率——99.9999999%。尽管这个模型拥有接近完美的准确率，但是在这个问题中准确率显然不是一个合适的度量指标。



## 精确率(Precision, 精准率)

* 精确率表示模型预测为正样本的实例中，真正为正样本的比例。
* 计算公式为:
```
精确率 = TP / (TP + FP)
    = 真正例 / (真正例 + 假正例)
```

* 精确率关注的是预测为正样本的实例中，有多少是真正的正样本。
* 示例：
    * 在上面的信用卡欺诈检测例子中，我们希望模型预测的欺诈交易中有尽可能多的真实欺诈交易，即精确率要高。
        * 精确率衡量的是所有被分类为欺诈的交易中，真正是欺诈的比例。
        * 在信用卡欺诈检测中，如果精确率很高，
        * 则分类为欺诈的交易中，绝大多数确实是欺诈，这可以降低误报率，减少对客户的不必要困扰和调查。
    * 在所有判为恐怖分子中，真正的恐怖分子的比例。

## 召回率(Recall)

* 召回率表示所有实际为正样本的实例中，被模型预测为正样本的比例。
* 计算公式为::
```
召回率 = TP / (TP + FN)
    = 真正例 / (真正例 + 假反例)
```

* 召回率关注的是所有正样本中，有多少被模型正确地预测出来。
* 示例：
    * 在信用卡欺诈检测中，我们希望尽可能多地找出所有的欺诈交易，即使这意味着可能会误判一些正常交易，即召回率要高。
        * 召回率衡量的是所有实际欺诈交易中，被正确分类为欺诈的比例。
        * 在信用卡欺诈检测中，高召回率意味着检测到了更多的欺诈交易，减少了漏报率，有助于防止欺诈行为。



## F1 Score

* F1 Score 是一种用于评估二分类问题的性能指标。它结合了精确度和召回率，以计算出模型的总体性能。
* 它的计算公式是：
```
F1 = 2 * (precision * recall) / (precision + recall)
```

$$
F1 = \frac{2}{\frac{1}{precision} + \frac{1}{recall}}
   = \frac{2 * precision * recall}{precision + recall}
$$


## 可视化精度和召回率

### 混淆矩阵（confusion matrix）

![](https://img.zhaoweiguo.com/uPic/2025/06/N1D1T4.png)


### 受试者特征曲线（ROC 曲线，Receiver Operating Characteristic curve）

![](https://img.zhaoweiguo.com/uPic/2025/06/EOOYTA.png)

* ROC 曲线展示了当改变在模型中识别为正例的阈值时，召回率和精度的关系会如何变化。
* 为了将某个病人标记为患有某种疾病（一个正例标签），我们为每种疾病在这个范围内设置一个阈值，通过改变这个阈值，我们可以尝试实现合适的精度和召回率之间的平衡。如果我们有一个用来识别疾病的模型，我们的模型可能会为每一种疾病输出介于 0 到 1 之间的一个分数，为了将某个病人标记为患有某种疾病（一个正例标签），我们为每种疾病在这个范围内设置一个阈值，通过改变这个阈值，我们可以尝试实现合适的精度和召回率之间的平衡。
* ROC 曲线在 Y 轴上画出了真正例率（TPR），在 X 轴上画出了假正例率 (FPR)。TPR 是召回率，FPR 是反例被报告为正例的概率。这两者都可以通过混淆矩阵计算得到。






## Recall@k


### 核心思想一句话概括

**Recall@k 衡量的是：在系统给出的前k个推荐结果中，有多少比例的用户真正感兴趣（相关）的物品被成功“召回”了。**

它关注的是系统的**覆盖能力**——是否能把用户喜欢的东西尽可能多地找出来，而不是关注意外发现或排序精准度。

---

### 公式与计算

Recall@k 的计算公式非常直观：

**Recall@k = （在前k个推荐结果中，与用户相关的物品数量） / （用户所有相关的物品总数）**

**结果是一个介于 0 到 1 之间的值，值越高，说明模型的召回能力越好。**

---

### 举例说明

假设我们有一个电影推荐系统，为某个用户推荐电影。我们知道这个用户**总共喜欢（相关）** 10部电影（例如，在测试集中他给这10部电影打了高分）。

* **场景一：k=5**
    * 系统为该用户推荐了5部电影（Top-5）。
    * 在这5部中，有2部是用户真正喜欢的（即出现在那10部相关的电影里）。
    * **Recall@5 = 2 / 10 = 0.20 (或 20%)**
    * 这意味着系统找出了用户20%的心仪电影。

* **场景二：k=10**
    * 系统推荐了10部电影（Top-10）。
    * 在这10部中，有4部是用户喜欢的。
    * **Recall@10 = 4 / 10 = 0.40 (或 40%)**
    * 推荐列表变长后，我们成功找出了更多用户喜欢的电影，召回率提升了。
    * **极端情况：如果k大于等于用户相关物品总数，并且系统足够好，Recall@k 可以达到 1.0 (100%)**。

---

### 为什么需要 Recall@k？

1. **评估召回阶段（Matching/Candidate Generation）的核心指标**：在推荐系统的两阶段流程中，召回阶段负责从百万级物品库中快速筛选出几百个候选物品。Recall@k 直接衡量了这个阶段的效果——是否把用户可能喜欢的东西都“捞”出来了。如果召回率很低，后续排序阶段再优秀也无济于事。

2. **反映结果的覆盖度**：它回答的是“我是否漏掉了用户喜欢的东西？”这个问题。对于新闻推荐、视频推荐等场景，确保用户能看到他们感兴趣的所有重要内容至关重要，Recall@k 非常适合衡量这一点。

3. **与业务目标对齐**：在很多业务中（如电商、音乐平台），目标之一是最大化用户消费其感兴趣内容的机会。高召回率意味着用户有更多机会发现并消费他们本就会喜欢的内容，从而提升满意度和留存率。

---

### 重要特性和注意事项

1. **k值的选择至关重要**：k是一个超参数。不同的k值会导致不同的结果。
    * 较小的k（如5, 10）用来模拟真实的用户体验（用户通常只看第一页）。
    * 较大的k（如100, 200）用来评估召回模型本身的能力（从海量物品中筛选候选集的能力）。

2. **不关心排序位置**：Recall@k **只关心相关的物品是否出现在前k个里面，而不关心它具体排第1名还是第k名**。即使所有相关结果都排在最末尾（但仍在k以内），Recall@k 的值依然是100%。

3. **通常与 Precision@k 结合使用**：Recall（召回率）和 Precision（精确率）是一对权衡指标（Trade-off）。
    * **Precision@k**：在前k个结果中，有多少比例是真正相关的。它关注的是**推荐结果是否精准**。
    * 提高k值通常会**提高Recall**（因为更容易包含相关物品），但会**降低Precision**（因为会混入更多不相关的物品）。必须根据实际业务需求来平衡这两者。

4. **适用于高度稀疏的场景**：在用户相关物品很少（正样本稀疏）的情况下，Recall@k 比一些聚合指标（如AUC）更能敏感地反映出模型性能的变化。

### 总结

**Recall@k 是一个用于评估模型发现相关物品能力的核心指标。它计算的是系统推荐列表的前k个结果中对用户相关物品的覆盖率。k值的选择取决于具体应用场景，并且它常与Precision@k结合使用，以全面评估推荐系统的性能和权衡。它是推荐系统、搜索引擎等领域不可或缺的评估工具。**



## Precision@k


### 核心思想一句话概括

**Precision@k 衡量的是：在系统给出的前k个推荐结果中，到底有多少比例是真正用户感兴趣（相关）的。**

它关注的是推荐结果的 **精准性** 和 **有效性** ————给你的推荐是不是“干货”，有没有很多垃圾内容。

---

### 公式与计算

Precision@k 的计算公式非常直接：

**Precision@k = （在前k个推荐结果中，与用户相关的物品数量） / （k）**

**结果是一个介于 0 到 1 之间的值，值越高，说明模型的推荐越精准、质量越高。**

---

### 举例说明

假设我们有一个音乐推荐系统，为某个用户推荐歌曲。

* **场景一：k=5**
    * 系统为该用户推荐了5首歌（Top-5）。
    * 用户听了之后，表示其中3首是他喜欢的（相关），另外2首他不喜欢（不相关）。
    * **Precision@5 = 3 / 5 = 0.60 (或 60%)**
    * 这意味着系统推荐的前5首歌里，有60%是用户喜欢的，推荐质量不错。

* **场景二：k=10**
    * 系统推荐了10首歌（Top-10）。
    * 用户表示其中有4首是他喜欢的。
    * **Precision@10 = 4 / 10 = 0.40 (或 40%)**
    * 当推荐列表变长后，虽然喜欢的歌曲绝对数量增加了（从3到4），但不喜欢的歌曲增加得更多，导致推荐的整体精准度下降了。

---

### 为什么需要 Precision@k？

1. **衡量用户体验和满意度**：Precision@k 直接关系到用户的直观感受。如果前几条推荐总是用户不感兴趣的内容（低Precision），用户会很快失去耐心并离开平台。它回答的是“你推荐给我的东西是不是我想要的？”。

2. **评估排序阶段（Ranking）的关键指标**：在推荐系统的两阶段流程中，排序阶段负责对召回后的几百个候选物品进行精细排序。Precision@k（尤其是较小的k，如5或10）**直接反映了排序模型的好坏**，因为它衡量的是“最顶尖”的推荐结果是否足够相关。

3. **与商业收益相关**：高的 Precision@k 意味着更高的点击率（CTR）、转化率（CVR）和用户 engagement（参与度）。因为展示给用户的都是他们更可能感兴趣的内容，从而更容易引发交互行为。

---

### 重要特性和注意事项

1. **k值的选择模拟真实场景**：k的选择通常与产品UI（用户界面）紧密结合。
    * **k=1**：衡量第一个推荐位（“首条”）的质量，极其重要。
    * **k=5/10**：模拟手机一屏或电脑首屏能看到的结果数量，是核心关注点。
    * **k=100**：这时Precision@k 更接近于衡量整个候选集的平均质量，而非排序能力。

2. **不关心未召回的物品**：Precision@k **只评估已经推荐出来的物品的质量，完全不关心那些没有被推荐出来的相关物品**。即使系统漏掉了用户非常喜欢的物品（低Recall），只要它推荐出来的东西用户都喜欢，Precision@k 依然可以很高。

3. **必须与 Recall@k 结合看待**：这是理解 Precision 最关键的一点。两者是一对“冤家”，存在 **权衡（Trade-off）**。
    * **提高Recall**：意味着要推荐更多物品（增大k），试图覆盖更多用户喜好。但这通常会 **降低Precision**，因为会混入更多不相关的物品。
    * **提高Precision**：意味着只推荐那些最有把握、最相关的物品（减小k或提高阈值）。但这通常会**降低Recall**，因为一些边缘但用户可能也喜欢的东西被过滤掉了。
    * 一个好的系统需要在Precision和Recall之间找到一个良好的 **平衡**，这个平衡点取决于业务目标（是更看重用户体验还是内容覆盖度）。

4. **计算方式**：通常会对测试集中的所有用户计算 Precision@k，然后取平均值（**Mean Precision@k**）来评估模型的整体性能。

### 总结

**Precision@k** 是一个用于评估推荐结果精准性和有效性的核心指标。它计算的是系统推荐列表的前k个结果中相关物品所占的比例。k值的选择通常与实际的用户体验场景相对应。它常与Recall@k成对使用，两者之间的权衡是推荐系统优化的核心所在。Precision@k 直接关系到用户满意度和商业效果，是评估排序模型性能的最重要指标之一。


## HR@k


### 一、核心定义：什么是 HR@N？

**HR@N** 的全称是 **Hit Rate @ N**，中文可翻译为 **“命中率@N”**。

它的定义非常直观：**在测试集中，如果推荐给用户的Top-N个物品列表（即排名最高的N个物品）中，包含了用户真实交互过的物品（即“相关物品”），那么这次推荐就视为一次“命中”（Hit）。HR@N 就是所有测试用户中，发生“命中”的用户所占的比例。**

简单来说，它衡量的是**推荐系统“是否至少猜中一个用户喜欢的东西”的能力**。

---

### 二、计算公式

$$
HR@N = \frac{\text{Number of Hits}}{\text{Total Number of Users}}
$$

或者更具体地：

$$
HR@N = \frac{1}{M} \sum_{i=1}^{M} \mathbb{I}(\text{相关物品} \in \text{Top-N推荐列表}_i)
$$

其中：
* $M$ 是测试集中的用户总数。
* $\mathbb{I}(·)$ 是一个指示函数（Indicator Function），如果括号内的条件为真（即命中了），则返回 1；否则返回 0。

---

### 三、举例说明

假设我们有一个测试集，包含 3 个用户（User A, B, C），他们的真实交互物品和系统给出的 Top-3 推荐列表如下：

| 用户 | 真实交互过的物品（Ground Truth） | 系统推荐的 Top-3 列表 | 是否命中？ (HR@3) |
| :--- | :--- | :--- | :--- |
| A | {物品 2} | [物品 1, **物品 2**, 物品 5] | **是 (1)** |
| B | {物品 3, 物品 4} | [物品 1, 物品 5, 物品 6] | 否 (0) |
| C | {物品 4, 物品 7} | [物品 3, **物品 7**, 物品 9] | **是 (1)** |

根据上表：
* 命中（Hit）的用户是 A 和 C，共 2 个。
* 总用户数是 3 个。
* 所以，$HR@3 = \frac{2}{3} \approx 0.667$ 或者说 66.7%。

这意味着，对于 66.7% 的用户，我们的推荐系统在Top-3的推荐中至少成功预测到了一个他们真正喜欢的物品。

---

### 四、指标的特点与解读

#### 优点：
1.  **直观易懂**：概念非常简单，业务方和技术方都能轻松理解。“推荐的前N个里有没有用户买过的？” 这个问题非常直接。
2.  **计算高效**：只需要判断是否存在，不需要复杂的排序计算，计算速度很快。
3.  **反映“捕捉能力”**：它很好地反映了模型将用户感兴趣的物品“捕捉”到推荐列表前列的能力。

#### 缺点和注意事项：
1.  **“二元性”缺陷**：HR@N 是一个二元指标（要么命中，要么没命中）。它**不关心命中的物品在列表中的具体位置**。
    * 例如，推荐列表 `[相关物品, 其他, 其他]` 和 `[其他, 其他, 相关物品]` 对于 HR@3 来说贡献是完全一样的（都是命中）。但显然，前者用户的体验更好，因为喜欢的物品排在第一。
2.  **不关心命中数量**：它不关心Top-N列表中命中了几个相关物品，只关心是否至少命中一个。
    * 用户B的真实物品有 {3,4}，但推荐列表 `[物品1, 物品2, 物品3]`（命中1个）和 `[物品3, 物品4, 物品1]`（命中2个）对于 HR@3 来说贡献也一样。
3.  **受N值影响大**：N 的取值对结果影响巨大。
    * N 越大，越容易命中，HR@N 的值自然就越高。例如，HR@100 几乎总是接近 100%。因此，**必须在相同的N值下比较不同模型的HR@N**，常见的N值有 1, 5, 10。
4.  **无法评估排序质量**：因为它不区分排名位置，所以不能单独用来评估推荐列表的排序好坏。

---

### 五、与其他指标的关系和对比

为了克服 HR@N 的缺点，通常需要与其他指标结合使用，以全面评估系统性能：

1.  **MRR (Mean Reciprocal Rank) / MAP (Mean Average Precision)**：
    * **功能**：这些指标**既关心是否命中，也关心命中的位置**。排名越靠前，得分越高。它们能更好地衡量排序质量。

2.  **NDCG (Normalized Discounted Cumulative Gain)**：
    * **功能**：这是一个更精细的指标，它**既考虑物品的相关程度（例如用户评分高低），也考虑物品的排名位置**。排名越靠前的高分物品，对得分贡献越大。这是目前最常用的衡量排序质量的指标之一。

**总结一下对比：**
* **HR@N**：问的是“**有没有？**”
* **MRR/MAP**：问的是“**有的话，排第几？**”
* **NDCG**：问的是“**排第几的这个东西，用户有多喜欢？**”

### 六、典型应用场景

HR@N 由于其简单性，被广泛应用于：

* **模型快速对比**：在算法开发初期，快速筛选和对比多个模型的基线性能。
* **线上A/B测试**：作为一个核心的线上监控指标，例如报告 HR@5 或 HR@10 的提升情况，业务方很容易理解。
* **召回阶段评估**：在推荐系统的召回阶段，目标就是从海量物品中快速找出用户可能感兴趣的几百个物品。HR@N（N取50或100）常被用来评估召回模型的效果，即“是否能把用户喜欢的物品成功召回”。

### 总结

**HR@N（命中率）** 是一个基础、直观且重要的推荐系统评估指标，它核心衡量的是模型将相关物品推荐到前列的**覆盖能力**。但由于其二元性，它无法评估排序质量。在实际工作中，它几乎总是与 **NDCG**、**MRR** 等更精细的排序指标一起使用，共同为推荐系统的性能提供全面的评估视角。





## NDCG@k

**适用于**：重排序任务

### 核心思想一句话概括

**NDCG@k 衡量的是：系统返回的前k个结果的排序列表，其质量有多好；它不仅考虑“是否相关”，还考虑“相关程度的高低”以及“相关结果所在的位置”。**

它是对 **DCG@k** 的标准化处理，使其取值范围在0到1之间，便于不同查询之间的比较。

---

### 从 CG 到 DCG 再到 NDCG

要理解NDCG，我们需要一步步拆解：

1. **CG (Cumulative Gain) - 累计增益**：
    * 最简单的想法：把前k个结果的“相关性分数”加起来。
    * **CG@k = sum(relevance_scores of top k items)**
    * **问题**：它只关心相关性总和，**完全不考虑这些结果的位置顺序**。把最相关的结果排在第10名和第1名，CG@10的值是一样的。

2. **DCG (Discounted Cumulative Gain) - 折损累计增益**：
    * **核心改进**：认为排名越靠前的结果，其价值越高。因此，高相关性的结果排位越低，其贡献就应该被“折扣”得越多。
    * **公式**：**DCG@k = sum( (relevance_score_i) / log2(position_i + 1) )**
        * 分母中的 `log` 函数就是“折扣因子”。位置（rank）越大，分母越大，该项的贡献就越小。
        * 一个更常用的、对高相关项给予更多强调的公式是：**DCG@k = sum( (2^(relevance_score_i) - 1 ) / log2(position_i + 1) )**
    * **例子**：一个相关性为3（高度相关）的结果：
        * 排在第1位：贡献为 `(2^3 - 1) / log2(2) = 7 / 1 = 7`
        * 排在第2位：贡献为 `7 / log2(3) ≈ 7 / 1.585 ≈ 4.41`
        * 排在第3位：贡献为 `7 / log2(4) = 7 / 2 = 3.5`
    * **DCG 的优点**：它同时考虑了相关性的强度和位置的高低。

3. **NDCG (Normalized Discounted Cumulative Gain) - 归一化折损累计增益**：
    * **最终改进**：DCG值本身没有上限，不同查询（用户）之间的DCG值无法直接比较。一个用户可能有10个相关物品，另一个只有2个，前者的 ``DCG@10`` 天然就比后者高。
    * **解决方案**：进行**归一化**。我们将计算出的 ``DCG@k`` 除以“理想情况下的最大可能 ``DCG@k``”，这个最大值叫做 **IDCG@k (Ideal DCG)**。
    * **IDCG@k** 的计算方法是：将**所有相关物品按照相关性程度从高到低排序**，取前k个，再用DCG的公式计算这个“理想列表”的得分。
    * **最终公式**：**NDCG@k = DCG@k / IDCG@k**
    * 由于NDCG是DCG与理想值的比值，它的取值范围是**[0, 1]**。**1表示达到了完美的排序**，0则表示最差的排序。

---

### 计算步骤举例

假设我们为一个用户推荐了5个物品（k=5），用户对它们的真实相关性评分是 `[3, 2, 3, 0, 1]`（评分范围0-3，3最高）。

1. **计算 DCG@5**：
    * 使用公式：$DCG@k = \sum_{i=1}^{k} \frac{2^{rel_i} - 1}{\log_2(i + 1)}$
    * Position 1: rel=3 -> `(2^3-1)/log2(2) = 7/1 = 7.00`
    * Position 2: rel=2 -> `(2^2-1)/log2(3) = 3/1.585 ≈ 1.89`
    * Position 3: rel=3 -> `(2^3-1)/log2(4) = 7/2 = 3.50`
    * Position 4: rel=0 -> `(2^0-1)/log2(5) = 0/2.322 = 0`
    * Position 5: rel=1 -> `(2^1-1)/log2(6) = 1/2.585 ≈ 0.39`
    * **DCG@5 = 7.00 + 1.89 + 3.50 + 0 + 0.39 ≈ 12.78**

2. **计算 IDCG@5**：
    * 首先，将真实相关性分数降序排列，得到理想列表：`[3, 3, 2, 1, 0]`
    * 计算这个理想列表的DCG：
        * Position 1: rel=3 -> `7/1 = 7.00`
        * Position 2: rel=3 -> `7/1.585 ≈ 4.41`
        * Position 3: rel=2 -> `3/2 = 1.50`
        * Position 4: rel=1 -> `1/2.322 ≈ 0.43`
        * Position 5: rel=0 -> `0/2.585 = 0`
    * **IDCG@5 = 7.00 + 4.41 + 1.50 + 0.43 + 0 ≈ 13.34**

3. **计算 NDCG@5**：
    * **NDCG@5 = DCG@5 / IDCG@5 ≈ 12.78 / 13.34 ≈ 0.958**

这个值非常接近1，说明我们的排序质量很高，几乎达到了理想状态。

---

### 为什么 NDCG@k 如此重要？

1. **同时考量位置和相关性等级**：这是它相比Precision@k和Recall@k的最大优势。它不会把“相关性为3排第1”和“相关性为1排第1”视为相同。
2. **归一化结果**：结果被规范到0-1之间，使得不同查询、不同用户之间的评估结果可以公平地进行比较和平均。
3. **符合用户体验**：用户确实更希望最相关、最好的结果出现在最前面。NDCG精准地量化了这种体验。
4. **业界标准**：它是评估搜索引擎和推荐系统**排序阶段（Ranking）效果的最主流、最可靠的指标之一**。

### 总结

**NDCG@k 是一个用于评估排序列表质量的权威指标。它通过结合相关性程度和结果位置信息，并经过归一化处理，提供了一个强大、公平且符合用户心理的衡量标准。值越接近1，说明排序效果越完美。它是信息检索和推荐系统领域不可或缺的核心评估工具。**

## MRR@k


### 一、核心概念：什么是 MRR@K？

**MRR@K** 的全称是 **Mean Reciprocal Rank at K**，即 **前K个结果的平均倒数排名**。

它的核心思想是：**评估一个系统是否能够将第一个正确答案尽可能地排在靠前的位置。**

* **Mean (平均)**：对多个查询（或请求）的结果取平均值。
* **Reciprocal Rank (倒数排名)**：对于单个查询，其得分是第一个正确答案所在排名的倒数。
* **@K**：只考虑系统返回的前 K 个结果。如果前 K 个结果里没有正确答案，则这个查询的得分记为 0。

---

### 二、为什么需要 MRR@K？

想象一下你在使用搜索引擎：
*  Scenario A：你搜索一个关键词，第一个结果就是你想要的。
*  Scenario B：你搜索一个关键词，翻了五页（第50个结果）才找到你想要的内容。

显然，Scenario A 的用户体验要好得多。MRR@K 就是为了量化这种“好”的程度。它特别强调**第一个相关结果的位置**，位置越靠前（排名数值越小），得分越高。

---

### 三、如何计算 MRR@K？

计算分为两步：

1.  **计算每个查询的倒数排名 (Reciprocal Rank @K)**
    * 对于一个给定的查询，从上到下查看系统返回的前 K 个结果。
    * **找到第一个相关（正确）的结果**，记下它的排名位置（`rank`）。注意，是 **第一个**出现的，后面的即使相关也不考虑。
    * 该查询的得分为 **1 / rank**。
    * 如果前 K 个结果中**没有任何**相关结果，则该查询得分为 **0**。

2.  **对所有查询的得分求平均值 (Mean)**
    * 将所有查询的 Reciprocal Rank @K 得分相加，然后除以查询的总数。

**公式表示：**
$$ MRR@K = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{rank_i} $$
其中：
* $Q$ 是查询的总数量。
* $rank_i$ 是第 `i` 个查询中，第一个正确答案的排名。如果前K个没有，则 $\frac{1}{rank_i}$ 为 0。

---

### 四、举例说明

假设我们有一个推荐系统，测试集包含 3 个查询（用户请求）。我们设定 K=5，即只考察系统返回的前5个结果。`✓` 表示该结果是相关的（正确的），`✗` 表示不相关。

* **查询 1** 的返回结果排名：`[✓, ✗, ✗, ✓, ✗]`
    * 第一个相关结果在**位置1**。
    * `RR@5 = 1 / 1 = 1.0`

* **查询 2** 的返回结果排名：`[✗, ✓, ✗, ✗, ✗]`
    * 第一个相关结果在**位置2**。
    * `RR@5 = 1 / 2 = 0.5`

* **查询 3** 的返回结果排名：`[✗, ✗, ✗, ✗, ✓]`
    * 第一个相关结果在**位置5**。
    * `RR@5 = 1 / 5 = 0.2`

* **查询 4** 的返回结果排名：`[✗, ✗, ✗, ✗, ✗]` (假设前5个都没有相关结果)
    * 没有相关结果。
    * `RR@5 = 0`

**现在计算 MRR@5：**
`MRR@5 = (1.0 + 0.5 + 0.2 + 0) / 4 = 1.7 / 4 = 0.425`

---

### 五、MRR@K 的特点和注意事项

**优点：**
1.  **简单直观**：计算简单，易于理解和实现。
2.  **关注首位相关性**：非常适用于那些用户只关心第一个正确答案的场景（例如，语音助手回答问题、搜索引擎的“最佳答案”）。
3.  **排序敏感性**：对排名高度敏感，将正确答案从第2位提升到第1位带来的收益（从0.5到1.0）远大于从第3位提升到第2位（从0.33到0.5）。

**缺点和局限：**
1.  **只关心第一个**：它完全忽略第一个正确答案之后的其他相关结果。如果一个查询有10个相关结果，系统把1个排在第1，另外9个排在第100开外，它的得分和只排第1个结果是一样的。
2.  **粗粒度**：它只考虑排名位置，而不考虑相关性的程度（是有点相关还是完全相关？）。
3.  **K值的选择**：K的选择非常重要。`MRR@10` 通常会比 `MRR@5` 高，因为更有可能在前10个结果中找到正确答案。比较不同系统时必须在相同的K值下进行。

---

### 六、与其他指标的区别

* **MAP@K (Mean Average Precision)**：**同时考虑**排名顺序和**所有**相关结果的平均精度。当需要关注多个相关结果时（如图片搜索、论文检索），MAP 是比 MRR 更好的选择。
* **NDCG@K (Normalized Discounted Cumulative Gain)**：可以考虑**不同等级的相关性**（例如，非常相关=3分，一般相关=2分，不相关=0分），并对排名靠前的结果赋予更高的权重。比 MRR 和 MAP 更精细。


### 为什么 MRR@10 常和 Recall@1000 一起使用？

这两个指标从**完全不同但互补**的角度评估系统性能：

1.  **MRR@10：衡量“精度”和“排名质量”**
    *   它关注用户 **最先看到**的结果有多好。
    *   用户体验：用户是否需要翻很多页才能找到第一个想要的结果？MRR越高，用户体验越好，因为他们能更快地找到答案。
    *   它非常“苛刻”，只要第一个正确答案没排到第一，就会罚分。

2.  **Recall@1000：衡量“召回能力”**
    *   它关注系统 **能否把所有相关的物品都找出来**，哪怕排名很靠后。
    *   它计算在前1000个结果中，出现了多少比例的标准答案。
    *   这对于需要“海捞”的场景很重要，比如为后续的排序模型准备候选集，或者用户不介意进行深度搜索。

**结合使用的意义：**

一个理想的系统应该同时具备：
*   **高 Recall@1000**：证明系统有能力检索到绝大部分相关物品，没有遗漏。
*   **高 MRR@10**：证明系统不仅能找到，还能把最好的结果精准地排在最前面，直接满足用户。

如果只有一个指标高：
*   **高 Recall，低 MRR**：系统能找到所有东西，但排序算法很差，用户体验不佳。
*   **低 Recall，高 MRR**：系统排在前面的结果很准，但可能遗漏了很多其他的相关结果，不够全面。

### 总结

| 指标 | 中文名 | 关注点 | 比喻 |
| :--- | :--- | :--- | :--- |
| **MRR@10** | **平均倒数排名@10** | **第一个正确答案的排名位置**（前10名内） | 你的最佳员工是否在第一排？ |
| **Recall@1000** | **召回率@1000** | **能找到所有相关答案的比例**（前1000名内） | 你的所有员工是否都在这个大厅里？ |


### 总结

| 特性 | 描述 |
| :--- | :--- |
| **全称** | Mean Reciprocal Rank at K |
| **用途** | 评估系统将**第一个**正确答案排在靠前位置的能力 |
| **核心思想** | 第一个相关结果排名越靠前，得分越高 |
| **计算** | 对所有查询的**倒数排名（1/rank）** 求平均，前K个无结果则得0 |
| **适用场景** | 用户只关心top1结果的场景（问答系统、语音助手、搜索引擎） |
| **局限性** | 忽略第一个正确结果之后的所有其他相关结果 |



## MAP@k

* **适用场景**：排序任务的核心指标，如：搜索列表、推荐系统、广告排序、重排序等。
* **全称**：Mean Average Precision at K
* **衡量**：在所有用户的推荐列表中，相关物品是否被排在了前K个的靠前位置。
* **取值范围**：0 到 1。越高越好。
* **优点**
    1. 考虑排名顺序（惩罚排名靠后）
    2. 对顶部结果敏感
    3. 在用户间公平比较
* **缺点**
    * 只考虑二元相关性（喜欢/不喜欢），不考虑喜欢/不喜欢的程度（这点NDCG可以弥补）。

好的，我们来彻底讲清楚 **MAP@K** 这个在推荐系统和信息检索中至关重要的评估指标。

### 一句话理解

**MAP@K 衡量的是：在所有用户的推荐列表中，我们排在前K个的“好东西”的平均排名有多好、多靠前。**

它同时考虑了两个方面：
1. **有没有推荐对？**（相关性）
2. **推荐对的物品，排得够不够靠前？**（排名位置）

---

### 拆解 acronym (首字母缩略词)

* **K**： 我们只关心推荐列表的前K个结果。比如 `MAP@10` 就是只看给你的前10个推荐。
* **P@K (Precision at K)**： **前K个的准确率**。这是基础。
    * `P@K = (前K个中用户喜欢的物品数量) / K`
    * 例如，推荐了10个电影（K=10），用户喜欢其中的3个，那么 `P@10 = 3/10 = 0.3`。
* **AP@K (Average Precision at K)**： **平均准确率**。这是关键，它升级了P@K。
    * 它不是简单计算整个K里的准确率，而是**从排名第一的结果开始，每出现一个用户喜欢的物品，就计算一次到当前位置的P@K，然后对这些值取平均**。
    * **AP@K 惩罚了“好的推荐但排得太靠后”的情况。**
* **MAP@K (Mean Average Precision at K)**： **平均的平均准确率**。这是最终指标。
    * 顾名思义，就是把**所有用户**的 `AP@K` 分数再求一个平均值。

**简单记忆：`MAP@K = Mean( AP@K ) = Mean( Average( P@K at each hit ) )`**

---

### 通过一个例子彻底搞懂

假设我们有3个用户（A, B, C），我们给每个人推荐6个物品（K=6）。`1` 代表用户喜欢（相关），`0` 代表不喜欢（不相关）。

**用户A的真实喜好：[1, 0, 0, 1, 0, 1]**
*(这意味着我们排在第1位的他喜欢，第4位的喜欢，第6位的也喜欢)*

我们来计算用户A的 `AP@6`：

1. **第一个相关项在位置1**：
    * 计算此时的 `P@1`：看前1个，有1个相关。`P@1 = 1/1 = 1.0`
2. **第二个相关项在位置4**：
    * 计算此时的 `P@4`：看前4个，有2个相关。`P@4 = 2/4 = 0.5`
3. **第三个相关项在位置6**：
    * 计算此时的 `P@6`：看前6个，有3个相关。`P@6 = 3/6 = 0.5`
4. **计算AP@6**：
    * 将以上所有**相关项位置**的P值加起来求平均：`AP@6 = (1.0 + 0.5 + 0.5) / 3 = 2.0 / 3 ≈ 0.6667`

*为什么不用P@2, P@3, P@5？因为在那几个位置没有出现新的相关项，所以我们不计算。*

**用户B的真实喜好：[0, 1, 0, 0, 1, 0]**
1. 位置2：`P@2 = 1/2 = 0.5` (前2个有1个相关)
2. 位置5：`P@5 = 2/5 = 0.4` (前5个有2个相关)
3. `AP@6 = (0.5 + 0.4) / 2 = 0.9 / 2 = 0.45`

**用户C的真实喜好：[1, 1, 0, 1, 0, 0]**
1. 位置1：`P@1 = 1/1 = 1.0`
2. 位置2：`P@2 = 2/2 = 1.0`
3. 位置4：`P@4 = 3/4 = 0.75`
4. `AP@6 = (1.0 + 1.0 + 0.75) / 3 = 2.75 / 3 ≈ 0.9167`

**最后，计算MAP@6**
`MAP@6 = (AP_A + AP_B + AP_C) / 3 = (0.6667 + 0.45 + 0.9167) / 3 ≈ 2.0334 / 3 ≈ 0.6778`

---

### 为什么MAP@K如此重要？

1. **考虑了排名顺序**：这是它比简单准确率（Precision）强大得多的地方。
    * 对比用户A `[1, 0, 0, 1, 0, 1]` 和 `[1, 1, 1, 0, 0, 0]`，两者的 `P@6` 都是 3/6=0.5。
    * 但前者的 `AP@6` 是 0.6667，后者的 `AP@6` 是 (1.0 + 1.0 + 1.0)/3=1.0。MAP@K能清晰地分辨出后者（把喜欢的都排前面）的模型更优秀。

2. **适用于用户间比较**：有些用户喜欢的物品多，有些喜欢的少。MAP通过对每个用户计算AP再平均，避免了对活跃用户的偏向。

3. **面向业务目标**：在推荐、搜索等场景中，**顶部结果的准确性远比底部重要**。用户主要看第一屏。MAP@K完美契合这个需求，重点关注前K个结果的质量。

### 总结

| 特性 | 解释 |
| :--- | :--- |
| **全称** | Mean Average Precision at K |
| **衡量什么** | 在所有用户的推荐列表中，**相关物品**是否被排在了**前K个**的**靠前位置**。 |
| **取值范围** | 0 到 1。越高越好。 |
| **优点** | 1. **考虑排名顺序**（惩罚排名靠后）<br>2. **对顶部结果敏感**<br>3. **在用户间公平比较** |
| **缺点** | 只考虑二元相关性（喜欢/不喜欢），不考虑喜欢/不喜欢的**程度**（这点NDCG可以弥补）。 |
| **适用场景** | **排序任务**的核心指标，如：搜索列表、推荐系统、广告排序、重排序等。 |

所以，当你的任务是**重排序**时，MAP@K是一个非常直接且强大的指标，它能告诉你新的排序算法是否真正地把用户更感兴趣的内容提到了更显眼的位置。

---

## AUC (Area Under the ROC Curve)

**适用于**：CTR任务

**核心思想：衡量模型的排序能力。**

**AUC越高越好**，最高为1.0，表示模型完美；0.5表示模型和随机猜测一样（即没有区分能力）。

---

### 为什么要用AUC？

在CTR任务中，我们的最终目标往往不是100%准确地预测用户 **会不会**点击（这在极度稀疏的数据上几乎不可能），而是更关心能否将**最有可能点击**的物品排在前面。例如，在信息流推荐中，我们只需要把用户最可能点击的几条内容放在顶部即可。

AUC衡量的正是模型 **将“点击”样本（正样本）排在“未点击”样本（负样本）前面** 的能力。

---

### 详细拆解：

![](https://img.zhaoweiguo.com/uPic/2025/09/3e6KfN.png)

ROC曲线图解读

1. **ROC曲线 (Receiver Operating Characteristic Curve)**
    * **横轴（False Positive Rate, FPR）**：在所有真实的负样本中，被模型错误地预测为正的比例。**越低越好**。
        * `FPR = FP / (FP + TN)`
    * **纵轴（True Positive Rate, TPR, 又称Recall）**：在所有真实的正样本中，被模型正确地预测为正的比例。**越高越好**。
        * `TPR = TP / (TP + FN)`

2. **ROC曲线是如何画出来的？**
    * 模型输出的不是非0即1的标签，而是一个介于0-1之间的概率值（例如0.8、0.3等）。
    * 我们通过设置不同的**阈值（Threshold）**，来将概率值转化为二分类的标签。例如，设定阈值为0.5，大于0.5的预测为点击(1)，小于0.5的预测为未点击(0)。
    * 每设定一个阈值，我们就能在所有测试样本上得到一组（FPR, TPR）点。将所有阈值对应的点连接起来，就形成了ROC曲线。

3. **AUC的含义**
    * AUC就是ROC曲线下方的面积。
    * **最直观的理解**：**AUC值等于模型随机抽取一个正样本和一个负样本，模型给正样本的打分高于给负样本打分的概率**。
        * AUC=1.0：所有正样本的得分都高于所有负样本。完美模型。
        * AUC=0.5：正负样本的得分完全随机分布，模型没有区分能力。
        * AUC<0.5：模型比随机猜测还差，通常意味着模型学到了反规律。

**在CTR任务中的优点：**
* **对样本不平衡不敏感**：CTR数据中绝大部分都是未点击（负样本），点击（正样本）很少。AUC关注的是样本之间的排序关系，而不是绝对的概率值，因此即使正负样本比例极度失衡，AUC依然是一个稳定的指标。

---

## LogLoss(Logarithmic Loss)

**适用于**：CTR任务

**核心思想：衡量模型预测概率的“准确程度”和“信心程度”。**

**LogLoss越低越好**，最低为0，表示模型完美预测；越高表示预测越差。

---

### 为什么要用LogLoss？

AUC只关心排序，不关心具体预测的概率值是否准确。举个例子：
* **模型A**：对一个点击样本预测为0.51，对一个未点击样本预测为0.49。AUC会认为这个排序正确。
* **模型B**：对同一个点击样本预测为0.99，对同一个未点击样本预测为0.01。AUC同样认为排序正确。

但从概率校准的角度看，模型B显然更好、更自信、更准确。LogLoss就是用来捕捉这种差异的。

---

### 详细拆解：

对数损失的公式如下：

$LogLoss = -\frac{1}{N} \sum_{i=1}^{N} [y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i)]$

其中：
* $N$：样本总数。
* $y_i$：第 $i$ 个样本的真实标签，点击为1，未点击为0。
* $p_i$：模型预测第 $i$ 个样本会点击的概率（介于0~1之间）。

**这个公式是如何工作的？**
* 对于一条**点击**了的样本（$y_i = 1$），损失函数变为 $-\log(p_i)$。预测概率 $p_i$ **越接近1**，$\log(p_i)$ 越接近0，损失越小；预测概率 $p_i$ **越接近0**，$\log(p_i)$ 会成为一个很大的负数，整个损失项就会变成一个很大的正数（因为前面有负号），惩罚非常重。
* 对于一条**未点击**的样本（$y_i = 0$），损失函数变为 $-\log(1 - p_i)$。预测概率 $p_i$ **越接近0**，$(1-p_i)$ 越接近1，损失越小；预测概率 $p_i$ **越接近1**，$(1-p_i)$ 接近0，$\log(1-p_i)$ 会成为一个很大的负数，整个损失项就会变成一个很大的正数，惩罚非常重。

**核心特点：**
* **严厉的惩罚**：LogLoss会对“ confidently wrong ”的预测（即非常肯定但预测错了）施加指数级增长的巨大惩罚。例如，将一个实际点击的样本预测为不点击的概率是0.999，其产生的损失会非常大。
* **衡量概率校准**：它要求模型不仅要把正负样本分开，还要输出一个尽可能接近真实概率的、有意义的置信度。

---

### 总结与对比

| 指标 | AUC | LogLoss |
| :--- | :--- | :--- |
| **核心目标** | **排序能力** (Ranking) | **概率校准** (Calibration) |
| **关心什么** | 正样本是否排在负样本之前 | 预测的概率是否准确和自信 |
| **取值范围** | [0, 1] | [0, +∞) |
| **最佳值** | **越高越好** (1.0完美) | **越低越好** (0.0完美) |
| **对样本不平衡** | **不敏感** | **敏感** (可通过样本加权缓解) |
| **业务意义** | 决定了推荐/广告列表的**整体效果** | 影响了**预算估计**、**竞价策略**等对概率值敏感的后续系统 |

### 在实际业务中如何看？

通常需要将这两个指标结合来看：

1. **AUC高，LogLoss低**：**理想情况**。模型既具有良好的排序能力，输出的概率值又非常准确。这是追求的目标。
2. **AUC高，LogLoss高**：模型排序能力不错，**但预测的概率值不够准确**（可能普遍偏高或偏低）。例如，模型总是将点击样本预测为0.9，将未点击预测为0.4，虽然排序正确，但概率值过于“激进”或“保守”。这种情况很常见，可能需要对预测概率进行**校准**（Calibration）。
3. **AUC低，LogLoss高**：**最差情况**。模型既没有排序能力，预测的概率也不准。需要重新检查特征、模型或数据。
4. **AUC低，LogLoss低**：这种情况比较少见。可能意味着模型虽然整体排序不好，但它在某些子集上预测的概率值相对准确。

因此，在CTR任务中，**AUC是模型能力的“天花板”指标，而LogLoss是模型输出质量的“精细化”指标**，两者相辅相成，缺一不可。





























