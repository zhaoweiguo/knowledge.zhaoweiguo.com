# 2301.12597_BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

* 首页: [https://arxiv.org/abs/2301.12597](https://arxiv.org/abs/2301.12597)
* PDF: [https://arxiv.org/pdf/2301.12597](https://arxiv.org/pdf/2301.12597)


## Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models



文章《BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models》提出了一种新的多模态预训练模型BLIP-2。BLIP-2 通过“冻结图像编码器”和“引入大规模语言模型（LLMs）”的方法，在语言-图像预训练（Vision-Language Pre-training, VLP）中取得了显著进展。

### 核心内容总结：
1. **模型结构**：
   - BLIP-2 采用两阶段结构：
     - **冻结图像编码器**：采用预训练的视觉模型（如 ViT），其参数在训练过程中保持不变。
     - **引入大型语言模型（LLMs）**：作为文本生成模块，用于从图像中生成描述或进行多模态推理任务。

2. **训练方法**：
   - 使用一种称为“知识蒸馏”的技术，将大型语言模型的输出作为训练目标，从而提升图像编码器与语言模型之间的对齐效果。
   - 通过“多模态指令调优”（Multimodal Instruction Tuning）进一步提升模型在下游任务中的表现。

3. **优势与特点**：
   - **高效性**：由于图像编码器是冻结的，BLIP-2 在训练过程中只需优化语言模型部分，大大降低了计算成本。
   - **灵活性**：通过引入不同的 LLM（如 OPT、FLAN-T5 等），可以方便地扩展模型能力。
   - **性能提升**：在多个视觉-语言基准测试中，BLIP-2 取得了优于现有方法的性能，尤其是在图像描述、视觉问答等任务中。

4. **应用场景**：
   - BLIP-2 适用于多种视觉-语言任务，包括图像描述生成、视觉推理、视觉问答（VQA）等。

### 总结：
BLIP-2 是一种结构轻量、训练高效、性能强大的视觉-语言预训练框架。通过冻结图像编码器和利用大型语言模型的生成能力，BLIP-2 在多模态任务中实现了较高的效果，为后续的视觉-语言模型设计提供了新的思路和方法。


## Abstract



该段内容总结如下：

本文提出了一种高效、通用的视觉-语言预训练方法 **BLIP-2**，旨在降低大规模端到端模型训练的高昂成本。BLIP-2 通过使用**现成的、冻结的图像编码器**和**大型语言模型**，结合一个轻量级的 **Querying Transformer** 来弥合视觉与语言之间的模态差距。BLIP-2 的预训练分为两个阶段：第一阶段基于冻结的图像编码器进行视觉-语言表征学习，第二阶段基于冻结的语言模型进行视觉到语言的生成学习。尽管 BLIP-2 的可训练参数远少于现有方法，它在多种视觉-语言任务上达到了**最先进的性能**。例如，在零样本 VQAv2 任务上，BLIP-2 比 Flamingo80B 模型高出 8.7%，且参数量仅为后者的 1/54。此外，BLIP-2 还展现出了**零样本图像到文本生成**的能力，能够根据自然语言指令生成相应文本。


## 1 Introduction



本文介绍了 **BLIP-2**（Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models）这一视觉语言预训练模型，旨在解决现有视觉语言模型预训练计算成本高、模态对齐困难的问题。

### 总结内容如下：

1. **研究背景**：
   - 近年来，视觉语言预训练（VLP）模型在多个下游任务中表现优异，但这些模型通常规模庞大，训练成本高。
   - 本文提出一种计算高效的 VLP 方法，通过利用已有的预训练视觉模型和语言模型（尤其是大语言模型，LLMs）来降低训练成本。

2. **核心思想**：
   - 提出**Querying Transformer（Q-Former）**，作为轻量级模块，用于在冻结的图像编码器和冻结的 LLM 之间进行跨模态对齐。
   - 通过**两阶段预训练策略**：
     - **第一阶段**：基于冻结图像编码器进行视觉-语言表示学习，使 Q-Former 学习与文本相关的视觉表示。
     - **第二阶段**：将 Q-Former 的输出连接到冻结的 LLM，进行生成式训练，使其输出能被 LLM 解码为自然语言。

3. **主要优势**：
   - **高效性**：BLIP-2 使用冻结的单模态模型和轻量的 Q-Former，显著降低了训练成本。
   - **高性能**：在视觉问答（VQA）、图像描述生成（Image Captioning）、图文检索等任务上达到 SOTA。
   - **零样本生成能力**：借助 LLM 的强大生成能力，BLIP-2 能够在没有微调的情况下完成图像到文本的生成，支持视觉知识推理、对话等任务。
   - **通用性**：BLIP-2 是一个通用框架，可以灵活结合更先进的视觉或语言模型以提升性能。

4. **实验结果**：
   - 相较于现有 SOTA 模型（如 Flamingo），BLIP-2 在少样本 VQA 任务（zero-shot VQAv2）上性能提升 8.7%，同时参数量仅为对方的 1/54，体现了其高效性与有效性。

总之，BLIP-2 通过两阶段的 Q-Former 预训练策略和冻结单模态模型的利用，提供了一种高效、通用的视觉语言预训练方法，并在多种任务上取得了优异表现。


## 2 Related Work



本章节“相关工作”主要分为两部分：**端到端视觉-语言预训练**和**模块化视觉-语言预训练**，对当前主流的视觉-语言模型方法进行了系统回顾与比较，重点突出本文方法（BLIP-2）的优势。

### 2.1 端到端视觉-语言预训练
- **目标**：通过大规模图像-文本对数据进行预训练，提升多种视觉-语言任务的性能。
- **模型架构**：
  - **双编码器架构**：分别处理视觉和语言模态，适用于需要快速检索的任务。
  - **融合编码器架构**：将图像和文本信息融合处理。
  - **编码器-解码器架构**：适用于生成任务，如图像描述生成。
  - **统一Transformer架构**：最近提出，统一处理视觉和语言模态。
- **预训练目标**：
  - 图像-文本对比学习（Image-Text Contrastive Learning）
  - 图像-文本匹配（Image-Text Matching）
  - （掩码）语言建模（Masked Language Modeling）
- **问题与挑战**：
  - 计算成本高，模型规模大导致训练代价高昂。
  - 模型不够灵活，难以有效利用现成的单模态预训练模型（如大语言模型LLM）。

### 2.2 模块化视觉-语言预训练
- **思路**：利用已有的预训练模型（如图像编码器、语言模型），在预训练过程中保持其参数冻结，仅训练中间模块。
- **冻结图像编码器**：
  - 早期方法使用固定的目标检测器提取视觉特征。
  - 近年的LiT方法采用冻结的图像编码器（如CLIP）进行预训练。
- **冻结语言模型**：
  - 用于图像到语言生成任务，通过视觉信息引导语言模型生成文本。
  - 关键挑战是将视觉特征与语言模型的文本空间对齐。
  - **Frozen**：微调图像编码器，将输出作为LLM的软提示。
  - **Flamingo**：在LLM中插入新的交叉注意力层，通过大规模图像-文本对进行预训练。
- **本文贡献**：
  - **BLIP-2**能够高效地同时利用冻结的图像编码器和冻结的语言模型，完成多种视觉-语言任务。
  - 相较于现有方法，在性能和计算效率上均有提升。

### 总结
本章系统梳理了视觉-语言预训练的发展脉络，对比了端到端与模块化两种主流方法的特点与不足。通过分析现有方法的优缺点，突出了BLIP-2在灵活性、效率和性能方面的创新优势，为后续提出方法的合理性奠定了基础。


## 3 Method



本节主要介绍了 **BLIP-2** 模型的方法部分，包括其创新性的架构和两阶段的预训练方式。以下是内容总结：

---

### **1. Q-Former 架构**
- **Q-Former** 是 BLIP-2 的核心模块，用于连接**冻结的图像编码器**与**冻结的大语言模型 (LLM)**，从而弥合视觉与语言之间的模态差距。
- Q-Former 由两个共享自注意力层的 Transformer 模块构成：
  - **图像 Transformer**：从冻结的图像编码器中提取视觉特征。
  - **文本 Transformer**：兼具文本编码和解码功能。
- 使用一组可学习的查询嵌入（queries）作为输入，通过自注意力和交叉注意力机制与图像特征和文本进行交互。
- 查询数量固定为 32，每个查询维度为 768，输出为维度为 32×768 的查询表示 Z，比原始图像特征（如 257×1024）小得多，形成“瓶颈”，迫使查询提取与文本相关性高的视觉信息。
- Q-Former 初始化使用 BERT 的预训练参数，交叉注意力层随机初始化，总参数量为 188M。

---

### **2. 第一阶段：视觉-语言表示学习**
- 在冻结图像编码器的基础上，使用图像-文本对进行预训练，目标是训练 Q-Former 使查询能够提取与文本相关的视觉表示。
- **联合优化三个目标**，共享相同模型参数但使用不同的注意力掩码策略：
  1. **图像-文本对比学习（ITC）**：
     - 对齐图像与文本的表示，最大化它们之间的互信息。
     - 使用图像查询表示 Z 与文本 [CLS] 表示进行对比。
     - 使用单模态注意力掩码避免信息泄露。
     - 使用批次内负样本而非动量队列。
  2. **图像引导的文本生成（ITG）**：
     - 通过图像生成文本。
     - 查询必须提取图像中的所有相关视觉信息，并通过自注意力传递给文本。
     - 使用多模态因果掩码，限制文本与查询的交互。
     - 用新的 [DEC] 代替 [CLS] 标记，表示生成任务开始。
  3. **图像-文本匹配（ITM）**：
     - 判别图像-文本对是否匹配。
     - 使用双向注意力掩码，允许查询与文本相互关注。
     - 将查询结果输入二分类器，输出匹配分数。
     - 使用硬负样本挖掘策略提高负样本的代表性。

---

### **3. 第二阶段：视觉到语言的生成式预训练**
- 将 Q-Former 与冻结的大语言模型（LLM）连接，利用其强大的生成能力。
- 通过全连接层将查询表示 Z 映射为与 LLM 输入维度相同，并将其作为“软视觉提示”加入文本输入中。
- 由于 Q-Former 已经训练为提取语言相关的信息，因此可有效减少 LLM 的视觉-语言对齐负担，缓解灾难性遗忘问题。
- 支持两种类型的 LLM：
  1. **解码器型 LLM（如 OPT）**：使用语言建模损失，让 LLM 根据视觉表示生成文本。
  2. **编码器-解码器型 LLM（如 FlanT5）**：使用前缀语言建模损失，将文本分为前缀和后缀，前缀与视觉表示共同输入编码器，后缀作为生成目标。

---

### **4. 预训练设置**
- **数据**：
  - 使用与 BLIP 相同的 129M 图像-文本对数据集，包括 COCO、Visual Genome、CC3M、CC12M、SBU 和 LAION。
  - 使用 CapFilt 方法生成和筛选合成 caption。
- **模型**：
  - 冻结图像编码器使用 ViT-L/14 或 ViT-g/14。
  - 冻结语言模型使用 OPT（解码器型）和 FlanT5（编码器-解码器型）。
- **训练过程**：
  - 第一阶段训练 250k 步，第二阶段 80k 步。
  - 使用 AdamW 优化器，学习率采用余弦衰减，峰值为 1e-4，第二阶段最小为 5e-5。
  - 使用 FP16 或 BFloat16 混合精度训练，节省计算资源。
  - 因为冻结模型，预训练效率高，可在单个 A100 GPU 上完成。

---

### **总结**
BLIP-2 通过 Q-Former 架构和两阶段预训练方法（视觉语言表示学习 + 生成式训练），在冻结图像编码器和大语言模型的基础上实现高效、强大的视觉-语言建模能力。Q-Former 通过学习可解释的查询表示，使模型能够聚焦于与文本相关性强的视觉信息，从而提升生成和理解能力。


## 4 Experiment



本文的第四部分“实验”详细评估了BLIP-2在多个零样本视觉-语言任务上的性能，包括图像问答（VQA）、图像描述生成（Image Captioning）、图像-文本检索（Image-Text Retrieval）等。以下是该章节内容的总结：

---

### **4.1 零样本图像到文本生成**
BLIP-2能够使大型语言模型（LLM）理解图像内容，并在遵循文本提示的情况下生成对应的文本。通过将视觉提示与文本提示拼接输入LLM，可以实现多种零样本图像到文本生成任务，如视觉推理、常识推理、个性化生成等。该部分展示了BLIP-2模型在零样本VQA任务上的定量评估结果，使用OPT和FlanT5模型进行测试，通过调整提示语和生成参数（如beam search和长度惩罚），实现了高质量的生成。

---

### **4.2 图像描述生成**
BLIP-2在图像描述生成任务上的表现也进行了微调验证。通过在COCO数据集上进行微调，并在NoCaps数据集上进行零样本评估，结果表明BLIP-2在多个评价指标（CIDEr、SPICE、BLEU@4）上均达到了SOTA（State-of-the-Art）水平。此外，BLIP-2在不需要进行微调的情况下也展示了强大的泛化能力。

---

### **4.3 图像问答（VQA）**
BLIP-2通过微调Q-Former和图像编码器（LLM保持冻结）来解决开放式的图像问答任务。模型在VQAv2、OK-VQA和GQA数据集上表现优异，尤其在VQAv2数据集上优于Flamingo80B，尽管其可训练参数数量仅为Flamingo80B的1/54。此外，研究发现，使用更强的图像编码器和更大的语言模型均有助于提升任务性能。

---

### **4.4 图像-文本检索**
BLIP-2在图像-文本检索任务上同样表现突出。该任务不需要语言生成，因此在训练阶段仅微调图像编码器和Q-Former。模型在COCO和Flickr30K数据集上的零样本评估中达到了SOTA结果。实验表明，结合图像文本对比损失（ITC）、图像文本匹配损失（ITM）和图像依据文本生成损失（ITG）可以显著提升检索性能，其中ITG损失能进一步增强视觉-语言对齐能力。

---

### **总结**
BLIP-2在多个零样本视觉-语言任务中均表现出色，尤其在参数效率方面具有显著优势。相比现有SOTA模型，BLIP-2在保持高精度的同时，所需的可训练参数更少，展示了其作为通用视觉-语言预训练方法的强大潜力。此外，BLIP-2的模块化设计（如冻结图像编码器、使用Q-Former桥接模态）使得其能够灵活适应不同任务，如图像生成、问答和检索。


## 5 Limitation



该章节主要讨论了BLIP-2模型的局限性，主要包括以下几点：

1. **缺乏上下文学习能力**：尽管当前大语言模型（LLMs）具备通过少量示例进行上下文学习的能力，但BLIP-2在使用提供上下文的视觉问答（VQA）示例时，并未表现出性能提升。作者认为这是因为BLIP-2的预训练数据集中每个样本只包含一个图像-文本对，无法让模型学习多个图像-文本对之间的关联性。类似的问题也在Flamingo论文中被提及，该模型使用了包含多个图像-文本对的数据集。

2. **生成结果可能不理想**：BLIP-2的图像到文本生成可能存在不准确的问题，主要原因包括语言模型的知识不准确、激活了错误的推理路径，或对新图像内容的信息不够更新。

3. **继承LLMs的风险**：由于BLIP-2使用的是冻结的图像编码器和大语言模型，因此继承了LLMs的潜在风险，例如输出冒犯性语言、传播社会偏见或泄露隐私信息。

4. **缓解措施**：为应对这些问题，可以采取一些缓解方法，例如通过指令引导模型生成内容，或在经过过滤的、去除了有害内容的数据集上进行训练。

总结来看，BLIP-2在上下文学习、生成能力和伦理风险方面仍存在一定的局限性，未来需要更丰富的训练数据和更有效的控制机制来改善这些问题。


## 6 Conclusion



本节总结如下：

本文提出了BLIP-2，这是一种通用且计算高效的视觉-语言预训练方法，利用了冻结的预训练图像编码器和大语言模型（LLM）。BLIP-2在多种视觉-语言任务中实现了最先进的性能，同时在预训练过程中仅需少量可训练参数。此外，BLIP-2在零样本图像到文本生成任务中也展现出新兴的能力。作者认为，BLIP-2是构建多模态对话AI代理的重要一步。
