# 2508.06471_GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models



* 首页: <https://arxiv.org/abs/2508.06471>
* PDF: <https://arxiv.org/pdf/2508.06471>
* 引用: 112(2026-01-24)
* 组织: 
    * Zhipu AI
    * Tsinghua University
* 链接
  * <https://github.com/zai-org/GLM-4.5>
  * <https://docs.z.ai/guides/llm/glm-4.7>


## 总结

### 相关LLM


#### GLM-4.7

**GLM-4.7**，您的新编程伙伴，具备以下特性：

- **核心编程能力**：与前代 GLM-4.6 相比，GLM-4.7 在多语言智能体编程和终端任务方面取得了显著提升，包括 SWE-bench（73.8%，+5.8%）、SWE-bench Multilingual（66.7%，+12.9%）和 Terminal Bench 2.0（41%，+16.5%）。GLM-4.7 还支持先思考后行动，在 Claude Code、Kilo Code、Cline 和 Roo Code 等主流智能体框架的复杂任务上有显著改进。
- **氛围编程**：GLM-4.7 在 UI 质量提升方面迈出了重要一步。它能生成更简洁、更现代的网页，并制作出布局和尺寸更精准、外观更美观的幻灯片。
- **工具使用**：GLM-4.7 在工具使用能力上取得了显著改进。在 τ²-Bench 等基准测试以及 BrowseComp 网页浏览测试中表现出明显更优的性能。
- **复杂推理**：GLM-4.7 在数学和推理能力上实现了大幅提升，在 HLE（人类终极考试）基准测试中相比 GLM-4.6 取得了（42.8%，+12.4%）的成绩。

#### GLM-4.6

与 GLM-4.5 相比，**GLM-4.6** 带来了几个关键改进：

- **更长的上下文窗口：** 上下文窗口从 128K 扩展到 200K tokens，使模型能够处理更复杂的智能体任务。
- **更强的代码性能：** 模型在代码基准测试中取得了更高的分数，并在实际应用中表现更佳，例如 Claude Code、Cline、Roo Code 和 Kilo Code，包括在生成视觉上更精美的前端页面方面的提升。
- **更先进的推理能力：** GLM-4.6 在推理性能上有明显提升，并在推理过程中支持工具调用，从而带来更强的整体能力。
- **更强大的智能体：** GLM-4.6 在工具使用和基于搜索的智能体方面表现更强，并能更高效地融入智能体框架。
- **更精细的写作：** 更好地符合人类在风格和可读性上的偏好，并在角色扮演场景中表现得更加自然。

#### GLM-4.5

**GLM-4.5** 系列模型是专为智能体设计的基础模型。GLM-4.5拥有 **3550** 亿总参数量，其中 **320** 亿活跃参数；GLM-4.5-Air 采用更紧凑的设计，拥有
 **1060** 亿总参数量，其中 **120** 亿活跃参数。GLM-4.5模型统一了推理、编码和智能体能力，以满足智能体应用的复杂需求。

GLM-4.5 和 GLM-4.5-Air 都是混合推理模型，提供两种模式：用于复杂推理和工具使用的思考模式，以及用于即时响应的非思考模式。

#### 模型表


| 模型               | 下载链接                                                                                                                                          | 模型大小      | 精度   |
|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-----------|------|
| GLM-4.7          | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.7)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.7)                   | 355B-A32B  | BF16      |
| GLM-4.7-FP8      | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.7-FP8)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.7-FP8)           | 355B-A32B  | FP8       |
| GLM-4.7-Flash    | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.7-Flash)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.7-Flash)       | 30B-A3B    | BF16      |
| GLM-4.6          | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.6)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.6)                   | 355B-A32B  | BF16      |
| GLM-4.6-FP8      | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.6-FP8)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.6-FP8)           | 355B-A32B  | FP8       |
| GLM-4.5          | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5)                   | 355B-A32B  | BF16      |
| GLM-4.5-Air      | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air)           | 106B-A12B  | BF16      |
| GLM-4.5-FP8      | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-FP8)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-FP8)           | 355B-A32B  | FP8       |
| GLM-4.5-Air-FP8  | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air-FP8)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air-FP8)   | 106B-A12B  | FP8       |
| GLM-4.5-Base     | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Base)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Base)         | 355B-A32B  | BF16      |
| GLM-4.5-Air-Base | [🤗 Hugging Face](https://huggingface.co/zai-org/GLM-4.5-Air-Base)<br> [🤖 ModelScope](https://modelscope.cn/models/ZhipuAI/GLM-4.5-Air-Base) | 106B-A12B  | BF16      |



### 图解

![](https://img.zhaoweiguo.com/uPic/2026/01/2HMdyB.png)

Figure 3:Pre-training and mid-training stages for GLM-4.5. We adapt a multi-stage training recipe and extend the sequence length from 4K to 128K.

![](https://img.zhaoweiguo.com/uPic/2026/01/Kr4TG3.png)

Figure 10:Overview of the Slime RL infrastructure.

**图解**
* The system consists of three core modules: 
    * Training (Megatron)
        * handles the main training process,
        * reads data from the Data Buffer, and synchronizes parameters with the rollout module after training; 
    * Rollout (SGLang + Router)
        * generates new data, including rewards and verifier outputs, and writes it to the Data Buffer;
    *  Data Buffer
        * serves as a bridge module that manages prompt initialization, custom data, and rollout generation strategies.


### 系统要求-推理(英伟达GPU)


模型可在下表配置下运行：

| 模型            | 精度   | GPU类型和数量   |
|---------------|------|------------|
| GLM-4.5       | BF16 | H100 x 16  |
| GLM-4.5       | FP8  | H100 x 8   |
| GLM-4.5-Air   | BF16 | H100 x 4   |
| GLM-4.5-Air   | FP8  | H100 x 2   |
| GLM-4.7-Flash | BF16 | H100 x 1   |


### prompt

#### Figure 4: One example of function call template.
```markdown
<|system|>
# Tools
You may call one or more functions to assist with the user query.
You are provided with function signatures within <tools></tools> XML tags:
<tools>
{"name": "get_weather", "description": "Get the weather of a city for a specific date.", "parameters": {"
type": "object", "properties": {"city": {"type": "string", "description": "The city to get weather for,
in Chinese."}, "date": {"type": "string", "description": "The date in YYYY-MM-DD format."}}, "required":
["city"]}}
</tools>
For each function call, output the function name and arguments within the following XML format:
<tool_call>{function-name}
<arg_key>{arg-key-1}</arg_key>
<arg_value>{arg-value-1}</arg_value>
<arg_key>{arg-key-2}</arg_key>
<arg_value>{arg-value-2}</arg_value>
...
</tool_call><|system|>
You are a helpful assistant.<|user|>
Today is June 26, 2024. Could you please check the weather in Beijing and Shanghai for tomorrow<|
assistant|>
<think>The user wants to check the weather of Beijing and Shanghai tomorrow. I need to call the
get_weather function respectively to check Beijing and Shanghai.</think>
I will call the get_weather function to check the weather in Beijing and Shanghai.
<tool_call>get_weather
<arg_key>city</arg_key>
<arg_value>Beijing</arg_value>
<arg_key>date</arg_key>
<arg_value>2024-06-27</arg_value>
</tool_call>
<tool_call>get_weather
<arg_key>city</arg_key>
<arg_value>Shanghai</arg_value>
<arg_key>date</arg_key>
<arg_value>2024-06-27</arg_value>
</tool_call><|observation|>
<tool_response>
{"city": "Beijing", "date": "2024-06-27", "weather": "Sunny", "temperature": "26C"}
</tool_response>
<tool_response>
{"city": "Shanghai", "date": "2024-06-27", "weather": "Overcast", "temperature": "29C"}
</tool_response><|assistant|>
<think>I have obtained the weather query results of get_weather for Beijing and Shanghai respectively and
can reply to users directly.</think>
It will be sunny in Beijing tomorrow with a temperature of 26 degrees Celsius. The weather in Shanghai is
overcast with a temperature of 29 degrees Celsius.<|user|>
```

#### Figure 11: One example of user prompt we used for TAU-bench.

```markdown
You are a user interacting with an agent.{instruction_display}
# Rules:
- Just generate one line at a time to simulate the user’s message.
- Do not give away all the instruction at once. Only provide the information that
is necessary for the current step.
- Do not hallucinate information that is not provided in the instruction. Follow
these guidelines:
1. If the agent asks for information NOT in the instruction:
- Say you don’t remember or don’t have it
- Offer alternative information that IS mentioned in the instruction
2. Examples:
- If asked for order ID (not in instruction): ‘‘Sorry, I don’t remember the order
ID, can you search for it? My name/email/phone number/zipcode is ...’’
- If asked for email (not in instruction): ‘‘I don’t have my email handy, but I
can give you my name and zip code which are...’’
- Do not repeat the exact instruction in the conversation. Instead, use your own
words to convey the same information.
- Try to make the conversation as natural as possible, and stick to the
personalities in the instruction.
# Constraint Handling:
- Provide requests strictly based on what is explicitly stated in the instruction.
- Do not assume, extend, substitute, or generalize in any form.
- Do not modify or relax constraints on:
- Time / Date
- Budget
- Specific terms (e.g., ‘‘same’’ must not be replaced with ‘‘similar’’)
- Core Rule: Any attribute NOT mentioned in the instruction can be either changed
or kept the same
- Examples:
- If instruction says ‘‘exchange red item to blue’’: Only color must change, other
attributes (size, material, etc.) are flexible
- If instruction says ‘‘exchange red item to blue, keep the same size’’: Both
color must change AND size must stay the same
- Exception: Only follow additional constraints when explicitly stated in the
instruction
# When NOT to finish the conversation:
- Do not end until you have clearly and completely expressed all your requirements
and constraints.
- Do not end until the agent has completed all tasks mentioned in the instruction
and verified no operations were missed.
- Do not end if the agent’s execution results do not match your expectations or
are incorrect/incomplete.
# When you CAN finish the conversation:
- Only when all above conditions are satisfied AND all tasks are completed
correctly.
- OR when you have clearly expressed complete requirements but the system
explicitly states it cannot complete them due to technical limitations - in this
case, accept transfer to human.
# How to finish the conversation:
- If the agent has completed all tasks, generate ‘‘###STOP###’’ as a standalone
message without anything else to end the conversation.
# Note:
- You should carefully check if the agent has completed all tasks mentioned in the
instruction before generating ‘‘###STOP###’’.
```



## From Moonlight

### 三句摘要

1. GLM-4.5是一种355B总参数和32B激活参数的开源Mixture-of-Experts (MoE)大型语言模型，采用混合推理方法，在Agentic、Reasoning和Coding (ARC)任务上表现出色，总体排名第三，其中Agentic基准测试排名第二。
2. 该模型经过23T token的多阶段预训练和“中训练”阶段，以增强代码、推理和长上下文能力，并通过包括Supervised Fine-Tuning、Reasoning RL、Agentic RL和General RL在内的专家模型迭代进行全面的后训练。
3. GLM-4.5在SWE-bench Verified、AIME 24等ARC基准测试中取得领先成绩，并在人工评估中展现出卓越的通用对话、编码Agent和逻辑推理能力，尤其在翻译任务上显著优于专业模型。

### 关键词

- GLM-4.5: 智谱AI与清华大学合作发布的一个开源大型语言模型系列中的主模型，拥有3550亿总参数和320亿激活参数，采用混合推理方法，支持思考模式和直接响应模式，在智能体、推理和编码（ARC）任务上表现出色。
- GLM-4.5-Air: GLM-4.5系列中的紧凑版本，拥有1060亿总参数，同样采用MoE架构和混合推理模式，在1000亿参数量级模型中表现领先。
- Mixture-of-Experts (MoE):
- 混合推理: GLM-4.5和GLM-4.5-Air的一项关键功能，允许模型在“思考模式”下进行复杂推理和智能体任务，并在“非思考模式”下进行即时响应，根据任务需求灵活切换。
- ARC: 指代模型在**智能体能力 (Agentic)**、**复杂推理 (Reasoning)** 和**高级编码 (Coding)** 三个核心领域的表现，是评估模型作为通用问题解决者能力的关键指标。
- 多阶段训练: GLM-4.5模型采用的一种训练策略，模型首先在大规模通用语料上进行预训练，然后在后续阶段逐渐引入特定领域的数据，如代码、数学和科学数据，以提升模型在不同任务上的能力。
- 中期训练: GLM-4.5多阶段训练中的一个阶段，在通用预训练之后进行，旨在进一步提升模型在特定应用领域（如代码、推理和智能体能力）的表现，通过使用中等规模的领域特定数据集和指令数据进行训练。
- 后训练: GLM-4.5模型训练过程中的重要阶段，分为两个主要部分：第一阶段是构建专门针对推理、智能体和通用聊天领域的专家模型；第二阶段是利用自蒸馏技术将多个专家模型的能力整合到一个全面的模型中，以支持混合推理模式。
- 专家模型迭代: 后训练的第一阶段，旨在通过训练专门的专家模型来提升模型在特定领域的能力，这些专家模型专注于推理、智能体和通用聊天。
- 强化学习 (RL): 在GLM-4.5的后训练阶段广泛使用的一种技术，用于增强模型在推理、智能体任务和通用对话中的能力，通过优化奖励信号来提升模型表现。
- 有监督微调 (SFT): 在GLM-4.5后训练开始时进行的一项技术，用于为专家模型提供基础能力（冷启动），并随后用于将不同专家模型的能力蒸馏到一个统一的模型中，使其具备处理多种任务的能力。
- 函数调用模板: GLM-4.5中提出的一种新颖的机制，旨在解决传统JSON格式中函数调用参数（特别是包含代码段时）需要大量字符转义的问题。该模板使用XML风格的特殊标记封装键值对，大大减少了转义的必要性，提高了模型生成函数调用的效率和鲁棒性。
- 拒接采样: 在从专家模型中采样时采用的一种过滤策略，通过多阶段过滤流程去除重复、过短、不符合推理格式或不正确的样本，并对主观问题的响应进行奖励模型过滤，以确保数据的质量。
- 基于难度的课程学习: 在强化学习中采用的一种两阶段学习策略，旨在应对模型能力提升与静态训练数据之间的不匹配问题。通过在初期使用中等难度数据，后期切换到极高难度且有正确答案验证的问题，确保RL训练始终获得有用的梯度信号，避免奖励方差不足。
- 单阶段RL: 在GLM-4.5的强化学习实验中发现的一种有效策略，即直接在最大目标输出长度（例如64K）进行强化学习，而非采用逐步增加最大输出长度的多阶段RL。研究表明，多阶段RL可能导致模型“遗忘”其长上下文能力，影响最终性能。
- 动态采样温度: 在RL训练中，根据模型收敛情况动态调整采样温度的策略。当平均奖励稳定时，增加采样温度以鼓励模型进行更大范围的探索；同时，通过质量控制机制确保新引入的探索性样本不会过度降低模型性能，从而在准确性和探索性之间取得平衡。
- 智能体强化学习: 强化学习在智能体（如网络搜索和代码生成智能体）领域的应用，特点是其行动或答案通常可以被自动验证，从而提供密集的、可靠的奖励信号，有效扩展RL训练。
- 通用强化学习: 旨在全面提升模型整体性能、纠正潜在问题并强化关键能力的RL阶段，它融合了基于规则的反馈、人工反馈（RLHF）和基于模型的反馈（RLAIF）多种来源，以提供更强大的训练信号。
- 指令遵循强化学习: 一种强化学习方法，通过创建细粒度的约束类型分类和挑战性训练集，并结合确定性验证规则、训练的奖励模型和批判模型组成的反馈系统，旨在提升模型理解和满足复杂指令的能力。
- 函数调用强化学习: 一种强化学习方法，分为步进式基于规则的RL和端到端多轮RL。前者通过严格的奖励函数强制模型生成正确的工具调用格式；后者允许模型通过与环境的持续交互和试错来优化其行动策略，提升自主规划和决策能力。
- 病理强化学习: 后训练的最后阶段，通过针对性地选择易触发语言混合、过度重复和格式错误等“病理行为”的提示，并施加高效的惩罚，以纠正这些问题，降低模型输出中问题行为的残余错误率。
- Slime: GLM-4.5团队开发的一个开源强化学习框架，旨在提供灵活、高效和可扩展的RL训练和数据生成架构，支持同步和异步模式，并针对智能体任务进行了优化。
- SWE-bench Verified: 
- Terminal-Bench: 一个衡量模型在终端环境中完成复杂任务能力的基准测试，评估模型在命令行界面下的操作和问题解决能力。
- TAU-Bench: 一个衡量模型智能体能力的基准测试，评估模型在零售和航空等领域通过函数调用响应用户查询的能力。
- BFCL V3: 全称Berkeley Function Call Leaderboard V3，是一个衡量模型调用用户定义函数以响应用户查询能力的基准测试。

### 摘要

GLM-4.5系列模型，包括GLM-4.5和更紧凑的GLM-4.5-Air，是由智谱AI与清华大学合作推出的开源MoE大语言模型。GLM-4.5总参数量为355B，激活参数量为32B；GLM-4.5-Air总参数量为106B，激活参数量为12B。该系列模型采用混合推理方法，支持“思考”（thinking）模式进行复杂推理和agentic任务，以及“非思考”（non-thinking）模式进行即时响应。

**核心方法论（Core Methodology）：**

1.  **架构（Architecture）**
    GLM-4.5系列采用了Mixture-of-Experts (MoE) 架构，以提升训练和推理的计算效率。
    *   **MoE层设计：** 使用了loss-free balance routing [40] 和sigmoid gates [23]。
    *   **深度优先：** 相比DeepSeek-V3和Kimi K2，GLM-4.5减少了模型宽度（hidden dimension和routed experts数量），增加了高度（层数），研究发现更深的模型展现出更好的推理能力。
    *   **注意力机制：** 采用带partial RoPE的Grouped-Query Attention，并使用了2.5倍多的attention heads (5120 hidden dimension对应96个头)，以提升推理性能。
    *   **稳定性：** 引入QK-Norm [15] 来稳定注意力logits的范围。
    *   **MTP层：** 额外添加了一个MoE层作为Multi-Token Prediction (MTP) 层 [12]，以支持推理时的推测解码（speculative decoding）。

2.  **预训练（Pre-Training）**
    模型在23T tokens的数据上进行了多阶段训练。
    *   **预训练数据：** 包含网页、社交媒体、书籍、论文和代码仓库等多种来源。
        *   **网页数据：** 根据质量分数分桶，高质量数据进行up-sample，低质量数据舍弃。使用SemDedup [1] 管道基于文档嵌入移除相似网页。
        *   **多语言数据：** 包含来自爬取网页和Fineweb-2 [27] 的多语言文档，通过质量分类器进行up-sample。
        *   **代码数据：** 来自GitHub及其他代码托管平台，经规则过滤、语言特定质量模型分类（高、中、低质量），高质量代码up-sample，低质量舍弃。代码数据应用Fill-In-the-Middle [5] 训练目标。代码相关网页文档也经过两阶段检索和质量评估。
        *   **数学与科学数据：** 从网页、书籍、论文收集，通过LLM评分并up-sample教育性内容。
    *   **预训练阶段：**
        *   **第一阶段：** 主要在通用网页文档上训练。
        *   **第二阶段：** up-sample来自GitHub、与编码、数学和科学相关的网页数据。

3.  **中训练（Mid-Training）：提升推理与Agentic能力**
    预训练后，模型进行多阶段中训练以进一步提升关键应用领域性能，使用中等规模的特定领域数据集，包括指令数据。
    *   **Repo-level Code Training：** 整合来自同一代码仓库的串联文件以学习跨文件依赖。加入模型过滤的GitHub issues, pull requests (PRs) 和commits，扩展序列长度从4K到32K。
    *   **Synthetic Reasoning Data Training：** 添加数学、科学和编程竞赛的合成推理内容，通过推理模型合成推理过程。
    *   **Long-context & Agent Training：** 将训练序列长度从32K扩展到128K，up-sample长文档，并整合大规模合成agent trajectories。
    *   **序列长度策略：** 预训练阶段最大序列长度为4,096，中训练阶段逐步扩展到32,768和131,072。中训练阶段采用best-fit packing。
    *   **RoPE调整：** 扩展序列长度至32K时，RoPE的base frequency从10,000调整为1,000,000。
    *   **优化器：** 使用Muon optimizer [21; 24]，对word embedding, bias, RMSNorm权重除外。学习率采用cosine decay schedule。

4.  **后训练（Post-Training）：专家模型迭代与强化学习**
    后训练分为两个阶段：专家训练（Stage 1）和统一训练（Stage 2）。
    *   **Stage 1: 专家训练（Expert Training）**
        构建专门的专家模型，专注于推理（Reasoning）、Agent和通用聊天（General chat）三个领域。
    *   **Stage 2: 统一训练（Unified Training）**
        采用自蒸馏（self-distillation）技术整合多个专家模型，最终得到一个能够通过深思熟虑的推理和直接响应模式生成回复的综合模型。

    **4.1 监督微调（Supervised Fine-Tuning, SFT）**
    *   **冷启动SFT：** 在Stage 1开始时，使用少量带有Extended Chain-of-Thought (CoT) 响应的SFT数据，为专家模型奠定基础。
    *   **整体SFT：** 在Stage 2中，收集来自先前训练的专家模型的数百万样本（涵盖推理、通用聊天、agentic任务、长文本理解），以最长128K tokens的上下文长度训练基础模型。通过蒸馏专家模型输出，模型学习为每项任务应用最有效的长CoT推理，同时平衡包含完整推理和缺乏显式思维过程的数据，以支持混合推理模式。
    *   **函数调用模板优化：** 针对agentic模型中函数调用参数常为JSON格式但包含代码段时字符转义过多的问题，提出将函数调用键值封装在XML-like特殊token标签内的新模板，大幅减少了代码段的转义需求。

    **4.2 推理RL (Reasoning RL)**
    专注于增强模型在数学、代码生成和科学推理等领域的逻辑演绎和结构化问题解决能力。
    *   **RL算法：** 基于GRPO [31] 框架，排除了KL损失项。
    *   **Difficulty-based Curriculum Learning：** 采用两阶段难度课程学习，解决模型能力提升与静态数据不匹配问题，确保训练效率和信号质量。
    *   **Single-Stage RL at 64K Output Length：** 实验发现，直接在最大目标长度64K进行单阶段RL比多阶段逐步增加长度更有效，避免了模型“unlearn”长上下文能力。
    *   **Dynamic Sampling Temperature：** 动态调整采样温度，以在训练后期策略分布集中时，鼓励更大程度的探索，并通过质量控制机制避免引入过多噪声。
    *   **Code RL：** 发现token-weighted mean loss相比sequence-mean loss能带来更快收敛和更稳定的梯度信号，减轻长度偏差。
    *   **Science RL：** 在GPQA-Diamond基准上，发现仅使用少量高质量、专家验证的多项选择题进行RL能显著提升性能。

    **4.3 Agentic RL**
    在web搜索和代码生成agentic设置中，利用其固有的可验证性提供密集、可靠的奖励信号，有效扩展RL训练。
    *   **数据收集与合成：**
        *   **Web搜索任务：** 开发自动化数据合成管道，通过知识图谱进行多跳推理，或人工参与从网页提取和混淆内容。
        *   **软件工程任务：** 整理GitHub pull requests和issues，创建包含用户提示和可执行单元测试的基准。
    *   **RL算法：** 采用group-wise policy optimization算法进行RL训练，目标函数为：
        $$L_{RL}(\theta) = E_{x \sim D} \left[ \frac{1}{K} \sum_{i=1}^K (r(x, y_i) - \bar{r}(x)) \right]$$
        其中，$y_i$是从旧策略$\pi_{old}$中采样的agent traces，$r(x, y_i)$是轨迹奖励，$\bar{r}(x) = \frac{1}{K} \sum_{i=1}^K r(x, y_i)$是采样响应的平均奖励。仅模型生成的token用于优化，环境反馈在损失计算中被忽略。
    *   **Outcome Supervision with Process Action Format Penalty：** 对于web搜索任务，最终答案的准确性作为整个agent轨迹的奖励。对于编码agent，主要使用SWE数据进行RL训练。若模型未能生成正确的工具调用格式，轨迹将获得零奖励。
    *   **Iterative Distillation：** 采用自蒸馏方法，通过RL训练后模型的响应来替代SFT冷启动数据，创建更优秀的SFT模型，然后在此基础上继续RL训练，逐步增加训练难度。
    *   **Scaling Test-time Compute via Interaction Turns：** 观察到agent任务通过与环境的持续交互（例如web搜索、代码自验证）能显著提升性能。

    **4.4 通用RL (General RL)**
    旨在全面提升模型的整体性能，纠正潜在问题，并强化关键能力。
    *   **多源反馈系统：** 结合基于规则的反馈、人工反馈（RLHF）和基于模型的反馈（RLAIF），提供更鲁棒的训练信号。
    *   **Holistic RL：** 构建平衡数据集，涵盖7个主要、33个次要和139个三级类别，奖励信号来自人工偏好标注和AI反馈（设计评分标准）。
    *   **指令遵循RL (Instruction Following RL)：** 创建细粒度分类法（7个主要，151个次要约束类型），构建挑战性指令训练集。反馈系统包含确定性验证规则、训练的奖励模型和critique模型。
    *   **函数调用RL (Function Calling RL)：**
        *   **Step-wise Rule-based RL：** 对于有明确工具调用流程的任务，在训练数据中为每一步/轮标注ground truth函数调用。使用严格的奖励函数：
            $$\text{Reward} = \begin{cases} 1, & \text{if FormatCorrect}(a_t) \text{ and Match}(a_t, a_t^*) \\ 0, & \text{otherwise} \end{cases}$$
            其中$a_t$是模型生成的第$t$个函数调用，$a_t^*$是对应的ground truth。此奖励强制输出格式化。
        *   **End-to-end Multi-turn RL：** 模型生成完整轨迹，并根据任务完成情况获得奖励。考虑单轮多步骤和多轮多步骤任务。奖励函数为：
            $$\text{Reward} = \begin{cases} 1, & \text{if FormatCorrect}(a_1, \dots, a_T) \text{ and TaskCompleted}(I, o_0, a_1, o_1, \dots, a_T, o_T) \\ 0, & \text{otherwise} \end{cases}$$
            其中$I$是原始任务，$a_t$是第$t$个函数调用，$o_t$是工具反馈或用户信息。
    *   **Pathology RL：** 针对语言混合、过度重复和格式错误等常见病态行为，策划有针对性的数据集进行惩罚训练，提高样本效率。

    **4.5 RL基础设施 (RL Infrastructure)**
    基于开源框架Slime构建，旨在增强灵活性、效率和可扩展性。
    *   **灵活的混合训练与数据生成架构：** 支持同地同步模式（适用于通用RL或推理能力提升，训练与推理引擎在同一worker，减少GPU空闲时间）和分布式异步模式（适用于agentic任务，agent环境可连续生成数据，训练与推理GPU独立调度）。
    *   **加速Rollout与混合精度推理：** 训练采用BF16，推理采用FP8以加速数据生成。在每次策略更新迭代中，对模型参数进行在线、块级FP8量化。
    *   **面向Agent的RL基础设施设计：** 采用异步解耦基础设施，处理长周期agent rollouts，支持跨不同agent框架的灵活多任务RL训练。设计高并发的基于Docker的运行时环境，以及统一的HTTP端点接口和中心化数据池，使不同agent框架的数据能够无缝集成和共享。

GLM-4.5系列模型在ARC基准测试上取得了显著性能，例如TAU-Bench 70.1%、AIME 24 91.0%、SWE-bench Verified 64.2%。在整体评估中排名第三，在agentic基准测试中排名第二。尽管参数量远少于某些竞争对手（例如，参数量约为DeepSeek-R1的一半，Kimi K2的三分之一），但仍能保持优异性能，展现出高效的参数利用率。在人工评估中，GLM-4.5在通用聊天、agentic编码和逻辑推理方面也表现出色，尤其在翻译任务中显著优于专业模型，体现了其对互联网俚语、文化背景和领域特定术语的深刻理解。


## Abstract


本节介绍了GLM-4.5，一个开源的**Mixture-of-Experts（MoE）大语言模型**，具有**3550亿总参数**，其中**每次激活320亿参数**。该模型引入了一种**混合推理方法**，支持**思考模式**（reasoning模式）和**直接响应模式**（chat模式），提升了在复杂任务中的灵活性。

GLM-4.5通过**多阶段训练**，共训练了**23万亿token**，并结合了**专家模型迭代**和**强化学习**的后训练策略，使其在**代理（agentic）、推理（reasoning）和代码（coding）任务**上表现优异：

- **TAU-Bench**（推理任务）得分：**70.1%**
- **AIME 24**（数学推理）得分：**91.0%**
- **SWE-bench Verified**（代码生成）得分：**64.2%**

尽管参数总量远少于多个竞品模型，GLM-4.5在所有评估模型中**综合排名第三**，在**代理任务（agentic）中排名第二**。

此外，作者还发布了轻量版本**GLM-4.5-Air**，参数为**1060亿**，便于研究和部署。两者均已开源，相关代码和模型可在GitHub上获取：[https://github.com/zai-org/GLM-4.5](https://github.com/zai-org/GLM-4.5)

### 图表说明：
图1展示了GLM-4.5及其Air版本在**代理、推理和代码（ARC）基准测试**中的平均性能。GLM-4.5综合排名第三，GLM-4.5-Air排名第六。数据截至2025年7月28日。

---

**重点内容总结：**
- GLM-4.5是开源MoE结构模型，参数总量355B，激活参数32B。
- 支持两种响应模式：思考模式和直接响应模式。
- 经过大规模训练和强化学习优化，在多个高难度任务中表现优异。
- 在参数量更少的情况下，综合性能排名第三，agentic任务中排名第二。
- 同时发布轻量版本GLM-4.5-Air（106B参数），便于研究和部署。


## 1 Introduction


本节介绍了大型语言模型（LLMs）从通用知识库向通用问题求解器的演进趋势，并提出了实现通用人工智能（AGI）的关键能力要求。文章重点指出，真正具备通用能力的模型应具备三大核心能力：**代理能力（Agentic）**、**复杂推理能力（Reasoning）** 和 **高级编程能力（Coding）**，这三者合称为 ARC 能力。

### 核心内容总结：

#### 1.1 研究背景与目标
- LLMs 正从知识存储向解决复杂问题的方向发展，目标是实现类人水平的跨领域认知能力。
- 实现 AGI 需要模型具备复杂问题解决、泛化和自我提升能力，而不仅仅是特定任务的优异表现。

#### 1.2 当前挑战
- 尽管如 OpenAI 的 o1/o3 和 Anthropic 的 Claude Sonnet 4 等闭源模型在某些 ARC 领域（如数学推理、代码修复）表现出色，但**尚无一个强大的开源模型能在代理、推理和编程三方面全面领先**。

#### 1.3 新模型介绍：GLM-4.5 与 GLM-4.5-Air
- **GLM-4.5**：
  - 是首个 MoE（Mixture of Experts）结构模型，总参数为 355B，激活参数为 32B。
  - 支持两种推理模式：用于复杂任务的“思考模式”与用于快速响应的“非思考模式”。
  - 在多个 ARC 基准测试中表现优异：
    - **代理能力**：TAU-Bench 得分 70.1%，BFCL v3 得分 77.8%（与 Claude Sonnet 4 相当）；在 BrowseComp 上得分为 26.4%，优于 Claude Opus 4（18.8%），接近 o4-mini-high（28.3%）。
    - **推理能力**：AIME 24 得分 91.0%，GPQA 得分 79.1%，LiveCodeBench 得分 72.9%，HLE 得分 14.4%。
    - **编程能力**：SWE-bench Verified 得分 64.2%，Terminal-Bench 得分 37.5%，超过 GPT-4.1 和 Gemini-2.5-pro，接近 Claude Sonnet 4。

- **GLM-4.5-Air**：
  - 较小的 MoE 模型，参数为 106B。
  - 在 100B 级模型中表现突出，超过 Qwen3-235B-A22B 和 MiniMax-M1。

#### 1.4 性能对比与参数效率
- 在 12 个 ARC 基准测试的平均排名中：
  - GLM-4.5 排名第 3，GLM-4.5-Air 排名第 6。
  - 在代理任务中，GLM-4.5 排名第二，仅次于 OpenAI o3。
  - 在编程任务中，GLM-4.5 排名第三，接近 Claude Sonnet 4。
- **参数效率高**：相比 DeepSeek-R1（参数为两倍）和 Kimi K2（参数为三倍），GLM-4.5 参数更少但性能更优。
- 在 SWE-bench Verified 与模型参数的对比图中，GLM-4.5 和 GLM-4.5-Air 位于帕累托前沿（Pareto Frontier），表明其在性能与参数之间达到了最优平衡。

#### 1.5 开源与可复现性
- 模型已开源，可在 Z.ai、BigModel.cn 和 HuggingFace（https://huggingface.co/zai-org/GLM-4.5）获取。
- 提供开源评估工具包：https://github.com/zai-org/glm-simple-evals。

---

### 总结：
本节明确了 ARC（代理、推理、编程）能力作为衡量通用 LLM 的关键标准，介绍了 GLM-4.5 和 GLM-4.5-Air 两款新模型的结构与性能优势，并通过大量基准测试验证其在开源模型中的领先地位。模型的开源与评估工具的发布也增强了研究的可复现性与社区影响力。


## 2 Pre-Training


### 2.1 架构（Architecture）

**重点内容：**
- **MoE 架构**：GLM-4.5 系列采用 **混合专家（MoE）架构**，提升训练和推理效率。使用 **loss-free balance routing** 和 **sigmoid 门控机制**。
- **模型结构优化**：相比 DeepSeek-V3 和 Kimi K2，GLM-4.5 **减少模型宽度（隐藏维度和专家数量）并增加层数（深度）**，以提升推理能力。
- **注意力机制改进**：
  - 使用 **Grouped-Query Attention + 部分 RoPE**；
  - 注意力头数量为 **96 个（隐藏维度为 5120）**，是 DeepSeek-V3 的 2.5 倍；
  - 虽然训练损失未改善，但在 **MMLU 和 BBH 等推理基准上表现更好**；
  - 引入 **QK-Norm** 以稳定注意力 logits 的范围。
- **MTP 层**：在 GLM-4.5 和 GLM-4.5-Air 中加入 MoE 层作为 **MTP（Multi-Token Prediction）层**，支持推理阶段的 **推测解码（speculative decoding）**。

**表格 1：模型结构对比**
| 模型 | GLM-4.5 | GLM-4.5-Air | DeepSeek-V3 | Kimi K2 |
| --- | --- | --- | --- | --- |
| 总参数量 | 355B | 106B | 671B | 1043B |
| 激活参数量 | 32B | 12B | 37B | 32B |
| MoE 层数 | 89 | 45 | 58 | 60 |
| 注意力头数 | 96 | 96 | 128 | 64 |
| 每 token 激活专家数 | 8 | 8 | 8 | 8 |
| QK-Norm | ✅ | ❌ | ❌ | ❌ |

> **总结**：GLM-4.5 系列通过 **深度增加、MoE 架构、注意力机制优化和 MTP 层设计**，在保持高效的同时提升了推理能力。

---

### 2.2 预训练数据（Pre-Training Data）

**重点内容：**

- **数据来源**：网页、社交媒体、书籍、论文、代码仓库。
- **网页数据处理**：
  - 按质量评分分桶，**高分桶上采样，低分桶丢弃**；
  - 使用 **SemDedup** 去除模板生成的重复网页。
- **多语言数据**：
  - 来自爬取网页和 Fineweb-2；
  - 使用质量分类器筛选高价值多语言文档。
- **代码数据**：
  - 来自 GitHub 和其他平台；
  - 使用语言专用质量模型分类为高、中、低三档；
  - **高质代码上采样，低质代码排除**；
  - 使用 **Fill-In-the-Middle 目标函数**；
  - 对网页中的代码内容进行两阶段检索和质量筛选。
- **数学与科学数据**：
  - 收集相关网页、书籍、论文；
  - 使用小模型预测内容质量，**高质量内容上采样**。

**预训练阶段划分**：
- 第一阶段：主要训练通用网页数据；
- 第二阶段：**上采样代码、数学、科学相关内容**。

> **总结**：数据构建强调 **高质量、多样性、领域覆盖**，并通过多阶段训练逐步强化模型在关键领域的表现。

---

### 2.3 中期训练：提升推理与代理能力（Mid-Training: Boost Reasoning & Agentic Capacity）

**重点内容：**

- **中期训练阶段**：在预训练后加入多个阶段，使用 **中等规模的领域特定数据和指令数据**，称为 **mid-training**。
- **训练阶段划分**：
  1. **仓库级代码训练（Repo-level Code Training）**：
     - 拼接同一仓库的代码文件，学习跨文件依赖；
     - 包括 GitHub 的 issue、PR、commit 数据；
     - 序列长度扩展至 **32K**。
  2. **合成推理数据训练（Synthetic Reasoning Data Training）**：
     - 收集竞赛类数学、科学、编程问题；
     - 使用推理模型生成推理过程。
  3. **长上下文与代理训练（Long-context & Agent Training）**：
     - 序列长度扩展至 **128K**；
     - 上采样长文档；
     - 引入大规模合成代理轨迹。

**图 3：训练阶段与序列长度扩展**
- 预训练阶段：最大序列长度为 **4K**；
- mid-training 阶段：逐步扩展至 **32K → 128K**；
- mid-training 使用 **best-fit packing** 避免截断重要推理过程或代码。

> **总结**：通过 **多阶段训练 + 长上下文建模 + 合成数据增强**，显著提升模型在推理、代码、长上下文等任务上的能力。

---

### 2.4 超参数设置（Hyper-Parameters）

**重点内容：**

- **优化器**：使用 **Muon 优化器**（除词嵌入、bias、RMSNorm 外）；
  - Newton-Schulz 迭代步数 NN = 5；
  - 动量 μ = 0.95；
  - Muon 更新 RMS 缩放 = 0.2；
  - **优势**：加速收敛、容忍更大 batch size。
- **学习率调度**：
  - 使用 **cosine decay**，而非 WSD；
  - warmup 阶段：从 0 到 2.5e-4；
  - decay 阶段：降至 2.5e-5。
- **批大小策略**：
  - 前 500B token：从 16M tokens 逐步增加到 64M；
  - 后续保持 64M。
- **正则化**：
  - 权重衰减率 = 0.1；
  - 不使用 dropout。
- **序列长度扩展**：
  - RoPE 基频从 10,000 调整为 1,000,000，以支持长上下文。
- **MoE 相关设置**：
  - loss-free balance routing 的 bias 更新率：
    - 前 15T token：0.001；
    - 后续：0.0；
  - 辅助序列级平衡损失权重 = 0.0001；
  - MTP 损失权重 λ：
    - 前 15T token：0.3；
    - 后续：0.1。

> **总结**：通过 **优化器选择、学习率策略、序列扩展、MoE 平衡控制** 等精细调参，确保模型在复杂任务上的稳定训练与高性能表现。


## 3 Post-Training: Expert Model Iteration


我们将后训练过程分为两个阶段。第一阶段（专家训练）构建专精于推理、智能体和通用对话的专家模型。第二阶段（统一训练）通过自蒸馏技术整合多个专家模型，最终生成一个综合性模型，能够通过深思熟虑的推理和直接响应两种模式生成回答。

### 3.1 监督微调

我们在两个阶段的开始都进行监督微调（SFT）。在专家训练阶段，SFT的主要作用是提供冷启动，赋予模型基本的对话、推理和工具使用能力。在统一训练阶段，SFT的目的是将不同专家模型的能力蒸馏到一个能够处理不同类型任务的混合推理模型中。

#### 冷启动SFT

在冷启动阶段，我们使用包含扩展思维链（CoT）回答的小型SFT数据集，确保每个专家模型在强化学习阶段之前具备足够的基础能力。

#### 全面SFT

在全面SFT阶段，我们收集数百万个样本，涵盖推理任务（数学、代码、科学等）、通用对话（写作、翻译、摘要、闲聊等）、智能体任务（基本工具使用、编码能力等）和长上下文理解任务。我们训练基础模型的最大上下文长度为128K tokens。通过从不同专家的输出中蒸馏，模型学会为每个任务应用最有效的长CoT推理以得出准确答案。

#### 减少函数调用模板中的字符转义

我们提出了一种新的函数调用模板，使用XML-like特殊标记封装函数调用键和值，大幅减少代码段中的字符转义需求。

#### 拒绝采样

我们采用多阶段过滤管道，包括去除重复、过短或截断样本，验证样本的正确性，使用奖励模型过滤主观问题的回答，确保工具调用场景的正确协议。

#### 提示选择和响应级扩展

过滤具有挑战性的提示并进行响应扩展证明是有效的。我们通过去除响应长度在底部50%的提示，训练数据减少一半的情况下，数学和科学任务提高了2%-4%。生成四个响应进一步提高了1%-2%。

#### 自动智能体SFT数据构建

构建智能体SFT数据包括四个步骤：1. 智能体框架和工具收集；2. 任务合成；3. 轨迹生成；4. 质量过滤。

### 3.2 推理RL

推理RL专注于增强模型在逻辑推理、结构化问题解决和可验证准确性方面的能力。我们采用GRPO框架，排除KL损失项。

#### 基于难度的课程学习

我们采用两阶段基于难度的课程学习，模型在第二阶段切换到极其困难的问题，持续提升性能。

#### 单阶段RL在64K输出长度

实验表明，单阶段RL在最大目标长度64K上比多阶段RL更有效。SFT已经使模型适应生成64K长度的响应，引入较短长度的RL阶段会导致模型“遗忘”其长上下文能力。

#### 动态采样温度

我们动态调整采样温度以保持准确性和探索之间的平衡。当rollout的平均奖励稳定时，增加采样温度以鼓励更大的多样性。

#### 代码和科学RL

对于代码RL，采用token加权平均损失比序列平均损失更有效。对于科学RL，使用专家验证的多项选择题进行RL训练效果最佳。

### 3.3 智能体RL

我们专注于网络搜索和代码生成智能体，其中每个动作或答案都可以自动检查。内置的可验证性提供了密集、可靠的奖励，使我们能够更有效地扩展RL训练。

#### 数据收集和合成

我们开发了一个数据合成管道，生成需要多步推理的问答对。对于软件工程任务，我们整理了GitHub的拉取请求和问题，创建了一个现实的软件开发基准。

#### 强化学习和迭代自蒸馏

我们采用组策略优化算法进行RL训练。对于每个问题x，我们从之前的策略πold中采样K个智能体轨迹，优化模型πθ。

#### 结果监督与过程格式惩罚

对于网络搜索任务，我们使用最终答案的准确性作为奖励。对于编码智能体，我们主要使用带有可验证测试用例的SWE数据进行RL训练。

#### 迭代蒸馏

我们采用自蒸馏方法迭代增强SFT冷启动模型的性能，然后在改进后的模型上继续RL训练。

#### 通过交互回合扩展测试时计算

我们观察到，随着与环境的交互回合增加，性能显著提升。

### 3.4 通用RL

通用RL旨在全面提升模型的整体性能，解决潜在问题，并加强关键能力。我们采用多源反馈系统，结合基于规则的反馈、人类反馈（RLHF）和模型反馈（RLAIF）。

#### 整体RL

整体RL旨在实现跨多个领域的广泛性能提升。我们构建了一个包含约5,000个提示的平衡数据集，奖励信号来自人类和AI反馈。

#### 指令遵循RL

指令遵循RL提高模型理解和满足复杂指令的能力。我们创建了一个细粒度的分类法，覆盖内容要求、格式规则等。

#### 函数调用RL

函数调用RL分为逐步基于规则的RL和端到端多轮RL。我们设计了严格的奖励函数，确保模型生成正确的函数调用。

#### 病理RL

病理RL旨在纠正潜在问题，如语言混合、过度重复和格式错误。我们整理了一个针对病理RL的数据集，进一步降低这些问题的残余错误率。

### 3.5 RL基础设施

我们的RL基础设施基于Slime，一个我们开发的开源框架。该框架经过多项关键优化，以增强灵活性、效率和可扩展性。

#### 灵活的混合训练和数据生成架构

我们的基础设施支持高度灵活的训练范式和数据生成策略，适应不同RL任务的需求。

#### 混合精度推理加速rollout

我们支持BF16训练，同时利用FP8推理加速数据生成阶段。

#### 面向智能体的RL基础设施设计

我们设计了一个完全异步和解耦的RL基础设施，高效处理长视野智能体rollout，支持跨多种智能体框架的灵活多任务RL训练。


## 4 Evaluation

### 4.1 基础模型评估
本节评估了基础模型 GLM-4.5-Base 的性能。从表2可以看出，GLM-4.5-Base 在多个基准测试中表现稳定，包括英文、代码、数学和中文任务。这验证了将多种能力统一到一个模型中的理念。

**表2：GLM-4.5-Base 与其他开源基础模型的比较**

|  | Benchmark (Metric) | Qwen3-235B | Llama4-Maverick | DeepSeek-V3 | Kimi-K2 | GLM-4.5 |
| --- | --- | --- | --- | --- | --- | --- |
|  |  | -A22B Base | 400B Base | Base | Base | Base |
|  | Architecture | MoE | MoE | MoE | MoE | MoE |
| # Activated Params | 22B | 17B | 37B | 32B | 32B |
| # Total Params | 235B | 400B | 671B | 1043B | 355B |
| English | SimpleQA (EM) | - | - | 26.6 | 35.3 | 30.0 |
| BBH (EM) | 88.9 | 87.1 | 88.4 | 88.7 | 86.2 |
| MMLU (EM) | 87.8 | 85.2 | 87.2 | 87.8 | 86.1 |
| HellaSwag (EM) | - | - | 88.9 | 94.6 | 87.1 |
| PIQA (EM) | - | - | 84.7 | - | 85.3 |
| TriviaQA (EM) | - | - | 82.9 | 85.1 | 80.0 |
| Code | EvalPlus (Pass@1) | 77.6 | 65.5 | 65.6 | 80.3 | 78.1 |
| LiveCodeBench-Base (Pass@1) | - | 25.1 | 24.6 | 26.3 | 28.1 |
| Math | GSM8K (EM) | 94.4 | 87.7 | 87.6 | 92.1 | 79.4 |
| MATH (EM) | 71.8 | 63.3 | 62.6 | 70.2 | 61.0 |
| Chinese | CLUEWSC (EM) | - | - | 82.7 | - | 83.5 |
| C-Eval (EM) | - | 80.9 | 90.1 | 92.5 | 86.9 |
| C3 (EM) | - | - | 78.6 | - | 83.1 |
|  | Chinese-SimpleQA (EM) | - | 53.5 | 72.1 | 77.6 | 70.1 |

### 4.2 在12个（ARC）基准测试中的评估
本节评估了GLM-4.5在完成代理（Agentic）、推理（Reasoning）和编码（Coding）任务后的性能，共涉及12个基准测试。

#### 4.2.1 代理能力评估
评估了GLM-4.5在TAU-bench 和 Berkeley Function Call Leaderboard V3 (BFCL V3) 上的表现。GLM-4.5在TAU-bench上的表现优于Gemini 2.5 Pro，接近Claude Sonnet 4。在BFCL V3上，GLM-4.5在所有基线模型中得分最高。在BrowseComp上，OpenAI o3的表现远超其他模型，GLM-4.5的表现接近第二名（o4-mini），显著优于Claude Opus 4。

**表3：代理基准测试结果**

| Benchmark | GLM- 4.5 | GLM- 4.5-Air | o3 | o4 mini | GPT-4.1 | Claude Opus 4 | Claude Sonnet 4 | Gemini 2.5 Pro | Kimi K2 | Grok 4 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| TAU-Retail | 79.7 | 77.9 | 70.4 | 65.6 | 75.1 | 81.4 | 80.5 | 77.0 | 73.9 | 76.5 |
| TAU-Airline | 60.4 | 60.8 | 52.0 | 49.2 | 48.8 | 59.6 | 60.0 | 48.0 | 51.2 | 58.4 |
| BFCL V3 | 77.8 | 76.4 | 72.4 | 67.2 | 68.9 | 74.4 | 75.2 | 61.2 | 71.1 | 66.2 |
| BrowseComp | 26.4 | 21.3 | 49.7 | 28.3 | 4.1 | 18.8 | 14.7 | 7.6 | 7.9 | 32.6 |
| Average | 58.1 | 55.7 | 61.1 | 50.1 | 45.0 | 54.6 | 53.4 | 43.8 | 47.2 | 55.4 |

#### 4.2.2 推理能力评估
评估了GLM-4.5和GLM-4.5-Air在七个基准测试中的表现，包括MMLU-Pro、AIME 24、MATH 500、SciCode、GPQA、HLE和LCB。GLM-4.5在AIME 24和SciCode上的表现优于OpenAI o3。平均而言，GLM-4.5的表现优于Claude Opus 4，接近DeepSeek-R1-0528。

**表4：推理基准测试结果**

| Benchmark | GLM- 4.5 | GLM- 4.5-Air | o3 | Claude Opus 4 | Gemini 2.5 Pro | DeepSeek R1 0528 | Qwen3 235B 2507 | Grok 4 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MMLU Pro | 84.6 | 81.4 | 85.3 | 87.3 | 86.2 | 84.9 | 84.5 | 86.6 |
| AIME 24 | 91.0 | 89.4 | 90.3 | 75.7 | 88.7 | 89.3 | 94.1 | 94.3 |
| MATH 500 | 98.2 | 98.1 | 99.2 | 98.2 | 96.7 | 98.3 | 98.0 | 99.0 |
| SciCode | 41.7 | 37.3 | 41.0 | 39.8 | 42.8 | 40.3 | 42.9 | 45.7 |
| GPQA | 79.1 | 75.0 | 82.7 | 79.6 | 84.4 | 81.3 | 81.1 | 87.7 |
| HLE | 14.4 | 10.6 | 20.0 | 11.7 | 21.1 | 14.9 | 15.8 | 23.9 |
| LCB | 72.9 | 70.7 | 78.4 | 63.6 | 80.1 | 77.0 | 78.2 | 81.9 |
| AA-Index (Est.) | 67.7 | 64.8 | 70.0 | 64.4 | 70.5 | 68.3 | 69.4 | 73.2 |

#### 4.2.3 编码能力评估
评估了GLM-4.5在SWE-bench Verified和Terminal-Bench上的表现。GLM-4.5在SWE-bench Verified上的表现优于GPT-4.1和Gemini-2.5-Pro。在Terminal-Bench上的表现优于Claude Sonnet 4。平均而言，GLM-4.5是Claude Sonnet 4在编码任务上的最佳竞争对手。

**表5：SWE-bench Verified和Terminal-Bench的结果**

| Benchmark | GLM- 4.5 | GLM- 4.5-Air | o3 | GPT-4.1 | Claude Opus 4 | Claude Sonnet 4 | Gemini 2.5 Pro | DeepSeek R1 0528 | Kimi K2 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| SWE-bench Verified | 64.2 | 57.6 | 69.1 | 48.6 | 67.8 | 70.4 | 49.0 | 41.4 | 65.4 |
| Terminal-Bench | 37.5 | 30.0 | 30.2 | 30.3 | 43.2 | 35.5 | 25.3 | 17.5 | 25.0 |
| Average | 50.9 | 43.8 | 49.7 | 39.5 | 55.5 | 53.0 | 37.2 | 29.5 | 45.2 |

#### 4.2.4 通用能力评估
评估了GLM-4.5在多个广泛采用的基准测试中的表现，包括MMLU、SimpleQA、IFEval、SysBench和MultiChallenge。GLM-4.5在MMLU上的表现与其他旗舰模型相当。在SimpleQA上，GLM-4.5的表现与DeepSeek V3和R1相当，尽管参数数量少了一半。在IFEval上，GLM-4.5的表现优于DeepSeek R1。在Sysbench评估中，GLM-4.5的表现优于GPT-4.1、DeepSeek V3和Kimi K2。在MultiChallenge基准测试中，GLM-4.5的表现优于GPT-4.1和DeepSeek R1。

**表6：常用通用聊天基准测试结果**

| Benchmark | GLM- 4.5 | GLM- 4.5-Air | GPT-4.1 | Claude Sonnet 4 | Gemini 2.5 Pro | Grok 4 | Qwen3 235B | Deepseek R1 0528 | DeepSeek V3 0324 | Kimi K2 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MMLU | 90.0 | 87.4 | 90.2 | 91.9 | 91.9 | 91.9 | 90.2 | 89.9 | 89.1 | 89.5 |
| SimpleQA | 26.4 | 14.5 | 42.3 | 18.5 | 54.0 | 51.9 | 45.8 | 27.8 | 27.7 | 31.0 |
| IFEval | 86.1 | 86.3 | 87.4 | 88.7 | 90.8 | 92.4 | 87.8 | 80.0 | 83.4 | 89.8 |
| SysBench | 81.0 | 77.4 | 80.6 | 80.6 | 82.2 | 81.5 | 83.3 | 81.2 | 79.8 | 79.0 |
| MultiChallenge | 52.8 | 42.5 | 38.3 | 55.3 | 57.5 | 65.2 | 58.2 | 46.5 | 37.0 | 54.1 |

#### 4.2.5 安全性评估
使用SafetyBench评估了GLM-4.5的安全对齐能力。GLM-4.5在多个安全类别上的表现强劲，总体得分为89.87，与Kimi-K2（90.48）和GPT-4.1（89.71）相当。在伦理与道德（94.33）、心理健康（94.67）和身体健康（96.67）方面表现突出。在非法活动（90.97）和隐私与财产（92.00）方面的表现良好，但在公平性与偏见方面仍有改进空间。

**表7：SafetyBench评估结果**

| Model | Average | Ethics & Morality | Illegal Activities | Mental Health | Offensiveness | Physical Health | Privacy & Property | Unfairness & Bias |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GLM-4.5 | 89.9 | 94.3 | 91.0 | 94.7 | 83.0 | 96.7 | 92.0 | 77.4 |
| GLM-4.5-Air | 87.8 | 91.0 | 90.3 | 92.7 | 83.3 | 92.3 | 90.3 | 74.7 |
| Gemini 2.5 Pro | 90.5 | 94.7 | 91.7 | 95.3 | 84.3 | 97.0 | 92.3 | 78.0 |
| Kimi K2 | 90.5 | 93.0 | 93.3 | 95.0 | 90.3 | 97.3 | 93.0 | 71.3 |
| GPT-4.1 | 89.7 | 92.0 | 94.3 | 95.3 | 85.3 | 95.7 | 91.3 | 74.0 |
| DeepSeek-V3-0324 | 88.8 | 92.3 | 90.7 | 95.0 | 84.7 | 95.7 | 91.0 | 72.3 |
| DeepSeek-R1-0528 | 83.5 | 87.0 | 81.7 | 86.7 | 77.7 | 92.0 | 85.7 | 73.7 |

### 4.3 实际体验评估

#### 4.3.1 通用聊天评估
评估了GLM-4.5、Deepseek-R1-0528和Kimi K2在多个语言和类别上的表现。GLM-4.5在英语、中文和其他语言的多个类别上均表现最佳。

**表8：英语提示的人工评估得分**

| Model | Overall | Math | Text Proc. | Subj. QA | Obj. QA | Text Gen. | Logic | Code |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GLM-4.5 | 8.66 | 8.72 | 8.00 | 8.36 | 8.82 | 8.61 | 9.25 | 8.53 |
| DeepSeek-R1-0528 | 8.62 | 8.56 | 8.27 | 7.91 | 9.00 | 7.83 | 9.07 | 8.65 |
| Kimi-K2 | 8.13 | 7.22 | 8.00 | 7.45 | 8.86 | 7.06 | 7.07 | 8.71 |

**表9：中文提示的人工评估得分**

| Model | Overall | Math | Text Proc. | Subj. QA | Obj. QA | Text Gen. | Logic | Code |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GLM-4.5 | 8.37 | 7.68 | 8.20 | 8.50 | 8.66 | 9.00 | 9.27 | 8.89 |
| DeepSeek-R1-0528 | 8.05 | 7.76 | 8.07 | 8.00 | 7.89 | 8.59 | 9.00 | 8.67 |
| Kimi-K2 | 7.03 | 7.37 | 6.43 | 7.71 | 6.45 | 8.28 | 7.55 | 8.26 |

**表10：其他语言提示的人工评估得分**

| Model | Overall | Math | Text Proc. | Text Gen. | Subj. QA | Obj. QA | Code | Logic |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| GLM-4.5 | 8.49 | 8.67 | 8.13 | 8.90 | 9.33 | 8.71 | 7.86 | 8.33 |
| DeepSeek-R1-0528 | 8.27 | 9.44 | 8.38 | 7.86 | 9.44 | 8.22 | 7.64 | 8.17 |
| Kimi-K2 | 6.63 | 7.22 | 6.38 | 7.62 | 7.78 | 6.22 | 6.68 | 7.17 |

#### 4.3.2 编码代理评估
构建了CC-Bench来评估GLM-4.5在真实场景中的编码代理能力。GLM-4.5在任务完成率、工具调用成功率和令牌使用效率方面均表现优异。

**图12：GLM-4.5与其他模型在CC-Bench上的头对头评估结果**

* GLM-4.5 vs Claude Sonnet 4: 40.4% 胜, 9.6% 平, 50.0% 败
* GLM-4.5 vs Kimi K2: 53.9% 胜, 17.3% 平, 28.8% 败
* GLM-4.5 vs Qwen3-Coder: 80.8% 胜, 7.7% 平, 11.5% 败

**图13：CC-Bench上不同模型的平均工具调用成功率和每次交互的令牌使用量**

* GLM-4.5: 90.6%
* Claude Sonnet 4: 89.5%
* Kimi-K2: 86.2%
* Qwen3-Coder: 77.1%

#### 4.3.3 逻辑推理评估
构建了一个新的、具有挑战性的逻辑推理评估集，评估了GLM-4.5在多个模型中的表现。GLM-4.5的表现与领先模型相当。

**表11：新逻辑推理问题的专家评估得分**

| Model | Score |
| --- | --- |
| Gemini 2.5 Pro | 65.8 |
| DeepSeek-R1-0528 | 62.1 |
| GLM-4.5 | 62.0 |
| GLM-4.5-Air | 53.4 |
| Kimi K2 | 51.9 |

### 4.4 翻译评估

#### 新的翻译范式
翻译不仅仅是简单的文本转换，还包括对网络俚语、文化背景和领域术语的理解。

#### 评估结果
测试了100个具有挑战性的现实案例，比较了GLM-4.5与专用翻译模型的表现。GLM-4.5显著优于专用模型。

**表12：选定挑战性翻译数据的人工评分**

| Model | Average Score |
| --- | --- |
| GLM-4.5 | 1.71 |
| Qwen-MT-plus | 0.38 |
| Qwen-MT-turbo | 0.55 |
| Seed-X | 0.65 |


## 5 Conclusion


本章节总结了GLM-4.5模型系列的发布与优势。该系列包括GLM-4.5和GLM-4.5-Air两个版本，均采用了MoE（Mixture of Experts）架构，相比之前的GLM模型，在计算效率上有了显著提升。

重点内容如下：

- **模型架构改进**：通过引入MoE架构，GLM-4.5在处理复杂任务时具有更高的效率。
- **性能表现优异**：GLM-4.5在推理、编程和代理任务（agentic tasks）方面表现出色，在全球范围内（包括开源和闭源模型）排名第三。
- **开放模型权重**：为了推动大语言模型的研究与应用，GLM-4.5和GLM-4.5-Air的模型权重已被公开发布。

本节未涉及数学公式、算法步骤或表格数据，主要为模型发布的技术总结与战略意义说明。


## 6 Contribution


本章节主要列出了对项目做出贡献的个人和团队，包括核心贡献者、普通贡献者、技术负责人、顾问以及致谢部分。

### 核心贡献者（Core Contributors）

该部分列出了以姓氏首字母排序的项目核心成员名单，共计37人，包括 Bin Chen、Chengxing Xie、Cunxiang Wang 等。其中部分名字后带有星号（*）表示这些成员已离开团队。

> **重点内容**：  
> - 所有姓名按首字母排序，格式统一。  
> - 带星号（*）的成员为已离职人员。

### 普通贡献者（Contributors）

该部分列出更多参与项目的贡献者，共计170人以上，如 Bohan Zhang、Bosi Wen、Bowen Wu 等。同样，部分成员名字后带有星号（*）表示已离职。

> **重点内容**：  
> - 同样按首字母排序，格式与核心贡献者一致。  
> - 星号（*）标记离职人员。

### 技术负责人（Tech Leads）

列出四位技术负责人：
- Aohan Zeng  
- Xin Lv  
- Qinkai Zheng  
- Zhenyu Hou

> **重点内容**：  
> - 项目技术方向的关键决策者。

### 顾问（Advisors）

列出八位项目顾问，包括：
- Jie Tang  
- Yuxiao Dong  
- Juanzi Li  
- Hongning Wang  
- Minlie Huang  
- Bin Xu  
- Jidong Zhai  
- Wenguang Chen

> **重点内容**：  
> - 项目提供战略和技术指导的专家。

### 致谢（Acknowledgement）

感谢来自北京、上海、天津、杭州、珠海和成都的支持。  
特别感谢客户和社区开发者对项目的帮助。

> **重点内容**：  
> - 表达对城市和合作方的感谢。  
> - 强调了社区开发者的重要性。

---

**总结**：  
本章为项目贡献者名单，结构清晰，分为核心成员、普通贡献者、技术负责人、顾问和致谢五部分。重点在于展示团队构成和人员贡献，并对离职人员做了标注（*）。未涉及数学公式、算法或数据表格。
