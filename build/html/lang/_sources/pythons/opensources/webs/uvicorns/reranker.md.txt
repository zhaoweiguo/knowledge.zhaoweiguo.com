# uvicorn


## 示例

```python
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main_rerank:app",
        host="0.0.0.0",
        port=myconfig.APP_RERANK_PORT,
        workers=myconfig.UVICORN_WORKERS,
        limit_concurrency=myconfig.UVICORN_LIMIT_CONCURRENCY,
        backlog=myconfig.UVICORN_BACKLOG,
        log_level="info"
    )


if myconfig.USE_HALF_PRECISION:
    model_kwargs['torch_dtype'] = torch.float16
    log.info("------- 使用半精度 (float16) 加载模型以节省内存")
if myconfig.USE_FLASH_ATTENTION:
    model_kwargs['attn_implementation'] = "flash_attention_2"
    log.info("------- 使用 flash_attention_2 以提升性能和节省内存")

model = AutoModelForCausalLM.from_pretrained(myconfig.PATH_RERANK,**model_kwargs).cuda().eval()
tokenizer = AutoTokenizer.from_pretrained(myconfig.PATH_RERANK, padding_side='left')

def format_instruction(instruction: Optional[str], query: str, doc: str) -> str:
    doc = mytool_truncate.truncate_text_middle(doc)
    query=mytool_truncate.truncate_text_middle(query)
    instruction = 'Given a query, retrieve relevant passages that answer the query'
    output = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
    return output

def process_inputs(pairs: List[str]) -> dict:
    inputs = tokenizer(...)
    # ... ...
    # Move to GPU
    for key in inputs:
        inputs[key] = inputs[key].to(model.device)

    return inputs

@torch.no_grad()
def compute_logits(inputs: dict) -> List[float]:
    batch_scores = model(**inputs).logits[:, -1, :]
    # ... ...

def compute_scores_in_batches(texts: List[str], batch_size: int) -> List[float]:
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        # ... ...
        inputs = process_inputs(batch_texts)
        scores = compute_logits(inputs)
        # Clean up intermediate tensors
        del inputs
    # Clean up GPU memory once after all batches complete
    if myconfig.ENABLE_MEMORY_CLEANUP:
        cleanup_gpu_memory()


def cleanup_gpu_memory():
    """Clean up GPU memory cache."""
    torch.cuda.empty_cache()

@app.post("/v1/rerank", response_model=RerankResponse)
def rerank(req: RerankRequest):
    # Format all query-document pairs
    texts = [
        format_instruction(req.instruction, req.query, doc.text)
        for doc in req.documents
    ]

    scores = compute_scores_in_batches(texts, myconfig.RERANK_BATCH_SIZE)
    # 省略部分代码........

```


## 显存影响配置参数

###  1. USE_HALF_PRECISION (最大影响 ~50%)

  if myconfig.USE_HALF_PRECISION:
      model_kwargs['torch_dtype'] = torch.float16

  - 作用: 使用 float16 代替 float32
  - 显存节省: 模型权重占用减半
  - 对比:
    - float32: ~2.4-2.6 GB
    - float16: ~1.2-1.3 GB

###  2. RERANK_BATCH_SIZE (动态影响)

  if len(texts) > myconfig.RERANK_BATCH_SIZE:
      scores = compute_scores_in_batches(texts, myconfig.RERANK_BATCH_SIZE)

  - 作用: 控制每批处理的文档数量
  - 显存影响:
    - 批次越大 → 推理时的激活值越大 → 峰值显存越高
    - 每个文档都会占用 batch_size × sequence_length × hidden_dim 的激活值空间
  - 建议值:
    - 3GB GPU: 4
    - 6GB+ GPU: 8-16

###  3. USE_FLASH_ATTENTION (~20-30% 优化)

  if myconfig.USE_FLASH_ATTENTION:
      model_kwargs['attn_implementation'] = "flash_attention_2"

  - 作用: 使用 Flash Attention 2 算法
  - 显存节省: 减少注意力机制的 KV cache 占用
  - 要求: 需要安装 flash-attn，且 GPU 支持 (A100/H100/L40 等)

###  4. MAX_LENGTH (次要影响)

  MAX_LENGTH = 8192

  - 作用: 输入序列的最大长度
  - 显存影响: 序列越长，attention cache 越大
  - 实际情况: 多数文档会被截断，峰值影响有限

###  5. UVICORN_WORKERS (成倍影响)

  workers=myconfig.UVICORN_WORKERS

  - 作用: 多进程 worker 数量
  - 显存影响: 每个 worker 都会加载一份完整的模型到显存

###  6. ENABLE_MEMORY_CLEANUP (释放缓存)

  if myconfig.ENABLE_MEMORY_CLEANUP:
      cleanup_gpu_memory()  # torch.cuda.empty_cache()

  - 作用: 调用 torch.cuda.empty_cache()
  - 显存影响: 不会立即降低占用，但会释放碎片化缓存，防止 OOM
  - 建议: 始终开启

  ---
  显存占用估算公式

  基础显存 = 模型权重 (1.2GB float16 / 2.4GB float32)
           + 基础运行时 (~0.1GB)

  峰值显存 = 基础显存
           + (batch_size × sequence_length × hidden_dim × 2 bytes)
           × safety_margin (1.2-1.5x)



## 96G建议配置

  ┌─────────────────────────────┬─────────┬─────────────────┬───────────┐
  │            参数             │ 当前值  │  显存占用估算   │   评价    │
  ├─────────────────────────────┼─────────┼─────────────────┼───────────┤
  │ USE_HALF_PRECISION=True     │ float16 │ 每worker ~1.3GB │ ✅ 推荐   │
  ├─────────────────────────────┼─────────┼─────────────────┼───────────┤
  │ RERANK_BATCH_SIZE=16        │ 16      │ 峰值 ~1GB       │ ⚠️ 偏保守 │
  ├─────────────────────────────┼─────────┼─────────────────┼───────────┤
  │ EMBEDDING_BATCH_SIZE=512    │ 512     │ 峰值 ~2GB       │ ✅ 合理   │
  ├─────────────────────────────┼─────────┼─────────────────┼───────────┤
  │ UVICORN_WORKERS=4           │ 4       │ ~5.2GB 基础     │ ⚠️ 偏保守 │
  ├─────────────────────────────┼─────────┼─────────────────┼───────────┤
  │ ENABLE_MEMORY_CLEANUP=False │ -       │ -               │ ✅ 正确   │
  └─────────────────────────────┴─────────┴─────────────────┴───────────┘


  方案 1: 高吞吐量模式 (推荐)

  # config/.env
  USE_HALF_PRECISION=True          # 保持半精度，吞吐量更高
  USE_FLASH_ATTENTION=True         # 必须开启，+20-30% 性能
  RERANK_BATCH_SIZE=32             # 16→32，充分利用显存
  EMBEDDING_BATCH_SIZE=1024        # 512→1024
  UVICORN_WORKERS=8                # 4→8，多核并行
  UVICORN_LIMIT_CONCURRENCY=1000   # 512→1000
  ENABLE_MEMORY_CLEANUP=False      # 保持关闭

  预期效果:
  - 显存占用: ~10-15GB
  - QPS 提升: 2-3倍
  - Latency: 基本不变或略降


  方案 2: 低延迟模式 (在线服务)

  # config/.env
  USE_HALF_PRECISION=True          # 必须开启，减少推理时间
  USE_FLASH_ATTENTION=True         # 必须开启
  RERANK_BATCH_SIZE=4              # 小批次，低延迟
  EMBEDDING_BATCH_SIZE=64          # 小批次
  UVICORN_WORKERS=16               # 16个worker，高并发
  UVICORN_LIMIT_CONCURRENCY=5000   # 支持更高并发
  ENABLE_MEMORY_CLEANUP=False


















