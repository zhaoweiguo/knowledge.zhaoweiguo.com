nvidia-smi命令
##############



示例::

    +---------------------------------------------------------------------------------------+
    | NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |
    |-----------------------------------------+----------------------+----------------------+
    | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
    |                                         |                      |               MIG M. |
    |=========================================+======================+======================|
    |   0  Tesla V100-SXM2-32GB           Off | 00000000:00:09.0 Off |                    0 |
    | N/A   38C    P0              37W / 300W |  12670MiB / 32768MiB |      0%      Default |
    |                                         |                      |                  N/A |
    +-----------------------------------------+----------------------+----------------------+



    顶部信息
        NVIDIA-SMI 535.183.01: 
            显示的是 NVIDIA 系统管理接口（nvidia-smi）的版本号。
            这是 NVIDIA 提供的工具，用于管理和监控 GPU。
        Driver Version: 535.183.01: 
            表示当前安装的 NVIDIA 驱动程序版本号。
        CUDA Version: 12.2: 
            显示当前支持的 CUDA 版本。
            CUDA 是 NVIDIA 提供的用于并行计算的工具包。

    GPU 状态
        GPU Name: 
            显示 GPU 的型号和配置。
            在这里是 Tesla V100-SXM2-32GB，这是 NVIDIA 的一款高性能计算 GPU，拥有 32GB 的显存。
        Persistence-M: 
            这表明 GPU 持久化模式（Persistence Mode）的状态。
            Off 表示该模式关闭。
            开启此模式可以保持 GPU 驱动在系统重启后不被卸载。
        Bus-Id: 
            显示 GPU 在系统总线上的位置（在 PCIe 总线上的标识）。
            这里 00000000:00:09.0 表示 GPU 的总线 ID。
        Disp.A: 
            表示显示器活动状态。
            Off 表示 GPU 没有连接到显示器或不用于显示任务。
        Volatile Uncorr. ECC: 
            表示易失性非纠错（ECC）内存错误的数量。
            0 表示没有检测到此类错误。

    温度和功耗
        Fan: 
            风扇状态。
            N/A 表示不适用或没有风扇信息。
            Tesla V100 可能没有风扇或其风扇速度不可用。
        Temp: 
            当前 GPU 的温度。
            这里是 38C，表示 GPU 温度为 38 摄氏度。
        Perf: 
            显示 GPU 性能状态。
            P0 表示 GPU 处于最高性能状态。
        Pwr:Usage/Cap: 
            显示 GPU 当前功耗和最大功耗。
            37W / 300W 表示当前功耗为 37 瓦特，最大功耗为 300 瓦特。

    内存使用情况
        Memory-Usage: 
            显示 GPU 内存的使用情况。
            12670MiB / 32768MiB 表示 GPU 总共 32GB 内存中，已使用 12.67GB。
        GPU-Util: 
            GPU 的使用率。
            0% 表示 GPU 当前没有进行计算任务或完全空闲。
        Compute M.: 
            计算模式。
            Default 表示使用默认计算模式。
        MIG M.: 
            显示多实例 GPU（MIG）模式的状态。
            N/A 表示此 GPU 不支持 MIG 功能。



GPU 属性
========

查询 NVIDIA GPU 的 计算能力（Compute Capability） 的命令::

    $ nvidia-smi --query-gpu=compute_cap --format=csv
    compute_cap
    8.9


计算能力的作用
--------------

* GPU 的计算能力（Compute Capability）是 NVIDIA 用来标识 GPU 架构和支持的功能的一个重要指标。
* 不同的计算能力对应不同的 GPU 架构和功能支持，例如::

    | 架构代号          | 架构名称           | 代表 GPU 型号            | Compute Capability |
    |------------------|------------------|-------------------------|---------------------|
    | Tesla            | Tesla            | GTX 200 系列             | 1.0 - 1.3           |
    | Fermi            | Fermi            | GTX 400/500 系列         | 2.0 - 2.1           |
    | Kepler           | Kepler           | GTX 600/700             | 3.0 - 3.7           |
    | Maxwell          | Maxwell          | GTX 750, 900 系列        | 5.0 - 5.3           |
    | Pascal           | Pascal           | GTX 10 系列              | 6.0 - 6.2           |
    | Volta            | Volta            | Tesla V100              | 7.0                 |
    | Turing           | Turing           | RTX 20 系列，GTX 16 系列  | 7.5                 |
    | Ampere           | Ampere           | RTX 30 系列，A100, L4     | 8.0 - 8.6           |
    | Hopper           | Hopper           | H100, H20                | 9.0                 |
    | Ada Lovelace     | Ada              | RTX 40 系列(4090), L20    | 8.9                 |
    | Blackwell (新)   | Blackwell        | B100 等                   | 9.0+（待正式发布）    |



查询所有 GPU 属性
-----------------

查询所有 GPU 属性::

    $ nvidia-smi --query-gpu=name,compute_cap,memory.total --format=csv
    name, compute_cap, memory.total [MiB]
    NVIDIA L20, 8.9, 46068 MiB




实例
====


nvidia-smi::

    Thu Feb 29 19:54:59 2024
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  Tesla V100-SXM2...  On   | 00000000:00:09.0 Off |                    0 |
    | N/A   41C    P0    37W / 300W |   4792MiB / 32768MiB |      0%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+

    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      7562      C   python                            884MiB |
    |    0   N/A  N/A   1425768      C   ./main                            836MiB |
    |    0   N/A  N/A   2854007      C   python                           1092MiB |
    |    0   N/A  N/A   3937597      C   ...3/envs/btc_env/bin/python     1164MiB |
    |    0   N/A  N/A   4027652      C   ...3/envs/btc_env/bin/python      808MiB |
    +-----------------------------------------------------------------------------+






自动刷新::

    nvidia-smi -l <sec>   # 默认5s

    nvidia-smi -l 1

    watch -n 1 nvidia-smi
































